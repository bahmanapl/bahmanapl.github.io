{"title":"Writing Stable Diffusion from Scratch 3","markdown":{"yaml":{"title":"Writing Stable Diffusion from Scratch 3","reading-time":null,"date":"2023-3-19","categories":["fastaipart2","Stable-Diffusion"]},"headingText":"Matrix multiplication from foundations","containsRefs":false,"markdown":"\n\nThings you should know and practice after this lecture : <br>\n1- Einsum <br>\n2- 2 ways you can do einsum / matmul in pytorch <br>\n3- Using GPU / Cuda <br>\n4- args and kwargs <br>\n\n\nAll credits go to fastai and all mistakes most likely is mine. This notebook connected to pass two notebooks. So the code is here but I remove most of explanation. You can check past two posts. Enjoy learning ...\n\n\n## Get data\n\n[urlretrieve](https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve) - (read the docs!)\n\n[islice](https://docs.python.org/3/library/itertools.html#itertools.islice)\n\nUse this link to learn more about [iter](https://docs.python.org/3.10/library/functions.html?highlight=iter#iter)\n\n## Matrix and tensor\n\nNow we can use pytorch.\n\n[Tensor documentation](https://pytorch.org/docs/stable/tensors.html)\n\nvector rank one tensor \nmatrix is a rank 2 tensor\nscalor in APL(depend of programming languages) is rank zero tensor\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nin y_train we can find min and max of it. \n\n## Random numbers\n\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\n%timeit check the time of excution. \n\npytorch version is faster. \n\n## Matrix multiplication\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n## Numba\n\nNow only two of our loops are running in Python, not three:\n\nThis is the test. \n\n2000 time faster. We change inner most loop. \n\n## Elementwise ops\n\n[TryAPL](https://tryapl.org/)\n\nElementwise addition\n\nCheck lecture for awesome implementation of mean. \n\nRank two tensor , aka Matrix. \n\n# Frobenius norm:\n\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n\n*Hint*: you don't normally need to write equations in LaTeX yourself, instead, you can click 'edit' in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click \"Download: Other formats\" in the top right, then \"Download source\"; rename the downloaded file to end in `.tgz` if it doesn't already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:\n\n```latex\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n```\n\nWe can use elementwise operation and get ride of inner loop. \n\nTest to see they are the same. \n\nNow that we wrote it , we can use equivalent of pytorch. (torch.dot)\n\n## Broadcasting\n\nThe term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations.\n\nFrom the [Numpy Documentation](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html):\n\n    The term broadcasting describes how numpy treats arrays with \n    different shapes during arithmetic operations. Subject to certain \n    constraints, the smaller array is “broadcast” across the larger \n    array so that they have compatible shapes. Broadcasting provides a \n    means of vectorizing array operations so that looping occurs in C\n    instead of Python. It does this without making needless copies of \n    data and usually leads to efficient algorithm implementations.\n    \nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\n\n*This section was adapted from [Chapter 4](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression) of the fast.ai [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course.*\n\n### Broadcasting with a scalar\n\nSimplest way of broadcasting. \n\nmultiply\n\n### Broadcasting a vector to a matrix\n\nwe can avoid : and say c[None].\n\nThis adding the vector to each row. \n\n### Broadcasting Rules\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when\nIt goes from right to left.\n- they are equal, or\n- one of them is 1, in which case that dimension is broadcasted to make it the same size\n\nArrays do not need to have the same number of dimensions. For example, if you have a `256*256*3` array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n\n    Image  (3d array): 256 x 256 x 3\n    Scale  (1d array):             3\n    Result (3d array): 256 x 256 x 3\n\nThe [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules) includes several examples of what dimensions can and can not be broadcast together.\n\n## Matmul with broadcasting\n\n## Einstein summation\n\nSo we're 5000 times faster than we started out. So another trick that we can use, which I'm a big fan of, is something called Einstein summation. And Einstein summation is a compact representation for representing products and sums. And this is an example of an Einstein summation. And what we're going to do now is begin to replicate our matrix product with an Einstein summation. And believe it or not, the entire thing can be pushed down to just these characters,(ik,kj->ikj) which is pretty amazing. So let me explain what's happening here. The arrow is separating the left hand side from the right hand side. The left hand side is the inputs. The right hand side is the output. The comma is between each inputs. So there are two inputs. The letters are just names that you're giving to the number of rise in the number of columns. So the first matrix we're multiplying by has i rows and k columns, the second has k rows and j columns. It's going to go through a process which creates a new matrix that actually this is not even doing this is not yet doing the matrix multiplication. This is without the sum. This one's going to create a new matrix that contains i rows, Well, how do we set i faces and k rows and j columns are rank three tensor. So the number of letters is going to be the rank and the rules of how this works is that if you repeat letters between input arrays,(ik,kj) so here's my inputs (ik,kj) and we've got a repeated letter. \nIt means that values along those axes will be multiplied together. So it means that each item across a row will be multiplied by each item down each column to create this i by k by j output tensor. So to remind you, our first matrix is five by 734 that's m1. Our second matrix is 784 by ten that's m2. So i is 5, k is 784 and J is 10. So if I do this torch.einsum then I will end up with a i k by k, it'll be five by 784 by ten. And if you have a look, I've run it here on these two tensor and m1 and m2 and the shape of the result is five by 784 by ten. And what it contains is the original five rows of m1 the original ten columns of m2, and then for the other 784 that I mentioned, they all multiplied together because it's been copied. It's been copied between the two arguments to the einsum. And so if we now sum up that over this dimension, we get back, if we have a look, it was that we printed this somewhere. Oh, there it is. So what we get back, if we go back to the original matrix multiply, we do. We had 10.94 negative, negative point six, eight, etc. And so now with this Einstein summation version, we've got back exactly the same thing because what it's done is it's taken each of these columns by rows, multiplied them together to get this five by seven, eight, four by ten, and then add it up that 784 for each one, which is exactly what matrix multiplication does. But we're going to use one of the two things from Einstein. The second one says if we omit a letter from the output. So the bit on the right of the arrow, it means those values will be summed. So if we remove this K, which gives us i , k and k,j goes to i,j, so we've removed the k entirely. That means that sum happens automatically. So if we run this, as you say, we get back again. Matrix multiplication. So Einstein summation notation is, you know, it takes some practice getting used to you, but it's very convenient and once you get used to it, it's actually a really nice way of thinking about what's going on. \nAnd as we'll see in lots of examples, often you can really simplify your code by using just a tiny little Einstein summation, and it doesn't even have to be a sum, right? You can you don't have to omit any letters if you're just doing products. So maybe it's a bit misnamed. So we can now define a matmul as simply this torch.einsum. So if we now check it, the test_close that the original result is equal to this new matmul. And yes, it is. And let's see how the speed looks. Okay. And that was for the whole thing. So compared to 600 milliseconds. So as you can see, this is much faster than even the very fast broadcasting approach we used. So this is a pretty good trick is torch.einsum. Some okay, but of course we don't have to do any of those things because PyTorch already knows how to do matmul. So there's two ways we can run matmul directly. In PyTorch, you can use a special @ operator. So x_train@weights is the same as matmul train comma weights as you say, test_close or you can say torch.matmul. And interestingly, as you can see here, the speed is about the same as the  einsum. So there's no particular harm that people reason not to do an einsum. So when I say einsum, that stands for Einstein summation notation. All right, let's go faster. Still. Currently we're just using my CPU, but I have a GPU. It would be nice to use it. So how does a GPU work at in video? GPU and indeed pretty much all GPU use. The way they work is that they do lots and lots of things in parallel and you have to actually tell the GPU what are all the things you want to do in parallel or one a time. And so what we're going to do is we're going to write in Pure Python something that works like a GPU. You expect it won't actually be in parallel, so it won't be fast at all. But the first thing we have to do if we're going to get something working in parallel is we have to create a function that can calculate just one thing even if a thousand other things are happening at the same time, it won't interact with anything else. And there's actually a very easy way to think about matrix multiplication in this way, which is what if we try to create something which, just as we've done here, fills in a single, the single item of the result? \n\n[Einstein summation](https://ajcr.net/Basic-guide-to-einsum/) ([`einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)) is a compact representation for combining products and sums in a general way. The key rules are:\n\n- Repeating letters between input arrays means that values along those axes will be multiplied together.\n- Omitting a letter from the output means that values along that axis will be summed.\n\nSo far we removed 2 inner loop and we are 5k faster than original for loop. It is more cleaner code and also faster. \n\n## pytorch op\n\nNow that we wrote matmul we can use pytorch matmul version. The speed is almost the same. We can use pytorch's function or operator directly for matrix multiplication.\n\nSo how do we create something that just fills in row zero column zero? Well, what we could do is we could create a new matmul where we're going to pass in the coordinates of the place that we want to fill in. So I'm going to start by passing that zero comma zero we'll pass at the matrices We want to multiply and we are passing a tensor that we've pre-filled in with zeros to put the result into. So they're going to say, okay, the result is torch.zeros() rows by columns, cal matmul for location zero comma zero passing in those two matrices and the bunch of zeros matrix ready to put the result in. And if we call that we get the answer in cell zero zero. So here's an implementation of that. So the implementation is first of all, we've been past the zero comma zero coordinates, so let's de structure them. So hopefully you've been experimenting with de structuring that so important. You said all the time into i and j throw in the column, make sure that that is inside the bounds of our output matrix and we're going to start by start at zero and loop through all of the all of the columns of a in the rows of b for i and j, just like the very innermost loop of our very first Python attempt and then at the end pop that into the output. So here's something that fills in one piece with a grid successfully. So we could call this row by columns times each time passing in a different grid. And we could do that in parallel because none of those different locations interact with any other location. So something which can calculate a little piece of, of an output on a GPU is called a kernel. So we call this a kernel. And so now we can create something called launch kernel, we pass at the kernel. So that's the function. So here's an example launch kernel passing in the function and how many rows and how many columns are there in the output grid. And then give me any arguments that you need to calculate it. So in python *args just says any additional arguments that you pass are going to be put into an array called args. If you do something like C, you might have seen like variadic arguments parameters. It's a same basic idea. So we're going to be calling launch kernel. \n\n## CUDA\n\nHow to use GPU instead of CPU. GPU does many more thing at the same time. CPU does not do that. We can compute each cell at the same time because none of those computation interact with other location.\n\nSo we're going to be calling launch kernel. We're going to be saying launch the kernel matmul using all the rows of a or the columns of b, and then the args which are going to be in star args are going to be m1, the first matrix, m2 the second matrix n res  another touched zeros we just created. So launch kernel, it's going to loop through the rows of a and then for each row of a loop through the columns of b and call the kernel which is matmul on that grid location and passing in m1, m2 and res. So I star args here is going to unpack that and pass them as three separate arguments. And if I run that, run all of that, you'll see it's done it, it's filled in the exact same matrix. Okay. So that's actually not fast at all. It's not doing anything in parallel, but it's the basic idea. So now to actually do it in parallel, we have to use something called Cuda. So Cuda is a programing model for Nvidia GPUs and to program in CUDA from Python. The easiest way currently to do that is be something called Numba. And Numba is a compiler where you've seen it actually already for non GPU. It's a compiler that takes Python code and spits out, you know, compiled fast machine code. If you use its CUDA module, it'll actually spit out GPU accelerated CUDA code. So rather than using an @njit like before, we now say @cuda.jit  and it behaves a little bit differently but you'll see that this matmul let me copy the other one over so you can compare cup it, compare it to our Python one, our Python matmul and this @cuda.jit matmul Look I think identical except for one thing. Instead of passing in the grid, there's a special magic thing called cuda.gird()  And you say how many dimensions just my grid have? And you unpack it so that's you don't have to. It's just a little convenience. That Numba does for you. You don't have to pass over the grid, it passes it over for you. So it doesn't need this grid. Other than that, these two are identical, but the decorator is going to compile that into your GPU code. So now we need to create our output tensor just like before, and we need to do something else, which is we have to take our input matrices and our output. So our input tenses, the matrices in this case and the output tensor and we have to move them to the GPU, you I should say, copy them to the GPU. \n\n*args any additional argument(s) will be put into an array called args. This is the idea. \n\nNow we use cuda so it will do multiply computation at the same time. \n\nSo cuda.to_device() copies a tensor to the GPU. And so we've got three things getting copied to the GPU here and therefore we store the three things over here. Another way I could have written this is I could have said map, which I kind of quite like doing a function which is cuda.to_device to each of these arguments and this would be the same thing. This is going to call CUDA dot device on x_train and put it in here on weights and put it in here and an r and put it in rg. That's a slightly more convenient way to do it. Okay, so we've got our 50,000 by ten output. That's just all zeros. Of course, that's just how we created it. And now we're going to try and fill it in. There is a there's a particular detail that you don't have to worry about too much, which is in CUDA They don't just have a grid, but there's also a concept of blocks and there's something we call here TPP, which is threads per block. This is just a detail of the kind of programing model you don't have to worry about too much. You can just basically copy this. And what it's going to do is it's going to call each grid item in parallel and with a number of different processes, basically. So this is just the code which turns the grid into blocks. And so you don't have to worry too much about the details of that. You just always run it. \n\nDecorator compile it to GPU code. \n\nOkay. And so now how do you call the equivalent of launch kernal?  it's it's a slightly weird way to do it, but it works fine. You call matmul, but because matmul has cuda.jit, it's got a special thing, which is you have to put something in square brackets afterwards, which is you have to tell it how many blocks per grid. That's just the result from the previous cell and how many threads per block in each of the two dimensions. So again, you can just copy and paste this from my version, but then you pass in the three arguments to the function. This will be a, c, and c, and this. Okay, this is, this is how you launch a kernel. So this will launch the kernel matmul on the GPU. You at the end of it, rg is going to get filled in. It's gone. It's on the GPU, which is not much good to us so we don't have to copy it back to the CPU, which is called the host copy to host to a run that and it's done and test_close shows us that result is similar to our original results. So it seems to be working. So that's great. So I see Sylvor on the YouTube chat is finding that it's not working on his Mac. That's right. So this will only work on it in NVIDIA CPU as basically all of the GPU, nearly all the CPU stuff we look at only works on video.\n\nMac GPUs are gradually starting to get a little bit of support from machine learning libraries, but it's taking quite a while. It's been, you know, it's got quite a way to go. As I say this at least towards the end of 2022, if this works for you and later on that's yeah, that's great. Okay, so let's time how fast that is. Okay, so that was 3.6 1 milliseconds. And so if we compare that to the PyTorch matmul on CPU, that was 15 milliseconds. So that's great. So it's faster still. So how much faster? Oh, by the way, we can actually go faster than that, which is we can use the exact same code we had from the PyTorch up. But here's a trick. If you just take your tensor and write .cuda after it, it copies it over to the GPU. If it's on a if it's on a Nvidia GPUs, you do the same for weights.cuda. So these are two cuda versions and now I can do the whole thing. And this will actually run on the GPU and then to copy it back to the host, you just say .cpu(). So if we look to see how fast that is, 458 ms .So yeah, that is somebody you just pointed out that I wrote the wrong thing here 1e-3. Okay, so how much faster is that? Well full 458 microseconds original on the whole data set was 663 microseconds. So compared to our broadcast version, we are another 1000 times faster.\nSo overall, this version here, compared to our original version, which was here, the difference in performance is 5 million x , So when you say people say, Yeah, Python can be pretty slow, it can be better to run the stuff on the GPU if possible. We're not talking about a 20% change, we're talking about a 5 million x change. So that's a big deal. And so that's why you need to be running stuff on the GPU. All right. Some folks on YT are wondering how on earth I'm running cuda when I'm on a mac and given it sets localhost here, that's because I'm using something called SSH tunneling, which we might get to sometime. I suspect my life coding from the previous course might have covered that already, but this is basically you can use a Jupyter notebook that's running anywhere in the world from your own machine using something called SSH Tunneling, which is a good thing to look up a OK when a person asks if Einstein summation borrows anything from APL.\nOh, yes, it does, actually. So it's kind of the other way around. Actually. APL borrows it from Einstein notation. I don't know if you remember I mentioned that Iverson, when he developed APL was heavily influenced by tensor analysis. And so this Einstein notation is very heavily used there. If you'll notice a key thing that happens in Einstein notation is there's no loop. You know, there isn't this kind of sigma, you know, i from here to here and then you put the i inside the function that you're summing up, everything's implicit and APL takes that a very long way and, and J takes it even further, which is what Iverson developed after APL and this kind of general idea of of removing the index is very important in APL and it's become very important in numpy PyTorch TensorFlow and so forth.\nAll right. So finally we know how to multiply matrices. Congratulations. So let's practice that. That's practice what we've learned. So we're going to go to zero two main shift to practice this. And so we're going to try to exercise our kind of tensor manipulation operation muscles in this section. And the key actually endpoint for this is the homework. And so what you need to be doing is getting yourself to a point that you could implement something like this, but for a different algorithm, why do we care about this? \n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"toc-depth":3,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"cyborg","title-block-banner":false,"author":"Bahman Sadeghi","title":"Writing Stable Diffusion from Scratch 3","reading-time":null,"date":"2023-3-19","categories":["fastaipart2","Stable-Diffusion"]},"extensions":{"book":{"multiFile":true}}}}}