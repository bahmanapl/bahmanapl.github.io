{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "title: \"Writing Stable Diffusion from Scratch 10\"\n",
        "reading-time: \n",
        "date: \"2023-4-2\"\n",
        "categories: [fastaipart2,Stable-Diffusion]\n",
        "---"
      ],
      "metadata": {
        "id": "c-YwIOS0SYG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All credits goes to fast.ai All mistakes are mine.\n",
        "You should know and practice following after this blog post : \n",
        "1- Convolution in concept and in coding"
      ],
      "metadata": {
        "id": "2pnH1sxQSXv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq git+https://github.com/fastai/course22p2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfNN4ko-SZwQ",
        "outputId": "b852a311-62af-48e0-9630-080f71d445fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.9/934.9 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for miniai (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi, all, and welcome to lesson 15. And what we're going to endeavor to do today is to create a convolutional auto encoder. And in the process, we will see why doing that well is a tricky thing to do. And time permitting, we will begin to work on a framework, a deep learning framework to make life a lot easier. Not sure how far we'll get on that today. Time wise, let's see how we go and get straight into it. So, okay, so today let's start by talking before we can create a convolutional auto encoder we need to talk about convolutions and and what are they and what are they for.\n",
        "\n",
        "Broadly speaking, convolutions are something that allows us to to tell our neural network a little bit about the structure of the problem. That's going to make it a lot easier for it to solve the problem. And in particular, the structure of our problem is we're doing things with images. Images are laid out on a grid, a 2D grid with black and white, or a 3 to 4 color or a 44 color video or whatever. And so we would say, you know, there's a relationship between the pixels going across the pixels going down. They tend to be similar to each other. Differences in those pixels across those dimensions tend to have meaning . Patterns of pixels that appear in different places often represent the same thing. So, for example, a cat in the top left is still a cat, even if it's in the bottom right. These kinds of this kind of prior information is something that is naturally captured by a convolutional neural network, something that is convolutions. Generally speaking, this is a good thing because it means that we will be able to use less parameters and less computation, and because more of that information about the problem solving is kind of encoded directly into our architecture.Maybe I should plug my other life her. Let me do that as well as how to get enough light. Okay. Let me say yes, there are other architectures that don't encode that prior information as strongly, such as a multilayer perceptron, which we've been looking at so far, or a Transformers network, which we haven't looked at yet. \n",
        "\n",
        "Those kinds of architectures could potentially give us what they do, give us more flexibility and given enough time, compute and data, they could potentially find things that maybe CNN's would struggle to find. So we're not always going to use convolutional neural networks, but they're pretty good starting point and certainly something important to understand. They're not just used for images. We can also take advantage of one dimensional convolutions for language based tasks. For instance, the convolutions come up a lot. So in this notebook, one thing you'll notice that might be of interest is we are importing stuff from miniai now. Now miniai is this little library that we're starting to create and we're creating nbdev. So we've got a miniai training and a mini A.I. data sets. And so if we look, for example, at the Datasets notebook, it starts with something that says that the default export module is called datasets, and some of the cells have a export directive on them. And at the very bottom we had something that called nbdev export. Now what that's going to do is it's going to create a file called dataset.py Just here, datasets.py why and it contains that's those cells that we exported. And why does it why is it called miniai.datasets? That's because everything miniai is stored in settings.mini and there's something here. So create a library lib_name called miniai. I you can't use this library until you install it. Now we haven't uploaded it to 2  the pipi installable package from the public server, but you can actually install a local directory as if it's a python module that you've kind of installed from the internet. And to do that you say pip install in the usual way, but you say minus e is set to editable and that means set up the current directory as a python module. Well, current directory actually any directory you like. I just put dot to be in the current directory and so you'll see that's going to go ahead and actually install all my library. And so after I've done that I can now import things from that library, as you say. Okay, so this is just the same as before. We're going to grab Our MNIST data set and we're going to create a convolutional neural network on it. So before we do that, we're going to talk about what are convolutions. And one of my favorite descriptions of convolutions comes from the student in I think was our very first course, Matt Kleinsmith, who wrote this really nice medium article, CNN's from Different Viewpoints.\n",
        "https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c\n",
        "\n",
        "which I'm going to steal from. And here's the basic idea. I say that this is our image. It's a three by three image with nine pixels labeled from A-J as capital letters.Now a convolution uses something called a kernel, and a kernel is just another tensor. In this case, it's a two by two matrix again. So it's this one. We're going to have Alpha, beta, gamma Delta as our four values in this convolution now in this kernel. Now one thing I mentioned I can't remember I've said this before is the Greek letters are things that you want to be able to I think I have mentioned this. You want to be able to pronounce them. So if you don't know how to read these and say what these names are, make sure you head over to Wikipedia or whatever and learn the names of all the Greek letters so that you can because they come up all the time. Okay, so what happens when we apply a convolution with this two by two kernel, two this three by three image? I mean, it doesn't have to be an image. It's in this case it's just a rank two tensor, but it might represent an image. What happens is we take the kernel and we overlay it over the first. They don't two by two separate like so. And specifically what we do is we match color the color. So the output of this first two by two overlay would be alpha times A plus, beta times B plus gamma times D plus delta times E, and that would yield some value P And that's going to end up in the top left of a two by two output. So the top right of the two by two output, we're going to slide. It's like a slide in window. We're going to slide our kernel over to here and apply each of our coefficients to these respectively colored squares. And then ditto for the bottom left and then ditto for the bottom right. So we end up with this equation. P, as we discussed, is Alpha A plus, beta B plus eight plus delta E plus some bias term. Q So the top right as you can say, it's just alpha in this case times B And so we're just multiplying them together and adding them up. Multiply together at the map, multiplied together and add them up. So we're basically you can imagine that we're basically flattening these out into rank one tensors into vectors. And then doing a dot product would be one way of thinking about what's happening as we slide this kernel over these windows. And so this is called a convolution. So let's try and create a convolution. So for example, let's grab our training images and take a look at one and let's create a three by three kernel. So remember, a kernel is just we've already got all appears has a lot of times in computer science and math. We've already seen the term kernel to mean a piece of code that we run on a year across lots of parallel kind of virtual devices or potentially in a grid. There's a similar idea here. We've got a computation which is in this case kind of this dot product or something like a dot product, okay, sliding over occurring lots of times over a grid. But it's yeah, it's a bit different. So that's kind of another use of the word kernel. \n",
        "\n"
      ],
      "metadata": {
        "id": "-Xn-0B2OuMv2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN9LH3Kxt_bB"
      },
      "source": [
        "# Convolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w6Fg2glt_bC"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import default_collate\n",
        "from typing import Mapping\n",
        "\n",
        "from miniai.training import *\n",
        "from miniai.datasets import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8iSoyUpt_bD"
      },
      "outputs": [],
      "source": [
        "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
        "import pandas as pd,matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from torch import tensor,nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Mapping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastcore.test import test_close\n",
        "\n",
        "mpl.rcParams['image.cmap'] = 'gray'\n",
        "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
        "np.set_printoptions(precision=2, linewidth=125)\n",
        "\n",
        "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
        "path_data = Path('data')\n",
        "path_data.mkdir(exist_ok=True)\n",
        "path_gz = path_data/'mnist.pkl.gz'"
      ],
      "metadata": {
        "id": "vN_-5N7gUFRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)"
      ],
      "metadata": {
        "id": "T6G2c85lUMK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQseRhrrUM0R",
        "outputId": "0903d0e9-fe4f-4385-e858-c5af78285165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16656\n",
            "-rw-r--r-- 1 root root 17051982 May 21 12:38 mnist.pkl.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
        "\n"
      ],
      "metadata": {
        "id": "xry7TIjdUPb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxaV-Przt_bD"
      },
      "outputs": [],
      "source": [
        "mpl.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doAfpdbxt_bE"
      },
      "source": [
        "In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that.\n",
        "\n",
        "It turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use a *convolution*. A convolution requires nothing more than multiplication, and addition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUNwoe92t_bF"
      },
      "source": [
        "### Understanding the Convolution Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7E9sOCnt_bF"
      },
      "source": [
        "To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing [CNNs from different viewpoints](https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c).\n",
        "\n",
        "Here's the input:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cR_Jiset_bF"
      },
      "source": [
        "<img alt=\"The image\" width=\"75\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00032.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8mH8aCGt_bG"
      },
      "source": [
        "Here's our kernel:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwec_xSkt_bG"
      },
      "source": [
        "<img alt=\"The kernel\" width=\"55\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00033.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUg3mylUt_bG"
      },
      "source": [
        "Since the filter fits in the image four times, we have four results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g53PMnq2t_bH"
      },
      "source": [
        "<img alt=\"The activations\" width=\"52\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00034.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1iEhR8Jt_bH"
      },
      "source": [
        "<img alt=\"Applying the kernel\" width=\"366\" caption=\"Applying the kernel\" id=\"apply_kernel\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00035.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT-NO_F2t_bI"
      },
      "source": [
        "<img alt=\"The equation\" width=\"436\" caption=\"The equation\" id=\"eq_view\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00036.png?raw=1\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN7Giuj7t_bI"
      },
      "outputs": [],
      "source": [
        "x_imgs = x_train.view(-1,28,28)\n",
        "xv_imgs = x_valid.view(-1,28,28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghsclEJ-t_bI"
      },
      "outputs": [],
      "source": [
        "mpl.rcParams['figure.dpi'] = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9ubCYLLt_bJ",
        "outputId": "f2ce2149-e88e-416c-927d-b1abb836111d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAAEFUlEQVR4nO2dTSh8URjG/6ORsiB2LETJhoUsJSx8RGSlFDKSpVgqW2tlQykbe2VMSEpK9ha2VixQNpTvjJVpnjdm/nO/5nh6fqv7NOOeq5/j7Zy579xYOp1O/xM0lBT7AkSwxLNDLBYr1nUIn3z/o9UMJUNCyZBQMiSUDAklQ0LJkFAyJJQMCSVDQsmQUDIklAwJJUNCyZBQMiSUDAklQ0LJkFAy4vnf4i5NTU2QS0tLIXd2dmaO19bW4LXPz89AryWZTEIeGxuD/Pb2Fuh4v6EZSoaEkiGhZMSy75x37b7c5uZmyIlEAvLo6CjkkhL8+6ytrc0c298t7IaBra0tyAsLC5AfHh4CHU/35ZIioWRIKBlO19Dd3V3Ig4ODns8VdQ21dHV1QT47Owv0/KqhpEgoGRJKhtN7uUdHR5Dz1dC7uzvIm5ubmWO7Rs23l9ve3g7Z1kBX0QwlQ0LJkFAynF6HxuNY4mtqanK+//39HfLNzY3nsSsqKiBfXFxAzt4n/omdnR3I4+PjkF9fXz1f209oHUqKhJIhoWQ4vQ79+PiAfHV1FdnY/f39kKuqqgr6+evra8hB18zf0AwlQ0LJkFAynF6HRom9j3Z2dhZyoXu51dXVkIO+h8iidSgpEkqGhJLh9Do0SOxe6uLiIuTGxkbItk8mH+fn55DtvnJUaIaSIaFkSCgZTtfQ+vp6yJOTk5B7enr++1wdHR2QC70v164jbQ3e39+H/Pz8XND5g0IzlAwJJcOprb+WlhbIthWirq7O87n9tkLs7e1BHhkZ8XwtYaCtP1IklAwJJcPpZYute35qfKGtEJahoSHIAwMDkA8ODrxdWMBohpIhoWRIKBlO1VDbbtDd3Q15YmIC8uHhIeSXlxfPY8/MzECem5vzfK5iohlKhoSSIaFkOLWXW0wqKysh39/f53z/8PAw5GKvQ7WXS4qEkiGhZDi1Di0mtn3wr6IZSoaEkiGhZERaQ217QV9fH+Tj42PIYd4KOT09DXl1dTW0saJEM5QMCSVDQskItYba9oOlpSXIvb29kBsaGiD7/Rqb7LZ4+9WsKysrkMvLy3Oey9ZzP5+9holmKBkSSoaEkhHq56G2Td32rljW19chPz4++ho/u0a3tbXBa/l6W05OTnJe2/b2tq9rCxp9HkqKhJIhoWQ4VUPDxP5ut7e3kFOpFOT5+XnIrq47v1ENJUVCyZBQMkKtoa2trZBtv8jU1FSg411eXkJ+enrKHJ+ensJrGxsbkG1fzV9DNZQUCSVDQsmItLelrKwMciKRgLy8vAzZPlrDPn7KPo4ymUxC9vOorL+GaigpEkqGhJKh/lASVENJkVAyJJQMCSVDQsmQUDIklAwJJUNCyZBQMiSUDOgPLfRZJsI9NEPJkFAyvgD8hTC7+8Q8YwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "im3 = x_imgs[7]\n",
        "show_image(im3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4es4Wpxt_bK"
      },
      "outputs": [],
      "source": [
        "top_edge = tensor([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]]).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXjPpcYDt_bK"
      },
      "source": [
        "We're going to call this our kernel (because that's what fancy computer vision researchers call these)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAgfWhZWt_bL",
        "outputId": "7de2403d-161c-4b6f-e41b-6fb91e2cb809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAABrklEQVR4nO3cIauDYBSH8f+GsGpas1j9ZvuuRttYsZmsprNyHWuXuxtkD88PXngFw4GHg81TVVWE0eyXdV3zeDxyuVyOnEcf2LYtfd+nbdukfozjWEk8X3rGcayqqvNe2c38bnu/8y/v6csYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDDN+8Ptdsv1ej1qFn1oWZbX3Q2FMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApzqqpKkmmakiTDMBw6kP7uvZ0bCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwjT7Zdu2zPN85Cz60P1+T9d1Sd6C9n1/2ED6n67rXv1ef0ERg99QmCfCL6s7xKOy5wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_image(top_edge, noframe=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_Q2Fm5t_bL"
      },
      "source": [
        "The filter will take any window of size 3×3 in our images, and if we name the pixel values like this:\n",
        "\n",
        "$$\\begin{matrix} a1 & a2 & a3 \\\\ a4 & a5 & a6 \\\\ a7 & a8 & a9 \\end{matrix}$$\n",
        "\n",
        "it will return $-a1-a2-a3+a7+a8+a9$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So in this case, a kernel is a in this case it's going to be a rank two tensor. And so let's create a kernel with these values in the three by three matrix rank, two tensor and we could draw what that looks like. Not surprisingly, this looks like a bunch of lines perhaps. Okay, so what would happen if we slide this over , nine pixels over this 28 by 28? Well, what's going to happen is if we've got some the top left, for example, three by three section has these names, then we're going to end up with negative a1 because the top three or negative, right. Negative a1, minus a2 minus a3 the next to exist zero. So that won't do anything. And then plus a7 plus a8 plus a9. Why is that interesting? That's interesting. Well, let's try here. What I've done here is I've grabbed just the first 13 rows and first 23 columns of our image, and I'm actually showing the numbers and also using Gray kind of conditional formatting, if you like, or the equivalent in PANDAS to show this top bit. So we're looking at just this top bit. So what happens if we take rows three, four and five? Remember, this is not inclusive, right? So it's rows three, four and five columns 14,15 , 16 , So we're looking at this these three here. What's that going to give us if we multiply it by this kernel, it gives us a fairly large positive value because the three that we have negatives on is the top, right? Well, they're all zero. And the three that we have positives on there or close to one. So we end up with quite a large number. What about the same columns but four rows 7,8,9  Here the top is all positive and the bottom is all zero. So that means that we're going to get a lot of negative terms. And not surprisingly, that's exactly what we see. This if we do this kind of dot product equivalent, which all you need a numpad to do, that is just an element. My multiplication followed by a sum, right? So that's going to be a quite a large negative number. And so perhaps you're saying what this is doing and maybe you got a hint from the name of the tensor. So we created it, something that is going to find the top edge. All right. So this one is a top edge. So it's a positive and this one is a bottom edge. So it's a negative. So we would like to apply that this kernel to every single three by three section in here. So we could do that by creating a little apply kernel function that takes some particular row in some particular column and some particular tensor as a kernel. And does that multiplication that some that we just saw. So for example, we could replicate this one by calling apply kernel and this here is the center of that three by three grid area. "
      ],
      "metadata": {
        "id": "sXsbZemb0v-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT7CJOOgt_bL",
        "outputId": "5b49405f-3010-47e1-90b6-5330c0c6599b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fdf242c3fa0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_8149e_row0_col0, #T_8149e_row0_col1, #T_8149e_row0_col2, #T_8149e_row0_col3, #T_8149e_row0_col4, #T_8149e_row0_col5, #T_8149e_row0_col6, #T_8149e_row0_col7, #T_8149e_row0_col8, #T_8149e_row0_col9, #T_8149e_row0_col10, #T_8149e_row0_col11, #T_8149e_row0_col12, #T_8149e_row0_col13, #T_8149e_row0_col14, #T_8149e_row0_col15, #T_8149e_row0_col16, #T_8149e_row0_col17, #T_8149e_row0_col18, #T_8149e_row0_col19, #T_8149e_row0_col20, #T_8149e_row0_col21, #T_8149e_row0_col22, #T_8149e_row1_col0, #T_8149e_row1_col1, #T_8149e_row1_col2, #T_8149e_row1_col3, #T_8149e_row1_col4, #T_8149e_row1_col5, #T_8149e_row1_col6, #T_8149e_row1_col7, #T_8149e_row1_col8, #T_8149e_row1_col9, #T_8149e_row1_col10, #T_8149e_row1_col11, #T_8149e_row1_col12, #T_8149e_row1_col13, #T_8149e_row1_col14, #T_8149e_row1_col15, #T_8149e_row1_col16, #T_8149e_row1_col17, #T_8149e_row1_col18, #T_8149e_row1_col19, #T_8149e_row1_col20, #T_8149e_row1_col21, #T_8149e_row1_col22, #T_8149e_row2_col0, #T_8149e_row2_col1, #T_8149e_row2_col2, #T_8149e_row2_col3, #T_8149e_row2_col4, #T_8149e_row2_col5, #T_8149e_row2_col6, #T_8149e_row2_col7, #T_8149e_row2_col8, #T_8149e_row2_col9, #T_8149e_row2_col10, #T_8149e_row2_col11, #T_8149e_row2_col12, #T_8149e_row2_col13, #T_8149e_row2_col14, #T_8149e_row2_col15, #T_8149e_row2_col16, #T_8149e_row2_col17, #T_8149e_row2_col18, #T_8149e_row2_col19, #T_8149e_row2_col20, #T_8149e_row2_col21, #T_8149e_row2_col22, #T_8149e_row3_col0, #T_8149e_row3_col1, #T_8149e_row3_col2, #T_8149e_row3_col3, #T_8149e_row3_col4, #T_8149e_row3_col5, #T_8149e_row3_col6, #T_8149e_row3_col7, #T_8149e_row3_col8, #T_8149e_row3_col9, #T_8149e_row3_col10, #T_8149e_row3_col11, #T_8149e_row3_col12, #T_8149e_row3_col13, #T_8149e_row3_col14, #T_8149e_row3_col15, #T_8149e_row3_col16, #T_8149e_row3_col17, #T_8149e_row3_col18, #T_8149e_row3_col19, #T_8149e_row3_col20, #T_8149e_row3_col21, #T_8149e_row3_col22, #T_8149e_row4_col0, #T_8149e_row4_col1, #T_8149e_row4_col2, #T_8149e_row4_col3, #T_8149e_row4_col4, #T_8149e_row4_col5, #T_8149e_row4_col6, #T_8149e_row4_col7, #T_8149e_row4_col8, #T_8149e_row4_col9, #T_8149e_row4_col10, #T_8149e_row4_col11, #T_8149e_row4_col12, #T_8149e_row4_col13, #T_8149e_row4_col14, #T_8149e_row4_col15, #T_8149e_row4_col16, #T_8149e_row4_col17, #T_8149e_row4_col18, #T_8149e_row4_col19, #T_8149e_row4_col20, #T_8149e_row4_col21, #T_8149e_row4_col22, #T_8149e_row5_col0, #T_8149e_row5_col1, #T_8149e_row5_col2, #T_8149e_row5_col3, #T_8149e_row5_col4, #T_8149e_row5_col5, #T_8149e_row5_col6, #T_8149e_row5_col7, #T_8149e_row5_col8, #T_8149e_row5_col9, #T_8149e_row5_col10, #T_8149e_row5_col22, #T_8149e_row6_col0, #T_8149e_row6_col1, #T_8149e_row6_col2, #T_8149e_row6_col3, #T_8149e_row6_col4, #T_8149e_row6_col5, #T_8149e_row6_col6, #T_8149e_row6_col7, #T_8149e_row6_col8, #T_8149e_row7_col0, #T_8149e_row7_col1, #T_8149e_row7_col2, #T_8149e_row7_col3, #T_8149e_row7_col4, #T_8149e_row7_col5, #T_8149e_row7_col6, #T_8149e_row7_col7, #T_8149e_row7_col8, #T_8149e_row8_col0, #T_8149e_row8_col1, #T_8149e_row8_col2, #T_8149e_row8_col3, #T_8149e_row8_col4, #T_8149e_row8_col5, #T_8149e_row8_col6, #T_8149e_row8_col7, #T_8149e_row8_col8, #T_8149e_row9_col0, #T_8149e_row9_col1, #T_8149e_row9_col2, #T_8149e_row9_col3, #T_8149e_row9_col4, #T_8149e_row9_col5, #T_8149e_row9_col6, #T_8149e_row9_col7, #T_8149e_row9_col8, #T_8149e_row9_col13, #T_8149e_row9_col14, #T_8149e_row9_col15, #T_8149e_row9_col16, #T_8149e_row10_col0, #T_8149e_row10_col1, #T_8149e_row10_col2, #T_8149e_row10_col3, #T_8149e_row10_col4, #T_8149e_row10_col5, #T_8149e_row10_col6, #T_8149e_row10_col7, #T_8149e_row10_col8, #T_8149e_row10_col9, #T_8149e_row10_col10, #T_8149e_row10_col11, #T_8149e_row10_col12, #T_8149e_row10_col13, #T_8149e_row10_col14, #T_8149e_row10_col15, #T_8149e_row10_col16, #T_8149e_row10_col22, #T_8149e_row11_col0, #T_8149e_row11_col1, #T_8149e_row11_col2, #T_8149e_row11_col3, #T_8149e_row11_col4, #T_8149e_row11_col5, #T_8149e_row11_col6, #T_8149e_row11_col7, #T_8149e_row11_col8, #T_8149e_row11_col9, #T_8149e_row11_col10, #T_8149e_row11_col11, #T_8149e_row11_col12, #T_8149e_row11_col13, #T_8149e_row11_col14, #T_8149e_row11_col15, #T_8149e_row11_col22, #T_8149e_row12_col0, #T_8149e_row12_col1, #T_8149e_row12_col2, #T_8149e_row12_col3, #T_8149e_row12_col4, #T_8149e_row12_col5, #T_8149e_row12_col6, #T_8149e_row12_col7, #T_8149e_row12_col8, #T_8149e_row12_col9, #T_8149e_row12_col10, #T_8149e_row12_col11, #T_8149e_row12_col12, #T_8149e_row12_col13, #T_8149e_row12_col22 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #ffffff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row5_col11 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #ececec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row5_col12 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #e8e8e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row5_col13 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #b0b0b0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row5_col14, #T_8149e_row5_col15, #T_8149e_row5_col16, #T_8149e_row5_col17, #T_8149e_row5_col18, #T_8149e_row5_col19, #T_8149e_row6_col13, #T_8149e_row6_col20, #T_8149e_row7_col9, #T_8149e_row7_col10, #T_8149e_row7_col11, #T_8149e_row7_col12, #T_8149e_row7_col13, #T_8149e_row7_col20, #T_8149e_row7_col21, #T_8149e_row7_col22, #T_8149e_row8_col10, #T_8149e_row8_col11, #T_8149e_row8_col20, #T_8149e_row8_col21, #T_8149e_row8_col22, #T_8149e_row9_col20, #T_8149e_row10_col20, #T_8149e_row11_col20 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #000000;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row5_col20 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #626262;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row5_col21 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #fcfcfc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row6_col9 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #dbdbdb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row6_col10 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #878787;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row6_col11 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #212121;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row6_col12 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #1e1e1e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row6_col14, #T_8149e_row7_col14 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #020202;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row6_col15, #T_8149e_row6_col16, #T_8149e_row6_col17, #T_8149e_row6_col18, #T_8149e_row6_col19, #T_8149e_row7_col15, #T_8149e_row7_col16, #T_8149e_row7_col17, #T_8149e_row7_col18, #T_8149e_row7_col19, #T_8149e_row8_col18, #T_8149e_row8_col19, #T_8149e_row9_col19, #T_8149e_row10_col19, #T_8149e_row11_col18, #T_8149e_row11_col19, #T_8149e_row12_col17, #T_8149e_row12_col18, #T_8149e_row12_col19 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #010101;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row6_col21 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #727272;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row6_col22 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #dcdcdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row8_col9 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #777777;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row8_col12 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #1a1a1a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row8_col13 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #8f8f8f;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row8_col14, #T_8149e_row8_col15, #T_8149e_row8_col16 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #909090;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row8_col17, #T_8149e_row11_col17 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #525252;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row9_col9 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #fdfdfd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row9_col10, #T_8149e_row9_col11, #T_8149e_row9_col22 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #f1f1f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row9_col12 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #f4f4f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row9_col17, #T_8149e_row11_col21 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #f8f8f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row9_col18 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #1f1f1f;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row9_col21 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #646464;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row10_col17 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #c5c5c5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row10_col18 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #0c0c0c;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row10_col21 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #828282;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row11_col16 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #c3c3c3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row12_col14 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #c1c1c1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8149e_row12_col15 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #323232;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row12_col16 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #070707;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row12_col20 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #3c3c3c;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_8149e_row12_col21 {\n",
              "  font-size: 7pt;\n",
              "  background-color: #fbfbfb;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_8149e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_8149e_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
              "      <th id=\"T_8149e_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
              "      <th id=\"T_8149e_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
              "      <th id=\"T_8149e_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
              "      <th id=\"T_8149e_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
              "      <th id=\"T_8149e_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
              "      <th id=\"T_8149e_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
              "      <th id=\"T_8149e_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
              "      <th id=\"T_8149e_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
              "      <th id=\"T_8149e_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
              "      <th id=\"T_8149e_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
              "      <th id=\"T_8149e_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n",
              "      <th id=\"T_8149e_level0_col12\" class=\"col_heading level0 col12\" >12</th>\n",
              "      <th id=\"T_8149e_level0_col13\" class=\"col_heading level0 col13\" >13</th>\n",
              "      <th id=\"T_8149e_level0_col14\" class=\"col_heading level0 col14\" >14</th>\n",
              "      <th id=\"T_8149e_level0_col15\" class=\"col_heading level0 col15\" >15</th>\n",
              "      <th id=\"T_8149e_level0_col16\" class=\"col_heading level0 col16\" >16</th>\n",
              "      <th id=\"T_8149e_level0_col17\" class=\"col_heading level0 col17\" >17</th>\n",
              "      <th id=\"T_8149e_level0_col18\" class=\"col_heading level0 col18\" >18</th>\n",
              "      <th id=\"T_8149e_level0_col19\" class=\"col_heading level0 col19\" >19</th>\n",
              "      <th id=\"T_8149e_level0_col20\" class=\"col_heading level0 col20\" >20</th>\n",
              "      <th id=\"T_8149e_level0_col21\" class=\"col_heading level0 col21\" >21</th>\n",
              "      <th id=\"T_8149e_level0_col22\" class=\"col_heading level0 col22\" >22</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_8149e_row0_col0\" class=\"data row0 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col1\" class=\"data row0 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col2\" class=\"data row0 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col3\" class=\"data row0 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col4\" class=\"data row0 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col5\" class=\"data row0 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col6\" class=\"data row0 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col7\" class=\"data row0 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col8\" class=\"data row0 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col9\" class=\"data row0 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col10\" class=\"data row0 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col11\" class=\"data row0 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col12\" class=\"data row0 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col13\" class=\"data row0 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col14\" class=\"data row0 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col15\" class=\"data row0 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col16\" class=\"data row0 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col17\" class=\"data row0 col17\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col18\" class=\"data row0 col18\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col19\" class=\"data row0 col19\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col20\" class=\"data row0 col20\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col21\" class=\"data row0 col21\" >0.00</td>\n",
              "      <td id=\"T_8149e_row0_col22\" class=\"data row0 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_8149e_row1_col0\" class=\"data row1 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col1\" class=\"data row1 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col3\" class=\"data row1 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col4\" class=\"data row1 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col5\" class=\"data row1 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col6\" class=\"data row1 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col7\" class=\"data row1 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col8\" class=\"data row1 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col9\" class=\"data row1 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col10\" class=\"data row1 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col13\" class=\"data row1 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col14\" class=\"data row1 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col15\" class=\"data row1 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col17\" class=\"data row1 col17\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col18\" class=\"data row1 col18\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col21\" class=\"data row1 col21\" >0.00</td>\n",
              "      <td id=\"T_8149e_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_8149e_row2_col0\" class=\"data row2 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col1\" class=\"data row2 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col2\" class=\"data row2 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col4\" class=\"data row2 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col6\" class=\"data row2 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col7\" class=\"data row2 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col8\" class=\"data row2 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col9\" class=\"data row2 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col10\" class=\"data row2 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col11\" class=\"data row2 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col12\" class=\"data row2 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col13\" class=\"data row2 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col14\" class=\"data row2 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col15\" class=\"data row2 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col16\" class=\"data row2 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col17\" class=\"data row2 col17\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col18\" class=\"data row2 col18\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col19\" class=\"data row2 col19\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col20\" class=\"data row2 col20\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col21\" class=\"data row2 col21\" >0.00</td>\n",
              "      <td id=\"T_8149e_row2_col22\" class=\"data row2 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_8149e_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col1\" class=\"data row3 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col2\" class=\"data row3 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
              "      <td id=\"T_8149e_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_8149e_row4_col0\" class=\"data row4 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col1\" class=\"data row4 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col3\" class=\"data row4 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col4\" class=\"data row4 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col14\" class=\"data row4 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
              "      <td id=\"T_8149e_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_8149e_row5_col0\" class=\"data row5 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col1\" class=\"data row5 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col2\" class=\"data row5 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col3\" class=\"data row5 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col4\" class=\"data row5 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row5_col11\" class=\"data row5 col11\" >0.15</td>\n",
              "      <td id=\"T_8149e_row5_col12\" class=\"data row5 col12\" >0.17</td>\n",
              "      <td id=\"T_8149e_row5_col13\" class=\"data row5 col13\" >0.41</td>\n",
              "      <td id=\"T_8149e_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
              "      <td id=\"T_8149e_row5_col15\" class=\"data row5 col15\" >0.99</td>\n",
              "      <td id=\"T_8149e_row5_col16\" class=\"data row5 col16\" >0.99</td>\n",
              "      <td id=\"T_8149e_row5_col17\" class=\"data row5 col17\" >0.99</td>\n",
              "      <td id=\"T_8149e_row5_col18\" class=\"data row5 col18\" >0.99</td>\n",
              "      <td id=\"T_8149e_row5_col19\" class=\"data row5 col19\" >0.99</td>\n",
              "      <td id=\"T_8149e_row5_col20\" class=\"data row5 col20\" >0.68</td>\n",
              "      <td id=\"T_8149e_row5_col21\" class=\"data row5 col21\" >0.02</td>\n",
              "      <td id=\"T_8149e_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_8149e_row6_col0\" class=\"data row6 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col1\" class=\"data row6 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col2\" class=\"data row6 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col3\" class=\"data row6 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col4\" class=\"data row6 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row6_col9\" class=\"data row6 col9\" >0.17</td>\n",
              "      <td id=\"T_8149e_row6_col10\" class=\"data row6 col10\" >0.54</td>\n",
              "      <td id=\"T_8149e_row6_col11\" class=\"data row6 col11\" >0.88</td>\n",
              "      <td id=\"T_8149e_row6_col12\" class=\"data row6 col12\" >0.88</td>\n",
              "      <td id=\"T_8149e_row6_col13\" class=\"data row6 col13\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col14\" class=\"data row6 col14\" >0.99</td>\n",
              "      <td id=\"T_8149e_row6_col15\" class=\"data row6 col15\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col16\" class=\"data row6 col16\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col17\" class=\"data row6 col17\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col18\" class=\"data row6 col18\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col19\" class=\"data row6 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col20\" class=\"data row6 col20\" >0.98</td>\n",
              "      <td id=\"T_8149e_row6_col21\" class=\"data row6 col21\" >0.62</td>\n",
              "      <td id=\"T_8149e_row6_col22\" class=\"data row6 col22\" >0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_8149e_row7_col0\" class=\"data row7 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col1\" class=\"data row7 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col2\" class=\"data row7 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col3\" class=\"data row7 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col4\" class=\"data row7 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col5\" class=\"data row7 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col6\" class=\"data row7 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col7\" class=\"data row7 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col8\" class=\"data row7 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row7_col9\" class=\"data row7 col9\" >0.70</td>\n",
              "      <td id=\"T_8149e_row7_col10\" class=\"data row7 col10\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col11\" class=\"data row7 col11\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col12\" class=\"data row7 col12\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col13\" class=\"data row7 col13\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col14\" class=\"data row7 col14\" >0.99</td>\n",
              "      <td id=\"T_8149e_row7_col15\" class=\"data row7 col15\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col16\" class=\"data row7 col16\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col17\" class=\"data row7 col17\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col18\" class=\"data row7 col18\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col19\" class=\"data row7 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col20\" class=\"data row7 col20\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col21\" class=\"data row7 col21\" >0.98</td>\n",
              "      <td id=\"T_8149e_row7_col22\" class=\"data row7 col22\" >0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_8149e_row8_col0\" class=\"data row8 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col1\" class=\"data row8 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col2\" class=\"data row8 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col3\" class=\"data row8 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col4\" class=\"data row8 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col5\" class=\"data row8 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col6\" class=\"data row8 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col7\" class=\"data row8 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col8\" class=\"data row8 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row8_col9\" class=\"data row8 col9\" >0.43</td>\n",
              "      <td id=\"T_8149e_row8_col10\" class=\"data row8 col10\" >0.98</td>\n",
              "      <td id=\"T_8149e_row8_col11\" class=\"data row8 col11\" >0.98</td>\n",
              "      <td id=\"T_8149e_row8_col12\" class=\"data row8 col12\" >0.90</td>\n",
              "      <td id=\"T_8149e_row8_col13\" class=\"data row8 col13\" >0.52</td>\n",
              "      <td id=\"T_8149e_row8_col14\" class=\"data row8 col14\" >0.52</td>\n",
              "      <td id=\"T_8149e_row8_col15\" class=\"data row8 col15\" >0.52</td>\n",
              "      <td id=\"T_8149e_row8_col16\" class=\"data row8 col16\" >0.52</td>\n",
              "      <td id=\"T_8149e_row8_col17\" class=\"data row8 col17\" >0.74</td>\n",
              "      <td id=\"T_8149e_row8_col18\" class=\"data row8 col18\" >0.98</td>\n",
              "      <td id=\"T_8149e_row8_col19\" class=\"data row8 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row8_col20\" class=\"data row8 col20\" >0.98</td>\n",
              "      <td id=\"T_8149e_row8_col21\" class=\"data row8 col21\" >0.98</td>\n",
              "      <td id=\"T_8149e_row8_col22\" class=\"data row8 col22\" >0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_8149e_row9_col0\" class=\"data row9 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col1\" class=\"data row9 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col2\" class=\"data row9 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col3\" class=\"data row9 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col4\" class=\"data row9 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col5\" class=\"data row9 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col6\" class=\"data row9 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col7\" class=\"data row9 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col8\" class=\"data row9 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col9\" class=\"data row9 col9\" >0.02</td>\n",
              "      <td id=\"T_8149e_row9_col10\" class=\"data row9 col10\" >0.11</td>\n",
              "      <td id=\"T_8149e_row9_col11\" class=\"data row9 col11\" >0.11</td>\n",
              "      <td id=\"T_8149e_row9_col12\" class=\"data row9 col12\" >0.09</td>\n",
              "      <td id=\"T_8149e_row9_col13\" class=\"data row9 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col14\" class=\"data row9 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col15\" class=\"data row9 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col16\" class=\"data row9 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row9_col17\" class=\"data row9 col17\" >0.05</td>\n",
              "      <td id=\"T_8149e_row9_col18\" class=\"data row9 col18\" >0.88</td>\n",
              "      <td id=\"T_8149e_row9_col19\" class=\"data row9 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row9_col20\" class=\"data row9 col20\" >0.98</td>\n",
              "      <td id=\"T_8149e_row9_col21\" class=\"data row9 col21\" >0.67</td>\n",
              "      <td id=\"T_8149e_row9_col22\" class=\"data row9 col22\" >0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_8149e_row10_col0\" class=\"data row10 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col1\" class=\"data row10 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col2\" class=\"data row10 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col3\" class=\"data row10 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col4\" class=\"data row10 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col5\" class=\"data row10 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col6\" class=\"data row10 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col7\" class=\"data row10 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col8\" class=\"data row10 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col9\" class=\"data row10 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col10\" class=\"data row10 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col11\" class=\"data row10 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col12\" class=\"data row10 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col13\" class=\"data row10 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col14\" class=\"data row10 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col15\" class=\"data row10 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col16\" class=\"data row10 col16\" >0.00</td>\n",
              "      <td id=\"T_8149e_row10_col17\" class=\"data row10 col17\" >0.33</td>\n",
              "      <td id=\"T_8149e_row10_col18\" class=\"data row10 col18\" >0.95</td>\n",
              "      <td id=\"T_8149e_row10_col19\" class=\"data row10 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row10_col20\" class=\"data row10 col20\" >0.98</td>\n",
              "      <td id=\"T_8149e_row10_col21\" class=\"data row10 col21\" >0.56</td>\n",
              "      <td id=\"T_8149e_row10_col22\" class=\"data row10 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_8149e_row11_col0\" class=\"data row11 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col1\" class=\"data row11 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col2\" class=\"data row11 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col3\" class=\"data row11 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col4\" class=\"data row11 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col5\" class=\"data row11 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col6\" class=\"data row11 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col7\" class=\"data row11 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col8\" class=\"data row11 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col9\" class=\"data row11 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col10\" class=\"data row11 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col11\" class=\"data row11 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col12\" class=\"data row11 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col13\" class=\"data row11 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col14\" class=\"data row11 col14\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col15\" class=\"data row11 col15\" >0.00</td>\n",
              "      <td id=\"T_8149e_row11_col16\" class=\"data row11 col16\" >0.34</td>\n",
              "      <td id=\"T_8149e_row11_col17\" class=\"data row11 col17\" >0.74</td>\n",
              "      <td id=\"T_8149e_row11_col18\" class=\"data row11 col18\" >0.98</td>\n",
              "      <td id=\"T_8149e_row11_col19\" class=\"data row11 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row11_col20\" class=\"data row11 col20\" >0.98</td>\n",
              "      <td id=\"T_8149e_row11_col21\" class=\"data row11 col21\" >0.05</td>\n",
              "      <td id=\"T_8149e_row11_col22\" class=\"data row11 col22\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8149e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_8149e_row12_col0\" class=\"data row12 col0\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col1\" class=\"data row12 col1\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col2\" class=\"data row12 col2\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col3\" class=\"data row12 col3\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col4\" class=\"data row12 col4\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col5\" class=\"data row12 col5\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col6\" class=\"data row12 col6\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col7\" class=\"data row12 col7\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col8\" class=\"data row12 col8\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col9\" class=\"data row12 col9\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col10\" class=\"data row12 col10\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col11\" class=\"data row12 col11\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col12\" class=\"data row12 col12\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col13\" class=\"data row12 col13\" >0.00</td>\n",
              "      <td id=\"T_8149e_row12_col14\" class=\"data row12 col14\" >0.36</td>\n",
              "      <td id=\"T_8149e_row12_col15\" class=\"data row12 col15\" >0.83</td>\n",
              "      <td id=\"T_8149e_row12_col16\" class=\"data row12 col16\" >0.96</td>\n",
              "      <td id=\"T_8149e_row12_col17\" class=\"data row12 col17\" >0.98</td>\n",
              "      <td id=\"T_8149e_row12_col18\" class=\"data row12 col18\" >0.98</td>\n",
              "      <td id=\"T_8149e_row12_col19\" class=\"data row12 col19\" >0.98</td>\n",
              "      <td id=\"T_8149e_row12_col20\" class=\"data row12 col20\" >0.80</td>\n",
              "      <td id=\"T_8149e_row12_col21\" class=\"data row12 col21\" >0.04</td>\n",
              "      <td id=\"T_8149e_row12_col22\" class=\"data row12 col22\" >0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df = pd.DataFrame(im3[:13,:23])\n",
        "df.style.format(precision=2).set_properties(**{'font-size':'7pt'}).background_gradient('Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNZHoEgct_bM",
        "outputId": "e38e70ad-d9af-4fae-c273-767644bd22a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.97)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "(im3[3:6,14:17] * top_edge).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCo5L06ft_bM",
        "outputId": "f793a80c-0f3b-4a61-98a8-7af3a5f4222d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.96)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "(im3[7:10,14:17] * top_edge).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzGYRz05t_bM"
      },
      "outputs": [],
      "source": [
        "def apply_kernel(row, col, kernel): return (im3[row-1:row+2,col-1:col+2] * kernel).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we could apply that kernel to every one of the three by three windows in this 28 by 28 image. So we're going to be sliding over like this red bit sliding over here, but we've actually got a 28 by 28 input, not just a five by five input. So to get all of the coordinates, let's just simplify it. To do this five by five, we can go, we can create a list comprehension. We can take i through every value in range five and then for each of those we can take a j for every value in range Five. And so if we just look at that tuple, you can say we get a list of lists containing all of those coordinates. So this is a list comprehension in list comprehension which when you first say it may be surprising or confusing, but it's a really helpful idiom and I certainly recommend getting used to it. Now, what we're going to do is we're not just going to create this tuple, but we're actually going to call applay kernel for each of those. So if we go through from 1 to 27, we'll actually 1 to 26 because 27 is exclusive. So we're going to go through everything from 1 to 26 and then for each of those go through from 1 to 26 again and call apply kernel and that's going to give us the result of applying that convolutional kernel to every one of those coordinates. And there's a result and you can see what it's done as we hoped is it is highlighting the top edges. So yeah, you might find that kind of surprising that it's that that easy to do this kind of image processing. We're literally just doing an element wise multiplication and a sum for each window. Okay. So,that is called a convolution. So we can do another convolution. This time we could do one with the left edge tensor.As You can see looks just a rotated version or transpose version I guess, of our top edge tensor. Here's what it looks like. And so if we apply that kernel, so this time we're going to apply the left edge kernel and so notice here that we're actually passing in a function, right? We're passing in a tensor , It's just a it's just a tensor actually. So we're going to pass in the left edge tensor for the same list Comprehension in a list comprehension. And this time we're getting back on the left edge as it's highlighting all of the left edges in the digit. So yeah, this is basically what's happening here is that a two by two can be looped over an image creating these outputs. Now you'll see here that in the process of doing so, we are losing the outermost pixels of our image. We'll learn about how to fix that later, but just for now, notice that that as we are putting in our three by three through for example, in this five by five, there's only 1 two 3 places that we can put it going across, not five places, because we need some kind of edge. "
      ],
      "metadata": {
        "id": "2EN1hWzt1Dnk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvbb3h9Kt_bN",
        "outputId": "1f5d3cfb-b523-4396-e0d6-b0cbea5c472a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.97)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "apply_kernel(4,15,top_edge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnkrwP9Ot_bN"
      },
      "source": [
        "<img src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/chapter9_nopadconv.svg?raw=1\" id=\"nopad_conv\" caption=\"Applying a kernel across a grid\" alt=\"Applying a kernel across a grid\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNAMUc1st_bN",
        "outputId": "cec6479b-2895-4325-ed96-f28b42f64db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)],\n",
              " [(1, 0), (1, 1), (1, 2), (1, 3), (1, 4)],\n",
              " [(2, 0), (2, 1), (2, 2), (2, 3), (2, 4)],\n",
              " [(3, 0), (3, 1), (3, 2), (3, 3), (3, 4)],\n",
              " [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "[[(i,j) for j in range(5)] for i in range(5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk-R26ttt_bN",
        "outputId": "b35d1be3-2cf4-4576-b2b3-1387212dfed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "rng = range(1,27)\n",
        "top_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n",
        "show_image(top_edge3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-AFfrWvt_bO"
      },
      "outputs": [],
      "source": [
        "left_edge = tensor([[-1,0,1],\n",
        "                    [-1,0,1],\n",
        "                    [-1,0,1]]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB00vKnMt_bP",
        "outputId": "8aae7015-2063-46db-efba-bbcef32a152c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAABtUlEQVR4nO3VoYrDQBhF4dulEBtVFxMbl75V37eujKmrih01VaUs3YpdFqYczqd+eeEwzK611iKM/ePYti2llAzD0HOP/qDWmnmeM47jM2gpJcfjseeuH51Op94TXnzapuv1miRZ1/UZ9FNf5uFw6D3hxbIsvSe89dV7gP6XQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUJj946i19tzx1u126z3hxfl87j3hm8vlkmmakiS71lpLkm3bUkrJMAxdx+n3aq2Z5znjOD6DisE/FOYOpVozu+0kyHoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_image(left_edge, noframe=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Qvx2P7t_bP",
        "outputId": "4e2d99e4-7bec-4036-a982-ce3a47e6a382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "left_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng])\n",
        "show_image(left_edge3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnk-cbzTt_bQ"
      },
      "source": [
        "<img alt=\"Result of applying a 3×3 kernel to a 4×4 image\" width=\"782\" caption=\"Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"three_ex_four_conv\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00028.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops. Somebody pointed out that this is not quite the same. It should be. It should be moved. Now, check this out. If you had an option or alt as you highlight, you can copy, you can copy and paste whole columns at once. Isn't that cool? Yeah. We go. All right. It doesn't change the output match, of course, but it's nice to have a consistent. All right, So that's cool. That's a convolution. And hopefully if you remember back to kind of the Zeiler and Fergus pictures from from lesson one, you might recognize that the kind of first layer of a convolutional network is often looking for kind of edges and gradients and things like that. And this is how, this is how it does it. And then the convolutions on top of convolutions with nonlinear activations between them can combine those into into curves or corners or stuff like that and so on and so forth. Okay, so how do we do this quickly? Because currently this is going to be super, super slow during this in Python. So one of the very earliest or probably the earliest publicly available general purpose deep learning GPU accelerator tape running thing I saw it was called Cafe that was created by somebody called Young Teenager and he actually described what happened where where cafe how cafe went about implementing a fast conversation on a GPU. And basically he said, well I had two months to do it and I had to finish my thesis. And so I ended up doing something where I said, Well, there was some other code out there. Could I ask who you might have come across him and, and Hinton set up a little start up which, which Google bought and that kind of became the start of Google's deep learning, the Google brain. Basically. Codrescu had all this fancy stuff in in his library, but Yuncheng , Oh, I didn't know how to do all that stuff. So I said, Well, I already know how to multiply matrices, so maybe I can convert a convolution into a matrix multiplication. And so that I became known as im2col, \n",
        "\n",
        "im2col a way of converting a convolution into a matrix. Multiply. And so actually, I don't know if a suspect young teenager kind of accidentally reinvented it because it actually had been around for a while, even at the point that he was writing his thesis. I believe. So it was actually this is this is the place I believe it was created in this paper.\n",
        "https://hal.inria.fr/inria-00112631/document\n",
        "\n",
        "\n",
        "So that was in 2006, which is a while ago. And so this is actually from that paper. And what they describe is, is let's say you are putting this two by two kernel over this, three by three bit of an image. So here you've got this this window needs to match to this part of this window. Right. What you could do is you could unwrap this to 1 2,1 2. \n",
        "\n",
        "unroll to here. One, two, one, two. To unroll it like so and you could unroll the kernel here. Yeah. So this is one, two, one one. So this is bit is here. One, two, one one and then you can unroll the kernel one, one, two, two, to here. One, one, two, two. And then once they, they've been moved, flattened out and moved in that way and then you're doing exactly the same thing for this next patch here.2, 0 , 1 , 3 flatten out and put it here if you basically take those kernels and flatten them out in this format, then you end up with a matrix multiply. If you multiply this matrix by this matrix, you'll end up with the output that you want from the convolution. So this is a basically a way of unrolling your kernels and your input features into matrices such as when you do the matrix multiply, you get the right answer. So it's kind of a nifty trick. And so that is called im2col and I guess we're kind of chatting a little bit. Implementing that is kind of boring is just a bunch of copying, intensive manipulation. So I actually haven't done it. Instead, I've linked to a numpy implementation, which is here. And it also part of it is this get indices which is here. And as you can see, it's a little bit tedious with repeats and tiles and re shapes and whatnot. So I'm not going to call it homework, but if you want to practice your tensor indexing manipulation skills, try creating a PyTorch version from scratch. I've got to admit I didn't bother. Instead, \n",
        "\n",
        "I used the one that's built into PyTorch and in PyTorch it's called unfold. So if we take our image and PyTorch expects there to be a batch axis and dimension and a channel dimension. So we'll add two unit leading dimensions to it, then we can unfold our input for a three by three and that will give us a 9 by 676 input. And so then we can take that We can take that and then we will make our we will take it out kernal and just flatten it out into a vector. So here changes the shape and minus one just says dump everything into this dimension. So that's going to create a nine long vector length, nine vector. And so now we can do the matrix multiply just like they've done here of the kernel matrix. That's our weights by the unrolled input features. And so that gives us a 676 long. We can then view that as 26 by 26 and we get back as we hoped, our left edge tensor result. And so this is yeah, this is how we can kind of from scratch create a a better implementation of convolutions. The reason I'm cheating I'm allowed to cheat here is because we did actually create convolution from scratch.\n",
        "\n",
        "So I think that's fair, but it's cool that we can kind of hack out GPU optimized version in the same way that the kind of original Deep Learning library did. So if we use apply kernel, we get nearly 9 milliseconds. If we use unfold with matrix multiply we get 20 microseconds. So that's what about 400 times faster. So that's pretty cool. Now of course we don't have to use unfold and matrix multiply because PyTorch has a copy of covv2d so we can run that. And that interestingly is about the same speed at least on GPU. But this would also work on on, on GPU just as well. Yeah. I'm not sure this will always be the case in this case. It's a pretty small image. I haven't experimented a whole lot to see whereabouts. There's a big difference in space between these. Obviously I always just use conv2d that if there's some more tricky convolution you need to do with some weird thing around channels or dimensions or something. You can always try this unfold trick. It's nice to know it's there. I think. So we could do the same thing for diagonal edges. So here's our diagonal edge kernel or the other diagonal. So if we just grab the first 16 images on our whole batch with all of our kernels at once. So this is a nice optimized thing that we can do. And you end up with your 26 by 26, you've got your four kernels and you've got your 16 images. And so that's summarized here. So that's generally what we're doing to get good GPU acceleration as we're doing a bunch of kernels and a bunch of images all at once across across all of their pixels. And so yeah, we go, that's what happens when we take a look at our various kernels for a particular image left edge, I guess, top edge and then diagonal, top left and top right. Okay, so that is optimized convolutions on and that works just as well in CPU or GPU. \n",
        "\n",
        "\n",
        "\n",
        "Obviously GPU will be faster if you have one. Now how do we deal with the problem that way? Losing one pixel on each side? What we can do is we can add something called padding and padding. What we basically do is rather than starting our window here, we start at a right over here and we actually would be up one as well. And so these three on the left here, we just take the input for each of those as zero. So we're basically just assuming that they're all zero. I mean, that's there's other options we could choose. We could assume they're the same as the one next to them\n",
        "\n",
        "There's various things we can do, but the simplest and the one we normally do is just assume that they're zero. So now so let's say, for example, this is this is called one pixel padding. \n",
        "\n",
        " we're just going to treated at zero. \n",
        " \n",
        "\n",
        " \n",
        " So generally odd numbered edge sized kernels are easier to deal with to make sure you end up with the same thing. You start with. Okay, so yeah. So as it says here with you've got an odd numbered size case by case size kernel, then case truncate divide two, that's what segments will give you the right size. And so another trick you can do is you don't always have to just move your window across by one each time. You could move it by a different amount. Each time the amount you move it by is called the stride. So, for example, here's a case of doing a stride two so stride two padding one. So we start out here and then we jump across two and then we jump across to then then we go to the next tray. \n",
        " \n",
        " So that's called a stride to convolution stride to convoluted ends are handy because they actually reduce the dimensionality of your input by a factor of two. And that's actually what we want to do a lot. For example, with an auto encoder, we want to do that. And in fact, for most classification architectures we do exactly that. We keep on reducing the kind of the grid size by a factor of two again and again and again, using stride to compilations with padding of one. So that strides in padding. So let's go ahead and create a convnet using these approaches. So we're going to put get our size of our training set. This is all the same as before a number of categories than per digit size of our hidden layer. So right previously with our sequential linear models with our MLPs, we basically went from size from the number of pixels to the number of hidden, and then relu and then the number of hidden to the number of outputs. So here's the equivalent with a convolution.\n",
        "\n"
      ],
      "metadata": {
        "id": "h0BZ3rLl2aVl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOl_q-_bt_bQ"
      },
      "source": [
        "### Convolutions in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKgPTgdRt_bR"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMHf9nZHt_bR"
      },
      "source": [
        "What to do if you have [2 months to complete your thesis](https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo)? Use [im2col](https://hal.inria.fr/inria-00112631/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGueQVrvt_bR"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaQ7FUlEt_bS"
      },
      "source": [
        "Here's a sample [numpy implementation](https://github.com/3outeille/CNNumpy/blob/5394f13e7ed67a808a3e39fd381f168825d65ff5/src/fast/utils.py#L360)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbQjASBQt_bS",
        "outputId": "19d0c347-285c-4829-83a2-b607a7d0afb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9, 676])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "inp = im3[None,None,:,:].float()\n",
        "inp_unf = F.unfold(inp, (3,3))[0]\n",
        "inp_unf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asjCZiyat_bS",
        "outputId": "a0535ac9-78fd-412f-c6b7-16e9100b1a04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "w = left_edge.view(-1)\n",
        "w.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t45VO2-Ct_bT",
        "outputId": "67a14ba5-3dec-4587-833b-611065521262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([676])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "out_unf = w@inp_unf\n",
        "out_unf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulNOaz2Qt_bT",
        "outputId": "211652dd-b550-4950-ae53-154250e5c28f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAAFT0lEQVR4nO2dx04rQRRE22CCyMhkibAAETdIrPgL/ocl/8QfIBZIBAkkck4iCESOfrsnbnWDx4z9PK9UZ1f29HisUut2uH0nlc1ms07QUFbqBxCFJf1VzMzMlOo5REymp6edc+qhdMhQMmQoGTKUDBlKhgwlQ4aSIUPJkKFkyFAyZCgZMpSMdO5LSk9VVZX3WSqVMjqdtn+ls7PT6IeHB+8eJycnsZ/t7u7O6IODA6Pr6+u9Nt3d3Ubjf4mDeigZMpQMGUpGImJodXW10c3NzUY3NDR4bW5ubox+eXkxuqKiojAPlyf4XEdHR941ZWW2H3V1df34fT6oh5IhQ8mQoWQkIob29vYa3dPTY/T19bXXBmNVUtnZ2fE+w8zZpqYmo+vq6n79e+qhZMhQMmQoGTKUjEQMijKZjNGjo6NGr62teW3W19eNxgk86igDjZqaGqNra2u9ax4fH3+8Bz774uKid83q6qrR4+PjRmtQJP4iQ8mQoWQkIobiwvrV1ZXRx8fHXhtcWMDYVllZaXSUuFReXm700NCQd83z87PRS0tLRuPC+ufnp3cP/CzOYjyiHkqGDCVDhpKRiBi6v79v9MfHh9GYiBWF35SOaGxsNHpkZMS75uLiwuiVlRWjcSMhNG9tbW01Gue/cVAPJUOGkiFDyUhEDL28vDT69vY2ZxtMrMa4FCV5GTeW29vbjcbktSjgOm0ohg4PDxutGCq+RYaSIUPJkKFkJGJQhLy9veXdBjPlccCDgybn/AV8HGjhJrpz/iIILnrgRkLo5JxOn4nIyFAyZCgZiYihGMsw/uH3zvkb3Pf390Zjglco4QtjNS5ohE6OYcIantjGeDg4OOjdA0+XFxL1UDJkKBkylIySxFBc9B4YGDAaK4ecnZ0V5TkwDuMmAW5mO+fc/Py80e/v70bjwntLS4t3D0xGKyTqoWTIUDJkKBmJiKE4V8M5IyZeRwHjY+jEd6i62FdCiWY478QN7KmpKaOxwolzhV27RdRDyZChZMhQMmQoGYlYnMfBB+rQwsLp6anReLos14DHudyb4LiZ7Zy/oI8n53CTHHWxUQ8lQ4aSIUPJKEkMxVPQGxsbRuPi/Ovra1GeA+MbVjDp6Ojw2uCzYSzHjfZ/jXooGTKUDBlKRiJiKCY04/zwN4nXUcAT22NjY0aHSqMmHfVQMmQoGTKUDBlKRtEHRaF3f+GJLcxIwAXv0A5/6L4/gQMt5/wM9qenJ6NDte6TjnooGTKUDBlKRuwYihl6uEmMk3fnnJudnTV6b2/P6ImJCaNDp88QXGjHDe9QqVSMoXiyLFQaHUujYnwvZkZfFNRDyZChZMhQMmLH0P7+fqPxvWXn5+deG5zf4Tw0VNY7FxgzMZaHyrzhie2FhQWjl5eXvTaYwIb/N9/5caFRDyVDhpIhQ8mIHUPx9DXG1MPDQ68NxiGMO8WYy+HpbOec293dNXpubs7oULI2xkwsjaoYKgqKDCVDhpIhQ8mIPSjCsi6oQyVcMLsOFxawVGoIzKbHAQxukuMGgHP+QAnvEXonWV9fn9FYtiZUgu5foh5KhgwlQ4aSETuGbm9vG42xK/ROksnJSaPxpPTm5qbRofe4YDY9xl38XUwAc85fwMAF/lBZN9wUL/WGNqIeSoYMJUOGkhE7hm5tbRmNJdhC5biLWV70O0LVSDD5OpPJGB16RUfSUQ8lQ4aSIUPJiB1DcX6Hr4oKbRK3tbXF/Vlv3olJYDinDB1WYkQ9lAwZSoYMJUOGklHwE9yY9R46BZ3rZHSU95aJMOqhZMhQMmQoGals6OUk4r9FPZQMGUrGH2rPj/aSISC2AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "out = out_unf.view(26,26)\n",
        "show_image(out);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnAAoep4t_bU",
        "outputId": "a9998896-dbd2-4d4e-cbec-0ec040098fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.5 ms ± 353 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 1 tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh-1NKZ_t_bU",
        "outputId": "978740b9-dde5-47fa-eb15-e1854bc7b8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56.2 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 100 (w@F.unfold(inp, (3,3))[0]).view(26,26);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxN0QdWht_bU",
        "outputId": "58edb76c-4062-4305-ca7c-2fa316566c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 7.39 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "25.4 µs ± 29.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 100 F.conv2d(inp, left_edge[None,None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1cZpMqWt_bV"
      },
      "outputs": [],
      "source": [
        "diag1_edge = tensor([[ 0,-1, 1],\n",
        "                     [-1, 1, 0],\n",
        "                     [ 1, 0, 0]]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwytZW8rt_bV",
        "outputId": "1eb2d5a9-e34d-4e71-d226-8955e0167aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAAB6ElEQVR4nO3cIY7bUBhF4TvVSKaWQRZgapZlZRfB4VmZ8SNmBpHpQx7UptFMQatKz3N0PmSzKx1ZZv/bvu97hPH+82HbtpRS0nVdyz36B7XWjOOYvu+fQUspud1uGYah5bZP7vd76wmfzPPcesKLZVmSJOfz+Rm067oMw5DT6dRs2HcxTVPrCX/0o/UA/V8GhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKMz77y9HPFBxxKs71+u19YQX67rmcrkk8QvFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDArzcjRjnudM09Rqy5eOdqDi6PxCYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQmF83FmqtWZal5ZYvrevaesLhPR6P1FqTJG/7vu9Jsm1bSinpuq7pOP29WmvGcUzf98+gYvAfCvMBYExIpyUhQqYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_image(diag1_edge, noframe=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUhuxTFUt_bV"
      },
      "outputs": [],
      "source": [
        "diag2_edge = tensor([[ 1,-1, 0],\n",
        "                     [ 0, 1,-1],\n",
        "                     [ 0, 0, 1]]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p50Q-2Stt_bV",
        "outputId": "dc83ab5c-e690-4abb-e511-c291817e6a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAAB7UlEQVR4nO3cLY7CUBhG4a8TkmurcJimDhTLwrMANsHOcOQafFN7VccMP5OKyUwmueXkPKokiDc5KbivmaZpCmGs7g/jOEbOOVJKNffoD0op0XVdtG37DJpzjtvtFn3f19w2s9vtak+YORwOtSd8MwxDHI/H2O/3z6Appej7Prbbbc1tb2G9XteeMHP/Zf2ovEP/zKAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFWb1+OJ/Pi7sfsMSrO03T1J4wcz/k4RsKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBRm9fNX6jqdTrUnzCztkMflcnk8+4bCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwBoUxKIxBYQwKY1AYg8IYFMagMAaFMSiMQWEMCmNQGIPCGBTGoDAGhTEojEFhDApjUBiDwhgUxqAwjxsLpZQYhqHmlrfxetNgCa7Xa2w2m4iIaKavCxDjOEbOOVJKVcfp90op0XVdtG37DCoG/0NhPgHxUUTP26+IXwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_image(diag2_edge, noframe=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVYO-i46t_bV",
        "outputId": "e08baded-ece6-41ec-e728-cffa1cef4586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "xb = x_imgs[:16][:,None]\n",
        "xb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtB1dNT0t_bW",
        "outputId": "ec4860c2-0569-4021-c979-452d2ff0e4c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])[:,None]\n",
        "edge_kernels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS1av_QOt_bW",
        "outputId": "ddb2f48e-745d-4521-a7b2-03594f410e00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 4, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "batch_features = F.conv2d(xb, edge_kernels)\n",
        "batch_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3i9UjbHt_bW"
      },
      "source": [
        "The output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jakWs28Ot_bW",
        "outputId": "27a38693-30d4-49d8-a9b4-1dea3505f967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 192x144 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAASdAAAEnQF8NGuhAAAEYUlEQVR4nO2dzSs1YRjGHUQkFjaU7Fj6jOzIEkVZSGKrpCRZKGSnhEKRkiKKZIEkG7KxEv4AK4nykXykKM67cnLdvR0HM2fmXF2/1fw6H/Ocrp65e+bcMxMIBoPBOEFDvNcDEM6S+FUCgYBX4xB/5PNAqxlKhgIlQ4GSoUDJUKBkKFAyFCgZCpQMBUqGAiVDgZKhQMlQoGQoUDIUKBmJ37+Fk9LSUvDOzk7wtrY28MXFRfCpqSnw4+NjB0f3ezRDyVCgZChQMgJfu/6Ye4qKiorA9/b2wNPT03/0fQ8PD+CZmZm/GpdTqKeIFAVKhgIlg3odWl5eHtpeX1+H1zIyMsDtBQRPT0/gb29v4LZmVlRUgNt1qf28W2iGkqFAyVCgZMT0OjQ1NRW8pKQEfGlpKbSdk5MDr9nfamuorYEjIyPgKysrYb+vv78ffHh4OM5NtA4lRYGSoUDJiOl16OzsLHhzc7Nj323rcVpaGvjBwQF4VVUVeEFBgWNj+QmaoWQoUDIUKBkxVUNtH1BtbS14uHW0rXlbW1vgo6Oj4JeXl+AnJyfg9/f34NXV1RGPxU00Q8lQoGQoUDJ8fS73r31AOzs7oW27Rq2srAS368a5uTnwm5ubsPt6f38Hf3l5Cbs/p/t4dS6XFAVKhgIlw1fr0Pz8fPDe3l5w2wd0e3sLfnV1Bb6wsBDafn5+hte2t7fD+l9JSUkB7+npAW9paXF0f59ohpKhQMlQoGR4WkOTk5PB7fnUmpoacNsra6/hPDo6Ard1zEtyc3Ojsh/NUDIUKBkKlAxPa2hxcTG4rZmW+vp6cPsfp9AMpUOBkuHpIXd8fBzc/n1nD6l+PsTGx+Pc+Pj48GYcnuxVuIYCJUOBkhHVGlpXVwduW0zsJX2bm5tuD8kxbM20v+X09DQq49AMJUOBkqFAyYhqDbV/ZyUlJYFfX1+Dr66uuj6mSLF/9Q0NDYV9v2057evrc3pI/0UzlAwFSoYCJcNXbZyvr6/gti0zmtiaaW9TY1tMLy4uwMfGxsBtG6lbaIaSoUDJUKBk+KqGennu1p5XtjWyqakJfGNjA7yxsdGVcf0UzVAyFCgZCpSMqNZQ2zNkvaGhAbyrq8u1sXR3d4MPDAyA20sXl5eXwe1lGH5BM5QMBUqGAiUjqjXU9tlYz8rKAp+cnASfn58Hv7u7A7eP2mhtbQ1tFxYWwmv2luXn5+fgu7u74NPT03GxgGYoGQqUDAVKhq/O5SYkJIB3dHSA2/Olj4+P4Hl5eRHv6/DwEHx/fx98cHAw4u/yE5qhZChQMhQoGVG9vapd+62trYGXlZWF/fx3j7eyfF2n2kdbuXme2At0e1VSFCgZCpQMT29Rnp2dDd7e3g5ue2G/q6ETExPgMzMzoe2zs7NfjzMWUA0lRYGSoUDJ8PVjPkTkqIaSokDJUKBkKFAyFCgZCpQMBUqGAiVDgZKhQMlQoGQoUDIUKBkKlAwFSgZc2/Jdn6vwP5qhZChQMv4BO0YtvpyPIkUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "img0 = xb[1,0]\n",
        "show_image(img0);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn1440pyt_bX",
        "outputId": "a2dfeac7-886c-4233-c2dc-4d4dce74175a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 180x180 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_images([batch_features[1,i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCroKKSjt_bX"
      },
      "source": [
        "### Strides and Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yju-zV_Vt_bX"
      },
      "source": [
        "With appropriate padding, we can ensure that the output activation map is the same size as the original image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtIluL3Ft_bX"
      },
      "source": [
        "<img src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/chapter9_padconv.svg?raw=1\" id=\"pad_conv\" caption=\"A convolution with padding\" alt=\"A convolution with padding\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYp-hiDQt_bX"
      },
      "source": [
        "With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMuyiYQit_bX"
      },
      "source": [
        "<img alt=\"A 4×4 kernel with 5×5 input and 2 pixels of padding\" width=\"783\" caption=\"A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"four_by_five_conv\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00029.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6n8sDyIt_bX"
      },
      "source": [
        "If we add a kernel of size `ks` by `ks` (with `ks` an odd number), the necessary padding on each side to keep the same shape is `ks//2`.\n",
        "\n",
        "We could move over two pixels after each kernel application. This is known as a *stride-2* convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H5SvgkLt_bX"
      },
      "source": [
        "<img alt=\"A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding\" width=\"774\" caption=\"A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"three_by_five_conv\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00030.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the problem is that you can't just do that because the output is not now ten probabilities for each item in our batch, but it's ten probabilities for each item in our batch for each of 28 by 28 pixels, because we don't even have a stride or anything. So you can't just use the same simple approach that we had for MLP. We have to be a bit more careful. So to make life easier, let's create a little conv function that does a conv2d  with a stride of two. Optionally followed by an activation. So if act is true, we will add in every relu activation. So this is going to either return conv2d or a little sequential containing a conv2d followed by a relu. And so now we can create a cnn from, you know, from scratch. That's the sequential model. And so since activation is true by default, this is going to take out 28 by 28 image starting with one channel and creating an output of four channels. So this is the number of in this is the number of filters. Sometimes we'll say filters to describe the number of kind of channels that our convolution has. That's the number of outputs. And it's very similar to the idea of the number of outputs in a linear layer. This is the number of outputs in your convolution. So what I like to do when I create stuff like this is I add a little comment just to remind myself what is my grid size after this. So I had a 28 by 28 input. So then I've then put it through a stride to conv, so the output of this will be 14 by 14. So then we'll do the same thing again, but this time we'll go from a four channel input to an eight channel output and then from 8 to 16. So by this point we're now down to a 4x4 and then down to a two by two. And then finally we're down to a one by one. So on the very last layer, we want add an activation and the very last layer is going to create ten outputs. And since we're now down to a one by one, we can just call flatten and that's going to remove those unnecessary unit axes. So if we take that pop mini batch through it, we end up with exactly what we want a 16 by ten. So for each of our 16 images, we've got  ten probabilities of each possible digit. So if we take our training set and make it into 28 by 28 images and we do the same thing for our validation set, and then we create two data sets, one for each which are called train data set and valid data set. And we're now going to train this on the GPU."
      ],
      "metadata": {
        "id": "_cZU2pv3QqeO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wULzF979t_bY"
      },
      "source": [
        "## Creating the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5v8d9FYt_bY"
      },
      "outputs": [],
      "source": [
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "nh = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiRaQRQWt_bY"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNS1xupvt_bY"
      },
      "outputs": [],
      "source": [
        "broken_cnn = nn.Sequential(\n",
        "    nn.Conv2d(1,30, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(30,10, kernel_size=3, padding=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l4LAHZZt_bY",
        "outputId": "2cf902a1-c253-4f38-ab44-b0a2771cbca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 10, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "broken_cnn(xb).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4PNEwrkt_bY"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def conv(ni, nf, ks=3, stride=2, act=True):\n",
        "    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
        "    if act: res = nn.Sequential(res, nn.ReLU())\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dujRuCsst_bY"
      },
      "source": [
        "Refactoring parts of your neural networks like this makes it much less likely you'll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN_FEZF-t_bY"
      },
      "outputs": [],
      "source": [
        "simple_cnn = nn.Sequential(\n",
        "    conv(1 ,4),            #14x14\n",
        "    conv(4 ,8),            #7x7\n",
        "    conv(8 ,16),           #4x4\n",
        "    conv(16,16),           #2x2\n",
        "    conv(16,10, act=False), #1x1\n",
        "    nn.Flatten(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now if you've got a mac, you can use a device called Apple. If you've got an Apple Silicon Mac, you've got a device called NPS, which is going to use the know Macs GPU here or if you've got it in video, you can use Cuda, which will use your and video creators, you know, ten times or more, possibly much more faster than a mac. So you definitely want to use in video if you can, but if you just running it on a laptop or whatever, you can use amps. So basically you're going to know what device to use. Do we want to use Coda or amps? You can check if you can check torchlight back backhands that is available to see if you're running on a mac with amps you can check \n",
        "\n",
        "torch.backends.mps.is_available()\n",
        "\n",
        ".It is available to see if you've got an in video GPU, in which case you've got CUDA. And if you've got neither, of course you'll have to use the CPU to do computation. So I've created a little function to device which takes a tensor or a dictionary or a list of tensors or whatever, and a device to move it to. And it just goes through and moves everything onto that device. Or if it's a dictionary, a dictionary of things, value has moved onto that device that has a handy little function. And so we can create a custom collate function which calls the PyTorch default collation function and then puts those tensors onto our device. And so with that, we've now got enough to run train this neural net on the GPU. You we created this get_dls function in the last lesson. So we're going to use that passing in the datasets that we just created and our default collation function, we're going to create our optimizer using our CNN's parameters and then we call fit(), now fit for a member that we also created in our last lesson and it's done. Yeah, it's really, it's a lot less code than last time I ran it. I don't know if I've changed something weird. Let's say now. There we go. \n",
        "\n",
        "I must have done something weird. Okay, so I then what I did then was I reduced the learning rate by a factor of four and ran it again. \n",
        "\n"
      ],
      "metadata": {
        "id": "xq7Kh0TMSE28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERg4YMQmt_bZ",
        "outputId": "2dc7460f-678f-41b8-99f5-20ae7c394ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "simple_cnn(xb).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPs0bP2wt_bZ"
      },
      "outputs": [],
      "source": [
        "x_imgs = x_train.view(-1,1,28,28)\n",
        "xv_imgs = x_valid.view(-1,1,28,28)\n",
        "train_ds,valid_ds = Dataset(x_imgs, y_train),Dataset(xv_imgs, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULWavUySt_bZ"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def to_device(x, device=def_device):\n",
        "    if isinstance(x, torch.Tensor): return x.to(device)\n",
        "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
        "    return type(x)(to_device(o, device) for o in x)\n",
        "\n",
        "def collate_device(b): return to_device(default_collate(b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHRHJwpst_bZ"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "bs = 256\n",
        "lr = 0.4\n",
        "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\n",
        "opt = optim.SGD(simple_cnn.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB4zgjDxt_bZ",
        "outputId": "f7ff16e2-bbd3-4d85-e5ff-dee0372675c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.2963203880310057 0.10639999992847443\n",
            "1 0.35034381108283996 0.8865999997138977\n",
            "2 0.2884100193500519 0.9101999996185303\n",
            "3 0.3795499787807465 0.8783999994277955\n",
            "4 0.14102031185626984 0.957599999332428\n"
          ]
        }
      ],
      "source": [
        "loss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And eventually, yeah, I got to a fairly similar accuracy to what we did on our multi on our MLP. So yeah, we've got a convolutional network working. I think that's pretty encouraging and it's nice that to train it, we didn't have to write much code right? We were able to use code that we had already built. We were able to use the dataset class that we made to get_dls function that we made and the fit function that we made. And you know, because those things are written in a fairly general way, they work just as well for conv net as they did for an MLP. Nothing had to change. So that was nice. Notice we had to take the model and put it on the device as well. So that will go through and basically put all of the tensors that are in that model onto the MPS or CUDA  device, if appropriate. So if we've got a batch size of 64 and as we do one channel channel, height width. So normally this is referred to as NCW, so n generally when you see N in a in a paper or whatever, in this way it's referring to the batch size and being the number that's the mnemonic, the number of items in the batch C is the number of channels height by width, and W. TensorFlow doesn't use that TensorFlow uses an NHWC So we generally call these that channels last since channels are at the and this one we normally call channels first. Now of course it's not actually channels first, it's actually channel second, but we ignore the batch bit \n",
        "\n",
        "in some models, particularly some more modern models. It turns out the channels last is faster. So PyTorch has recently added support for channels last. And so you'll see that being used more and more as well. \n",
        "\n",
        "Yeah, we go. All right. So a couple of comments and questions from our chat. The first is Sam Watkins pointing out that where we've actually had a bit of a win here, which is that the number of parameters now CNN is is pretty small by comparison. So the of in the MLP version, the number of parameters is equal to basically the size of this matrix. Right. So M times and NH, plus the number in this, which will be an H times ten. And you know something that at some point we probably should do is actually create something that allows us to automatically calculate the number of parameters. And I'm ignoring the bias there. Of course, let's say what would be a good way to do that? Maybe np.product. Yeah. Okay. So what we could do, what we could do is just calculate this automatically by doing a little list comprehension here.\n",
        "\n",
        "So there's the number of parameters across all of the different layers. So both bias and weights. And then we could, I guess, just well, we could just use well, let's use pytouch so we could turn that into a tensor and sum it up. So that's the number in our MLP. And then the number in our simple CNN. So that's pretty cool. We've gone down from 40000 to 5000 and got about the same number there. Oh, thank you, Jonathan. Jonathan's reminding me that there's a better way than np.product shape, which is just to say I dot number of elements num of element. Very nice. \n",
        "\n",
        "Now, one person asked a very good question, which is I thought convolutional neural networks can handle any sized image and actually know this convolutional network cannot handle any sized image. This convolutional neural network only handles images that once they go through these tried to convs end up with a one by one because otherwise you can't  dot Flatten it and end up with 16 by ten. So we will learn how to create comv nets that can handle any sized input. But there's nothing particularly about a net that necessitates that it has to be any sized input that it can handle. Okay. \n",
        "\n",
        "So just let's briefly finish this section off by talking about this. Yeah. That this well, particularly on to talk about the idea of receptive field, consider this. Yeah. One input Channel four output. Channel three by three kernal. Right. So that's just here just to show you what we're doing here conv one. Well actually so simple. See it in simple CNN. This is the model we created. Remember, is like a sequential model containing sequential models because that's how our current function worked. So simple. CNN zero is our first layer. It contains both conv and a relu so I simple CNN zero zero is the actual conv. So if we grab that whole conf one, it's a four by one, by three by three. So number of outputs, number of input channels and height by weight for that kernal and then it's got its bias as well. So that's how we could kind of deconstruct what's going on with our weight matrices or our parameters inside a convolution. Now I'm going to switch over to Excel.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XinYGOAJTTBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F0UR1m7t_bZ",
        "outputId": "ab206540-caf3-4487-e40b-16e48da8ac8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.10183437039852142 0.9675999994277954\n",
            "1 0.10473719484806061 0.9674999994277954\n",
            "2 0.10602079322338104 0.9673999995231628\n",
            "3 0.09614700574874878 0.9709999995231628\n",
            "4 0.09545752574205399 0.9696999994277954\n"
          ]
        }
      ],
      "source": [
        "opt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\n",
        "loss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmD_F2Ltt_ba"
      },
      "source": [
        "### Understanding Convolution Arithmetic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUVD19Tqt_ba"
      },
      "source": [
        "In an input of size `64x1x28x28` the axes are `batch,channel,height,width`. This is often represented as `NCHW` (where `N` refers to batch size). Tensorflow, on the other hand, uses `NHWC` axis order (aka \"channels-last\"). Channels-last is faster for many models, so recently it's become more common to see this as an option in PyTorch too.\n",
        "\n",
        "We have 1 input channel, 4 output channels, and a 3×3 kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuMsEsaLt_ba",
        "outputId": "3d6b4a40-9da7-4888-dddf-3dcd648f36cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "simple_cnn[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTSpxZeVt_ba",
        "outputId": "6c03ec94-b415-4fe5-9ce7-b07ef9625a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "conv1 = simple_cnn[0][0]\n",
        "conv1.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lLKAIEbt_ba",
        "outputId": "c7cc6d2d-9bf0-497b-9849-074f3a24a26a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "conv1.bias.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p3KFhQyt_ba"
      },
      "source": [
        "The *receptive field* is the area of an image that is involved in the calculation of a layer. *conv-example.xlsx* shows the calculation of two stride-2 convolutional layers using an MNIST digit. Here's what we see if we click on one of the cells in the *conv2* section, which shows the output of the second convolutional layer, and click *trace precedents*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wk6TqDvt_ba"
      },
      "source": [
        "<img alt=\"Immediate precedents of conv2 layer\" width=\"308\" caption=\"Immediate precedents of Conv2 layer\" id=\"preced1\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00068.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0dUgLeWt_ba"
      },
      "source": [
        "The blue highlighted cells are its *precedents*—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Click *trace precedents* again:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY1xLZrvt_ba"
      },
      "source": [
        "<img alt=\"Secondary precedents of conv2 layer\" width=\"601\" caption=\"Secondary precedents of Conv2 layer\" id=\"preced2\" src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/att_00069.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMB2j65Ct_bb"
      },
      "source": [
        "In this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the *receptive field*\n",
        "\n",
        "The deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1aHD5wut_bb"
      },
      "source": [
        "## Color Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QOSq3JPt_bb"
      },
      "source": [
        "A colour picture is a rank-3 tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gxZbvAht_bb"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DHu1odt_bb"
      },
      "source": [
        "<img src=\"https://github.com/fastai/course22p2/blob/master/nbs/images/chapter9_rgbconv.svg?raw=1\" id=\"rgbconv\" caption=\"Convolution over an RGB image\" alt=\"Convolution over an RGB image\" width=\"550\">"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}