{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "title: \"Writing Stable Diffusion from Scratch 7\"\n",
        "reading-time: \n",
        "date: \"2023-3-27\"\n",
        "categories: [fastaipart2,Stable-Diffusion]\n",
        "---"
      ],
      "metadata": {
        "id": "LlG_dwsGIbnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All credits goes to fast.ai <br>\n",
        "All mistakes are mine. <br>\n",
        "I have to put code from previous lessons scence they are connected. "
      ],
      "metadata": {
        "id": "x0XMLRRFoIx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should know and practice following after this blog post : \n",
        "1- Refactor pervious code to make it cleaner <br>\n",
        "2- Know how nn module works in pytorch <br>\n",
        "3- how setattr works ? <br>\n",
        "4- __repr__ ? <br>\n",
        "5- yeild from ? <br>\n",
        "6- supper() and object ? <br>\n",
        "7- reduce, map ? <br>\n",
        "8- optimizer <br>\n",
        "9- learning rate ? <br>\n",
        "10- Sampler <br>\n",
        "11- Collate function <br> \n",
        "12- Multi processing data loader <br>\n"
      ],
      "metadata": {
        "id": "pPp5sI_LmLg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jMl5VKBlmMIv"
      },
      "outputs": [],
      "source": [
        "#| default_exp training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Re1jxXTkmMIw"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from torch import tensor,nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BCf_tBnTmMIy"
      },
      "outputs": [],
      "source": [
        "from fastcore.test import test_close\n",
        "\n",
        "mpl.rcParams['image.cmap'] = 'gray'\n",
        "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
        "np.set_printoptions(precision=2, linewidth=125)\n",
        "\n",
        "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
        "path_data = Path('data')\n",
        "path_data.mkdir(exist_ok=True)\n",
        "path_gz = path_data/'mnist.pkl.gz'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)"
      ],
      "metadata": {
        "id": "Oi2ZCSoxJGs2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5QZ8I7GJPGV",
        "outputId": "2272b66b-e737-422d-f60a-bb30d822ec39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16656\n",
            "-rw-r--r-- 1 root root 17051982 Mar 29 06:53 mnist.pkl.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
      ],
      "metadata": {
        "id": "dszCyfTJJYcP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAVNpRchmMIz"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCmjm3dAmMI0"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rww8ufytmMI1"
      },
      "outputs": [],
      "source": [
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "nh = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OLxh6uGCmMI2"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zcXTQr-lmMI2",
        "outputId": "6cffc6fb-58f5-4171-8df5-370f334dc612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = Model(m, nh, 10)\n",
        "pred = model(x_train)\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YJId-BgmMI3"
      },
      "source": [
        "### Cross entropy loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubJ21eGOmMI4"
      },
      "source": [
        "First, we will need to compute the softmax of our activations. This is defined by:\n",
        "\n",
        "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
        "\n",
        "or more concisely:\n",
        "\n",
        "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$ \n",
        "\n",
        "In practice, we will need the log of the softmax when we calculate the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G638zd8OmMI4"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uuYeNPWXmMI4",
        "outputId": "3b62999d-ea60-49d7-d28f-9255a042e070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
              "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
              "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
              "        ...,\n",
              "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
              "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
              "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "log_softmax(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd8YE6k6mMI5"
      },
      "source": [
        "Note that the formula \n",
        "\n",
        "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
        "\n",
        "gives a simplification when we compute the log softmax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sJ2jB06_mMI6"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKquNcDFmMI6"
      },
      "source": [
        "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
        "\n",
        "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
        "\n",
        "where a is the maximum of the $x_{j}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uWk4b1JOmMI6"
      },
      "outputs": [],
      "source": [
        "def logsumexp(x):\n",
        "    m = x.max(-1)[0]\n",
        "    return m + (x-m[:,None]).exp().sum(-1).log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNeWlNJCmMI8"
      },
      "source": [
        "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9DYj8EwbmMI8"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mRocr8h8mMI8",
        "outputId": "2b19a3b4-23b2-4f36-b017-d07625ec92de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
              "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
              "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
              "        ...,\n",
              "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
              "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
              "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "test_close(logsumexp(pred), pred.logsumexp(-1))\n",
        "sm_pred = log_softmax(pred)\n",
        "sm_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea-KgaHDmMI9"
      },
      "source": [
        "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
        "\n",
        "$$ -\\sum x\\, \\log p(x) $$\n",
        "\n",
        "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
        "\n",
        "This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5f8xfA7RmMI-",
        "outputId": "ee9347df-3ac0-42be-98b3-3fb5984d1d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "y_train[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "28f9pgDLmMI-",
        "outputId": "7605ed23-d3b5-4dcc-a852-a50a0292f21a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-2.20, grad_fn=<SelectBackward0>),\n",
              " tensor(-2.37, grad_fn=<SelectBackward0>),\n",
              " tensor(-2.36, grad_fn=<SelectBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "sm_pred[0,5],sm_pred[1,0],sm_pred[2,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SVt6vGCwmMI_",
        "outputId": "21bd364b-09c0-4f72-e186-47805b27b192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "sm_pred[[0,1,2], y_train[:3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MIR5zRi8mMI_"
      },
      "outputs": [],
      "source": [
        "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1hmx-SOdmMI_",
        "outputId": "e59f60e3-b676-4fba-b0b6-525f1845fe91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.30, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "loss = nll(sm_pred, y_train)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJrRhVzWmMJA"
      },
      "source": [
        "Then use PyTorch's implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wPOMBYfemMJA"
      },
      "outputs": [],
      "source": [
        "test_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scB6AudWmMJB"
      },
      "source": [
        "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9obxUYlEmMJB"
      },
      "outputs": [],
      "source": [
        "test_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMeX6U-6mMJB"
      },
      "source": [
        "## Basic training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCiCBDiYmMJB"
      },
      "source": [
        "Basically the training loop repeats over the following steps:\n",
        "- get the output of the model on a batch of inputs\n",
        "- compare the output to the labels we have and compute a loss\n",
        "- calculate the gradients of the loss with respect to every parameter of the model\n",
        "- update said parameters with those gradients to make them a little bit better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NFZNBiEPmMJC"
      },
      "outputs": [],
      "source": [
        "loss_func = F.cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sQS2ECzzmMJC",
        "outputId": "cc721e88-421a-4b84-dddb-6f4e188b4dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n",
              " torch.Size([50, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "bs=50                  # batch size\n",
        "\n",
        "xb = x_train[0:bs]     # a mini-batch from x\n",
        "preds = model(xb)      # predictions\n",
        "preds[0], preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZmfDdolbmMJC",
        "outputId": "d5695218-1855-43eb-b010-ac16d38c904f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,\n",
              "        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "yb = y_train[0:bs]\n",
        "yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zqYS-3f_mMJC",
        "outputId": "be947a14-7b6d-4afd-ffbf-230c17bd0874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.30, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "loss_func(preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3RnED3xWmMJC",
        "outputId": "192c81ce-fffa-478a-bd67-0617d7dc7533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3,\n",
              "        9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "preds.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IWIS8odJmMJD"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ba-ar4kvmMJD",
        "outputId": "48f75b1f-c246-4ff1-ba87-0691661e5170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.08)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "accuracy(preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0ouCO0AUmMJD"
      },
      "outputs": [],
      "source": [
        "lr = 0.5   # learning rate\n",
        "epochs = 3 # how many epochs to train for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Wn2ritaymMJD"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0RASPaVTmMJD",
        "outputId": "04924c1c-0921-4758-d9bb-61c2e8c11ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.30, 0.08\n"
          ]
        }
      ],
      "source": [
        "xb,yb = x_train[:bs],y_train[:bs]\n",
        "preds = model(xb)\n",
        "report(loss_func(preds, yb), preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZE6bvtDYmMJE",
        "outputId": "163cfbfa-2b56-458f-c858-2c2595310bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11, 0.96\n",
            "0.13, 0.96\n",
            "0.10, 0.96\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        s = slice(i, min(n,i+bs))\n",
        "        xb,yb = x_train[s],y_train[s]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for l in model.layers:\n",
        "                if hasattr(l, 'weight'):\n",
        "                    l.weight -= l.weight.grad * lr\n",
        "                    l.bias   -= l.bias.grad   * lr\n",
        "                    l.weight.grad.zero_()\n",
        "                    l.bias  .grad.zero_()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCD3dAVjmMJE"
      },
      "source": [
        "## Using parameters and optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And so we're now going to I'm going to show you something that's part of pytorch and then going to show you how to build it, and then you'll see why this is really useful. So PyTorch  has a sub Module starting nn. And in there there's something called the Module class. Now we can we don't normally use it this way, but I just want to show you how it works. We can create an instance of it in the usual way we create instances of classes and then we can assign things to attributes of that module. So for example, it's assign a linear layer to it. And if we now print out that, you'll see it says, Oh, this is a module containing something called f00, which is a linear layer. But here's something quite tricky. This module we can say, show me all of the named children of that module and it says, Oh, this one code foo and it's a linear layer. And we can say, Oh, show me all of the parameters of this module. And it says, Oh, okay, sure. There's two of them. There's this four by three tensor, that's the weights and there's this four long vector. That's the biases. And so somehow just by creating this module and assigning this to it, it's automatically tracked what's in this module and what are its parameters. That's pretty neat. So we're going to see both how and why it does that. I'm just going to point out, by the way, why did I add list here? If I just said m1.named_children(), it just prints out generate an object which is not very helpful and that's because this is a kind of iterator called a generator, and it's something which is going to only produce the contents of this when I actually do something with it, such as list them out. \n",
        "\n",
        "So just popping a list around a generator is one way to like run the generator and get its output. That little trick when you want to look inside a generator. \n",
        "\n",
        "Okay, so now, as I said, we don't normally use it this way. What we normally do is we create our own class. So, for example, we create our own multilayer perception and we inherit it. We inherit from a nn.Module. And so then in __init__, this is the thing that constructs an object of the class. This is the special magic method that does that well, say, okay, well, how many inputs are there to this multilayer perceptron? How many hidden activations and how many output activations are there? So just be one hidden layer. And then here we can do just like we did up here, where we assign things as attributes. We can do that in this constructor, so we create an l1 attribute, which is a linear layer from number into number. Hidden l2 is a linear layer from number hidden number out, and we'll also create a relu. And so when we call that module(__call__), we can take the input that we get and run the linear layer and then run the relu here and then run the l2. And so I can create one of these, as you say, and I can have a look and see like, Oh, here's the attribute l1 and there it is, like I said, and I can say print out the model and the model knows all the stuff that's in it. And I can go through each of the named children and print out the name and the layer. Now, of course, if you're a member, although you can use __call__, we actually showed how we can refactor things using forward such that it would automatically kind of do the things necessary to make all the, you know, or automatic gradient stuff work correctly.\n",
        "\n",
        " And so in practice we're actually not going to do it __call__ we would do forward. So this is an example of creating a custom PyTorch module. And the key thing to recognize is that it knows what are the attributes you added to it, and it also knows what all the parameters. \n",
        "\n",
        "So if I go through the parameters and print out the shapes, you can see I've got my linear layers, weights first,  my first linear layers weights, my first linear layers biases second linear layers weights, second linear layers biases. And this is because we set nh, the number of hidden to 50. \n",
        "\n",
        "So why is that interesting? Well, because now I don't have to write all this anymore. Going through layers and having to make sure that they've all been put into a list where you've just been able to add them as attributes and they're automatically going to appear as parameters. So we can just say, go through each parameter and update it based on the gradient and the learning rate. And furthermore, you can actually just go model.zero_grad and it'll zero out all of the gradients. So that's really made our code quite a lot nicer and quite a lot more flexible, which is cool. \n",
        "we do \n",
        "\n",
        "So let's check that this still works. There we go. So just to clarify with if I called report on this before I ran it, as you would expect, the accuracy is about 8% with about 10% less and the loss is pretty high. And so after I run this fit this model, the accuracy goes up and the loss goes down. So basically it's all of this is exactly the same as before. The only thing I've changed are these two lines of code, so that's a really useful refactoring. So what how on earth did this happen? How did it know what the parameters and layers are? Automatically it used a trick called dunder __setattr__. and we're going to create our own and nn.module. "
      ],
      "metadata": {
        "id": "b2gZ8-TGnOYo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xy0fRrTmMJE"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZvXXfYJimMJE",
        "outputId": "1b4fe53d-f021-4e9f-ba97-813bb609d500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Module(\n",
              "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "m1 = nn.Module()\n",
        "m1.foo = nn.Linear(3,4)\n",
        "m1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AyX9XinhmMJE",
        "outputId": "fdb8698c-bdd9-4717-bc9a-d70d578a3c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "list(m1.named_children())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zeAW_tj2mMJE",
        "outputId": "3bf1e94a-fa34-4218-a70e-a717d631174c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.named_children at 0x7f64aaeec190>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "m1.named_children()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "XMmeTB9nmMJF",
        "outputId": "3e3dfbcf-6a2d-446b-9afa-d8fae96bc92f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.57,  0.43, -0.30],\n",
              "         [ 0.13, -0.32, -0.24],\n",
              "         [ 0.51,  0.04,  0.22],\n",
              "         [ 0.13, -0.17, -0.24]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.01, -0.51, -0.39,  0.56], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "list(m1.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zc5hHq_BmMJF"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x): return self.l2(self.relu(self.l1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KeidnailmMJF",
        "outputId": "c7353b70-12cc-4cd0-aa92-5d770265dd43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=50, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "model = MLP(m, nh, 10)\n",
        "model.l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RburJXekmMJF",
        "outputId": "25aeb0a6-b23e-4981-b27b-5b650ece60fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "5ttiyVgUmMJF",
        "outputId": "dab066f2-cf78-4e9d-f157-65ddac0cf574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l1: Linear(in_features=784, out_features=50, bias=True)\n",
            "l2: Linear(in_features=50, out_features=10, bias=True)\n",
            "relu: ReLU()\n"
          ]
        }
      ],
      "source": [
        "for name,l in model.named_children(): print(f\"{name}: {l}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Y1k9uL-5mMJG",
        "outputId": "baa2b052-df5b-419c-8643-88c32fca8dde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 784])\n",
            "torch.Size([50])\n",
            "torch.Size([10, 50])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for p in model.parameters(): print(p.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "RE7Psc5_mMJG"
      },
      "outputs": [],
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, n, bs):\n",
        "            s = slice(i, min(n,i+bs))\n",
        "            xb,yb = x_train[s],y_train[s]\n",
        "            preds = model(xb)\n",
        "            loss = loss_func(preds, yb)\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters(): p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "        report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fPWw8vKxmMJG",
        "outputId": "4e134716-8bb9-49ae-caa6-612a095a23fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19, 0.96\n",
            "0.11, 0.96\n",
            "0.04, 1.00\n"
          ]
        }
      ],
      "source": [
        "fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FLEgfu1mMJG"
      },
      "source": [
        "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now. So if there was no such thing as an nn.module, here's how would build it. And so let's actually build it and also add some things to it. So in __init__, we would have to create a dictionary for our named children. This is going to contain a list dictionary of all of the layers. Okay. And then just like before, we'll create a couple of linear layers, right? And then what we're going to do is going to define this special magic thing that Python has __setattr__. And this is called automatically by Python if you have it, every time you set an attribute such as here or here and it's going to be past the name of the attribute, the key and the value is the actual thing on the right hand side of the equals sign. Now, generally speaking, things that start with an underscore where we use for either private stuff. So we check that it doesn't start with an underscore. And if it doesn't start with an underscore, __setattr__ will put this value into the modules dictionary with this key and then call Python's the normal python __setattr__, try to make sure it just actually does the attribute setting. So super is how you call whatever is in the the superclass, the base class. So another useful thing to know about is how do we how does how does it do this nifty thing where you can just type the name and it kind of lists out all this information about it.\n",
        "\n",
        " That's a special thing called __repr__. So here,  __repr__ will just have it return a stringified version of the module's dictionary. And then here we've got parameters. How did parameters work? So how did this thing work? Well, we can go through each of those modules, go through each value. So the values of the modules is all the actual layers and then go through each of the parameters in each module and yield p.  So that's going to, that's going to create an iterator. If you remember when we looked at iterates for all the parameters, So let's try it so we can create one of these modules. \n",
        " \n",
        " there they are now just mentioned something that's optional, kind of like advanced Python that a lot of people don't know about, which is there's no need to loop through a list or a generator or I guess I say look for an iterator and yield. There's actually a shortcut, which is you can just say yield from and then give it the iterator. \n",
        " \n",
        " And so with that we can get this all down to one line of code and it'll do exactly the same thing. So that's basically saying yield one at a time. Everything in here, that's what yield from does. So there's a little advanced python thing, totally optional. But if you're interested I think it can be kind of neat.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "r1gfaE2irDsm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DfRlOooWmMJG"
      },
      "outputs": [],
      "source": [
        "class MyModule:\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        self._modules = {}\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "\n",
        "    def __setattr__(self,k,v):\n",
        "        if not k.startswith(\"_\"): self._modules[k] = v\n",
        "        super().__setattr__(k,v)\n",
        "\n",
        "    def __repr__(self): return f'{self._modules}'\n",
        "    \n",
        "    def parameters(self):\n",
        "        for l in self._modules.values(): yield from l.parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Ko-fjjxNmMJH",
        "outputId": "bc056413-53f3-4a45-da0e-b04497ea826d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "mdl = MyModule(m,nh,10)\n",
        "mdl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nApa1RzImMJH",
        "outputId": "a1d75396-b272-427e-d32e-644aa8a57cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 784])\n",
            "torch.Size([50])\n",
            "torch.Size([10, 50])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for p in mdl.parameters(): print(p.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we've now learned how to create our own implementation of an   nn.module and therefore we are now allowed to use pytouches      nn.module. So that's good news. So how would we do using the PyTorch and nn.module? How would we create the model that we started with, which is where we had this self,layers because we want to somehow register all of these all at once? That's not going to happen based on the code we just wrote. So to do that, let's have a look. We can so let's make a list of the layers we want. And so we'll create again, a subclass of nn.module. Make sure you call the super classes in it first(super()__init__) and we're just store list of layers. And then to tell PyTorch about all those layers, we basically have to loop through them and call add_module and say what the name of the module is and what the module is. And again, because should probably should have used forward to here in the first place and you can say this is now done exactly the same thing. Okay. So if you've used a sequential model before, you'll see or you can say that we're on the path to creating a sequential model. \n",
        "\n",
        "Okay. So Gonash asked an interesting question, which is what on earth is super calling? Because we actually in fact, we don't even need the parentheses here. We actually don't have a base class. That's because if you don't put any parentheses or if you put empty parentheses, it's actually a shortcut for writing that. And so Python has stuff in object which does, you know, all the normal object, things like storing your attributes so that you can get them back later. So that's what's happening there. Okay. \n",
        "\n",
        "So this is a little bit awkward is to have to store the list and then enumerate and call add_module. So now that we've implemented that from scratch, we can use PyTorch is version, which is they've just got something called ModuleList that just does that for you. Okay. So if you use ModuleList and pass that list of layers, it will just go ahead and register them all those modules for you. So here's something called sequential model. So this is just like    nn.sequential now. So if I create it passing in the layers, there you go. You can see that's my module containing my module list with my layers. And so to know why I never used forward for these things, it's silly because it doesn't add a terribly in this stage. But anyhow, okay, so call fit. And there we go. Okay, so, so in forward here, I just go through each layer and I set the result of that equal to calling that layer on the previous result and then pass and return it at the end. "
      ],
      "metadata": {
        "id": "RDC6lbkhs2VC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY3Xf1nDmMJH"
      },
      "source": [
        "### Registering modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dcjOZSdFmMJH"
      },
      "outputs": [],
      "source": [
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6nhRrMvmMJH"
      },
      "source": [
        "We can use the original `layers` approach, but we have to register the modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "N-Ilvtl6mMJH"
      },
      "outputs": [],
      "source": [
        "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now there's little another way of doing this, which I think is kind of fun. It's not like shorter or anything at this stage. I just wanted to show an example of something that you see quite a lot in machine learning code, which is the use of reduce. \n",
        "\n",
        "This implementation here is exactly the same as this thing here. So let me explain how it works. A lot reduced. So reduced is a very common kind of like fundamental or computer science concept reductions. This is something that does a reduction.\n",
        "\n",
        " And what a reduction is??\n",
        " \n",
        " is it's something that says start with the third parameter, some initial value. So we're going to start with x, the thing with being passed and then loop through a sequence. So look through each of our layers and then for each layer, call some function. Here is our function and the function is going to get passed. First time around, it'll be past the initial value and the first thing in your list. So your first layer and x. So it's just going to call the layer function on x the second time around to take the output of that and passes that in as a second as the first parameter and passes in the second layer. So then the second time this goes through, it's going to be calling the second layer on the result of the first layer and so forth, and that's what a reduction is. And so you might see reduce, you'll certainly see it talked about quite a lot in in papers and books and you might sometimes also see it in code. It's a very general concept. And so here's how you can implement a sequential model using reduce. So there's no explicit loop there, although it's still happening internally. \n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "xlhCwI1gu8_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tf9SGKtMmMJI"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
        "\n",
        "    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "iJ5aT1W9mMJI",
        "outputId": "3fca77e4-0de3-4bfa-cc68-38ca74c56693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (layer_1): ReLU()\n",
              "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "model = Model(layers)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TICvP8Z4mMJI",
        "outputId": "35622aa5-051d-4bb5-c79d-94653832b175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "model(xb).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DERf4JBUmMJI"
      },
      "source": [
        "### nn.ModuleList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFxIqP7tmMJI"
      },
      "source": [
        "`nn.ModuleList` does this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fqHD6rv6mMJI"
      },
      "outputs": [],
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HU3MEq9lmMJI",
        "outputId": "d5f2157d-6f27-4179-a5f5-383c21769c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialModel(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "model = SequentialModel(layers)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "7B_fXK8MmMJJ",
        "outputId": "65105f38-d3be-4aa9-86fd-be0e9e4fe994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12, 0.96\n",
            "0.11, 0.96\n",
            "0.07, 0.98\n"
          ]
        }
      ],
      "source": [
        "fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All right. So now that we've re implemented sequential, we can just go ahead and use PyTorch as version.there's an nn.Sequential we can pass in our layers and we can fit, not surprisingly, we can see the model. So yeah, it looks very similar to the one we built ourselves. All right. \n",
        "\n"
      ],
      "metadata": {
        "id": "GbMgPyVuvvOo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-riZT5gmMJJ"
      },
      "source": [
        "### nn.Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZAOeOpzmMJJ"
      },
      "source": [
        "`nn.Sequential` is a convenient class which does the same as the above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-LJjOgVlmMJJ"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TiYEgfllmMJJ",
        "outputId": "b51a35bf-f618-47f9-d9f8-15daf46d6519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16, 0.94\n",
            "0.13, 0.96\n",
            "0.08, 0.96\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oJZaiLmlmMJK",
        "outputId": "7ee2efc3-5fcc-4b21-f951-e1132a337011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this thing of looping through parameters and updating our parameters based on gradients and aligning right and then zeroing them is very common.\n",
        "\n",
        "So common that there is something that does at all for us and that's called an optimizer. It's the stuff in Optim. So let's create our own optimizer.\n",
        " And as you can see, it's just going to do the two things we just saw.\n",
        "\n",
        "1-It's going to go through each of the parameters and update them using the gradient in the lending rate. \n",
        "\n",
        "2-And there's also zero grad which will go through each parameter and set their gradients to zero.\n",
        "\n",
        " If you used .data like it's just a way of avoiding having to say touch.no_grid basically. \n",
        " \n",
        " Okay, so in Optimizer we're going to pass at the parameters that we want to optimize. I'm going to pass at the learning, right? And we're just going to store them away. And since the parameters might be a generator, we'll call list to turn them into a list. So we are going to create our optimizer, pass it in the model. parameters which have been automatically constructed for us by an nn.module. And so here's our new loop. Now, we don't have to do any of the stuff manually. We can just say opt.step. So that's going to call this and opt.zero_grad and that's going to call this. There it is. So we've now built our own SGD optimizer from scratch. \n",
        " \n",
        "So I think this is really interesting right?\n",
        "Like these things which seem like they must be big and complicated once we have this nice structure in place, you know, an SGD to optimize, it doesn't take much code at all. And so it's all very transparent, simple clear. If you're having trouble using complex library code that you've found elsewhere, you know, this can be a really good approach is to actually just go all the way back and move as you know, as many of these abstractions as you can and like run everything by hand to see exactly what's going on. It can be really freeing to see that you can do all this anyway, since PyTorch has this for us In torch.optim. It's got a optim.SGD. And just like our version, you pass in the parameters and you pass in the learning, right? So you really see it is just the same. So let's define something called get model that's going to return the model, the sequential model and the optimizer for it. So if we go model, comma opt equals get model, and then we can call the lost function to see where it's starting. And so then we can write our training loop again, go through each epoch, go through each starting point for our for our batches, grab the slice, slice into our x and y in the training set to get a predictions, calculate our loss to the backward pass, to the optimizer, step to the zero gradient and print out how you're going at the end of each one. And then we go, "
      ],
      "metadata": {
        "id": "0AnincC7wGn2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqoo8PNFmMJK"
      },
      "source": [
        "### optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ZYA5LFzEmMJK"
      },
      "outputs": [],
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
        "\n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for p in self.params: p -= p.grad * self.lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.params: p.grad.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "JUpmZ3EwmMJK"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "eNJhGov9mMJK"
      },
      "outputs": [],
      "source": [
        "opt = Optimizer(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "FpwgSzDUmMJK",
        "outputId": "913c4481-e710-4497-dc6f-1962dbf7eb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18, 0.94\n",
            "0.13, 0.96\n",
            "0.11, 0.94\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        s = slice(i, min(n,i+bs))\n",
        "        xb,yb = x_train[s],y_train[s]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj1mptuEmMJL"
      },
      "source": [
        "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rBw948JdmMJL"
      },
      "outputs": [],
      "source": [
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "-NLTJybXmMJL"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "f8ohSJjQmMJL",
        "outputId": "3aea759a-a19b-4a3d-8ed7-f91e37151d99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.33, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "model,opt = get_model()\n",
        "loss_func(model(xb), yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0zJ0bwJJmMJL",
        "outputId": "83cac206-7a46-483d-cd8e-c48671e9901d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12, 0.98\n",
            "0.09, 0.98\n",
            "0.07, 0.98\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        s = slice(i, min(n,i+bs))\n",
        "        xb,yb = x_train[s],y_train[s]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7DCuSpqmMJM"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All right, so let's keep making this simpler. This don't say much code. So one thing we could do is we could replace these lines of code with one line of code by using something, call the dataset class. \n",
        "\n",
        "So the dataset class is just something that we're going to pass in our independent and dependent variable. Well, store them away as self.x and self.y. Why we'll have something. So if you if you define __len__, then that's the thing that allows the len function to work. So the length of the dataset would just be the length of the independent variables. \n",
        "\n",
        "And then  __getitem__ is a thing that will be called automatically any time you use square brackets in Python. So that task is going to call this function passing in the indices you want. So when we grab some items from our dataset, we're going to return a couple of the x values and the y values. \n",
        "\n",
        "\n",
        "So then we'll be able to do this. So let's create a data set using this tiny little tree line class. it's going to be a dataset containing the x and y training, and they'll create another dataset containing the x and y valid. And those two datasets will call train_ds and valid_ds. So let's check the length of those data sets should be the same as the length of the xs and they are. And so now we can do exactly what we hope we could do. We can say xb,yb equals train_ds and passing some slice. So that's going to give us back our check The shapes are correct. It should be five by 28, by 28. And the y is should just be five. And so here they are, the xs and the y's. So that's nice. We've created a dataset from scratch and again, it's not complicated at all. And if you look at the actual PyTorch source code, this is basically your data sets do so let's try it. We call get_model() And so now we've replaced our dataset line with this one and as usual it still runs. And so this is what I do when I'm writing code is I try to like always make sure that my starting code works as I refactor. And so you can see all the steps. And so somebody reading my code can then see exactly like, why am I building everything I'm building? How does it all fit in? Say that it still works and I can also keep it clear in my own head. So I think this is a really nice way of implementing libraries as well. All right. So now we're going to replace these two lines of code with this one line of code. So we're going to create something called a data loader.\n",
        "\n"
      ],
      "metadata": {
        "id": "uqIeyt50yF4-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZP9_u25mMJM"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ99f9SXmMJM"
      },
      "source": [
        "It's clunky to iterate through minibatches of x and y values separately:\n",
        "\n",
        "```python\n",
        "    xb = x_train[s]\n",
        "    yb = y_train[s]\n",
        "```\n",
        "\n",
        "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
        "\n",
        "```python\n",
        "    xb,yb = train_ds[s]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Blo-XPeJmMJM"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i],self.y[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "5SyeU4v8mMJM"
      },
      "outputs": [],
      "source": [
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "assert len(train_ds)==len(x_train)\n",
        "assert len(valid_ds)==len(x_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CdPvYj-7mMJM",
        "outputId": "491e4b5f-077e-4313-bb0f-db7393564124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "xb,yb = train_ds[0:5]\n",
        "assert xb.shape==(5,28*28)\n",
        "assert yb.shape==(5,)\n",
        "xb,yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "wC-bBJ-EmMJM"
      },
      "outputs": [],
      "source": [
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "XkuNz5YemMJM",
        "outputId": "b7411200-1752-4c52-8517-9c62d31c6413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.17, 0.96\n",
            "0.11, 0.94\n",
            "0.09, 0.96\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        xb,yb = train_ds[i:min(n,i+bs)]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch9LeA6CmMJN"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we're going to replace these two lines of code with this one line of code. So we're going to create something called a data loader. And a data loader is something that's just going to do this. Okay. So we need to create an iterator. So an iterator is a class that has a __iter__ method. When you say for in in Python behind the scenes, it's actually calling __iter__ to get a special object, which it can then loop through using yield. So it's basically getting this thing that you can iterate through using the yield. So a data loader is something that's going to have a data set and a batch size because we're going to go through the batches and grab one batch at a time. So we have to store away the data set in the batch size. And so when we when we call the for loop, it's going to code __iter__. We're going to want to do exactly what we saw before, go through the range just like we did before, and then yield that bit of the data set. And that's all. So that's a data letter. So we can now create a train data loader and a validator loader from our train data set and validator set. And so now we can, if you remember the way you can create one thing out of an iterator so you don't need to use a for loop, you can just say __iter__. And that will also code and data. Next, we'll just grab one value from it. So here we will run this and you can see we've now just confirmed wave xb is a 50 by 784. And why yb, there it is. And then we can check what it looks like. So let's grab the first element of our x batch, make it 28 by 28. And there it is. So now that we've got a date loader again, we can grab our model and we can simplify our fit function to just go for xb,yb and train_dl.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5LZFjTZwzxtO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A21vPFKmMJN"
      },
      "source": [
        "Previously, our loop iterated over batches (xb, yb) like this:\n",
        "\n",
        "```python\n",
        "for i in range(0, n, bs):\n",
        "    xb,yb = train_ds[i:min(n,i+bs)]\n",
        "    ...\n",
        "```\n",
        "\n",
        "Let's make our loop much cleaner, using a data loader:\n",
        "\n",
        "```python\n",
        "for xb,yb in train_dl:\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "CXlIxkpqmMJN"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ZN1w52U7mMJN"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, bs)\n",
        "valid_dl = DataLoader(valid_ds, bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "TulkQO72mMJN",
        "outputId": "f4e72342-a017-43be-9476-bcd92cfa3d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "xb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "kL3lLbYbmMJN",
        "outputId": "71be89a3-f09f-4d10-bec5-a131c8dcb5d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7,\n",
              "        6, 8, 9, 0, 3, 8, 3, 7, 7, 8, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "wgizv0KimMJN",
        "outputId": "f16e6854-91d0-43b1-a25b-35c1e21eb4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "WA82Sm0EmMJO"
      },
      "outputs": [],
      "source": [
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8VFdDm7LmMJO"
      },
      "outputs": [],
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "SyxXqNVymMJO",
        "outputId": "bc0706a7-8ad5-4e80-d6ac-45cd8d7d7ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11, 0.96\n",
            "0.09, 0.96\n",
            "0.06, 0.96\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCcz8sM2mMJO"
      },
      "source": [
        "### Random sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " So this is getting nice and small, don't you think? And it still works the same way. Okay, so this is really cool. And now that it's nice and concise, we can start adding features to it. So one feature I think we should add is that our training set each time we go through it, it should be in a different order. It should be randomized the order. So instead of always just going through these indexes in order, we want some way to say use random indexes. So the way we can do that is create a class called sampler. And what sampler is going to do here is if we create a sampler without shuffle, without randomizing it, it's going to simply return all the numbers from zero up to n in order and it'll be an iterator. \n",
        "\n",
        "So the way we can do that is create a class called sampler. And what sampler is going to do here is if we create a sampler without shuffle, without randomizing it, it's going to simply return all the numbers from zero up to end in order and it'll be an iterator. So this is done data, but if I do want to, then it will randomly shuffle them. So here you can see I've created a sampler without shuffle. So if I then make an iterator from that and print a few things in the iterator, you can see it's just printing out the indexes. It's going to want, or I can do exactly the same thing as we learned earlier in the course using Isolés, we can grab the first five. So here's the first five things from a sampler when it's not shuffled. So as you can see, these are just indexes. So we could add shuffle equals true. And now that's going to call random dot shuffle, which just randomly permits them. And now if I do the same thing, I've got random indexes. If my source data. So why is that useful? Well, what we can now do is create something called a batch sampler, and the batch sampler is going to do is it's going to basically do this isolates thing for us. So we're going to say, okay, pass in a sampler, set something that generates indices and pass in a batch size. And remember, we've looked at chunking before. It's going to chunk that iterator by that batch size. And so if I now say, all right, please take our sampler and create batches of four, as you can see here, it's creating batches of four indices at a time. So rather than just looping through them in order, I can now loop through this batch sampler. So we're going to change our data loader so that now it's going to take some batch sampler and it's going to look through the batch sampler that's going to give us indices and then we're going to get that data set item from that batch for everything in that batch.\n",
        "\n",
        "So that's going to give us a list and then we have to stack all of the (x es) and all of the (y eys) together into tensers. So I've created something here called Collate function and we're going to default that to this little function here, which is going to grab our batch, pull out the (x es) and (y eys) separately, and then stack them up into tensors. So this is called our collate function. Okay, So if we put all that together, we can create a training sampler, which is a batch sampler over the training set with shuffle True, a validation sampler will be a batch sampler over the validation set with shuffle false. And so then we can pass that into this data loader class, The training data set and the training sampler and the COLLATE function, which we don't really need because we're just using the default one. So I guess we can just get rid of that. And so now here we go. We can do exactly the same thing as before. xb,yb =next(iter(valid_dl)) this time we use the validator loader, check the shapes. And so now check. That still works. And it does. So this is how PyTorch is actual data loaders work. This is the this is all the pieces they have. They have samplers, they have batch samplers, they have a collation function and they have data letters. So remember that what I want you to be doing for your homework is experimenting with these carefully to see exactly what each thing is taking in. Okay, so here is asking on the chat what is this collate thing doing?\n",
        "\n",
        " Okay, so collate function and it defaults to collate. What does it do? Well, let's see. Let's go through each of these steps. Okay, so we need so we've got a batch sampler. So let's do just the valid sampler. No fit didn't work. We have to look at that to. Okay, so the batch sampler, here it is. So we're going to go through each thing in the batch sampler. So let's just grab one thing from the batch sampler. Okay? So the output with the batch sampler will be next. It's a okay, so here's what the batch sampler contains. All right. Just the first 50 digits, not surprisingly, because this is a validation sampler. If we did a training sampler, that would be randomized. Yeah, they are. Okay, so then what we then do is we go self.ds[i] for i and b, so let's copy that copy paste. And so rather than self.ds[i] will just say train_ds[i] for i in o ,Sorry. Training. Okay. So what it's created here is a list of tuples of tensors, I think. Let's have a look. So let's have a look. So we'll call this a whatever. So p zero. Okay, is a tuple. It's got the x and the y independent, independent variable. So that's not what we want. What we want is something that we can live through. We want to get batches. So what the collation function is going to do is it's going to take all of our x's and all of our y's and collect them into two tensors, one tensor x's and one tensor y's. So the way it does that is it. First of all, calls zip. So zip is a very, very commonly used python function. It's got nothing to do with the compression program zip. But instead what it does is it effectively allows us to like transpose things so that now, as you can see, we've got all of the second elements, index one elements all together and all of the index zero elements together. And so then we can stack those all up together and that gives us our y`s for our batch. So that's what collate it does. So the collate function is used an awful lot in in PyTorch increasingly nowadays where hackingface stuff uses it a lot and so we'll be using it a lot as well. And basically it's a thing that allows us to customize how the data that we get back from our dataset set. Once it's been kind of generating a list of things from the dataset, how do we put it together into some into a bunch of things that our model can take as inputs, because that's really what we want here. So that's what the collection function does. All right. So let's try this again. And the reason these are wrong, it'll be something to do with a report. It's a accuracy. So something about the accuracy is not printing out correctly. Let's see if we can figure it out. So probably does it down here. Yeah, it'd be something to do with that shuffling. If we tend to shuffle off, probably find out our work. Yes, it does. Okay, sir, my shuffling bracket. Hmm. Oh, I see why this should be preds. Better.So when we run our model and we call fit(), we get the same results. One trick I was just going to mention normally in __init__ the way we we very, very often want to grab all the stuff that we've been passed as parameters and store it away like so this is the wrong way around. Like so this is something that I do so often that fast core has a quick little shortcut for it. Just called Store at Trust all attributes. And so if you just put that in your __init__, then you just need one line of code and it does exactly the same thing. So there's a little shortcut as you see. And so you'll see that quite a bit. All right, let's have a seven minute break and see you back here very soon. "
      ],
      "metadata": {
        "id": "flux2cBu06J0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-uAo34NmMJO"
      },
      "source": [
        "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "j8R2R46FmMJO"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Tx3bDtxCmMJO"
      },
      "outputs": [],
      "source": [
        "class Sampler():\n",
        "    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n",
        "    def __iter__(self):\n",
        "        res = list(range(self.n))\n",
        "        if self.shuffle: random.shuffle(res)\n",
        "        return iter(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "IijZ19DNmMJO"
      },
      "outputs": [],
      "source": [
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9k5Vqyb6mMJO"
      },
      "outputs": [],
      "source": [
        "ss = Sampler(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "eO1YTggtmMJP",
        "outputId": "489aa89a-bb0a-4034-f54b-29f42437cf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "it = iter(ss)\n",
        "for o in range(5): print(next(it))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "BnYaWw8QmMJP",
        "outputId": "066ec75f-60c0-470d-9a92-d7aaeb1f8401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "list(islice(ss, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "jYZ8BC0AmMJP",
        "outputId": "14277b28-5703-442f-b67a-d3da68f37e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28659, 39049, 23211, 13983, 38058]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "ss = Sampler(train_ds, shuffle=True)\n",
        "list(islice(ss, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "KX07gevdmMJP"
      },
      "outputs": [],
      "source": [
        "import fastcore.all as fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1khLsfwTmMJP"
      },
      "outputs": [],
      "source": [
        "class BatchSampler():\n",
        "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
        "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "9gyVdFpImMJP",
        "outputId": "1ec22ab0-4142-4f55-b25b-cf7456f47a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[7445, 36933, 36891, 13229],\n",
              " [47783, 46860, 1239, 20962],\n",
              " [8646, 29897, 9202, 31355],\n",
              " [48398, 35167, 44700, 27769],\n",
              " [7834, 22128, 40411, 5830]]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "batchs = BatchSampler(ss, 4)\n",
        "list(islice(batchs, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "txERkZMBmMJP"
      },
      "outputs": [],
      "source": [
        "def collate(b):\n",
        "    xs,ys = zip(*b)\n",
        "    return torch.stack(xs),torch.stack(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "n1mR_mCImMJQ"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n",
        "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "vOQJ1UXqmMJQ"
      },
      "outputs": [],
      "source": [
        "train_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\n",
        "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "N-b0PmVjmMJQ"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batchs=train_samp)\n",
        "valid_dl = DataLoader(valid_ds, batchs=valid_samp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "75L554JzmMJQ",
        "outputId": "1f1c9e02-46e7-4d98-8604-3920bdb7cdd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "llLc03IKmMJQ",
        "outputId": "0b49d8a1-d436-4c3e-d399-323129d94f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([50, 784]), torch.Size([50]))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "xb.shape,yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "E-F_jZOsmMJQ"
      },
      "outputs": [],
      "source": [
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "HEQqnsSxmMJQ",
        "outputId": "21896cb5-8f7d-4280-9257-081b2d0d45a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.33, 0.12\n",
            "0.22, 0.06\n",
            "0.02, 0.04\n"
          ]
        }
      ],
      "source": [
        "fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zsLI6UkmMJQ"
      },
      "source": [
        "### Multiprocessing DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we're going to look at a multi processing data loader and then we'll have nearly finished this notebook.\n",
        " \n",
        "All right. So you send it because wants to know how it implements store_attr. You can always look it up. Right. So if we could go look at here is the docs and there's always a link to the source. Yeah, go. You can create a version that's a lot less code than this. This is dealing with a few extra things like slots and store_args and stuff. So basically the trick is to see who's calling you using some rather internal ish stuff on code. All right, let's keep going. So we've seen how to create a data loader and sampling from it. The PyTorch data loader works exactly like this, but it uses a lot more code because it implements multi processing. And so multi processing means that the actual this thing here, that code can be run in multiple processes, that can be run in parallel for multiple items. So this code, for example, might be opening up a jpeg, rotating it, flipping it, etc.. Right. So because remember, this is just calling the __getitem__ for a data set. So that could be doing a lot of work for each item and we're doing it for every item in the batch. So we'd love to do those all in parallel. So I'll show you a very quick and dirty way that basically does the job. So Python has a multiprocessor library. It doesn't work particularly well with PyTorch tensors, so PyTorch has created an exact implementation of it. So it's identical API wise, but it does work well with tensors. So this is basically what a script the multi processing. So this is not quite shading because multi processing is in the standard library and this is API equivalent. So I'm going to say we're allowed to do that. So as we've discussed, you know, when we call square brackets on a class, it's actually identical to calling the __getitem__ function on on the object. So you can see here, if we say give me items three, six, eight and one, it's the same as calling __getitem__ passing in three, six, eight and one.\n",
        "\n",
        " Now why does this matter??\n",
        " \n",
        "Well I'll show you why it matters because we're going to be able to use map and explain why we want to use map the moment map is a really important concept. You might have heard of MapReduce. So we've already talked about reductions and what those are maps are kind of the other key piece map is something which takes a sequence and calls a function on every element of that sequence.\n",
        "\n",
        " So imagine we had a couple of batches of indices three and six and eight and one. Then we're going to call __getitem__ on each of those batches. So that's what map does. map calls this function on every element of the sequence. And so that's going to give us the same stuff. But now this, same as this, but now batched into two batches. Now why do we want to do that? Because multi processing has something called Pool where you can tell it. How many workers do you want to read and how many processes you want to run? And it then has a map which works just like the python normal python map, but it runs this function in parallel over the items from this iterator. So this is how we can create a multi processing data loader. So here we are creating our data loader and again, we don't actually need to pass in the collate function because we using the default one. So if we say n_workers  equals two and then create that if we say next, see how it's taking a moment took a moment because it was firing off those two workers in the background. So the first batch actually comes out more slowly. But the reason that we would use a multi processing data loader is if this is doing a lot of work, we want it to run in parallel. And even though the first  item might come out a bit slower, once those processes are fired up, it's going to be faster to run. So this is yeah, this is a really simplified multi processing data loader because this needs to be super, super efficient. PyTorch has lots more code than this to make it much more efficient. But the idea is this and this is actually a perfectly good way of experimenting or building your own data loader to make things work exactly how you want. So now that we've really implemented all this from PyTorch, let's just grab the PyTorch. And as you can see, they're exactly the same data laoder. They don't have one thing called sampler that you pass shuffle to. They have two separate classes called sequential sampling random sampler. I don't know why they do it that way. It's a bit more work to me, but same idea. And they've got that sampler. And so it's exactly the same idea. That training sampler is a batch sampler with a random sampler. The validation sampler is a batch sampler with a sequential sampler passing in batch sizes. And so we can now pass those samplers to the data loader. This is now the PyTorch data letter. And just like ours, it also takes a collate function. Okay. And it works cool. So that's as you can see, it's it's doing exactly the same stuff that ours is doing with exactly the same API. And it's got some shortcuts, as I'm sure you've noticed when you've used data loaders.\n",
        " \n",
        "So, for example, calling batch sampler is very going to be very, very common. So you can actually just pass the batch size directly to a data loader and it will then auto create the batch samples for you so you don't have to pass in batch sampler at all. Instead, you can just say sampler and it will automatically wrap that in the batch sampler for you. That does exactly the same thing. And in fact, because it's so common to create a random sampler or a sequential sampler for a data set, you don't have to do that manually. You can just pass in shuffle equals true or shuffle equals false to the data loader. And that does again, exactly the same thing. There it is. \n",
        "\n",
        "Now, something that is very interesting is that when you think about it, the batch sampler and the collation function are things which are taking the result of the sampler looping through them and then collating them together. But what we could do is actually because our datasets know how to grab multiple indices at once, we can actually just use the batch sampler as a sampler. We don't actually have to look through them and collate them because they're basically instantly collated. They come pre collated. So this is a trick which actually huggingface stuff can use as I won't be saying it again. So this is an important thing to understand is how come we can pass a batch sample to the sampler. What's it doing?\n",
        "\n",
        " And so rather than trying to look through the PyTorch code, I suggest going back to our non multi processing pure Python code to see exactly how that would work, because it's a really nifty trick for things that you can grab multiple things from at once and it can save a whole lot of time. It can make your code a lot faster. Okay, so now that we've got all that nicely implemented, \n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "5fPquM0ni3u6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "EuTWYKuSmMJQ"
      },
      "outputs": [],
      "source": [
        "import torch.multiprocessing as mp\n",
        "from fastcore.basics import store_attr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "banJXNDlmMJQ",
        "outputId": "534a4508-f012-47d6-c049-c9cda44b43e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "train_ds[[3,6,8,1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ct4r2m4jmMJR",
        "outputId": "2c55749e-2747-4bb7-c847-f141abe36aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "train_ds.__getitem__([3,6,8,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "hDnQk4MamMJR",
        "outputId": "c31518f5-ed52-4b64-f388-7457e5ab0f82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n",
            "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n"
          ]
        }
      ],
      "source": [
        "for o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "YB-eLlw6mMJR"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n",
        "    def __iter__(self):\n",
        "        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "NP6X24HymMJR"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\n",
        "it = iter(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "E9vmF0dLmMJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0f005d-a495-41cd-eaae-5b53cbf49321"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([50, 784]), torch.Size([50]))"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "xb,yb = next(it)\n",
        "xb.shape,yb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXv1Sl70mMJR"
      },
      "source": [
        "### PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "L0LfwX3dmMJR"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "udbSulEmmMJR"
      },
      "outputs": [],
      "source": [
        "train_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\n",
        "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "pPDNrwsImMJR"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ItyVraT-mMJR",
        "outputId": "8cbf63e3-c417-4ac9-e0c5-1412f59b0e75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10, 0.06\n",
            "0.10, 0.04\n",
            "0.27, 0.06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.04, grad_fn=<NllLossBackward0>), tensor(0.98))"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdGmdBYemMJS"
      },
      "source": [
        "PyTorch can auto-generate the BatchSampler for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "eqaUuoxjmMJS"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0NOEtiSmMJS"
      },
      "source": [
        "PyTorch can also generate the Sequential/RandomSamplers too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "3BHKovpnmMJS"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
        "valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "A7xxongCmMJS",
        "outputId": "44e38b96-b346-47d2-feaf-1da219692926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21, 0.14\n",
            "0.15, 0.16\n",
            "0.05, 0.10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.06, grad_fn=<NllLossBackward0>), tensor(0.98))"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2MylC1KmMJS"
      },
      "source": [
        "Our dataset actually already knows how to sample a batch of indices all at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "r_uXg6zrmMJS",
        "outputId": "5fa470c7-5fe7-4925-fa4c-589cfef5027d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([9, 1, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "train_ds[[4,6,7]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvg6tX_NmMJS"
      },
      "source": [
        "...that means that we can actually skip the batch_sampler and collate_fn entirely:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "WsdJbg-MmMJS"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, sampler=train_samp)\n",
        "valid_dl = DataLoader(valid_ds, sampler=valid_samp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "qiRhdHVJmMJS",
        "outputId": "6091c2ce-4c67-4a7f-d25a-c3beecbcf459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "xb,yb = next(iter(train_dl))\n",
        "xb.shape,yb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbIWnvlAmMJT"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we should now add a validation set and there's not really too much to talk here. We'll just take our fit function. And this is exactly the same code that we had before. And then we're just going to add something which goes through the validation set and gets the predictions and sums up the losses and accuracies and from time to time prints out the loss and accuracy. And so get deals we will implement by using the PyTorch data loader now. And so now our whole process will be get deals passing in the training and validation data set that is set for our validation data loader. \n",
        "\n",
        "???I'm doubling the batch size because\n",
        "\n",
        " it doesn't have to do back propagation,???\n",
        "\n",
        "\n",
        "\n",
        "so it should use about half as much memory. So I can use a bigger batch size, get our model and then call this fit. And now it's printing out the loss and accuracy on the validation set. So finally we actually know how we're doing, which is that we're getting 97% accuracy on the validation set, and that's on the whole thing, not just on the last batch. So that's cool. We've now implemented a proper working, sensible training loop. It's still, you know, a bit more code than I would like, but it's not bad. And every line of code in there and every line of code it's calling, it's all stuff that we have built ourselves, reimplemented ourselves. So we know what's going on. And that means it's going to be much easier for us to create anything we can think of. We don't have to rely on other people's code, so hopefully you're as excited about that as I am because it really opens up a whole world for us. "
      ],
      "metadata": {
        "id": "lrXbcMv1nBpp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCHuMvdbmMJT"
      },
      "source": [
        "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
        "\n",
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "xA6ERKF5mMJT"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_dl:\n",
        "            loss = loss_func(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc,count = 0.,0.,0\n",
        "            for xb,yb in valid_dl:\n",
        "                pred = model(xb)\n",
        "                n = len(xb)\n",
        "                count += n\n",
        "                tot_loss += loss_func(pred,yb).item()*n\n",
        "                tot_acc  += accuracy (pred,yb).item()*n\n",
        "        print(epoch, tot_loss/count, tot_acc/count)\n",
        "    return tot_loss/count, tot_acc/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "08asJ0semMJT"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zbIKvbBmMJT"
      },
      "source": [
        "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "ECrrfJ53mMJT"
      },
      "outputs": [],
      "source": [
        "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "UUNcjiyOmMJT",
        "outputId": "be5c9c8e-ba61-4f91-a9b4-10dc56858f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.14236383258365096 0.958100004196167\n",
            "1 0.12564025239087642 0.9632000041007995\n",
            "2 0.13069150418043138 0.9645000052452087\n",
            "3 0.10988456704188138 0.9670000064373017\n",
            "4 0.11636368061415851 0.9678000068664551\n",
            "CPU times: user 6.57 s, sys: 25.2 ms, total: 6.59 s\n",
            "Wall time: 6.66 s\n"
          ]
        }
      ],
      "source": [
        "%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}