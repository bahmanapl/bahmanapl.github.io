[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bahman Sadeghi",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\nMar 20, 2023\n\n\nWriting Stable Diffusion from Scratch 4\n\n\n51 min\n\n\n\n\n\nMar 19, 2023\n\n\nWriting Stable Diffusion from Scratch 3\n\n\n20 min\n\n\n\n\n\nMar 15, 2023\n\n\nWriting Stable Diffusion from Scratch 2\n\n\n41 min\n\n\n\n\n\nMar 6, 2023\n\n\nWriting stable diffusion from scratch\n\n\n37 min\n\n\n\n\nFeb 27, 2023\n\n\nWelcome To My Blog\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bahman Sadeghi",
    "section": "",
    "text": "Meet Bahman, a software developer passionate for exploring new cultures, languages and foods. With experience living in Asia, North America and Europe, He loves traveling and immersing himself in different ways of life. He considers North California his home. In his free time, you can find him delving into novels, watching captivating TV shows, and expanding his knowledge of deep learning. As a minimalist and follower of Stoic principles, Bahman approaches his personal and professional life with intention and purpose.This bio was written by the help of ChatGPT."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/mynotebook/index.html#matrix-multiplication-from-foundations",
    "href": "posts/mynotebook/index.html#matrix-multiplication-from-foundations",
    "title": "Bahman Sadeghi",
    "section": "\nMatrix multiplication from foundations\n",
    "text": "Matrix multiplication from foundations\n\n\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/mynotebook/index.html#get-data",
    "href": "posts/mynotebook/index.html#get-data",
    "title": "my notebook post",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Feb 22 13:37 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/mynotebook/index.html#matrix-and-tensor",
    "href": "posts/mynotebook/index.html#matrix-and-tensor",
    "title": "my notebook post",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use pytorch.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/mynotebook/index.html#random-numbers",
    "href": "posts/mynotebook/index.html#random-numbers",
    "title": "my notebook post",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn child: tensor([0.5702])In parent: tensor([0.5702])\n\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n21.4 ms ± 5.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\nThe slowest run took 4.18 times longer than the fastest. This could mean that an intermediate result is being cached.\n167 µs ± 116 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/mynotebook/index.html#matrix-multiplication",
    "href": "posts/mynotebook/index.html#matrix-multiplication",
    "title": "my notebook post",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nOkay, so let’s move on with our from the foundations now. And so we were working on trying to at least get the start of a forward pass of a linear model or a simple multi-layer perceptron for MNIST going. And we had successfully created a basic tensor. We’ve got some random numbers going. So what we now need to do is we now need to be able to multiply these things together, matrix multiplication. So matrix multiplication to remind you in this case. So we’re doing MNIST, right? So we’ve got about we’re going to use a subset, let’s see. Yeah, Okay. So we’re going to create a matrix called m1, which is just the first five digits, So m1 will be the first five digits. So five rows and. Well, dot, dot dot dot dot dot. And then 780. What is it again. because it’s 28 by 28 pixels and reflect that out. So this is our first matrix and matrix multiplication, and then we’re going to multiply that by some some weights. So the weights are going to be 784 by 10 random numbers. So for every one of thes 784 pixels, each one is going to have a weight. So 784 down here, so 94 by ten. So this first column, for example, is going to tell us all the weights in order to figure out if something’s a zero. And the second column will have all the weights in deciding of the probability of something. So one, so forth, assuming we just doing a linear model. And so then we’re going to multiply these two matrices together. So when we multiply matrices together, we take row one of matrix one and we take column one of matrix two and we take each one in turn. So we take this one and we take this one, we multiply them together and then we take this one and this one and we multiply them together. And we do that for every element wise pair, and then we add them all up and that would give us the value for the very first cell that would go in here. That’s what matrix multiplication is. Okay, so let’s go ahead and create our random numbers for the weights since we’re allowed to use random number generator now and for the bias, but just use a bunch of zeros to start with. So the bias is just what we’re going to add to each one. And so for our matrix multiplication, we’re going to be doing a little mini batch. I’m going to be doing five rows of, as we discussed, five rows of so five, five images flattened out and then multiplied by this weights matrix.\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\nSo here are the shapes and one is five by seven, eight four as we saw, and m2 is seven, eight, four by ten. Okay, so keep those in mind. So here’s a handy thing. And one touch shape contains two numbers and I want to pull them out. I want to call the I’m going to think of that as I’m going to actually think of this as like a and b rather than I wanted them to. So this is like a and b, so the number of rows in a and the number of columns in b, if I say equals and one shape that will put five in ar and 784 in ac, So I’ll notice I do this a lot, this restructuring, we talked about it last week too so can do the same for m2 dot shape, put that into b rows and b columns. And so now if I write out ar,ac and br , br , you can again see the same things from the sizes. So that’s a good way to kind of give us the stuff we have to look through. So here’s our results.\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nget matrix dimsions and put them in variables to make it readable for future looping.\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\nSo here’s our results. So our resultant tensor, well, we’re multiplying, we’re multiplying together all of these seven, eight, four things and adding them up. So the resultant tensor is going to be five by ten. And then each thing in here is the result of multiplying and adding So the result here is going to start with zeros and there is this is the result and it’s going to contain ar rows, five rows and bc columns, ten columns, five coma ten. Okay, so now we have to fill that in. And so to do a matrix multiplication, so we have to first we have to go through each row one at a time and here we have that go through each row one at a time and then go through each column one at a time. And then we have to go through each pair in that row column one at a time. So it’s going to be a loop in a loop in a So here’s quick over each row, and here we’re going to loop over each column and then here we’re going to loop so each column of c, and then here we’re going to leap over each column of a, which is going to be the same as the number of rows of b, which we can see here. I say ac or br they are seven, eight, four. They’re the same. So it wouldn’t matter whether we day, ac or br, so then our result for that row and that column, we have to add onto it the product of i,k in the first matrix by k,j in the second matrix. So k, it’s going up through those seven, eight, four. And so we’re going to go across the columns and down so across the rows and down the columns, it’s going to go across the row where it goes down this column. So here is the world’s most naive, slow, uninteresting matrix multiplication. And if we run it, okay, it’s done something we have successfully hopefully successfully multiplied the matrices m1 and m2.It’s a little hard to read this, I find because because punch cards used to be 80 columns wide. We still assume screens 80 columns wide. Everything defaults to 80 wide, which is ridiculous, but you can easily change it. So if you say sit print options, you can choose your own line width. Oh, you can say it’s five by ten. We did it before. So if we change the line width, okay, that’s much easier to rate. Now we can see here the five rows and here are the ten columns for that matrix multiplication. I tend to always put this at the top of my notebooks and you can do the same thing for numpy as well. So what I’d like to do this is really important is when I’m working on code, particularly numeric code, I like to do it all step by step and Jupiter. And then what I do is once I’ve got it working is a copy all the cells that have implemented that and I paste them and then I select them all and I hit shift+m to merge. Get rid of anything that prints out stuff I don’t need. And then I put a header on the top, give it a function name, and then I select the whole lot and I hit control or f right square bracket and I’ve turned it into a function, but I still keep the stuff above it. So I can see all the step by step stuff for learning about it later. And so that’s what I’ve done here to create this function. And so this function does exactly the same things we just did, and we can see how long it takes to run by using %time. And it took about half a second, which gosh, that’s a long time to generate such a small matrix. This is just to do five MNIST digits. So that’s not going to be great. We’re going to have to speed that up. I’m actually quite surprised at how slow that is because there’s only 39,200. So, you know, if you look at the how, we’ve got a loop within a loop within a loop, it’s doing 39,200 of these. So Python. Yeah, Python. When you’re just doing python, it is it is slow. So we can’t we can’t do that. That’s why we can’t just write Python.\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\nGo through each row one at a time (5), then each column one at a time (10) and then go through each pair(784). go accross the rows , down the column multiply and add. t1[i,j] += m1[i,k] * m2[k,j]\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nDefault is 80 columns wide because of punch cards and we still do that. (Talking about legacy and network effect , haha)\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\nThis is only to show data more readable.\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\nDo this on the top of the notebook and make it easier.\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\nFor numerical programming , Jeremy recommend doing stuff line by line , check the results and dimensions and then when it works , copy all the cells and paste them after those cell and select them all and hit shift+M to merge cells get ride of everything that prints out stuff you dont need put a header on the top (def ….), select the rest of the code and hit control + ] now you have the function. Keep the same none function code above to remember what did you do and how you get there.\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n%time _=matmul(m1, m2)\n\nCPU times: user 1.13 s, sys: 10.7 ms, total: 1.14 s\nWall time: 2.1 s\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/mynotebook/index.html#numba",
    "href": "posts/mynotebook/index.html#numba",
    "title": "my notebook post",
    "section": "Numba",
    "text": "Numba\nBut there is something that kind of lets this write Python we could instead use Numba.Numba is a system that takes python and turns it into basically into machine code and it’s amazingly easy to do. You can basically take a function and write and @ngit on top. And what it’s going to do is it’s going to look the first time you call this function, it’s going to compile it down to machine code and will run much more quickly. So what I’ve done here is I’ve taken the innermost loop. So just looping through and adding up all these. So I start at zero, go through and add up all those just two vectors and return it, which is called a dot product. And linear algebra, so call it dot and so Numba only works with numpy, it doesn’t work with PyTorch. So we’re just going to use arrays instead of tensers for a moment. Now have a look at this. If I try to do a dot product of one, two, three and two, three, four, it’s pretty easy to do. It took a fifth of a second, which sounds terrible, but the reason it took a fifth of a second is because that’s actually how long it took to compile this and run it. Now that it’s compiled the second time, it just has to call it it’s now 21 microseconds. And so that’s actually very fast. So with Numba we can basically make Python run at C speed. So now the important thing to recognize is if I replace this loop in Python with a called a dot which is running in machine code, then we now have one two loops running in python not three. So our 448 MS, let’s make sure if I run it, run that matmul that should be close to my t1 one. t1 is what we got before.\nAnd so when I’m refactoring or performance improving or whatever, I always like to put every step in the notebook and then test. So this test close comes from fastcore.test and it just checks. The two things are very similar. They might not be exactly the same because of floating point differences, which is fine. Okay, our matmul is working correctly, or at least it’s doing the same thing it did before. So if we now run it, it’s taking 268 micro second, versus 448 milliseconds. So it’s taking, you know, about 2000 times faster just by changing the one in my loop. So really all we’ve done is we’ve had @ngit to make it 2000 times faster, so Numba is well worth knowing about. I can make your Python code very, very fast. Okay, let’s keep making it faster. So we’re going to use stuff again, which kind of goes back APL. And a lot of people say that learning APL is the thing that’s taught them more about programing than anything else. So it’s probably worth considering learning APL And let’s just look at these various things. You got a is ten six minus four. So remember at APL, we don’t say equals, equals actually means equals. Funny enough we to say set two, we use this arrow and it’s, this is a list of ten, six, four and then b is 287. Okay. And we’re going to add them up a plus b, So what’s going on here? So it’s really important that you can think of a symbol like a as representing a tensor or an array. APL calls them arrays, pytorch call them tensors, Numpy calls them arrays. They’re the same thing. So this is a single that contains a bunch of numbers. This is a single thing that contains a bunch of numbers. This is an operation that applies to arrays or tensors. Now what it does is it works what’s called elsment-wise. It takes each pair ten and two, and that’s them together. Each pair six and eight, add them together. This is element wise addition and Fred is asking in the chat, how do you put in these symbols? If you just mouse over any of them, it will show you how to write it and the one you want is the one at the very bottom, which is the one where it says prefix. Now the prefix is the backtick character. So here it’s saying prefix hyphen gives us times. So we’ve had hyphen. So I’ve of a backtick dash b is a times b for example. So yeah, they all have shortcut keys which you learn pretty quickly. I find, and there’s a fairly consistent kind of system for those shortcut keys too. All right, So we can do the same thing in PyTorch.\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 562 ms, sys: 53 ms, total: 615 ms\nWall time: 1.75 s\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 37 µs, sys: 0 ns, total: 37 µs\nWall time: 41 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\nThis is the test.\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n2000 time faster. We change inner most loop.\n\n%timeit -n 50 matmul(m1a,m2a)\n\n1.24 ms ± 419 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nSo we can do the same thing in PyTorch. It’s a little bit more verbose. PyTorch, which is one reason I often like to do my mathematical fiddling around in APL. I can often do it with less boilerplate, which means I can spend more time thinking, you know, I can see everything on the screen at once. I don’t have to spend as much time trying to like ignore the tensor around bracket square bracket dot com, blah blah. It’s all cognitive load, which I’d rather ignore. But anyway, it does the same thing so I can say a plus b work exactly like APL. So here’s an interesting example. I can go a less than (a < b).float().mean(). So let’s try that one over here less than b. So this is a really important idea, which I think was invented by Ken Iverson, the APL guy, which is that true and false represented zero and one. And because they’re represented by zero and one, we can do things to them. We can add them up and subtract and so forth. That’s a really important idea. So in this case, I want to take the main of them, and I’m going to tell you something amazing, which is that in APL there is no function called mean. Why not? That’s because we can write the mean function, which so that’s four letters mean and we can write the mean function from scratch with four characters. I’ll show you. Here’s the whole mean function we’re going to create a function called mean, and the mean is equal to the sum of a list divided by the of the list. So this here is some divided by count. And so I have now to find a new function called mean, which calculates the mean, mean of a is less than b, there we go. And so, you know in practice, I’m not sure why people would even bother defining a function called mean because it’s just as easy to actually write it’s implementation in APL, in numpy or whatever a python. It’s going to take a lot more than four letters to implement mean. So anyway, you know, it’s a math notation and so being a math notation we can do a lot with little, which I find out folks, I can say everything going on at once anyway. Okay, so that’s how we do the same thing in pytouch. And again, you can say that the less than in both cases, operating element wise. Okay, So a is less than b is saying ten is less than two six is less than eight four is less than seven and gives us back each of those trues and faleses as zeros and onces and according to our YouTube chat, had just exploded as it should. This is why APL is. Yeah life changing."
  },
  {
    "objectID": "posts/mynotebook/index.html#elementwise-ops",
    "href": "posts/mynotebook/index.html#elementwise-ops",
    "title": "my notebook post",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\nElementwise addition\n\na + b\n\ntensor([12., 14.,  3.])\n\n\nCheck lecture for awesome implementation of mean.\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\nRank two tensor , aka Matrix.\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nOkay, let’s now go up to higher rank. So this here is a rank one tensor. So a rank one tensor means it’s a list of things, it’s a vector, it’s where else a rank two tensor. It’s like a list of lists. They all have to be the same length lists, or it’s like a rectangular bunch of numbers. And we call it in math, we call it a matrix. So this is how we can create a tensor containing one, two, three, four, five, six, 789. And you can see also, like, what I like to do is I want to print out the thing I just created after I created it. Two ways to do it. You can say put an enter and then write m and that’s going to do that. Or if you want to put it all in the same line, that works too. You just use a semicolon. Neither one is better than the other. They’re just different. So we could do the same thing in APL. Of course in APL it’s going to be much easier. So we’re going to define a matrix called m which is going to be a three by 3 by 3 tensor containing the numbers from 1 to 9. Okay. And there we go. That’s done it in APL, a three by three tensor containing the numbers from 1 to 9. A lot of these ideas from APL you’ll find have made their way into other programing languages. For example, if you use GO you might recognize this. This is the iota character and go uses the word iota to spell it out in a somewhat similar way. A lot of these ideas from APL have found themselves into math notation and in other languages. It’s been around since the late fifties. Okay, so here’s a bit of fun. We’re going to learn about a new thing that looks kind of crazy code for Frobenius Norm and we’ll use that from time to time as we’re doing modeling. And here’s a definition of a four Frobenius norm. It’s the sum over all of the rows and columns of a matrix, and we’re going to take each one and square it. They’re going to add them up and they’re going to take the square root.\nAnd so to implement that in pytouch is as simple as (m*m).sum().sqrt(). So this looks like a pretty complicated thing when you kind of look at it. At first it looks like a lot of squiggly business or if you said this thing here you might be like, what on earth is that? Well, now you know, it’s just a square, some square root. So again, we could do the same thing in APL. So let’s do so in APL. We want the okay, so we got a case called S.F. Now it’s interesting, Apple does this a little bit differently. So dot some by default in PyTorch sums over everything. And if you want to sum over just one dimension, you have to pass in a dimension keyword for very good reasons. APL is the opposite. It just comes across rows or just down columns. So actually we have to say sum up the flattened out version of the Matrix and say flattened out. He is comma. So his sum up the flattened out version of the Matrix. Okay, so that’s our S.F.. Oh, Oh, sorry. And the Matrix is meant to be m times m There you go. So that’s the same thing. Sum up the flattened out and by a matrix and another interesting thing about APL is it always is read right to left. There’s no such thing as operator precedence, which makes life a lot easier. Okay, then we take the square root of that. There isn’t a square root function, so we have to do to the power of 0.5 and there we go. Same thing. All right, You got the idea. Yes. A very interesting question here from Marabou or other bars for norm or absolute value. And I like answer, which is the norm, is the same as the absolute value for scalar. So in this case, you can think of it as absolute value and it’s kind of not needed because it’s being squared anyway. But yes, in this case the norm. Well, in every case for a scale, the norm is the absolute value, which is kind of a cute discovery when you realize it. So thank you for pointing that out. See the. All right. So this is just fiddling around a little bit to kind of get a sense of how these things work. So really importantly, you can index into a matrix and you’ll say rows first and then columns."
  },
  {
    "objectID": "posts/mynotebook/index.html#matmul-with-broadcasting",
    "href": "posts/mynotebook/index.html#matmul-with-broadcasting",
    "title": "my notebook post",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\nSo let’s just grab a single digit. So here’s the first digit. So its shape is it’s a 784 long vector. Okay. And remember that our weight matrix is 784 by ten. Okay. So if we say digit colon coma None dot shape, then that is a 784 by one row matrix. Okay. So there’s our matrix. And so if we then take that 784 by one and expandas m2, it’s going to be the same shape as our weight matrix. So it’s copied our image data for that digit across all of the ten vectors, representing the ten kind of linear projections we’re doing for our linear model. And so that means that we can take the digit colon comma a None so 784 by one and multiply it by the weights. And so that’s going to get us back 784 by 10 so what it’s doing, remember, is it’s basically looping through each of these 10, 784 long vectors. And for each one of them it’s multiplying it by this digit. So that’s exactly what we want to do in our matrix multiplication. So originally we had when I originally most recently I should say, we had this dot product where we were actually looping over j, which was the columns of b, So we don’t have to do that anymore because we can do it all at once by doing exactly what we just did so we can take the ith and all the columns and add a access to the end. And then just like we did here, multiply it by b and then .sum(). And so that is again exactly the same thing.\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n539 µs ± 221 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nThat is another metrics, multiplication, doing it using broadcasting. Now this is like a tricky to get your head around. And so if you haven’t done this kind of broadcasting before, it’s a really good time to pause the video and look carefully at each of these four cells before and understand what did I do there, Why did I do it? What am I showing you? And then experiment with trying to and so remember that we started with m1 zero, right? So just like we have here, a[i , so that’s why we’ve got, i comma colon comma None because this digit is actually m1 zero. So this is like m1 zero colon None. So this line is doing exactly the same thing as this here plus the sum. So let’s check if this matmul is the same as it used to be, yet still working and the speed of it. Okay, not bad. So 137 microseconds. So we’ve now gone from a time from 500 milliseconds to about point 1 milliseconds. Funnily enough, my MacBook Air is an m2, whereas this Mac mini is an m1 that’s a little bit slower. So my error it was a bit faster than 0.1 milliseconds. So overall we’ve got about a 5000 times speed improvement. So that is pretty exciting. And since it’s so fast now, there’s no need to use a mini batch anymore. If you remember, we used a mini batch of five images, but now we can actually use the whole dataset so fast. So now we can do the whole data set. There it is. We’ve now got 15,000 by ten, which is what we want. And so it’s taking us only 656 milliseconds now to do the whole dataset. So this is actually getting to a point now where we could start to create and train some simple models in a reasonable enough time. So that’s good news. All right. I think that’s probably a good time to take a break. We don’t have too much more of this to go, but I don’t want to keep you guys up too late. So hopefully you learned something interesting about broadcasting today. I cannot overemphasize how widely useful is in all deep learning in machine learning code. It comes up all the time. It’s basically our number one most critical kind of foundational operation. So, yeah, take your time practicing it. And also good luck with your diffusion homework from the first half of the lesson. Thanks for joining us and I’ll see you next time.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.54 s, sys: 2.08 ms, total: 1.54 s\nWall time: 1.76 s"
  },
  {
    "objectID": "posts/mynotebook/index.html",
    "href": "posts/mynotebook/index.html",
    "title": "my notebook post",
    "section": "",
    "text": "from pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/about/index.html",
    "href": "posts/about/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "1- Currently I am doing fast AI course part 2. So I do write about it twice a week. One post every mondays one post every Tuesdays.\n2- I am trying something I call 52 projects in 52 weeks. I will write about that too (Without schedule).\n3- Other stuff that not technical , I will write about it , stuff that interest me. (Without schedule)."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html",
    "href": "posts/Writing stable diffusion from scratch/index.html",
    "title": "Writing stable diffusion from scratch",
    "section": "",
    "text": "All credit goes to www.fast.ai. All mistakes are mine. In the foundation series, I only write about part of the lecture that is related to writing stable difussion from scratch.Jeremy also talked about the big picture of the stable diffusion model. I will write about those in the big picture series of my blog posts.Almost all of the stuff in the subtitle from lectures."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#matrix-multiplication-from-foundations",
    "href": "posts/Writing stable diffusion from scratch/index.html#matrix-multiplication-from-foundations",
    "title": "Writing stable diffusion from scratch",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\nIt’s going to require some serious tenacity and a certain amount of patience,but I think you’re going to learn a lot.A lot of folks I’ve spoken to have said thatprevious iterations of this of the course is like the best course they’ve ever done, and this one’s going to be dramatically better than any previous version we’ve done of this. So hopefully you’ll find that the the hard work and patience pays off. So the goal is to get to stable diffusion from the foundations, which means we have to define what are the foundations. So I have decided to define them as follows : We’re allowed to use Python, we’re allowed to use the Python standard library. So that’s all the stuff that comes with Python by default we’re allowed to use matplotlib because I couldn’t be bothered creating my own plotting library and are allowed to use Jupyter notebooks and nbdev, which is something that creates modules from notebooks. So basically what we’re going to try to do is to yeah, rebuild everything starting from this foundation. Now to be clear, what we are allowed to use are the libraries Once we have re-implemented them correctly. And so if we if we re-implement something from NumPy or from PyTorch or whatever, we’re then allowed to use the Numpy or PyTorch or whatever version, sometimes we’ll be creating things that haven’t been created before, and that’s then going to be becoming our own library and we’re going to be calling that Library Mini AI. So we’re going to be building our own little framework as we go.\nThe foundations we’ll assume throughout this course are:\n\nPython\nmatplotlib\nThe Python standard library\nJupyter notebooks and nbdev\n\nSo, for example, here are some inputs and these inputs all come from the Python standard library, except for these two. Now, to be clear, one challenge we have is that the models we use in stable diffusion, what trained on millions of dollars worth of equipment per month, which we don’t have the time or money. So another we’re going to do is we’re going to create smaller, identical, but smaller versions of them. And so once we’ve got them working, well, then be allowed to use the big Pre-Trained versions. So that’s the basic idea. So we’re going to have to end up with our own VIE our own UNIT, our own clip encoder and, so forth. To some degree. I am assuming that you’ve completed part one of the course to some degree I will cover everything, at least briefly. But if I cover something about deep learning too fast for you to know what’s going on and you get lost, go back and watch part one or go and, you know, Google for that term for stuff that we haven’t In part one, I will go over it very thoroughly and carefully. All right. So I’m going to assume that, you know, the basic idea that which is that we’re going to need to be doing some matrix multiplication.\nSo we’re going to try to take a deep dive into matrix multiplication today and we’re going to need some input data. And I quite like working with MNIST data, MNIST is hand-written digits. It’s a classic data set they are 28 by 28 pixel grayscale images and so we can download them from this URL. So we use the path libs path object a lot. It’s got part of Python and it basically takes a string and turns it into something that you can treat as a path. For example, you can use slash to mean this file inside this subdirectory. So this is how we create a path object path objects have for example a make directory mkdir method. So I like to get everything set up, but I want to be able to rerun this so lots of times and not have it like give me errors if I run it more than once, if I read a second time, it still works. And in that case that’s because I put this exist_ok = True. How did I know that? I can say because otherwise would try to make the directory. It would already exist in a given error. How do I know what parameters I can pass to make? I just press shift tab. And so when I hit shift tab, it tells me what options there. If I press it a few times, it’ll actually puppet down at the bottom of the screen. Just to remind me I can press escape to get rid of it. Or you can just or else you can just hit tab inside and it’ll list all the things you can type as parametors. As you can see. All right. So we need to grab this URL. And so Python comes with something for doing that, which is the URL lib library that’s part of Python that has something you I will retrieve and something which I’m always a bit surprised is not widely used as people are reading the Python documentation. So you should do that a lot. So if I click on that, here is the documentation for urlretrive, I will retrieve and so I can find exactly what it can take and I can learn about exactly what it does.So I yeah, I read the documentation from the Python docs for every single method I use and I look at every single option that it takes and then I practice with it and to practice with it, I practice inside Jupyter. So if I want this import on its own, I can hit control shift hyphen and it’s going to split it into two cells and then I’ll hit ALT + Enter or Option Enter so I can create something underneath so I can type urlretrieve shift tab. And so there it all is if I’m like way down somewhere in in the notebook and I have no idea where urlretrieve comes from, I can just hit shift enter and it actually tells me exactly where it comes from. And if I want to know more about it, I can just hit question mark shift enter and it’s going to give me documentation and most of all, second question mark and it gives me the full source code and you say it’s not a lot. You know, reading the source code of Python standard library stuff is often quite revealing and you can see exactly how they do it.\nWe use MNIST (hand-written digits) as our data. It is a classic 28 by 28 pixcel data set. http://yann.lecun.com/exdb/mnist/ We can download them from GitHub URL.\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#get-data",
    "href": "posts/Writing stable diffusion from scratch/index.html#get-data",
    "title": "Writing stable diffusion from scratch",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nThat’s a great way to learn more about more about this. So in this case, I’m just going to use a very simple functionality, which is I’m going to say the URL to retrieve and the file name to save it as and again, I’m made it so I can run this multiple times. So it’s only going to do the URL retrieve if the path doesn’t exist. If already downloaded it, I don’t want it downloaded again. So I run that cell and notice that I can put exclamation mark followed by a line of bash. And it actually runs this using bash. If you’re using windows, this this won’t work. And I would very, very strongly if you’re using Windows use WSL and if you used WSL, all of these notebooks will work perfectly. So yeah, do that. All right. It on paperspace or LambdaLabs or something like that, CoLab, etc..\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\nSo I run that cell and notice that I can put exclamation mark followed by a line of bash.\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar  6 09:39 mnist.pkl.gz\n\n\nOkay, so this is a zgip file. So thankfully, Python comes with a gzip module. Python comes with quite a lot actually. And so we can open a gzip file gzip.open and we can pass in the path and we say we’re going to read it as binary as opposed to text. Okay. So this is called a context manager. It’s it’s a width clause. And what it’s going to do is it’s going to open up this gzip file. The GC object will be called F and that it runs everything inside the the block. And when it’s done it will close the file. So with blocks can do all kinds of different things.\nBut in general, with blocks that involve files, it will going to close the file automatically for you. So We can now do that. And so you can see it’s opened up the gzip file and the gzip file contains what’s called pickle objects, pickled objects, It’s basically Python objects that are being saved to disk. It’s the main way that people in pure Python save stuff and it’s part of the standard library. So this is how we load in from that file. Now the file contains a couple of tuples, so when you put a tuple on the left hand side of an equal sign, it’s quite neat. It allows us to put the first couple into two variables called x_train, y_train and the second into x_valid, y_valid.And we’ve added this trick here where you put stuff like this on the left is called D structuring and it’s a super handy way to make your code kind of clear and concise. And lots of languages support that including Python.\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\nOkay, so we’ve now got some data and so we can have a look at it. Now it’s a bit tricky because we’re not allowed to use Numpy according to our rules, but unfortunately this actually comes as Numpy, so I’ve turned it into a list. All right. So I’ve taken the first image and I’ve turned it into a list. And so we can look at a few examples of some values in that list. And here they are. So it looks like the numbers between zero and one and this is what I do, you know, when I learn about a new dataset. So when I started writing this notebook, what you see here other than the pros here is, is what I actually did when I was working with this data. This I wanted to know what it was. So I just grab a little bit of it and look at it. So I kind of got a sense now of what it is now. Interestingly, this image is 784 long list.People already have people freaking out in the comments. No numpy. Yeah, the numpy. Do you say numpy then. NumPy. Why 784 ?What is that. Well that’s because he’s a 28 by 28 images. So it’s just a flat list here of 784 long. So do I turn this 784 long thing into 28 by 28. So I want to take a list of 28 lists of 28, basically because we don’t have matrices. So how do we do that? And so we’re going to be learning a lot of cool stuff in Python here.\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\nSorry, I got to start laughing at all the stuff in that chat. The people are quite reasonably freaking out. That’s okay. We’ll get there, I promise. I hope. Otherwise I’ll embarrass myself. All right, So how do I convert a 784 long list into 28 lists? I’m going to use something called chunks. And first of all, I’ll show you what this thing does and then I show you how it works. So vals is currently a list of ten things. So I take vals and I pass it to chunks with five, it creates two lists of five is list number one of five elements. And here’s list number two of five elements. Hopefully you can say it’s doing its chunk defying this list, and this is the length of each chunk. Now, how did you do that? The way I did it is using a very, very useful thing in Python that far too many people don’t know about, just called yield. And what it does is you can see here what a loop it’s going to go through from zero up to the length of my list and it’s going to jump by five at a time. That’s going to go, in this case, 0 to 5. And then it’s going to think of this as being like return for now, it’s going to return the list from zero up to five. So it returns the first bit of the list. But yield doesn’t just return. It kind of like returns a bit and then it continues and it returns a bit more. And so specifically, what yield does is it creates an iterator and iterator is an iterator is basically something you can actually just use it that you can call next on a bunch of times.So what is iterator? Well, iter it is something that I can basically I can call next on and next basically says yield the next thing. So this should yield vals[0,5]. There it is. It did write this vals[0,5]. Now, if I run that again, it’s going to give me a different answer because it’s now up to the second part of this loop. Now it returns the last five. Okay. So this is what a iterator does. Now, if you pass an iterator to Python’s List, it runs through the entire letter, iterater it until it’s finished and creates a list of the results. And what is finished? Looks like this is what finish looks like. If you call next and get stop iteration, that means you’ve run out. And that makes sense, right? Because my loop, there’s nothing left in it. So all of that is to say we now have a way of taking a list and chunkifying it. So what if I now take my full image? Image number one chunkify it into chunks of 28 long and turn that into a list and plot it that we have successfully created an image. So that’s good. Now we are done. But there are other ways to create this iterator. And because iterate is and generators which are closely related are so important. I wanted to show you more about how to do them in Python. It’s one of these things that if you understand this, you will often find that you can throw away huge pieces of enterprise software and basically replace it with an iterator that lets you stream things one bit at a time. It doesn’t store it all in memory. It’s this really powerful thing that once I often find, once I show it to people, they suddenly go like, Oh wow, I know we’ve been using all this third party software and we could have just created a python iterator. Python comes with a whole standard library module called itertools just to make it easier to work with iterators.\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\nIn order to convert 784 to 28*28 matrix, we use yeild and chunk function to do this. chunk going through the loop and go through whole list but jump 28 at a time. yeild return and continue. It create a iterator.\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nI’ll show you one example or something from it at all, such as islice. So let’s grab our values again. These ten values. Well, that was a mistake. I should not have called this iter. Let’s just do that again. Okay, so let’s take these ten values and we can any list and turn it into an iterator by passing it to iter, which I should call it that. So I don’t override this python. That’s not a keyword. But this thing, I don’t want to override. So this is now basically something that I can call. Actually, let’s do this. I’ll show you that. I can call next on it. So if I now go next.it you can see it’s giving me each item one at a time. Okay. So that’s what converting it into an iterator does. I slice convert it into a different kind of iterator? Let’s call this isislice iterator. I write a and so you can see here what it did was it jumped stop here. Yeah. So that’s what it being better so I should query create the iterator and then call next a few times. Sorry, this is what I meant to do. It’s now only returning the first five before it calls stop iteration before it raises stop iteration. So what I does is it grabs the first and things from an iterable, something that you can iterate. Why is that interesting? Because I can pass it to list. For example. Right? And now if I pass it to list again, this iterator has now grabbed the first five things. So it’s now up to thing number six. So if I call it again, it’s the next five things. And if I call it again, then there’s nothing left. And maybe you can see we’ve actually now got this defined, but we can do it with islice. And here’s how we can do it. It’s actually pretty tricky. iter in Python or you can pass it something like a list to create an iterator or you can pass it,this is a really important word, a callable. What’s a callable? A callable is generally speaking, it’s a function. It’s something that you can put parentheses after. Could even be a class, anything you can put parentheses after. You can just think of it for now as a function. So we’ve got a pass it a function and in the second form it’s going to be called until the function returns this value here, which in this case is empty list. And we just saw that islice will return empty list when it’s done. So this here is going to keep calling this function again and again and again. And we’ve seen exactly what happens because we caught it ourselves before. There it is. Until it gets an empty list. So if we do it with 28, then we’re going to get our image again. So we’ve now got two different ways of creating exactly the same thing.If you’ve never used iterate as before, now’s a good time to pause the video and play with them. Right? So for example, you can take this here, right? And if you’ve not seen Lambdas before, they’re exactly the same as functions, but you can define them in line. So let’s, let’s replace that with a function. Okay? So now I’ve turned it into a function and then you can experiment with it. So let’s create our iterator and call F on it.\nWell,F and you can say this the first 28 and each time I do it, I’m getting another 28. Now the first few rows are all empty. But finally, look, now I’ve got some values. Call it again. See how each time I’m getting something else. This calling it again and again. And that is the values in a iterator. So that gives you a sense of like how you can use Jupyter to experiment. So what you should do is as soon as you hit something in my code that doesn’t look familiar to you, I recommend pausing the video and experimenting with that in Jupyter. And for example, iter, Most people probably have not used it at all, and certainly very few people have use this to argument form so hit shift tab a few times and now you’ve got at the bottom to the description of what it is or find that more Python iter. Yeah go to the docs. Well that’s not the right but if the docs say API wow crazy that’s terrible let’s try searching here. Yeah okay iter that’s more like it. it so now you’ve got links so it’s like okay it returns an iterated object. What’s that. Well click on it find out that that’s really important to note is that stop exception that we saw so stop iteration exception we saw next already we can find out what iterable is. And here’s an example. And as you can see, it’s using exactly the same approach that we did. But here it’s being used to read from a file. This is really cool. Here’s how to read from a file 64 bytes at a time until you get nothing processing it right so that the docs the python are quite fantastic as long as you use them. If you don’t use them, they’re not very useful at all. And I say Safer in the comments : Our local Haskell programmer appreciating this Haskell illness in Python. So that’s good. It’s not quite Haskell, I’m afraid, but it’s the closest we’re going to come. All right, here we go for time. Pretty good.\n\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\nislice\nlets grab our 10 values and learn about islice from itertools module in python.\n\nfrom itertools import islice\n\nyou can call next in it and give you next 5 items. (5 in islice). So islice only return the first five (use next to understand this).\n\nit = iter(vals)\nisit = islice(it, 5)\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#matrix-and-tensor",
    "href": "posts/Writing stable diffusion from scratch/index.html#matrix-and-tensor",
    "title": "Writing stable diffusion from scratch",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\nOkay, so now that we’ve got image, which is a list of lists and each list is 25 long, we can index into it so we can say image 20. Well, let’s do it. Image 20. Okay. Is a list of 28 numbers and then we could index into that. Okay, so we can index into it. Now normally we don’t like to do that for matrices. We would normally rather write it like this : img[20,15] Okay, So that means we’re going to have to create our own class to make that work. So to create a class in Python, you’re write Class. And then you write the name of it and then you write some really weird things. The weird things you write have two underscore is a special word And then two underscore is these things with two underscores Each side are called dunder methods, and they’re all the special magically named methods which have particular meanings to Python, and you’re just going to let them. But they’re all documented in the Python object model. dunder init. No, it’s actually terrible search. We probably maybe need to look for object model. Then also absolutely terrible. All right. So maybe try Google Python and it object model. Yeah. Finally. Okay. So what’s your manually for. Oh it’s got data or not object model. And so this is basically where all the documentation is about absolutely everything and I can click done to edit and it tells you basically this is the thing that constructs objects. So any time you want to create a class that you want to, that you want a constructor that’s going to stores and stuff. So in this case it’s going to store image. You have to define dunder init. Python’s slightly weird in that every method you have to put self here for reasons we probably don’t really need to get into right now. And then any parameters. So we’re going to be creating image passing in the thing to store the x’s they’re going to be passing in the Xs. And so here we’re just going to store it inside the self. So once I’ve got this line of code, I’ve now got something that knows how to store stuff, the x’s inside itself. So now I want to be able to call square bracket 20 comma 15. So how do we do that? Well, basically part of the data model, this is a special thing called dunder getitem. And when you call square brackets on your object, that’s what Python uses and it’s going to pass across the [20,15]. Yeah that’s indices So we’re now but basically you’re going to return this so the self.x with the first index and the second index. So let’s create that matrix class and run that And you can now see m[20,15] is the same, quick note on, you know, ways in which my code is different to everybody else’s, which it is. It’s somewhat unusual to put definitions of methods on the same line as as the the signature like this. I do a quite a lot for one liners. As I mentioned before, I find it really helps me to be able to see all the code I’m working with on the screen at once. A lot of the world’s best program has actually had that approach as well. It seems to work quite well for some people that are extremely productive. It’s not common in Python, some people are quite against it. So if you’re at work and your colleagues don’t write Python this way, you probably shouldn’t either. But if you can get away with it, I think it works quite well anyway. Okay, so now that we’ve created something that lets us index into things like this, we’re allowed to use because we were allowed to use this one feature in PyTorch. Okay, so we can now do that. And so now to create a tensor, which is basically a lot like our matrix can now pass a list into tensor to get back that tensor version of that list. Or perhaps more interestingly, we could pass in a list of lists. Maybe this gives us a name.that needs to be a list of lists just like we had before. For our image. In fact, let’s do it for our image. Let’s just pass in our image. Yeah. Okay. And so now we should be able to say tens[20,15]. Okay, so we’ve successfully reinvented that. All right. So now we can convert all of our lists into tenses. There’s a convenient way to do this, which is to use the map function in the Python standard library. So shift tab map takes function, and then some iterables, in this case one iterable, and it’s going to apply this function to each of these four things and return those four things. And so then I can put four things on the left to receive those four things. So this is going to call tensor x_train and put it in a x_train so forth. So this is converting all of these lists to tensors and storing them back in the same name. So you can see that x_train now is a tensor. So that means it has a shape property. It 50,000 images in it which each 784 long and you can find out what kind of what kind of stuff it contains by calling x_train.type() . So it contains floats. So this is the tensor class we’ll be using a lot of it. So of course you should read its documentation in I don’t love the PyTorch documentation. Some of it’s good, some of it’s not good. It’s a bit all over the place. So here’s tensor, but it’s well worth scrolling through to get a sense of like this is actually not bad. Right? It tells you how you can construct it. This is how I constructed one before passing it lists of lists. You can also pass at Numpy Array. You can change types, so on and so forth. So you know, it’s well worth reading through and like you’re not going to look at every single method it takes, but you’re kind. If you browse through it, you’ll get a general sense, right? That tensors do just about everything you can think of for numeric programing. At some point you will want to know every single one of these, or at least be aware roughly what exists. So you know what to search for in the docs. Otherwise you will end up recreating stuff from scratch, which is much, much slower than simply reading the documentation to find out it’s there. All right. So instead of instead of calling chunks or islice the thing that is roughly equivalent in a tensor is the reshape method. So reshape. So the reshape our 15,000 by 784 thing we be able to turn it into a 50000,28 by 28 tensors. So I could write here reshape to 50,000 by 28 by 28. But I kind of don’t need to because I could just put -1 here and it can figure out that that must be 50,000, because it knows that I have 50,000 by 784 items so I can figure out. So -1 means just fill this with all the rest.\n\nimg[20]\n\n[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.09375,\n 0.4453125,\n 0.86328125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.78515625,\n 0.3046875,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0]\n\n\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use this one feature in pytorch.We do it cause it more look like math than original way.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\ntens = tensor(img)\n\n\ntens[20,15]\n\ntensor(0.9883)\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\nyou could also do this imgs = x_train.reshape((50000,28,28)), -1 is another way Jeremy prefer.\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\nOkay. Now what does the word tensor mean? So there’s some very interesting history here, and I’ll try not to get too far into it because I’m a bit overenthusiastic about this stuff. I must admit. I’m very, very interested in the history of tensor programing and array programing, and it basically goes back to a language called APL. APL is a basically originally a mathematical notation that was developed in the mid to late fifties, 1950s. And at first it was used to as a notation for defining how certain new IBM systems what would work. So it was all written out in this, in this notation, it’s kind of like a replacement for mathematical notation that was designed to be more consistent and and kind of more expressive in the early sixties. So the guy who wrote made it was called Kenneth E. Iverson.In the early sixties some implementations that actually allowed this notation to be executed on a computer appeared both in notation and the executable implementations. Slightly confusingly, both called APL. APL has been in constant development ever since that time, and today is one of the world’s most powerful programing languages. And you can try it by going to try APL. And why am I mentioning it here? Because one of the things Kenneth E. Iverson did well, he studied an area of physics called tensor analysis, and as he developed APL, he basically said like, Oh, what if we took these ideas from tensor analysis and put them into a programing language? So in yeah, in APL you, you and you know have been able to for some time can basically you can define a variable and rather than saying equals which is a terrible way to define things really mathematically because that has a very different meaning most of the time in math. Instead we use Arrow to define things. We can say, okay, that’s going to be a a tensor like so, and then we can look at their contents of a and we can do things like, Oh, what if we do a*3 or a-2 and so forth. And as you can see, what it’s doing is it’s taking all the contents of this tensor and it’s multiplying them all by three or subtracting two from all of them, or perhaps more fun we could put in to be a different tensor. And we can now do things like a divided by b, and you can see it’s taking each of a and dividing by each of b. Now, this is very interesting because now we don’t have to write loops anymore. We can just express things directly. We can multiply things by scales even if they’re this is called a rank one tensor. That is to say it’s basically a method called a vector. We can take two and can divide one by the other and so forth. It’s a really powerful idea. Funnily enough, APL didn’t call them tensor even though Kenneth E. Iverson said he got this idea from tensor analysis. APL calls them arrays. NumPy, which was heavily influenced by APL, also calls them arrays. For some reason PyTorch, which very heavily influenced by APL, so by numpy doesn’t call them arrays, it calls them tensors. They’re all the same thing. They are rectangular blocks of numbers. They can be one dimensional like a vector, they can be two dimensional, like a matrix, they can be three dimensional, which is like a bunch of stacked matrices, like a batch of matrices and so forth. If you are interested in APL, which I hope you are, we have a whole APL and a array programing section on our forums and also we’ve prepared a whole set of notes on every single glyph in APL, which also covers all kinds of interesting mathematical concepts like complex direction and magnitude and all kinds of fun stuff like that. That’s all totally optional, but a lot of people who do APL say that they feel like they’ve become a much better programmer in the process. And also you’ll find here at the forums a set of 17 study sessions of an hour or two each covering the entirety of the language, every single glyph. So that’s all like where this stuff comes from. So this, this batch of 50,000 images, is what we call a rank three tensor in PyTorch and in Numpy We would call it an array with three dimensions. Those are the same thing. So what is the rank? The rank is just the number of dimensions. It’s 50,000 images of 28 high by 28 wide. So there are three dimensions. That is the rank of the tensor. So if we then pick out a particular image right, then look at its shape. We could call this a matrix. It’s a 28 by 28 tensor, or we could call it a rank two tensor vector is a rank one tensor in APL, a scalar is a rank zero tensor, and that’s the way it should be. A lot of languages in libraries don’t unfortunately think of it that way. So what is a scalar is a bit dependent on the language. Okay, so we can index into the zeroth image,20 rows and 50s colomn,get back the same number. Okay. So we can take x_train.shape, which is 50,000 by 784 and you can destructure it into N, which is the number of images and C which is the number of the 4 number of columns, for example. And we can also well, this is actually part of the standard library. So reality is mean so we can find out in y_train what’s the smallest number and what’s the maximum number. So that goes from 0 to 9. So you see here it’s not just the number zero, it’s a scalar tensor zero. They act almost the same most of the time. So here’s some example of a bit of the the y_train. So you can see these are basically this is going to be the labels, right? These are our digits and this is its shape. So this is 50,000 of these labels. Okay. And so since we’re allowed to use this in the standard library, well, it also exists in PyTorch. So that means we’re also allowed to use torch.min() and torch.max() properties. All right. So before we wrap up, we’re going to do one more thing. And I don’t know what the we would call kind of anti cheating, but according to our rules, we’re allowed to use random numbers because there is a random number generator in the Python standard library. But we’re going to do random numbers from scratch ourselves. And the reason we’re going to do that is even though according to the rules, we could be allowed to use the standard library one, it’s actually extremely instructive to build our own random number generator from scratch well, at least I think so. Let’s see what you think. So there is no way normally in software to create a random number.\n\nplt.imshow(imgs[0]);\n\n\n\n\n\nplt.imshow(imgs[3]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))\n\n\nHere we go further and broke our rule and make it even harder so we gonna do random number from scratch. We use sodu random number generator."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#random-numbers",
    "href": "posts/Writing stable diffusion from scratch/index.html#random-numbers",
    "title": "Writing stable diffusion from scratch",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\nSo there is no way normally in software to create a random number. Unfortunately, computers, you know, add, subtract, times, logic gates, stuff like that. So how does one create random numbers? Well, you could go to the Australian National University Quantum random number generator, and this looks at the quantum fluctuations of the vacuum and provides an API which will actually hook you in and return quantum random fluctuations of the vacuum. So that’s about that’s the most random thing I’m aware of. So that would be one way to get random numbers and there’s actually an API for that. So that’s a bit of fun. You could do what Cloudflare does. Cloudflare has a huge wall full of larva lamps and it uses the pixels of a camera looking at those larva lamps to generate random numbers. Intel nowadays actually has something in its chips which you can call RDRAND, which will return random numbers on certain intel chips from 2012. All of these things are kind of slow. They can kind of get you one random number from time to time. We want some way of getting lots and lots of random numbers. And so what we do is we use something called a pseudo random number generator. A pseudo random number generator is a mathematical function that you can call lots of times, and each time you call it, it will give you a number that looks random to show you what I mean by that, I’m going to run some code and I’ve created a function which will look at the moment called rand. And if I call rand 50 times and plot it, there’s no obvious relationship between one call and the next. That’s one thing that I would expect to see from my random numbers. I would expect that each time I call rand, the numbers would look quite different to each other. The second thing is rand is meant to be returning uniformly distributed random numbers, and therefore if I call it lots and lots and lots of times and plot its, histogram, I would expect see exactly this, which is each from zero to point one. There’s a few from point one, 2.2, there’s a few 1.2, 2.3. That’s true. It’s a fairly evenly spread thing. These are the two key things I would expect to see an even distribution of random numbers and that there’s no correlation or no obvious correlation from one to the other. So we’re going to try and create a function that has these properties. We’re not going to derive it from scratch. I’m just going to tell you that we have a function here called the Wickman Hill algorithm. This is actually what Python used to use back in before Python 2.3. And the key reason we need to know about this is to understand really well the idea of random state. Random state is a global variable. It’s something which is, or at least it can be. Most of the time when we use it, we use it as a random variable and it’s just basically one or more numbers. So we’re going to start with no random state at all. I’m going to create a function called seed that we’re going to pass something to, and I just smashed the keyboard to create this number. Okay So this is my random number. You could get this from the and quantum vacuum generator or from cloudflare’s larva lamps or from your intel chips RDRAND. You know, in python land, which pretty much always is a number 42, any of those are fine. So you pass in some number or you can pass in the current tick count in nanosecond as there’s various ways of getting some random starting point. And if we pass it into seed, it’s going to do a bunch of modular divisions and create a tupple of three things, and it’s going to store them in this global state. So Rand State now contains three numbers. Okay, so why do we do that? The reason we did that is because now this function, which takes our random state, unpacks it into three things and does again a bunch of multiplications and modules and then sticks and together with various kind of weights, modulo one. So this is how you can pull out the decimal part. This returns random numbers. But the key thing I want you to understand is that we pull out the random state at the start. We do math thingies to it and then we store new random state. And so that means each time I call this, I’m going to get a different number, right? So this is a random number generator. And this is really important because lots of people in the deep learning world screw this up, including me. Sometimes, which is to remember that random number generators rely on the state. So let me show you where that will get you if you’re not careful. If we use a special thing called fork that creates a whole separate copy of this python process in one copy or.fork() returns true and in the other copy it returns false, roughly speaking. So this copy here is this. If I say this, this version here the true version is the original none copied. It’s called the parent and so on. My else here this. So this will only be called by the parent. This will only be called by the copy. It is called the child. And each one I’m calling rand. These are two different random numbers right? Wrong. Yeah. The same number. And why is that? That’s because this process here and this process here are copies of each other, and therefore they each contain the same numbers in random state. So this is something that comes up in deep learning all the time, because in deep learning we often do parallel processing, for example, to generate lots of augmented images at the same time using multiple processes fast.ai used to have a bug in fact, where we failed to correctly initialize the random number generator separately in each process.\nAnd in fact, to this day, at least as as of October 2022 torch.rand by default fails to initialize the random number generator. That’s the same number. Okay, so you got to be careful now. I have a feeling numpy gets it right. Let’s check import Numpy As an np. Okay. And so I don’t then I can’t remember which right now. Okay. NumPy also doesn’t have interesting. What about python. Right. And look at that. So python does actually remember to re initialize the random state and each fork. So you know this is something that like even if you’ve experimented in Python and you think everything’s working well in your data loader or whatever and you switch to PyTorch or numpy and now suddenly everything’s broken. So this is why we’ve spent some time re-implementing random, the random number generator from scratch, partly because it’s fun and interesting and partly because it’s important that you now understand that when you’re calling rand or any random number generator kind the default versions in numpy and PyTorch, this global state is going to be copied. So you’ve got to be a bit careful. Now I will mention our random number generator. Okay, So this is this is called percent timeout percent is a special Jupyter or Ipython function and percent time. It runs a piece of Python code this many times. So to call it ten times. Well, actually today seven loops and each one will be seven times and I’ll take the maiden standard deviation. So here I am going to generate random numbers, long chunks, and if I run that, it takes me 3 milliseconds. Like if I run it using PyTorch, this is the exact same thing in PyTorch. It’s going to take me 73 micro second. So as you can see, although we could use our version, we’re not going to because the PyTorch version is much, much faster. This is how we can create a 784 by ten. And why would we want this? That’s because this is our final layer of our neural net. Or if we’re doing a linear classifier, a linear weights want it to be 784 because that’s 28 by 28 by ten, because that’s the number of possible outputs, the number of possible digits. All right. That is it. So quite the intense lesson. I think we can all agree should keep you busy for a week. And thanks very much for joining and see you next time. Bye everybody.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\nBe carefull when you use Pytorch or Numpy\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.2706])\nIn child: tensor([0.2706])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n8.57 ms ± 368 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\n135 µs ± 54.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html",
    "href": "posts/Writing stable diffusion from scratch 2/index.html",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "",
    "text": "Important stuff you need to know after this lecture:\n1- Matrix Multiplication  2- Numba, how to compile python code to machine code  3- Frobenius Norm  4- Braodcasting Rules  5- expand_as , stride  6- unsqueeze , c[ None , : ] ,c[ : , None] 7- c[None], c[…,None] \n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#get-data",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#get-data",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar 15 09:50 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#matrix-and-tensor",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#matrix-and-tensor",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use pytorch.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#random-numbers",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#random-numbers",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.2262])\nIn child: tensor([0.2262])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n5.39 ms ± 223 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\n86.7 µs ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#matrix-multiplication",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#matrix-multiplication",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nOkay, so let’s move on with our from the foundations now. And so we were working on trying to at least get the start of a forward pass of a linear model or a simple multi-layer perceptron for MNIST going. And we had successfully created a basic tensor. We’ve got some random numbers going. So what we now need to do is we now need to be able to multiply these things together, matrix multiplication. So matrix multiplication to remind you in this case. So we’re doing MNIST, right? So we’ve got about we’re going to use a subset, let’s see. Yeah, Okay. So we’re going to create a matrix called m1, which is just the first five digits, So m1 will be the first five digits. So five rows and. Well, dot, dot dot dot dot dot. And then 780. What is it again. because it’s 28 by 28 pixels and reflect that out. So this is our first matrix and matrix multiplication, and then we’re going to multiply that by some some weights. So the weights are going to be 784 by 10 random numbers. So for every one of thes 784 pixels, each one is going to have a weight. So 784 down here, so 94 by ten. So this first column, for example, is going to tell us all the weights in order to figure out if something’s a zero. And the second column will have all the weights in deciding of the probability of something. So one, so forth, assuming we just doing a linear model. And so then we’re going to multiply these two matrices together. So when we multiply matrices together, we take row one of matrix one and we take column one of matrix two and we take each one in turn. So we take this one and we take this one, we multiply them together and then we take this one and this one and we multiply them together. And we do that for every element wise pair, and then we add them all up and that would give us the value for the very first cell that would go in here. That’s what matrix multiplication is. Okay, so let’s go ahead and create our random numbers for the weights since we’re allowed to use random number generator now and for the bias, but just use a bunch of zeros to start with. So the bias is just what we’re going to add to each one. And so for our matrix multiplication, we’re going to be doing a little mini batch. I’m going to be doing five rows of, as we discussed, five rows of so five, five images flattened out and then multiplied by this weights matrix.\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\nSo here are the shapes and one is five by seven, eight four as we saw, and m2 is seven, eight, four by ten. Okay, so keep those in mind. So here’s a handy thing. And one touch shape contains two numbers and I want to pull them out. I want to call the I’m going to think of that as I’m going to actually think of this as like a and b rather than I wanted them to. So this is like a and b, so the number of rows in a and the number of columns in b, if I say equals and one shape that will put five in ar and 784 in ac, So I’ll notice I do this a lot, this restructuring, we talked about it last week too so can do the same for m2 dot shape, put that into b rows and b columns. And so now if I write out ar,ac and br , br , you can again see the same things from the sizes. So that’s a good way to kind of give us the stuff we have to look through. So here’s our results.\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nget matrix dimsions and put them in variables to make it readable for future looping.\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\nSo here’s our results. So our resultant tensor, well, we’re multiplying, we’re multiplying together all of these seven, eight, four things and adding them up. So the resultant tensor is going to be five by ten. And then each thing in here is the result of multiplying and adding So the result here is going to start with zeros and there is this is the result and it’s going to contain ar rows, five rows and bc columns, ten columns, five coma ten. Okay, so now we have to fill that in. And so to do a matrix multiplication, so we have to first we have to go through each row one at a time and here we have that go through each row one at a time and then go through each column one at a time. And then we have to go through each pair in that row column one at a time. So it’s going to be a loop in a loop in a So here’s quick over each row, and here we’re going to loop over each column and then here we’re going to loop so each column of c, and then here we’re going to leap over each column of a, which is going to be the same as the number of rows of b, which we can see here. I say ac or br they are seven, eight, four. They’re the same. So it wouldn’t matter whether we day, ac or br, so then our result for that row and that column, we have to add onto it the product of i,k in the first matrix by k,j in the second matrix. So k, it’s going up through those seven, eight, four. And so we’re going to go across the columns and down so across the rows and down the columns, it’s going to go across the row where it goes down this column. So here is the world’s most naive, slow, uninteresting matrix multiplication. And if we run it, okay, it’s done something we have successfully hopefully successfully multiplied the matrices m1 and m2.It’s a little hard to read this, I find because because punch cards used to be 80 columns wide. We still assume screens 80 columns wide. Everything defaults to 80 wide, which is ridiculous, but you can easily change it. So if you say sit print options, you can choose your own line width. Oh, you can say it’s five by ten. We did it before. So if we change the line width, okay, that’s much easier to rate. Now we can see here the five rows and here are the ten columns for that matrix multiplication. I tend to always put this at the top of my notebooks and you can do the same thing for numpy as well. So what I’d like to do this is really important is when I’m working on code, particularly numeric code, I like to do it all step by step and Jupiter. And then what I do is once I’ve got it working is a copy all the cells that have implemented that and I paste them and then I select them all and I hit shift+m to merge. Get rid of anything that prints out stuff I don’t need. And then I put a header on the top, give it a function name, and then I select the whole lot and I hit control or f right square bracket and I’ve turned it into a function, but I still keep the stuff above it. So I can see all the step by step stuff for learning about it later. And so that’s what I’ve done here to create this function. And so this function does exactly the same things we just did, and we can see how long it takes to run by using %time. And it took about half a second, which gosh, that’s a long time to generate such a small matrix. This is just to do five MNIST digits. So that’s not going to be great. We’re going to have to speed that up. I’m actually quite surprised at how slow that is because there’s only 39,200. So, you know, if you look at the how, we’ve got a loop within a loop within a loop, it’s doing 39,200 of these. So Python. Yeah, Python. When you’re just doing python, it is it is slow. So we can’t we can’t do that. That’s why we can’t just write Python.\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\nGo through each row one at a time (5), then each column one at a time (10) and then go through each pair(784). go accross the rows , down the column multiply and add. t1[i,j] += m1[i,k] * m2[k,j]\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nDefault is 80 columns wide because of punch cards and we still do that. (Talking about legacy and network effect , haha)\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\nThis is only to show data more readable.\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\nDo this on the top of the notebook and make it easier.\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\nFor numerical programming , Jeremy recommend doing stuff line by line , check the results and dimensions and then when it works , copy all the cells and paste them after those cell and select them all and hit shift+M to merge cells get ride of everything that prints out stuff you dont need put a header on the top (def ….), select the rest of the code and hit control + ] now you have the function. Keep the same none function code above to remember what did you do and how you get there.\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n%time _=matmul(m1, m2)\n\nCPU times: user 771 ms, sys: 748 µs, total: 772 ms\nWall time: 774 ms\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#numba",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#numba",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Numba",
    "text": "Numba\nBut there is something that kind of lets this write Python we could instead use Numba.Numba is a system that takes python and turns it into basically into machine code and it’s amazingly easy to do. You can basically take a function and write and @ngit on top. And what it’s going to do is it’s going to look the first time you call this function, it’s going to compile it down to machine code and will run much more quickly. So what I’ve done here is I’ve taken the innermost loop. So just looping through and adding up all these. So I start at zero, go through and add up all those just two vectors and return it, which is called a dot product. And linear algebra, so call it dot and so Numba only works with numpy, it doesn’t work with PyTorch. So we’re just going to use arrays instead of tensers for a moment. Now have a look at this. If I try to do a dot product of one, two, three and two, three, four, it’s pretty easy to do. It took a fifth of a second, which sounds terrible, but the reason it took a fifth of a second is because that’s actually how long it took to compile this and run it. Now that it’s compiled the second time, it just has to call it it’s now 21 microseconds. And so that’s actually very fast. So with Numba we can basically make Python run at C speed. So now the important thing to recognize is if I replace this loop in Python with a called a dot which is running in machine code, then we now have one two loops running in python not three. So our 448 MS, let’s make sure if I run it, run that matmul that should be close to my t1 one. t1 is what we got before.\nAnd so when I’m refactoring or performance improving or whatever, I always like to put every step in the notebook and then test. So this test close comes from fastcore.test and it just checks. The two things are very similar. They might not be exactly the same because of floating point differences, which is fine. Okay, our matmul is working correctly, or at least it’s doing the same thing it did before. So if we now run it, it’s taking 268 micro second, versus 448 milliseconds. So it’s taking, you know, about 2000 times faster just by changing the one in my loop. So really all we’ve done is we’ve had @ngit to make it 2000 times faster, so Numba is well worth knowing about. I can make your Python code very, very fast. Okay, let’s keep making it faster. So we’re going to use stuff again, which kind of goes back APL. And a lot of people say that learning APL is the thing that’s taught them more about programing than anything else. So it’s probably worth considering learning APL And let’s just look at these various things. You got a is ten six minus four. So remember at APL, we don’t say equals, equals actually means equals. Funny enough we to say set two, we use this arrow and it’s, this is a list of ten, six, four and then b is 287. Okay. And we’re going to add them up a plus b, So what’s going on here? So it’s really important that you can think of a symbol like a as representing a tensor or an array. APL calls them arrays, pytorch call them tensors, Numpy calls them arrays. They’re the same thing. So this is a single that contains a bunch of numbers. This is a single thing that contains a bunch of numbers. This is an operation that applies to arrays or tensors. Now what it does is it works what’s called elsment-wise. It takes each pair ten and two, and that’s them together. Each pair six and eight, add them together. This is element wise addition and Fred is asking in the chat, how do you put in these symbols? If you just mouse over any of them, it will show you how to write it and the one you want is the one at the very bottom, which is the one where it says prefix. Now the prefix is the backtick character. So here it’s saying prefix hyphen gives us times. So we’ve had hyphen. So I’ve of a backtick dash b is a times b for example. So yeah, they all have shortcut keys which you learn pretty quickly. I find, and there’s a fairly consistent kind of system for those shortcut keys too. All right, So we can do the same thing in PyTorch.\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 479 ms, sys: 145 ms, total: 624 ms\nWall time: 563 ms\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 34 µs, sys: 6 µs, total: 40 µs\nWall time: 44.3 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\nThis is the test.\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n2000 time faster. We change inner most loop.\n\n%timeit -n 50 matmul(m1a,m2a)\n\n455 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nSo we can do the same thing in PyTorch. It’s a little bit more verbose. PyTorch, which is one reason I often like to do my mathematical fiddling around in APL. I can often do it with less boilerplate, which means I can spend more time thinking, you know, I can see everything on the screen at once. I don’t have to spend as much time trying to like ignore the tensor around bracket square bracket dot com, blah blah. It’s all cognitive load, which I’d rather ignore. But anyway, it does the same thing so I can say a plus b work exactly like APL. So here’s an interesting example. I can go a less than (a < b).float().mean(). So let’s try that one over here less than b. So this is a really important idea, which I think was invented by Ken Iverson, the APL guy, which is that true and false represented zero and one. And because they’re represented by zero and one, we can do things to them. We can add them up and subtract and so forth. That’s a really important idea. So in this case, I want to take the main of them, and I’m going to tell you something amazing, which is that in APL there is no function called mean. Why not? That’s because we can write the mean function, which so that’s four letters mean and we can write the mean function from scratch with four characters. I’ll show you. Here’s the whole mean function we’re going to create a function called mean, and the mean is equal to the sum of a list divided by the of the list. So this here is some divided by count. And so I have now to find a new function called mean, which calculates the mean, mean of a is less than b, there we go. And so, you know in practice, I’m not sure why people would even bother defining a function called mean because it’s just as easy to actually write it’s implementation in APL, in numpy or whatever a python. It’s going to take a lot more than four letters to implement mean. So anyway, you know, it’s a math notation and so being a math notation we can do a lot with little, which I find out folks, I can say everything going on at once anyway. Okay, so that’s how we do the same thing in pytouch. And again, you can say that the less than in both cases, operating element wise. Okay, So a is less than b is saying ten is less than two six is less than eight four is less than seven and gives us back each of those trues and faleses as zeros and onces and according to our YouTube chat, had just exploded as it should. This is why APL is. Yeah life changing."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#elementwise-ops",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#elementwise-ops",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\nElementwise addition\n\na + b\n\ntensor([12., 14.,  3.])\n\n\nCheck lecture for awesome implementation of mean.\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\nRank two tensor , aka Matrix.\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nOkay, let’s now go up to higher rank. So this here is a rank one tensor. So a rank one tensor means it’s a list of things, it’s a vector, it’s where else a rank two tensor. It’s like a list of lists. They all have to be the same length lists, or it’s like a rectangular bunch of numbers. And we call it in math, we call it a matrix. So this is how we can create a tensor containing one, two, three, four, five, six, 789. And you can see also, like, what I like to do is I want to print out the thing I just created after I created it. Two ways to do it. You can say put an enter and then write m and that’s going to do that. Or if you want to put it all in the same line, that works too. You just use a semicolon. Neither one is better than the other. They’re just different. So we could do the same thing in APL. Of course in APL it’s going to be much easier. So we’re going to define a matrix called m which is going to be a three by 3 by 3 tensor containing the numbers from 1 to 9. Okay. And there we go. That’s done it in APL, a three by three tensor containing the numbers from 1 to 9. A lot of these ideas from APL you’ll find have made their way into other programing languages. For example, if you use GO you might recognize this. This is the iota character and go uses the word iota to spell it out in a somewhat similar way. A lot of these ideas from APL have found themselves into math notation and in other languages. It’s been around since the late fifties. Okay, so here’s a bit of fun. We’re going to learn about a new thing that looks kind of crazy code for Frobenius Norm and we’ll use that from time to time as we’re doing modeling. And here’s a definition of a four Frobenius norm. It’s the sum over all of the rows and columns of a matrix, and we’re going to take each one and square it. They’re going to add them up and they’re going to take the square root.\nAnd so to implement that in pytouch is as simple as (m*m).sum().sqrt(). So this looks like a pretty complicated thing when you kind of look at it. At first it looks like a lot of squiggly business or if you said this thing here you might be like, what on earth is that? Well, now you know, it’s just a square, some square root. So again, we could do the same thing in APL. So let’s do so in APL. We want the okay, so we got a case called S.F. Now it’s interesting, Apple does this a little bit differently. So dot some by default in PyTorch sums over everything. And if you want to sum over just one dimension, you have to pass in a dimension keyword for very good reasons. APL is the opposite. It just comes across rows or just down columns. So actually we have to say sum up the flattened out version of the Matrix and say flattened out. He is comma. So his sum up the flattened out version of the Matrix. Okay, so that’s our S.F.. Oh, Oh, sorry. And the Matrix is meant to be m times m There you go. So that’s the same thing. Sum up the flattened out and by a matrix and another interesting thing about APL is it always is read right to left. There’s no such thing as operator precedence, which makes life a lot easier. Okay, then we take the square root of that. There isn’t a square root function, so we have to do to the power of 0.5 and there we go. Same thing. All right, You got the idea. Yes. A very interesting question here from Marabou or other bars for norm or absolute value. And I like answer, which is the norm, is the same as the absolute value for scalar. So in this case, you can think of it as absolute value and it’s kind of not needed because it’s being squared anyway. But yes, in this case the norm. Well, in every case for a scale, the norm is the absolute value, which is kind of a cute discovery when you realize it. So thank you for pointing that out. See the. All right. So this is just fiddling around a little bit to kind of get a sense of how these things work. So really importantly, you can index into a matrix and you’ll say rows first and then columns."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#matmul-with-broadcasting",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#matmul-with-broadcasting",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\nSo let’s just grab a single digit. So here’s the first digit. So its shape is it’s a 784 long vector. Okay. And remember that our weight matrix is 784 by ten. Okay. So if we say digit colon coma None dot shape, then that is a 784 by one row matrix. Okay. So there’s our matrix. And so if we then take that 784 by one and expandas m2, it’s going to be the same shape as our weight matrix. So it’s copied our image data for that digit across all of the ten vectors, representing the ten kind of linear projections we’re doing for our linear model. And so that means that we can take the digit colon comma a None so 784 by one and multiply it by the weights. And so that’s going to get us back 784 by 10 so what it’s doing, remember, is it’s basically looping through each of these 10, 784 long vectors. And for each one of them it’s multiplying it by this digit. So that’s exactly what we want to do in our matrix multiplication. So originally we had when I originally most recently I should say, we had this dot product where we were actually looping over j, which was the columns of b, So we don’t have to do that anymore because we can do it all at once by doing exactly what we just did so we can take the ith and all the columns and add a access to the end. And then just like we did here, multiply it by b and then .sum(). And so that is again exactly the same thing.\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n185 µs ± 54.5 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nThat is another metrics, multiplication, doing it using broadcasting. Now this is like a tricky to get your head around. And so if you haven’t done this kind of broadcasting before, it’s a really good time to pause the video and look carefully at each of these four cells before and understand what did I do there, Why did I do it? What am I showing you? And then experiment with trying to and so remember that we started with m1 zero, right? So just like we have here, a[i , so that’s why we’ve got, i comma colon comma None because this digit is actually m1 zero. So this is like m1 zero colon None. So this line is doing exactly the same thing as this here plus the sum. So let’s check if this matmul is the same as it used to be, yet still working and the speed of it. Okay, not bad. So 137 microseconds. So we’ve now gone from a time from 500 milliseconds to about point 1 milliseconds. Funnily enough, my MacBook Air is an m2, whereas this Mac mini is an m1 that’s a little bit slower. So my error it was a bit faster than 0.1 milliseconds. So overall we’ve got about a 5000 times speed improvement. So that is pretty exciting. And since it’s so fast now, there’s no need to use a mini batch anymore. If you remember, we used a mini batch of five images, but now we can actually use the whole dataset so fast. So now we can do the whole data set. There it is. We’ve now got 15,000 by ten, which is what we want. And so it’s taking us only 656 milliseconds now to do the whole dataset. So this is actually getting to a point now where we could start to create and train some simple models in a reasonable enough time. So that’s good news. All right. I think that’s probably a good time to take a break. We don’t have too much more of this to go, but I don’t want to keep you guys up too late. So hopefully you learned something interesting about broadcasting today. I cannot overemphasize how widely useful is in all deep learning in machine learning code. It comes up all the time. It’s basically our number one most critical kind of foundational operation. So, yeah, take your time practicing it. And also good luck with your diffusion homework from the first half of the lesson. Thanks for joining us and I’ll see you next time.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.92 s, sys: 1.61 ms, total: 1.93 s\nWall time: 2.3 s"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html",
    "href": "posts/Writing stable diffusion from scratch 3/index.html",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "",
    "text": "Things you should know and practice after this lecture :  1- Einsum  2- 2 ways you can do einsum / matmul in pytorch  3- Using GPU / Cuda  4- args and kwargs\nAll credits go to fastai and all mistakes most likely is mine. This notebook connected to pass two notebooks. So the code is here but I remove most of explanation. You can check past two posts. Enjoy learning …"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication-from-foundations",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication-from-foundations",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#get-data",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#get-data",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar 18 22:05 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice at 0x7fc00c934040>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matrix-and-tensor",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matrix-and-tensor",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use pytorch.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#random-numbers",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#random-numbers",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.6953])\nIn child: tensor([0.6953])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n4.6 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\n103 µs ± 42.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n%time _=matmul(m1, m2)\n\nCPU times: user 621 ms, sys: 1.34 ms, total: 623 ms\nWall time: 629 ms\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#numba",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#numba",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Numba",
    "text": "Numba\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 363 ms, sys: 113 ms, total: 476 ms\nWall time: 445 ms\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 19 µs, sys: 4 µs, total: 23 µs\nWall time: 26 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\nThis is the test.\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n2000 time faster. We change inner most loop.\n\n%timeit -n 50 matmul(m1a,m2a)\n\n383 µs ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#elementwise-ops",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#elementwise-ops",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\nElementwise addition\n\na + b\n\ntensor([12., 14.,  3.])\n\n\nCheck lecture for awesome implementation of mean.\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\nRank two tensor , aka Matrix.\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#broadcasting",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#broadcasting",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes how arrays with different shapes are treated during arithmetic operations.\nFrom the Numpy Documentation:\nThe term broadcasting describes how numpy treats arrays with \ndifferent shapes during arithmetic operations. Subject to certain \nconstraints, the smaller array is “broadcast” across the larger \narray so that they have compatible shapes. Broadcasting provides a \nmeans of vectorizing array operations so that looping occurs in C\ninstead of Python. It does this without making needless copies of \ndata and usually leads to efficient algorithm implementations.\nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\nThis section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course.\n\nBroadcasting with a scalar\n\na\n\ntensor([10.,  6., -4.])\n\n\nSimplest way of broadcasting.\n\na > 0\n\ntensor([ True,  True, False])\n\n\n\na + 1\n\ntensor([11.,  7., -3.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nmultiply\n\n2*m\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\n\nBroadcasting a vector to a matrix\n\nc = tensor([10.,20,30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nc + m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt = c.expand_as(m)\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nm + t\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\n\nc.unsqueeze(0), c[None, :]\n\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))\n\n\n\nc.shape, c.unsqueeze(0).shape\n\n(torch.Size([3]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(1), c[:, None]\n\n(tensor([[10.],\n         [20.],\n         [30.]]), tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([3, 1]))\n\n\nwe can avoid : and say c[None].\n\nc[None].shape,c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nc[:,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nm + c[:,None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\nThis adding the vector to each row.\n\nm + c[None,:]\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\n\nBroadcasting Rules\n\nc[None,:]\n\ntensor([[10., 20., 30.]])\n\n\n\nc[None,:].shape\n\ntorch.Size([1, 3])\n\n\n\nc[:,None]\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\n\nc[:,None].shape\n\ntorch.Size([3, 1])\n\n\n\nc[None,:] * c[:,None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[None] > c[:,None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\nm*m\n\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when It goes from right to left. - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size\nArrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matmul-with-broadcasting",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matmul-with-broadcasting",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n231 µs ± 71.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.26 s, sys: 2.76 ms, total: 1.26 s\nWall time: 1.28 s"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#einstein-summation",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#einstein-summation",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Einstein summation",
    "text": "Einstein summation\nSo we’re 5000 times faster than we started out. So another trick that we can use, which I’m a big fan of, is something called Einstein summation. And Einstein summation is a compact representation for representing products and sums. And this is an example of an Einstein summation. And what we’re going to do now is begin to replicate our matrix product with an Einstein summation. And believe it or not, the entire thing can be pushed down to just these characters,(ik,kj->ikj) which is pretty amazing. So let me explain what’s happening here. The arrow is separating the left hand side from the right hand side. The left hand side is the inputs. The right hand side is the output. The comma is between each inputs. So there are two inputs. The letters are just names that you’re giving to the number of rise in the number of columns. So the first matrix we’re multiplying by has i rows and k columns, the second has k rows and j columns. It’s going to go through a process which creates a new matrix that actually this is not even doing this is not yet doing the matrix multiplication. This is without the sum. This one’s going to create a new matrix that contains i rows, Well, how do we set i faces and k rows and j columns are rank three tensor. So the number of letters is going to be the rank and the rules of how this works is that if you repeat letters between input arrays,(ik,kj) so here’s my inputs (ik,kj) and we’ve got a repeated letter. It means that values along those axes will be multiplied together. So it means that each item across a row will be multiplied by each item down each column to create this i by k by j output tensor. So to remind you, our first matrix is five by 734 that’s m1. Our second matrix is 784 by ten that’s m2. So i is 5, k is 784 and J is 10. So if I do this torch.einsum then I will end up with a i k by k, it’ll be five by 784 by ten. And if you have a look, I’ve run it here on these two tensor and m1 and m2 and the shape of the result is five by 784 by ten. And what it contains is the original five rows of m1 the original ten columns of m2, and then for the other 784 that I mentioned, they all multiplied together because it’s been copied. It’s been copied between the two arguments to the einsum. And so if we now sum up that over this dimension, we get back, if we have a look, it was that we printed this somewhere. Oh, there it is. So what we get back, if we go back to the original matrix multiply, we do. We had 10.94 negative, negative point six, eight, etc. And so now with this Einstein summation version, we’ve got back exactly the same thing because what it’s done is it’s taken each of these columns by rows, multiplied them together to get this five by seven, eight, four by ten, and then add it up that 784 for each one, which is exactly what matrix multiplication does. But we’re going to use one of the two things from Einstein. The second one says if we omit a letter from the output. So the bit on the right of the arrow, it means those values will be summed. So if we remove this K, which gives us i , k and k,j goes to i,j, so we’ve removed the k entirely. That means that sum happens automatically. So if we run this, as you say, we get back again. Matrix multiplication. So Einstein summation notation is, you know, it takes some practice getting used to you, but it’s very convenient and once you get used to it, it’s actually a really nice way of thinking about what’s going on. And as we’ll see in lots of examples, often you can really simplify your code by using just a tiny little Einstein summation, and it doesn’t even have to be a sum, right? You can you don’t have to omit any letters if you’re just doing products. So maybe it’s a bit misnamed. So we can now define a matmul as simply this torch.einsum. So if we now check it, the test_close that the original result is equal to this new matmul. And yes, it is. And let’s see how the speed looks. Okay. And that was for the whole thing. So compared to 600 milliseconds. So as you can see, this is much faster than even the very fast broadcasting approach we used. So this is a pretty good trick is torch.einsum. Some okay, but of course we don’t have to do any of those things because PyTorch already knows how to do matmul. So there’s two ways we can run matmul directly. In PyTorch, you can use a special @ operator. So x_train@weights is the same as matmul train comma weights as you say, test_close or you can say torch.matmul. And interestingly, as you can see here, the speed is about the same as the einsum. So there’s no particular harm that people reason not to do an einsum. So when I say einsum, that stands for Einstein summation notation. All right, let’s go faster. Still. Currently we’re just using my CPU, but I have a GPU. It would be nice to use it. So how does a GPU work at in video? GPU and indeed pretty much all GPU use. The way they work is that they do lots and lots of things in parallel and you have to actually tell the GPU what are all the things you want to do in parallel or one a time. And so what we’re going to do is we’re going to write in Pure Python something that works like a GPU. You expect it won’t actually be in parallel, so it won’t be fast at all. But the first thing we have to do if we’re going to get something working in parallel is we have to create a function that can calculate just one thing even if a thousand other things are happening at the same time, it won’t interact with anything else. And there’s actually a very easy way to think about matrix multiplication in this way, which is what if we try to create something which, just as we’ve done here, fills in a single, the single item of the result?\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. The key rules are:\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\nOmitting a letter from the output means that values along that axis will be summed.\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nSo far we removed 2 inner loop and we are 5k faster than original for loop. It is more cleaner code and also faster.\n\n# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntorch.einsum('ik,kj->ij', m1, m2)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n\n\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n\n\n%timeit -n 5 _=matmul(x_train, weights)\n\n30.5 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#pytorch-op",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#pytorch-op",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "pytorch op",
    "text": "pytorch op\nNow that we wrote matmul we can use pytorch matmul version. The speed is almost the same. We can use pytorch’s function or operator directly for matrix multiplication.\n\ntest_close(tr, x_train@weights, eps=1e-3)\n\n\n%timeit -n 5 _=torch.matmul(x_train, weights)\n\n32 ms ± 4.72 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nSo how do we create something that just fills in row zero column zero? Well, what we could do is we could create a new matmul where we’re going to pass in the coordinates of the place that we want to fill in. So I’m going to start by passing that zero comma zero we’ll pass at the matrices We want to multiply and we are passing a tensor that we’ve pre-filled in with zeros to put the result into. So they’re going to say, okay, the result is torch.zeros() rows by columns, cal matmul for location zero comma zero passing in those two matrices and the bunch of zeros matrix ready to put the result in. And if we call that we get the answer in cell zero zero. So here’s an implementation of that. So the implementation is first of all, we’ve been past the zero comma zero coordinates, so let’s de structure them. So hopefully you’ve been experimenting with de structuring that so important. You said all the time into i and j throw in the column, make sure that that is inside the bounds of our output matrix and we’re going to start by start at zero and loop through all of the all of the columns of a in the rows of b for i and j, just like the very innermost loop of our very first Python attempt and then at the end pop that into the output. So here’s something that fills in one piece with a grid successfully. So we could call this row by columns times each time passing in a different grid. And we could do that in parallel because none of those different locations interact with any other location. So something which can calculate a little piece of, of an output on a GPU is called a kernel. So we call this a kernel. And so now we can create something called launch kernel, we pass at the kernel. So that’s the function. So here’s an example launch kernel passing in the function and how many rows and how many columns are there in the output grid. And then give me any arguments that you need to calculate it. So in python *args just says any additional arguments that you pass are going to be put into an array called args. If you do something like C, you might have seen like variadic arguments parameters. It’s a same basic idea. So we’re going to be calling launch kernel."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#cuda",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#cuda",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "CUDA",
    "text": "CUDA\nHow to use GPU instead of CPU. GPU does many more thing at the same time. CPU does not do that. We can compute each cell at the same time because none of those computation interact with other location.\nSo we’re going to be calling launch kernel. We’re going to be saying launch the kernel matmul using all the rows of a or the columns of b, and then the args which are going to be in star args are going to be m1, the first matrix, m2 the second matrix n res another touched zeros we just created. So launch kernel, it’s going to loop through the rows of a and then for each row of a loop through the columns of b and call the kernel which is matmul on that grid location and passing in m1, m2 and res. So I star args here is going to unpack that and pass them as three separate arguments. And if I run that, run all of that, you’ll see it’s done it, it’s filled in the exact same matrix. Okay. So that’s actually not fast at all. It’s not doing anything in parallel, but it’s the basic idea. So now to actually do it in parallel, we have to use something called Cuda. So Cuda is a programing model for Nvidia GPUs and to program in CUDA from Python. The easiest way currently to do that is be something called Numba. And Numba is a compiler where you’ve seen it actually already for non GPU. It’s a compiler that takes Python code and spits out, you know, compiled fast machine code. If you use its CUDA module, it’ll actually spit out GPU accelerated CUDA code. So rather than using an @njit like before, we now say @cuda.jit and it behaves a little bit differently but you’ll see that this matmul let me copy the other one over so you can compare cup it, compare it to our Python one, our Python matmul and this @cuda.jit matmul Look I think identical except for one thing. Instead of passing in the grid, there’s a special magic thing called cuda.gird() And you say how many dimensions just my grid have? And you unpack it so that’s you don’t have to. It’s just a little convenience. That Numba does for you. You don’t have to pass over the grid, it passes it over for you. So it doesn’t need this grid. Other than that, these two are identical, but the decorator is going to compile that into your GPU code. So now we need to create our output tensor just like before, and we need to do something else, which is we have to take our input matrices and our output. So our input tenses, the matrices in this case and the output tensor and we have to move them to the GPU, you I should say, copy them to the GPU.\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nres = torch.zeros(ar, bc)\nmatmul((0,0), m1, m2, res)\nres\n\ntensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])\n\n\n*args any additional argument(s) will be put into an array called args. This is the idea.\n\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): kernel((i,j), *args, **kwargs)\n\n\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\nNow we use cuda so it will do multiply computation at the same time.\n\nfrom numba import cuda\n\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\nSo cuda.to_device() copies a tensor to the GPU. And so we’ve got three things getting copied to the GPU here and therefore we store the three things over here. Another way I could have written this is I could have said map, which I kind of quite like doing a function which is cuda.to_device to each of these arguments and this would be the same thing. This is going to call CUDA dot device on x_train and put it in here on weights and put it in here and an r and put it in rg. That’s a slightly more convenient way to do it. Okay, so we’ve got our 50,000 by ten output. That’s just all zeros. Of course, that’s just how we created it. And now we’re going to try and fill it in. There is a there’s a particular detail that you don’t have to worry about too much, which is in CUDA They don’t just have a grid, but there’s also a concept of blocks and there’s something we call here TPP, which is threads per block. This is just a detail of the kind of programing model you don’t have to worry about too much. You can just basically copy this. And what it’s going to do is it’s going to call each grid item in parallel and with a number of different processes, basically. So this is just the code which turns the grid into blocks. And so you don’t have to worry too much about the details of that. You just always run it.\nDecorator compile it to GPU code.\n\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\n!pip install numba\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (0.56.4)\nRequirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.9/dist-packages (from numba) (1.22.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba) (63.4.3)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba) (0.39.1)\n\n\n\nfrom numba import cuda\nr = np.zeros(tr.shape)\nm1g,m2g,rg = cuda.to_device(x_train),cuda.to_device(weights),cuda.to_device(r)\n\n\nr.shape\n\n(50000, 10)\n\n\n\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(3125, 1)\n\n\nOkay. And so now how do you call the equivalent of launch kernal? it’s it’s a slightly weird way to do it, but it works fine. You call matmul, but because matmul has cuda.jit, it’s got a special thing, which is you have to put something in square brackets afterwards, which is you have to tell it how many blocks per grid. That’s just the result from the previous cell and how many threads per block in each of the two dimensions. So again, you can just copy and paste this from my version, but then you pass in the three arguments to the function. This will be a, c, and c, and this. Okay, this is, this is how you launch a kernel. So this will launch the kernel matmul on the GPU. You at the end of it, rg is going to get filled in. It’s gone. It’s on the GPU, which is not much good to us so we don’t have to copy it back to the CPU, which is called the host copy to host to a run that and it’s done and test_close shows us that result is similar to our original results. So it seems to be working. So that’s great. So I see Sylvor on the YouTube chat is finding that it’s not working on his Mac. That’s right. So this will only work on it in NVIDIA CPU as basically all of the GPU, nearly all the CPU stuff we look at only works on video.\n\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n\nMac GPUs are gradually starting to get a little bit of support from machine learning libraries, but it’s taking quite a while. It’s been, you know, it’s got quite a way to go. As I say this at least towards the end of 2022, if this works for you and later on that’s yeah, that’s great. Okay, so let’s time how fast that is. Okay, so that was 3.6 1 milliseconds. And so if we compare that to the PyTorch matmul on CPU, that was 15 milliseconds. So that’s great. So it’s faster still. So how much faster? Oh, by the way, we can actually go faster than that, which is we can use the exact same code we had from the PyTorch up. But here’s a trick. If you just take your tensor and write .cuda after it, it copies it over to the GPU. If it’s on a if it’s on a Nvidia GPUs, you do the same for weights.cuda. So these are two cuda versions and now I can do the whole thing. And this will actually run on the GPU and then to copy it back to the host, you just say .cpu(). So if we look to see how fast that is, 458 ms .So yeah, that is somebody you just pointed out that I wrote the wrong thing here 1e-3. Okay, so how much faster is that? Well full 458 microseconds original on the whole data set was 663 microseconds. So compared to our broadcast version, we are another 1000 times faster. So overall, this version here, compared to our original version, which was here, the difference in performance is 5 million x , So when you say people say, Yeah, Python can be pretty slow, it can be better to run the stuff on the GPU if possible. We’re not talking about a 20% change, we’re talking about a 5 million x change. So that’s a big deal. And so that’s why you need to be running stuff on the GPU. All right. Some folks on YT are wondering how on earth I’m running cuda when I’m on a mac and given it sets localhost here, that’s because I’m using something called SSH tunneling, which we might get to sometime. I suspect my life coding from the previous course might have covered that already, but this is basically you can use a Jupyter notebook that’s running anywhere in the world from your own machine using something called SSH Tunneling, which is a good thing to look up a OK when a person asks if Einstein summation borrows anything from APL. Oh, yes, it does, actually. So it’s kind of the other way around. Actually. APL borrows it from Einstein notation. I don’t know if you remember I mentioned that Iverson, when he developed APL was heavily influenced by tensor analysis. And so this Einstein notation is very heavily used there. If you’ll notice a key thing that happens in Einstein notation is there’s no loop. You know, there isn’t this kind of sigma, you know, i from here to here and then you put the i inside the function that you’re summing up, everything’s implicit and APL takes that a very long way and, and J takes it even further, which is what Iverson developed after APL and this kind of general idea of of removing the index is very important in APL and it’s become very important in numpy PyTorch TensorFlow and so forth. All right. So finally we know how to multiply matrices. Congratulations. So let’s practice that. That’s practice what we’ve learned. So we’re going to go to zero two main shift to practice this. And so we’re going to try to exercise our kind of tensor manipulation operation muscles in this section. And the key actually endpoint for this is the homework. And so what you need to be doing is getting yourself to a point that you could implement something like this, but for a different algorithm, why do we care about this?\n\n%%timeit -n 10\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n\n10.9 ms ± 3.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nm1c,m2c = x_train.cuda(),weights.cuda()\n\n\nr=(m1c@m2c).cpu()\n\n\n%timeit -n 10 r=(m1c@m2c).cpu()\n\n2.27 ms ± 288 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "",
    "text": "What you should know and practice after this lecture: 1- Easily plot matrix that is not easily plottable  2- Broadcasting roles  3- Creating sample data  4- meanshift algorithm  5- You can use peresentify to draw on screen  6- Animation"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#create-data",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#create-data",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "Create data",
    "text": "Create data\n\nn_clusters=6\nn_samples =250\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = torch.rand(n_clusters, 2)*70-35\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\nSo how does sample work? Well, we’re passing in the centroid, and so what we want is we’re going to get back. So each of those centroid contains an X in a Y. So multivariate normal is just like normal. It’s going to give you back normally distributed data, but more than one item. That’s why it’s multivariate. And so we passed in two means a main for X and a mean for our Y. And so that’s the mean that we’re going to get. And our standard deviation is going to be five. Why do we use torch.diag(tensor([5.,5.])))? That’s because we’re saying that because that for multivariate normal distributions, there’s not just one standard deviation. Each column that you get back, there could also be a connection between columns. The columns might not be independent. So you actually need so it’s called a covariance matrix, not just to make, not just a variance. We discussed that a little bit more in lesson 9B if you’re interested in learning more about that. Okay, So this is something that’s going to give us back random columns of data with this mean and this standard deviation.\nAnd this is the number of samples that we want and this is coming from PyTorch. So PyTorch has a whole bunch of different distributions that you can use, which can be very handy. So there is our data. Okay. So remember, for sample clustering, we we don’t know the different colors and we don’t know where the X is. That’s kind of our job is to figure that out. We might just briefly also look at how to plot. So in this case, we want to plot the X s and we want to plot the data so it looks like this. So what I do is I look through each centroid and I grab that centroid samples and they’re just all done in order. So I grab it from in_samples: to (i+1)n_samples, and then I create a scatterplot with the samples on them. And what I’ve done is I’ve created an axis here and you’ll see y later that we can also pass one in. But I’m not passing one it. And so we create a plot and an axis. And so in that matplotlib, you can keep plotting things on the same axis. So then I plot on the centroid a big x, which is black, and then a smaller x, which is what is that magenta? And so that’s how I get these X’s. So that’s how plot data works. Okay, so how do we create something now that starts with all the dots and returns where the X is are ? We’re going to use a particular algorithm, particular clustering algorithm called meanshift.\n\ndef sample(m): return MultivariateNormal(m, torch.diag(tensor([5.,5.]))).sample((n_samples,))\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None):\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#mean-shift",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#mean-shift",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "Mean shift",
    "text": "Mean shift\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\nAnd and meanshift is a nice clustering approach because you don’t have to say how many clusters there are. So it’s not that often that you’re actually going to know how many clusters there are. So we don’t have to say quite a few things, like the very popular K means required to say how many in step. You just have to pass them in quite a bandwidth, which we’ll learn about, which can actually be chosen automatically. And it can also handle clusters of any shape so they don’t have to be bold shaped like that. But they are here. They can be kind of like L-shaped or ellipse shaped or whatever. And so here’s what’s going to happen. We’re going to pick some point. So let’s say we pick that point just there. Okay? And so what we now do is we go through each data point, so we pick the first one, and so we then find the distance between that point and every other point. Okay. So we’re going to have to say what is the distance between that point and that point? And point and that point and that point and that point and also the ones further away, that point and that point. And you do it for every single point compared to the one that we’re currently looking at. Okay. So we get all of those as a big list. And now what we’re going to do is we’re going to take a weighted average of all of those points. Now That’s not interesting without the weighting. If we just take our average of all of the points and how far away they are, we’re going to end up somewhere here, right? This is the average of all the points. But the key is that we’re going to take an average and find the right spot. The key is we need to find an average that is weighted by how far away things are.\nSo, for example, this one over here is a very long way away from our point of interest. And so it should have a very low weight and the weighted average where else this point here, which is very close, should have a very high weight in our weighted average. So What we do is we create weights for every point compared to the one that we’re currently interested in using a what’s called a Gaussian kernel that we’ll look at. But the key thing to know is that points that are further away from our point of interest, which is this one, are going to have lower weights. That’s what we mean, that they’re penalized. The rate at which weights for a zero is determined by this thing that we set at the start called the bandwidth. And that’s going to be the standard deviation of our Gaussian. So we take an average of all the points in the dataset, a weighted average weighted by how far away they are. So for our point of interest, right, the this point is going to get a big weight. This point is going to get a big weight. This point is going to get a big weight. That point is going to get a tiny weight.\nThat point is going to get an even tiny weight. So it’s mainly going to be a weighted average of these points at a nearby. And the weighted average of those points, I would guess, is going to be somewhere around about here. Right. And would have a similar thing for the weighted average of the points near this one. That’s going to probably be somewhere around about here or maybe over here. And so it’s going to move all of these points in closer. It’s almost like a gravity right. They’re kind of going to be moved like closer and closer in towards this kind of gravitational center. And then these ones will go towards their own gravitational center and so forth. Okay.\n\nmidp = data.mean(0)\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\nSo let’s take a look at it. All right. So what’s the gaussian kernel? This is the gaussian kernel, which was a sign in the original March for science back in the days when the idea of not following scientists was considered socially unacceptable. We used to have a March for these things, if you remember. So this is this is not normal. So this is the definition of the gaussian kernel, which is also known as the normal distribution. This is the shape of it. So you’ve seen it before. And here is that formula copied directly off the science match sign. Okay, here we see the square root, two pi, etc..\nSo here’s the definition of the gaussian kernel, which you may remember from high school… This person at the science march certainly remembered!\n\nOkay. And bw is the standard deviation. Now what does that look like? It’s very helpful to have something that we can very quickly plot any function that doesn’t come with matplotlib , but it’s very easy to write one. Just say, oh, let’s as X, let’s use all the numbers from 0 to 10, a hundred of them spaced evenly. That’s what linspace Does. it linearly spaced 100 numbers in this range. That’s going to be our Xs. So plot those Xs and plot F of X is the Ys. So here’s a very nice little plot_func, we want. And here it is. And as you can see here, we’ve now got something where if you are this like very close to the point of interest, you’re going to get a very high weight. And if you’re a long way away from the point of interest, you’ll get a very low weight. So that’s the key thing that we wanted to remember is something that penalizes further away points more. Now, you’ll notice here I’ve managed to plot this function for a bandwidth of 2.5, and the way I did that was using this special thing from functools(functools.partial), . Now, the first thing to point out here is that very often drives me crazy. I see people trying to find out what something is in Jupiter, and the way they do it is they’ll scroll up to the top of the notebook and search through the imports and try to find it. That is the dumb way to do it. The smart way to do it is just to type it and press shift enter and it’ll tell you where it comes from and you can get its help with Question Mark and you can get it also source code with two question marks. Okay, So just type it to find out where it comes from. Okay. So this is as Sylver mentioned in the chat, also known as carrying or partial function application. This creates a new function. So let’s just grab it. We create a new function. And this function F is is the function Gaussian, but it’s going to automatically pass. BW equals 2.5. So this is a partially applied function.\n\ndef gaussian(d, bw): return torch.exp(-0.5*((d/bw))**2) / (bw*math.sqrt(2*math.pi))\n\n\ndef plot_func(f):\n    x = torch.linspace(0,10,100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\nSo I could type f of four f(tensor(4.0)), for example, that’s going to be a tensor. There we go. And you can see that’s exactly what this is got to for across. Yep, about .44. So we use partial function application all the time. It’s a very, very, very important tool. Without it, for example, plotting this function would have been more complicated with it. It was trivially easy. I guess the alternative, like one alternative which would be fine but slightly more clunky, would be we could create a little function in line so we could have said, Oh, plot a function. Then I’m going to define right now, which is called lamb, which is lambda X, which is Gaussian and of X with a bandwidth of 2.5. You could do that too. You know, it’s it’s fine, but, but yeah, partials I think are a bit neater, a bit less to think about.\nThey often produce some nature and clearer code. Okay. Why did we decide to make the bandwidth 2.5 as a as a rule of thumb, choose a bandwidth which covers about a third of the data. So if we kind of found ourselves somewhere over here, write a bandwith which covers about a third of the data would be enough to cover two clusters ish. So it would be kind of like this big. So somewhere in the middle there. So that’s the basic idea. Yeah. So but you can play around with bandwidth and get different amounts of clusters. I should mention, like often when you see something that’s kind of on the complicated side, like a Gaussian, you can often simplify things. I think most implementations and write ups I’ve seen talk about using Gaussians, but if you look at the shape of it, it looks a lot like this shape. So this is a triangular weighting which is just using clamp_min So it’s just using a linear with clamp_min And yeah, it occurred to me that we could probably use this just as well. So I did find it.\nI decided to define this triangular weighting and then we can try both anyway. So I will start with we’re going to use the Gaussian version. All right. So we’re going to be move literally moving all the points towards the kind of center of gravity.\n\nf = partial(gaussian,bw=2.5)\n\n\nf(tensor(4.0))\n\ntensor(0.044)\n\n\n\npartial\n\nfunctools.partial\n\n\n\nplot_func(lambda x : gaussian(x,bw=2.5))\n\n\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i\n\n\nplot_func(partial(tri, i=8))\n\n\n\n\nSo we don’t want to mess up our original data so we clone it. It’s a PyTorch thing is .clone (data.clone()), it’s very handy. And so Big X is our matrix of data. I mean, it’s actually a That’s right. Matrix of data. Yeah. And then little x will be our first point. And it’s pretty common to use big X a capital letters for matrices. So this is our data. This is the first point.\nOkay. So there it is. We’re going to start at 26.2, 26.3. So 26.2, 26.3. So somewhere up here, so little x, its shape is just it’s a rank one tensor of shape two. Big X is a rank two tensor of 1500 data points by two, the X and Y. And if we call x[None], that would add a unit axis to that. And the reason I’m going to show you that is because we want to find the distance from little x to everything in Big X and the way we do a distance is with minus, but you wouldn’t be able to go, you wouldn’t be able to go X minus big X and get the right actually to you get the right answer. Let’s think about that X shape. Oh, we’ve got that already. I know actually that is going to work isn’t it? So, yes. All right. So you can see why we’ve got these two versions here. If we do x[None], we’ve got something of shape. One comma, two. Now we can subtract that from something, a shape 1500 comma two, because the twos match up because they’re the same and the 1500 and the one matches up because we remember our Numpy roles, everything matches up to a unit axis. So it’s going to copy this matrix across every of this matrix and it works.\nBut you remember there’s a special trick which is if you’ve got two shapes of different lengths, we can use the shorter length and it’s going to add unit axes to the front to make it as long as necessary. So we actually don’t need the x[None]. We can just use little x and it works because it’s going to say, is this compatible with this? Well, the last axis, remember we go right to left the last axis matches the second last axis, Oh, it doesn’t exist. So we pretend that there’s a unit axis, and so it’s going to do exactly the same thing as this. So if you have not studied the broadcasting from last week carefully, that might not have made a lot of sense to you. And so definitely at this point, you might want to pause the video and go back and reread the NumPy broadcasting rules from last time and practice them because that’s what we just did. We use numpy broadcasting rules and we’re going to be doing this dozens more times throughout the rest of the course and many more times, in fact, in this lesson.\n\nX = data.clone()\nx = data[0]\n\n\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape,X.shape,x[None].shape\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\nx-X\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        ...,\n        [-4.568, 17.025],\n        [-3.151, 22.389],\n        [-4.964, 21.040]])\n\n\n\n(x[None]-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\nHi, Dan, The thing I’m using to write on the screen is called presentify.It’s this thing here. It’s very cool. And a graphics tablet graphics tablets are quite cheap nowadays. Oh, excuse me. I use a cheap Chinese equivalent of a webcam on tablet. All right. Hi, everybody. Welcome back. So we had got to the point where we had managed to get the distance between our first point x and all of the other points in the data. And so just looking at the first eight of them here. So the very first instance is of course zero on the X axis and zero on the Y axis because it is the first point. The other thing is that because we the way we created the clusters is they’re all kind of next to each other in the list. So these are all in the first cluster. So none of them are too far away from each other. So now that we’ve got all the distances, it’s easy enough to, well, not the distances on X and Y, it’s easy enough to get the distance, the kind of Euclidean distance, so we can just square their difference and sum and square root. And actually maybe this is a good time to talk about norms and to talk about what we just did there. Um, so we’ve got all these data points. So here’s one of our data points and here’s the other one of our data points, and there’s some, you know, distance across the X axis and there’s some distance along the Y axis. So we could call that change in X and change in Y.\nAnd one way to think about this distance then is it’s this distance here. So to calculate that we couldn’t use Pythagoras, so a squared plus b squared equals C squared or in our case so this would be c, a, and b, so, so in our case it would be the square root of the change in X squared plus the change in Y squared. And rather than saying square root, we could say to the power of a half another way of saying the same thing. But there’s a different way we could find the distance. We could first go along here and then go up here. And so that one would be change in X, if you like, to the one plus change in Y to the one to the power of one one. Yeah, I got a slightly odd way for reasons you’ll see in a moment. It’s just this otherwise, in general, if we’ve got a whole list of numbers, we can add them up. Let’s say they’re some list V, we can add them up, we can do each one to the power of some number alpha and take that sum to the one over alpha. And this thing here is called a norm. So you might have remember we came across that last week and we come across it again this week. They basically come up, I don’t know, they might end up coming up every week.\nThey come up all the time, particularly because the two norm, which we could write like this or we could write like this or we could write like this, they’re all the two norm this is just saying it’s this equation for alpha equals two And Stefano is pointing out we should actually have an absolute value. I’m not going to worry about that. We’re just doing real numbers. So I keep things simple. Oh, I guess first higher than one. Now you’re probably right for something like three. Yeah, I guess we do need an absolute value there. That’s a good point because okay, we could have this one. And so the distance actually has to be the absolute value. So the change in x is the absolute value of that distance. Yes. Thank you, Stefano. Okay. So we’ll have the absolute value. Okay. So the two, norm, is what happens when every calls to and we would call this in this case, we would call this the Euclidean distance. But actually where it comes up more often is when you’re doing like a lost function.\nSo the mean squared error is just while the root means squared error, I should say, is just the two norm. Where else the mean absolute error is the one norm. And these are also known as L2 and L1. And remember what we saw in that paper last week. We saw it in this form. There’s a two up here which is where they got rid of the square root again. So would have just been a change in x squared plus change in Y squared. And now we don’t even need the parentheses. Okay, so all of this is to say that for, you know, this comes up all the time because we’re very, very often interested in distances and errors and things like that. I’m trying to think I don’t feel like I’ve ever seen anything other than one or two. So although it is a general concept, I don’t think we’re going to see probably things other than one or two in this course. I’d be excited if we do, that would be kind of cool. So here we’re taking the Euclidean distance, which is the two on. So this has got eight things in it because we’ve summed it over dimension one. So here’s your first homework is to rewrite using torch.einsum, you won’t be able to get rid of the x minus x. You’ll still need to have that in there.\nBut when you’ve got a multiply followed by a sum, now you want to get rid of the square root. Either you should be able to get rid of the multiply in the sum by doing it in a single torch.einsum. So we’re summing up over the first dimension, which is this dimension. So in other words, with summing up the X in the Y axis, okay, so now we can get the, the weights by passing those distance is into our gaussian. And so as we would expect, the biggest weights, it gets up 0.16. So the closest one is itself, it’s going to be at a big weight. These other ones get reasonable weights and the ones that are in totally different clusters have weights small enough that at three significant figures they appear to be zero. Okay, so we’ve got our weights. So there the weights are 1500 long vector and of course our original data is 1500 by two, the X and the Y for each one. So we now want a weighted average. We want this data, we want it’s average weighted by this. So normally an average is the sum of your data divided by the count. That’s a normal average weighted average item in your data. It’s let’s put some i’s around here. Just to be more clear, each item in your data is going to have a different weight. And so you multiply each one by the weights. And so rather than dividing by n, which is just the sum of ones, we would divide by the sum of weights. So is an important concept to be familiar with. Weighted averages. So we need to multiply every one of these x says by this.\n\n# rewrite using torch.einsum\ndist = ((x-X)**2).sum(1).sqrt()\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\nOkay, so can we say weights times X? No. All right. Why didn’t that work? So remember, we go right to left. So first of all, it’s going to say, let’s look at the two and multiply that by the 15. Are they compatible? Things are compatible if they’re equal or if at least one of them is one. These are not equal and they’re not one, so they’re not compatible. That’s why it says the size of a tensor a, must match. Now, when it says match, it doesn’t mean they have to be the same. One of them can be one. Okay. That’s what it means to match. They’re either or. One of them is one. So that doesn’t work. On the other hand, what if this was 1500 comma one? If it was 1500 comma one, then they would match because the one and the two match because one of them’s a unit axis and the 1500 and the 1500 match because they had the same. So that’s what we’re going to do because that would then copy this to every one of these, which is what we want. We want weights for each of these (x,y) tuples. So to add the trailing unit axis, we say every row and a trailing unit axis.(weight[:,None]*X) So that’s what that shape looks like. So we can then multiply that by x and as you can see, it’s now weighting each of them. And so each of these x’s and y is down the bottom, they’re all zero. So we can sum that up and then divide by the sum of weights. So let’s now write a function that puts all this together so you can see this really important way of like to me, the only way that makes sense to do a particularly scientific numerical programing.\nI actually do all my programing this way, but particularly scientific numerical programing is write it all out step by step, check every piece, have it all that documented for you and for others, and then copy the cells, merge them together and indent them to indent its control+right+spare bracket and put a function header on top. So here’s all those things we just did. And now, rather than just grabbing the first x, we enumerate through all of them. So that’s the distance we had before. That’s the weight we had before. There’s the product we had before. And then finally some across the rows divide by the sum of the weights. So that’s going to calculate for is It's going to move. So it's actually changing Capital X, so it's changing the is thing and capital X so that it’s now the weighted sum. Oh, actually sorry, the weighted average of all of the other data weighted by how far it is away. So that’s going to do a single step. So the main shift update is extremely straightforward, which is clone the data, iterate a few times and do the update. So if we run it, take 600 milliseconds. And what I’ve done is I’ve plotted the centroid moved by two pixels or two one up two pixels, two units so that you can see them and so you can see the dots is where our data is. And they’re dots now because every single data point is on top of each other on a cluster. And so you can see they are now in the correct spots. So it is successfully clustered our data.\n\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight[:,None].shape\n\ntorch.Size([1500, 1])\n\n\n\nweight[:,None]*X\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x-X)**2).sum(1))\n#         weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:,None]*X).sum(0)/weight.sum()\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n%time X=meanshift(data)\n\nCPU times: user 1.3 s, sys: 13.4 ms, total: 1.32 s\nWall time: 1.44 s\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\nSo that’s great news. And so we could test out our hypothesis. Could we use triangular just as well as we could have used Gaussian. So control slash comments and on comments, yeah, we got exactly the same results. So that’s good. It’s really important to know these keyboard shortcuts hit H to get a list of them. Some things that are really important don’t have keyboard shortcuts. So if you click help edit keyboard shortcuts. This list of all the things Jupyter can do and you can add keyboard shortcuts to things that don’t have them. So for example, I always add keyboard shortcuts to run all cells above and run all cells below. As you can see, I type Q and then A for above and Q and then B for below. All right. Now that was kind of boring in a way, because it did five steps, but we just saw the result. What did it look like? One step at a time. This isn’t just fun. It’s really important to be able to see things happening one step at a time because there are so many algorithms we do which are like updating weights or updating data, you know? So for stable diffusion, for example, very likely to want to show, you know, your incrementally denoising and so forth. So in my opinion, it’s important to know how to do animations. And I found the documentation for this unnecessarily complicated because it’s a lot of it’s about how to make them performant. But most of the time we probably don’t care too much about that. So I want to show you a little trick, a simple way to create animations without any trouble. So that matplotlib animation has something called FuncAnimation. That’s what we’re going to use to create an animation. You have to create a function and the function you’re going to be calling FuncAnimation passing in the name of that function and saying how many times to run it. And that’s what this frames the argument that says run this function this many times and then create an animation that that basically contains the result of that with a 500 millisecond interval between each one. So what’s this do one going to do to create one frame of animation? We will call our one_update.\nHere it is one_update, right? We’re going to call this that’s going to update our access and then we’re going to have an access which we’ve created here. So we’re going to clear whatever was on the plot before and plot our new data on that access. And then the only other thing you need to do is that the very first time it calls it, we want to plot it before running and d is going to be passed automatically the frame number. So for the zeroth frame, we’re going to not do the update, but it’s going to plot the data as it is already. I guess another way we could have done that would have been just to say if d then do the update the update, I suppose that should work too. Maybe it’s even simpler. Let’s see if I just break it. Okay So we’re going to clone our data. We’re going to create our figure in our subplots vertical FuncAnimation calling do_one 5 times, and then we’re going to display the animation. And so let’s see, so HTML takes some HTML and displays it and to_jshtml(), creates some HTML.\nSo that’s why it’s created. This HTML includes JavaScript. And so I click run one, two, three, four, five. That’s the five steps. So if I click loop, you’ll see them running again and again. Fantastic. So that’s how easy it is to create a matplotlib animation. So hopefully now you can use that to play around with some fun stable fusion animations as well. You don’t just have to use to to_jshtml. You can also create Oopsie Daisy. You can also create movies. For example. So you can call to_html5_video would be another option. And you can save an animation as a movie file. So this okay, all these different options for that, but hopefully that’s enough to get you started. So for your homework, I would like you when you create your k means or whatever, to try to create your own animation or create an animation of some stable diffusion thing that you’re playing with. So don’t forget this important ax.chear().without the ax.chear(), it prints it on top of the last one, which sometimes is what you want To be fair. But in this case, it’s not what I wanted. All right, So kind of slow half a second for not that much data, I’m sure would be nice. It was faster. Well, the good news is we can GPU accelerate it. The bad news is it’s not going to GPU You accelerate that Well, because of this loop, this is looping 1500 times. If we so looping is not going to run on the GPU."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#animation",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#animation",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "Animation",
    "text": "Animation\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\n\n# create your own animation\nX = data.clone()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nSo the best we could do with this would be to move all this to the GPU. Now the problem is that calling something the GPU 1500 times from Python is a really bad idea because there’s this kind of huge communication overhead of this of flow of control and data switching back between the CPU and the GPU. It’s the kernel launching overhead. It’s bad news. So you don’t want to have a really big fast python loop that inside it calls cuda code. GPU code. So we need to make all of this run without the loop, which we could do with broadcasting. So let’s roll up our sleeves and try to get the broadcast version of this working. So generally speaking, the way we tend to do things with broadcasting on a GPU is we create batches or mini batches. So to create batches or mini batches, we don’t just call them batches. Nowadays, we create a batch size. So let’s say we’re going to do a batch size of five, so we’re going to do five at a time. All right, so how do we do five at a time? This is only doing one at a time. How do we do five at a time last before it’s final data and this time little x for our testing. So I’ve got to do everything ahead of time. Little tests as we always do. This is not now X[0] anymore, but it’s X colon bs (X[:bs]), so it’s the first five. This is now the first five items. Okay, so little x is now a five by two metrics. This is how mini batch the first five items as before. Our data itself is 1500 by two. All right. So we need a distance calculation.\nBut previously our distance calculation, previously a distance calculation only worked if Little x was a single number and it returned just the distance is from that to everything in Big X. But we need something that’s actually going to be return a Matrix right. We’ve got let’s say we’ve got five by two in little x and then in big X we’ve got something much bigger not to scale, obviously we’ve got 1500 by two. And what is the distance between these two things? Well, if you think about it, there’s going to be a distance between item one and item one, but there’s also going to be a distance between item one, item two, and there’s going to be a distance between let’s use a different color for the next one, item two and item one, right? So the output of this is actually going to be a matrix. The distances are actually going to give us a matrix where I mean, it doesn’t matter which way around, we do what we can decide, but if we it this way around for each of the five things in the mini batch, there will be 1500 distances. The distance between every one. So we’re going to need to do a broadcasting to do this calculation. So this is a function that we’re going to create and it’s going to create this, as you can see, five by 1500 output. But let’s say how we get it. So can we do X minus x? No, we can’t. Why is that? That’s because big X is 1500 by two and little x is five by two.\nSo it’s going to look at remember our roles right to left these compatible? Yes they are They’re the same these compatible. No, they’re not. Okay. Because they’re different. So that’s not possible to do What if though we want it to What if we insert in big X and axis at the start here and in little x we add an axis in the middle here then now these are compatible because you’ve got they’re the same because I should use arrows really? They are compatible because one of them is a one. And these are compatible because one of them is a one as well. So they are all compatible. And what it’s going to do is it’s going to do the subtraction between these directly and it’s going to copy this across all 1500 rows. It will copy it. This is going to be copied and then this across five rows, and then this will be copied across these 1500 rows because what broadcasting does, it’s not really copying, but it’s effectively copying. And so that gives us it can now subtract them and that gives us what we wanted, which is five by 1500 and then also by two because there’s both the x and the y. So that’s why this works. That’s what this is doing here. It’s taking the subtraction, it’s squaring them, and then summing over that last shortest axis, summing over the X and the Y squids and then take square root. I don’t know why as it touched that square root, we could just put dot square root at the end. But same, same. In fact, it’s worth mentioning that. So most things that can do on tensors, you can either write torch. as a function or you can write it as a method. Generally speaking, both should be fine. Not everything, but most things work in both ways. Okay, so now we’ve got this matrix, which is five by 1500. And the nice thing is that our Gaussian kernel doesn’t actually have to be changed to get the weights, believe it or not. And the reason for that is now how do we get the source code? I could move back up there or I can just type Gaussian question mark, question mark and see it. And the nice thing is that this is just this is a scalar, so it broadcasts over anything and then this is also just a scalar. So this is all going to work fine without any fiddling around. Okay, so now we’ve got a 5, 1500 a weight. So that’s the weight for each of the five things. There are mini batch each of the 1500 things, each of them as compared to. And then we’ve got the shape of the data itself, X.shape, which is the 1500 points. So now we want to apply each one of these weights to each of these columns. So we need to add a unit access to the end set at a unit, access to the end, we could say colon, comma, colon, common, none, but dot, dot, dot means all of the axes up until however many you need. So in this case, the last one comma None[…,None]."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#gpu-batched-algorithm",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#gpu-batched-algorithm",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\nbs=5\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a,b): return (((a[None]-b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None,:]-x[:,None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[...,None].shape, X[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\nSo this is going to add an access to the end. So this is going to turn this is going to turn weight dot shape from five comma 1500 to 5 comma 1500 from a one. And this is going to add an access to the start. Remember, it’s the same as X[None] = X[None,:,:]. And so let’s check our rules left, right to left. These are compatible because one of them is one. These are compatible because they’re both the same. And these are compatible because one of them is one. Okay? So it’s going to be copying each weight across to each of the X and Y, which is what we want. We want to we want to weight both of those components and it’s going to copy each of the 1500 points sorry, each of the point five times, because we do in fact want to wait every one of the five things now, mini batches, the separate set of weights for each of them. So that sounds perfect. So that’s how I think through these calculations. Okay. So we can now do that multiplication, which is going to give us something of five by 1500 by two, because we end up with the maximum of our ranks. And then we sum up over those 1500 points and that’s going to give us now five new data points. Now, something that you might notice here is that we’ve got a product and a sum\n\nnum = (weight[...,None]*X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\nAnd when you say a product and a sum that tells you maybe we should use einsum. So in this case, we’ve got our weight, we’ve got five by 1500. So let’s call those i and j As for the five and 1500 we’ve got, the X is 1500 by two. Now we want to take the product of that and that so wanted to use the same name for this row. So he use j again. And then k is the number of rows, that’s the two. And then we want to end up with ik. So einsum, exactly the same result. That’s great. But you might recognize this. That’s exactly the einsum Something we had just before when we were doing matrix multiplication. Oh, that is a matrix multiplication. We’ve just re-invented matrix multiplication using this rather nifty. So we could also just use that. And so, you know, again, this is like what I was playing around with this morning as I started to look at this and I was thinking like, Oh, you know, can we simplify this? I don’t like this kind of like messing around of axes and summing over dimensions and whatnot. And so it’s nice to get things down to Einstein or better still, get down to matrix multipliers. It’s just clearer, you know, it’s stuff that we recognize because we use them all the time they all work performance would be pretty similar. I suspect. Okay, so now that we’ve got that, we then need to do our sum and we’ve got our five points. This is our five denominators. So we’ve got our numerator that we calculated up here for our weighted for our weighted average. The denominator is just the sum of the weights, remember.\n\ntorch.einsum('ij,jk->ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\nAnd so numerator, divided by denominator is our answer. So again, we’ve gone through every we’ve checked out all the dimensions all along the way. So nothing’s going to surprise us. Don’t try and write a function like this. Just bang from scratch. Right. You’re going to drive yourself crazy. Instead, do it step by step. So here’s our meanshift algorithm, clone the data, go through five iterations, and now go from 0 to n and batch size at a time. So Python has something called slices so we can create a slice of X starting at one up to i + batch size. Right. Unless it’s gone past, in which case use n. And so then we’re just copying and pasting each of the lines of code that we had before. Actually had us copy the cells and merge them. Of course I don’t actually copy and paste because it’s slow and boring and there’s my final step to create the new X[s]. And so notice here s is not a single thing. It’s a slice of things you might not have seen slice before, but this is just internally what Python’s doing when he is :, And it’s very convenient when you to use the same slice multiple times. Okay, so let’s do that using Cuda. I would run it first without cuda, but I mean, I’ve done all the steps before, so it should be fine so puppet on the GPU and run meanshift and let’s see how long that takes. It takes one millisecond and previously without GPU, it took 400 milliseconds. And you know, the other thing we should probably think about doing is looking at other batch sizes as well because now we’re looping over batches, right? So if we make the batch size bigger that for loop, it’s going to do less looping. So what if we make that 16? Will that be any faster?\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs,n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n#             weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight@X/div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\nOh, I see. Thank you. People on YouTube pointing out that I’m passing batch size, so I actually need to put it here. All right. So if we used a batch size of five, I wonder is missing. Oh, look at that. I’ve totally made it slow now. And in 57 milliseconds. Haha. Okay. 64, All right. Finally, that makes much more sense. Okay, so the bigger, bigger is better. And I guess we could actually do all 5000 at once. Probably nice. All right. Thank you YouTube friends, for solving that bizarre mystery. Okay. All right. So that’s pretty great. I mean, you know, to say that we can you optimize a meanshift like actually google for this to see if it’s been done before. And it’s the kind of thing that people, like write papers about. So I think it’s great that we can do it so easily with PyTorch. And it’s the kind of thing that previously had been considered, you know, a very challenging academic problem to solve. So maybe you can do something similar with some of these. Now, I haven’t told you what these are. So part of the homework is to go read about them and learn about them. dbscan, funnily enough, actually is an algorithm that I accidentally invented and then discovered a year later had already been invented.\nLSH comes up all the time, so that’s great. And in fact I have a strong feeling and I’ve been thinking about this for a while, that something like LSH could be used to speed this whole thing up a lot. Because if you think about it and again, maybe already this already exists, I don’t know. But if you think about it, when we did that distance calculation, the vast majority of the the weights or nearly zero. And so it seems pointless to create that big you know kind of eventually 1500 by 1500 matrix. That’s like it’d be much better if we just found the ones that were like pretty close by and just took their average. And so you want an optimized nearest neighbors, basically. And so this is an example of something that can give you a, an, a kind of a fast nearest neighbors algorithm or, you know, there are things like. kd trees and trees and stuff like that. So if you want to, like have a bonus bonus, invent a new meanshift algorithm which picks only the closest points to avoid the quadratic time. All right. So not very often you get an assignment, which is to invent a new meanshift algorithm, I guess a super, super bonus. Super, super bonus. Publish a paper that describes it. All right. You definitely get four points. If you do that, we’ll give you a number of points equal to the impact factor of the journal. You get it published in. Okay. So what I want to do now is move on to calculus, which for some of us may not be our favorite topic. Yeah, that’s funny. I found out that I in some version here already, I didn’t notice. Okay. Or is ahead of his time. That guy. Let’s talk about calculus. If you’re not super comfortable with derivatives and what they are and why we care. called The Essence of Calculus, which I strongly recommend watching. It’s just a pleasure, actually, to watch, as is everything that is on 3blue1brown.\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\n%timeit -n 5 _=meanshift(data, 1250).cpu()\n\n6.06 ms ± 746 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\nHomework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.\nBonus: Implement it in APL too!\nSuper bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.\nSuper super bonus: Publish a paper that describes it :D\nWhere do we start. So the good news is just like you don’t have to know much linear algebra at all, you basically just need to know about matrix multiplication. You also don’t need to know much calculus at all. Just derivatives. So let’s think about like what derivatives are. So I’m going to borrow actually the same starting point that when 3blue1brown uses one of their videos is to consider a car, and we’re going to see how far away from home it is at various time points. Okay. So after a minute, let’s say after a second, it’s traveled five meters and then after 2 seconds, it’s traveled ten meters. Okay. And after 3 seconds, you can probably guess it’s traveled 15 meters. So there’s this concept here of a I got it the wrong way around. Obviously. So time, distance. Okay. So there’s this concept of Yeah, of like location. It’s like how far how far if you traveled at a particular point in time. So we can look at one of these points and find out how far that car has gone. We could also take two points and we can say, where did it start at the start of those two points and where did it finish at the end of those two points. And we can say between those two points, how much time passed and how far did they travel? In 2 seconds. They traveled ten meters. So we could now also say, all right, well, the slope of something, let’s rise over, run. You’ll see.ten meters in 2 seconds. And notice we don’t just divide the numbers. We also divide the units. We get five meters per second. So this here is now changed the dimensions entirely. We’re now not looking at distance, but we’re looking at speed or velocity. And it’s equal to rise over run. It’s equal to the rate of change. And what it says really is as time the X-axis goes up by one second, what happens to the distance in meters as one second passes? How does the number of meters change?\nAnd so maybe these aren’t points at all. Maybe there’s a function that it’s a continuum of points, and so you can do that for the function. So the function is a function of time. Distance is a function of time. And so we could say, what’s the slope of that function? And we can get the slope from point A to point B using over run. So from T one to T two the amount of distance, that’s the amount of time that’s passed is T2 minus T1. That’s how much time has passed that say this is t one, this is t two and the distance that they’ve traveled while they’ve moved from wherever they are at the end to wherever they were at the start. So that’s the change in distance, divided by the change in time, Change in distance, divided by change in time. Okay, So that’s why. So another way. Now the thing is, when we talk about calculus, we talk about finding a slope, but we talk about finding a slope of something that’s more often more tricky than this, right? We have slopes of things that look more like this and we say, what’s this slope absent Terrible attempt at drawing? Let’s maybe put it over here because I’m left handed. What’s this slope now? What does it mean to have like the idea of a velocity at an exact moment in time? It doesn’t mean anything, you know, at an exact moment in time, you’re just like it’s frozen. Right? What’s happening exactly now? But what you can do is you can say, well, what’s the change in time between a bit before a point and bit after a point? And what’s the change in distance between a bit before our point and a bit after our point? And so you can do the same kind of rise over run the thing, right? But you can make that distance between T2 and T1 smaller and smaller and smaller. So let’s rewrite this in a slightly different way. Let’s call the denominator the distance between T1 plus a little bit I’ll call it d, it’s that minus T1. So this is T2 = T1+d, right?\nIt’s T1 pluss a little bit. So we say oh his T1. Let’s add a little bit and notice that we when we write it this well let’s actually, let’s do the rest of it. So now f52 becomes f of t one plus a little bit and this is the same. And now notice here that t one plus t minus t one. We can delete all that because it just comes out to d. So this is another way of calculating the slope of our function. And as you get smaller and smaller and smaller, we’re kind of getting a triangle that’s tinier and tinier and tinier and it still makes sense it still that some time has passed and the car has moved, right? But it’s just smaller and smaller amounts of time. Now, if you did calculus at that college or at school, you might have done all this stuff messing around with limits, Epsilon Delta and blah, blah, blah. I’ve got really good news. It turns out you can actually just think of this d as a really small number where d is the difference if, uh, it’s. And so when we calculate the slope, we can write it in a slightly different way as the change in dY divided by the change in dX, this here is the change in dY, and this here is the change in dX. And so in other words, this here is a very small number. A very small number, and this here is the result in the function of changing by that very small number. And this way of thinking about calculus is known as the calculus of in infinitesimal. and it’s how Leibniz its originally developed it. And it’s been turned into a whole theory nowadays. And the reason I talk about it here is because when we do calculus, you’ll see me doing stuff all the time where. I act like the dx is a really small number. And when I was at school I was told I wasn’t allowed to do that. I’ve since learned that it’s totally fine, do that.\nSo, for example, next lesson, we’re going to be looking at the chain role, which looks like this. The dy/dx equals to dy/du * du/xd And I’m just going to say, Oh, these two small numbers can cancel out. And that’s why obviously they’re the same thing and that’s all going to work out nicely. So what do you know? What would be very helpful would be if before the next lesson, if you’re not totally up to date with your, you know, remembering all the stuff you did in high school about calculus is watch the 3blue1brown Course, we are not going to be looking. I don’t think at all that integration. So you don’t have to worry about that. Also we are not going to on the whole be doing any derivatives by hand. So for example, there are rules such as to why the dy/dx, if y equals x squared is 2x, these kind of rules, you’re not really going to have to learn because PyTorch is going to do them all for you. The one that we care about is going to be the chain role that we’re going to learn about that next time. Okay. I hope I don’t get beaten to a bloody pulp the next time I walk into a mathematician’s conference, I suspect I might. But hopefully I get away with this. I think it’s safe. We’ll see how we go. So thanks, everybody very much for joining me and really look forward to seeing you next time where we’re going to do backpropagation from scratch. We’ve already learned to multiply matrices, so once we’ve got backpropagation as well, we’ll be ready to train a neural network. All right. Thanks So."
  }
]