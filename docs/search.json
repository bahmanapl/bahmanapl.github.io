[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bahman Sadeghi",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nWriting Stable Diffusion from Scratch 12\n\n\n35 min\n\n\n\n\n\nApr 4, 2023\n\n\nWriting Stable Diffusion from Scratch 11\n\n\n15 min\n\n\n\n\n\nApr 2, 2023\n\n\nWriting Stable Diffusion from Scratch 10\n\n\n34 min\n\n\n\n\n\nMar 29, 2023\n\n\nWriting Stable Diffusion from Scratch 9\n\n\n15 min\n\n\n\n\n\nMar 28, 2023\n\n\nWriting Stable Diffusion from Scratch 8\n\n\n23 min\n\n\n\n\n\nMar 27, 2023\n\n\nWriting Stable Diffusion from Scratch 7\n\n\n37 min\n\n\n\n\n\nMar 25, 2023\n\n\nWriting Stable Diffusion from Scratch 6\n\n\n19 min\n\n\n\n\n\nMar 23, 2023\n\n\nWriting Stable Diffusion from Scratch 5\n\n\n50 min\n\n\n\n\n\nMar 20, 2023\n\n\nWriting Stable Diffusion from Scratch 4\n\n\n51 min\n\n\n\n\n\nMar 19, 2023\n\n\nWriting Stable Diffusion from Scratch 3\n\n\n20 min\n\n\n\n\n\nMar 15, 2023\n\n\nWriting Stable Diffusion from Scratch 2\n\n\n41 min\n\n\n\n\n\nMar 6, 2023\n\n\nWriting stable diffusion from scratch\n\n\n37 min\n\n\n\n\nFeb 27, 2023\n\n\nWelcome To My Blog\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bahman Sadeghi",
    "section": "",
    "text": "Meet Bahman, a software developer passionate for exploring new cultures, languages and foods. With experience living in Asia, North America and Europe, He loves traveling and immersing himself in different ways of life. He considers North California his home. In his free time, you can find him delving into novels, watching captivating TV shows, and expanding his knowledge of deep learning. As a minimalist and follower of Stoic principles, Bahman approaches his personal and professional life with intention and purpose.This bio was written by the help of ChatGPT."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/mynotebook/index.html#matrix-multiplication-from-foundations",
    "href": "posts/mynotebook/index.html#matrix-multiplication-from-foundations",
    "title": "Bahman Sadeghi",
    "section": "\nMatrix multiplication from foundations\n",
    "text": "Matrix multiplication from foundations\n\n\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/mynotebook/index.html#get-data",
    "href": "posts/mynotebook/index.html#get-data",
    "title": "my notebook post",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Feb 22 13:37 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/mynotebook/index.html#matrix-and-tensor",
    "href": "posts/mynotebook/index.html#matrix-and-tensor",
    "title": "my notebook post",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use pytorch.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/mynotebook/index.html#random-numbers",
    "href": "posts/mynotebook/index.html#random-numbers",
    "title": "my notebook post",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn child: tensor([0.5702])In parent: tensor([0.5702])\n\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n21.4 ms ± 5.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\nThe slowest run took 4.18 times longer than the fastest. This could mean that an intermediate result is being cached.\n167 µs ± 116 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/mynotebook/index.html#matrix-multiplication",
    "href": "posts/mynotebook/index.html#matrix-multiplication",
    "title": "my notebook post",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nOkay, so let’s move on with our from the foundations now. And so we were working on trying to at least get the start of a forward pass of a linear model or a simple multi-layer perceptron for MNIST going. And we had successfully created a basic tensor. We’ve got some random numbers going. So what we now need to do is we now need to be able to multiply these things together, matrix multiplication. So matrix multiplication to remind you in this case. So we’re doing MNIST, right? So we’ve got about we’re going to use a subset, let’s see. Yeah, Okay. So we’re going to create a matrix called m1, which is just the first five digits, So m1 will be the first five digits. So five rows and. Well, dot, dot dot dot dot dot. And then 780. What is it again. because it’s 28 by 28 pixels and reflect that out. So this is our first matrix and matrix multiplication, and then we’re going to multiply that by some some weights. So the weights are going to be 784 by 10 random numbers. So for every one of thes 784 pixels, each one is going to have a weight. So 784 down here, so 94 by ten. So this first column, for example, is going to tell us all the weights in order to figure out if something’s a zero. And the second column will have all the weights in deciding of the probability of something. So one, so forth, assuming we just doing a linear model. And so then we’re going to multiply these two matrices together. So when we multiply matrices together, we take row one of matrix one and we take column one of matrix two and we take each one in turn. So we take this one and we take this one, we multiply them together and then we take this one and this one and we multiply them together. And we do that for every element wise pair, and then we add them all up and that would give us the value for the very first cell that would go in here. That’s what matrix multiplication is. Okay, so let’s go ahead and create our random numbers for the weights since we’re allowed to use random number generator now and for the bias, but just use a bunch of zeros to start with. So the bias is just what we’re going to add to each one. And so for our matrix multiplication, we’re going to be doing a little mini batch. I’m going to be doing five rows of, as we discussed, five rows of so five, five images flattened out and then multiplied by this weights matrix.\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\nSo here are the shapes and one is five by seven, eight four as we saw, and m2 is seven, eight, four by ten. Okay, so keep those in mind. So here’s a handy thing. And one touch shape contains two numbers and I want to pull them out. I want to call the I’m going to think of that as I’m going to actually think of this as like a and b rather than I wanted them to. So this is like a and b, so the number of rows in a and the number of columns in b, if I say equals and one shape that will put five in ar and 784 in ac, So I’ll notice I do this a lot, this restructuring, we talked about it last week too so can do the same for m2 dot shape, put that into b rows and b columns. And so now if I write out ar,ac and br , br , you can again see the same things from the sizes. So that’s a good way to kind of give us the stuff we have to look through. So here’s our results.\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nget matrix dimsions and put them in variables to make it readable for future looping.\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\nSo here’s our results. So our resultant tensor, well, we’re multiplying, we’re multiplying together all of these seven, eight, four things and adding them up. So the resultant tensor is going to be five by ten. And then each thing in here is the result of multiplying and adding So the result here is going to start with zeros and there is this is the result and it’s going to contain ar rows, five rows and bc columns, ten columns, five coma ten. Okay, so now we have to fill that in. And so to do a matrix multiplication, so we have to first we have to go through each row one at a time and here we have that go through each row one at a time and then go through each column one at a time. And then we have to go through each pair in that row column one at a time. So it’s going to be a loop in a loop in a So here’s quick over each row, and here we’re going to loop over each column and then here we’re going to loop so each column of c, and then here we’re going to leap over each column of a, which is going to be the same as the number of rows of b, which we can see here. I say ac or br they are seven, eight, four. They’re the same. So it wouldn’t matter whether we day, ac or br, so then our result for that row and that column, we have to add onto it the product of i,k in the first matrix by k,j in the second matrix. So k, it’s going up through those seven, eight, four. And so we’re going to go across the columns and down so across the rows and down the columns, it’s going to go across the row where it goes down this column. So here is the world’s most naive, slow, uninteresting matrix multiplication. And if we run it, okay, it’s done something we have successfully hopefully successfully multiplied the matrices m1 and m2.It’s a little hard to read this, I find because because punch cards used to be 80 columns wide. We still assume screens 80 columns wide. Everything defaults to 80 wide, which is ridiculous, but you can easily change it. So if you say sit print options, you can choose your own line width. Oh, you can say it’s five by ten. We did it before. So if we change the line width, okay, that’s much easier to rate. Now we can see here the five rows and here are the ten columns for that matrix multiplication. I tend to always put this at the top of my notebooks and you can do the same thing for numpy as well. So what I’d like to do this is really important is when I’m working on code, particularly numeric code, I like to do it all step by step and Jupiter. And then what I do is once I’ve got it working is a copy all the cells that have implemented that and I paste them and then I select them all and I hit shift+m to merge. Get rid of anything that prints out stuff I don’t need. And then I put a header on the top, give it a function name, and then I select the whole lot and I hit control or f right square bracket and I’ve turned it into a function, but I still keep the stuff above it. So I can see all the step by step stuff for learning about it later. And so that’s what I’ve done here to create this function. And so this function does exactly the same things we just did, and we can see how long it takes to run by using %time. And it took about half a second, which gosh, that’s a long time to generate such a small matrix. This is just to do five MNIST digits. So that’s not going to be great. We’re going to have to speed that up. I’m actually quite surprised at how slow that is because there’s only 39,200. So, you know, if you look at the how, we’ve got a loop within a loop within a loop, it’s doing 39,200 of these. So Python. Yeah, Python. When you’re just doing python, it is it is slow. So we can’t we can’t do that. That’s why we can’t just write Python.\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\nGo through each row one at a time (5), then each column one at a time (10) and then go through each pair(784). go accross the rows , down the column multiply and add. t1[i,j] += m1[i,k] * m2[k,j]\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nDefault is 80 columns wide because of punch cards and we still do that. (Talking about legacy and network effect , haha)\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\nThis is only to show data more readable.\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\nDo this on the top of the notebook and make it easier.\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\nFor numerical programming , Jeremy recommend doing stuff line by line , check the results and dimensions and then when it works , copy all the cells and paste them after those cell and select them all and hit shift+M to merge cells get ride of everything that prints out stuff you dont need put a header on the top (def ….), select the rest of the code and hit control + ] now you have the function. Keep the same none function code above to remember what did you do and how you get there.\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n%time _=matmul(m1, m2)\n\nCPU times: user 1.13 s, sys: 10.7 ms, total: 1.14 s\nWall time: 2.1 s\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/mynotebook/index.html#numba",
    "href": "posts/mynotebook/index.html#numba",
    "title": "my notebook post",
    "section": "Numba",
    "text": "Numba\nBut there is something that kind of lets this write Python we could instead use Numba.Numba is a system that takes python and turns it into basically into machine code and it’s amazingly easy to do. You can basically take a function and write and @ngit on top. And what it’s going to do is it’s going to look the first time you call this function, it’s going to compile it down to machine code and will run much more quickly. So what I’ve done here is I’ve taken the innermost loop. So just looping through and adding up all these. So I start at zero, go through and add up all those just two vectors and return it, which is called a dot product. And linear algebra, so call it dot and so Numba only works with numpy, it doesn’t work with PyTorch. So we’re just going to use arrays instead of tensers for a moment. Now have a look at this. If I try to do a dot product of one, two, three and two, three, four, it’s pretty easy to do. It took a fifth of a second, which sounds terrible, but the reason it took a fifth of a second is because that’s actually how long it took to compile this and run it. Now that it’s compiled the second time, it just has to call it it’s now 21 microseconds. And so that’s actually very fast. So with Numba we can basically make Python run at C speed. So now the important thing to recognize is if I replace this loop in Python with a called a dot which is running in machine code, then we now have one two loops running in python not three. So our 448 MS, let’s make sure if I run it, run that matmul that should be close to my t1 one. t1 is what we got before.\nAnd so when I’m refactoring or performance improving or whatever, I always like to put every step in the notebook and then test. So this test close comes from fastcore.test and it just checks. The two things are very similar. They might not be exactly the same because of floating point differences, which is fine. Okay, our matmul is working correctly, or at least it’s doing the same thing it did before. So if we now run it, it’s taking 268 micro second, versus 448 milliseconds. So it’s taking, you know, about 2000 times faster just by changing the one in my loop. So really all we’ve done is we’ve had @ngit to make it 2000 times faster, so Numba is well worth knowing about. I can make your Python code very, very fast. Okay, let’s keep making it faster. So we’re going to use stuff again, which kind of goes back APL. And a lot of people say that learning APL is the thing that’s taught them more about programing than anything else. So it’s probably worth considering learning APL And let’s just look at these various things. You got a is ten six minus four. So remember at APL, we don’t say equals, equals actually means equals. Funny enough we to say set two, we use this arrow and it’s, this is a list of ten, six, four and then b is 287. Okay. And we’re going to add them up a plus b, So what’s going on here? So it’s really important that you can think of a symbol like a as representing a tensor or an array. APL calls them arrays, pytorch call them tensors, Numpy calls them arrays. They’re the same thing. So this is a single that contains a bunch of numbers. This is a single thing that contains a bunch of numbers. This is an operation that applies to arrays or tensors. Now what it does is it works what’s called elsment-wise. It takes each pair ten and two, and that’s them together. Each pair six and eight, add them together. This is element wise addition and Fred is asking in the chat, how do you put in these symbols? If you just mouse over any of them, it will show you how to write it and the one you want is the one at the very bottom, which is the one where it says prefix. Now the prefix is the backtick character. So here it’s saying prefix hyphen gives us times. So we’ve had hyphen. So I’ve of a backtick dash b is a times b for example. So yeah, they all have shortcut keys which you learn pretty quickly. I find, and there’s a fairly consistent kind of system for those shortcut keys too. All right, So we can do the same thing in PyTorch.\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 562 ms, sys: 53 ms, total: 615 ms\nWall time: 1.75 s\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 37 µs, sys: 0 ns, total: 37 µs\nWall time: 41 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\nThis is the test.\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n2000 time faster. We change inner most loop.\n\n%timeit -n 50 matmul(m1a,m2a)\n\n1.24 ms ± 419 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nSo we can do the same thing in PyTorch. It’s a little bit more verbose. PyTorch, which is one reason I often like to do my mathematical fiddling around in APL. I can often do it with less boilerplate, which means I can spend more time thinking, you know, I can see everything on the screen at once. I don’t have to spend as much time trying to like ignore the tensor around bracket square bracket dot com, blah blah. It’s all cognitive load, which I’d rather ignore. But anyway, it does the same thing so I can say a plus b work exactly like APL. So here’s an interesting example. I can go a less than (a < b).float().mean(). So let’s try that one over here less than b. So this is a really important idea, which I think was invented by Ken Iverson, the APL guy, which is that true and false represented zero and one. And because they’re represented by zero and one, we can do things to them. We can add them up and subtract and so forth. That’s a really important idea. So in this case, I want to take the main of them, and I’m going to tell you something amazing, which is that in APL there is no function called mean. Why not? That’s because we can write the mean function, which so that’s four letters mean and we can write the mean function from scratch with four characters. I’ll show you. Here’s the whole mean function we’re going to create a function called mean, and the mean is equal to the sum of a list divided by the of the list. So this here is some divided by count. And so I have now to find a new function called mean, which calculates the mean, mean of a is less than b, there we go. And so, you know in practice, I’m not sure why people would even bother defining a function called mean because it’s just as easy to actually write it’s implementation in APL, in numpy or whatever a python. It’s going to take a lot more than four letters to implement mean. So anyway, you know, it’s a math notation and so being a math notation we can do a lot with little, which I find out folks, I can say everything going on at once anyway. Okay, so that’s how we do the same thing in pytouch. And again, you can say that the less than in both cases, operating element wise. Okay, So a is less than b is saying ten is less than two six is less than eight four is less than seven and gives us back each of those trues and faleses as zeros and onces and according to our YouTube chat, had just exploded as it should. This is why APL is. Yeah life changing."
  },
  {
    "objectID": "posts/mynotebook/index.html#elementwise-ops",
    "href": "posts/mynotebook/index.html#elementwise-ops",
    "title": "my notebook post",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\nElementwise addition\n\na + b\n\ntensor([12., 14.,  3.])\n\n\nCheck lecture for awesome implementation of mean.\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\nRank two tensor , aka Matrix.\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nOkay, let’s now go up to higher rank. So this here is a rank one tensor. So a rank one tensor means it’s a list of things, it’s a vector, it’s where else a rank two tensor. It’s like a list of lists. They all have to be the same length lists, or it’s like a rectangular bunch of numbers. And we call it in math, we call it a matrix. So this is how we can create a tensor containing one, two, three, four, five, six, 789. And you can see also, like, what I like to do is I want to print out the thing I just created after I created it. Two ways to do it. You can say put an enter and then write m and that’s going to do that. Or if you want to put it all in the same line, that works too. You just use a semicolon. Neither one is better than the other. They’re just different. So we could do the same thing in APL. Of course in APL it’s going to be much easier. So we’re going to define a matrix called m which is going to be a three by 3 by 3 tensor containing the numbers from 1 to 9. Okay. And there we go. That’s done it in APL, a three by three tensor containing the numbers from 1 to 9. A lot of these ideas from APL you’ll find have made their way into other programing languages. For example, if you use GO you might recognize this. This is the iota character and go uses the word iota to spell it out in a somewhat similar way. A lot of these ideas from APL have found themselves into math notation and in other languages. It’s been around since the late fifties. Okay, so here’s a bit of fun. We’re going to learn about a new thing that looks kind of crazy code for Frobenius Norm and we’ll use that from time to time as we’re doing modeling. And here’s a definition of a four Frobenius norm. It’s the sum over all of the rows and columns of a matrix, and we’re going to take each one and square it. They’re going to add them up and they’re going to take the square root.\nAnd so to implement that in pytouch is as simple as (m*m).sum().sqrt(). So this looks like a pretty complicated thing when you kind of look at it. At first it looks like a lot of squiggly business or if you said this thing here you might be like, what on earth is that? Well, now you know, it’s just a square, some square root. So again, we could do the same thing in APL. So let’s do so in APL. We want the okay, so we got a case called S.F. Now it’s interesting, Apple does this a little bit differently. So dot some by default in PyTorch sums over everything. And if you want to sum over just one dimension, you have to pass in a dimension keyword for very good reasons. APL is the opposite. It just comes across rows or just down columns. So actually we have to say sum up the flattened out version of the Matrix and say flattened out. He is comma. So his sum up the flattened out version of the Matrix. Okay, so that’s our S.F.. Oh, Oh, sorry. And the Matrix is meant to be m times m There you go. So that’s the same thing. Sum up the flattened out and by a matrix and another interesting thing about APL is it always is read right to left. There’s no such thing as operator precedence, which makes life a lot easier. Okay, then we take the square root of that. There isn’t a square root function, so we have to do to the power of 0.5 and there we go. Same thing. All right, You got the idea. Yes. A very interesting question here from Marabou or other bars for norm or absolute value. And I like answer, which is the norm, is the same as the absolute value for scalar. So in this case, you can think of it as absolute value and it’s kind of not needed because it’s being squared anyway. But yes, in this case the norm. Well, in every case for a scale, the norm is the absolute value, which is kind of a cute discovery when you realize it. So thank you for pointing that out. See the. All right. So this is just fiddling around a little bit to kind of get a sense of how these things work. So really importantly, you can index into a matrix and you’ll say rows first and then columns."
  },
  {
    "objectID": "posts/mynotebook/index.html#matmul-with-broadcasting",
    "href": "posts/mynotebook/index.html#matmul-with-broadcasting",
    "title": "my notebook post",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\nSo let’s just grab a single digit. So here’s the first digit. So its shape is it’s a 784 long vector. Okay. And remember that our weight matrix is 784 by ten. Okay. So if we say digit colon coma None dot shape, then that is a 784 by one row matrix. Okay. So there’s our matrix. And so if we then take that 784 by one and expandas m2, it’s going to be the same shape as our weight matrix. So it’s copied our image data for that digit across all of the ten vectors, representing the ten kind of linear projections we’re doing for our linear model. And so that means that we can take the digit colon comma a None so 784 by one and multiply it by the weights. And so that’s going to get us back 784 by 10 so what it’s doing, remember, is it’s basically looping through each of these 10, 784 long vectors. And for each one of them it’s multiplying it by this digit. So that’s exactly what we want to do in our matrix multiplication. So originally we had when I originally most recently I should say, we had this dot product where we were actually looping over j, which was the columns of b, So we don’t have to do that anymore because we can do it all at once by doing exactly what we just did so we can take the ith and all the columns and add a access to the end. And then just like we did here, multiply it by b and then .sum(). And so that is again exactly the same thing.\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n539 µs ± 221 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nThat is another metrics, multiplication, doing it using broadcasting. Now this is like a tricky to get your head around. And so if you haven’t done this kind of broadcasting before, it’s a really good time to pause the video and look carefully at each of these four cells before and understand what did I do there, Why did I do it? What am I showing you? And then experiment with trying to and so remember that we started with m1 zero, right? So just like we have here, a[i , so that’s why we’ve got, i comma colon comma None because this digit is actually m1 zero. So this is like m1 zero colon None. So this line is doing exactly the same thing as this here plus the sum. So let’s check if this matmul is the same as it used to be, yet still working and the speed of it. Okay, not bad. So 137 microseconds. So we’ve now gone from a time from 500 milliseconds to about point 1 milliseconds. Funnily enough, my MacBook Air is an m2, whereas this Mac mini is an m1 that’s a little bit slower. So my error it was a bit faster than 0.1 milliseconds. So overall we’ve got about a 5000 times speed improvement. So that is pretty exciting. And since it’s so fast now, there’s no need to use a mini batch anymore. If you remember, we used a mini batch of five images, but now we can actually use the whole dataset so fast. So now we can do the whole data set. There it is. We’ve now got 15,000 by ten, which is what we want. And so it’s taking us only 656 milliseconds now to do the whole dataset. So this is actually getting to a point now where we could start to create and train some simple models in a reasonable enough time. So that’s good news. All right. I think that’s probably a good time to take a break. We don’t have too much more of this to go, but I don’t want to keep you guys up too late. So hopefully you learned something interesting about broadcasting today. I cannot overemphasize how widely useful is in all deep learning in machine learning code. It comes up all the time. It’s basically our number one most critical kind of foundational operation. So, yeah, take your time practicing it. And also good luck with your diffusion homework from the first half of the lesson. Thanks for joining us and I’ll see you next time.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.54 s, sys: 2.08 ms, total: 1.54 s\nWall time: 1.76 s"
  },
  {
    "objectID": "posts/mynotebook/index.html",
    "href": "posts/mynotebook/index.html",
    "title": "my notebook post",
    "section": "",
    "text": "from pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/about/index.html",
    "href": "posts/about/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "1- Currently I am doing fast AI course part 2. So I do write about it twice a week. One post every mondays one post every Tuesdays.\n2- I am trying something I call 52 projects in 52 weeks. I will write about that too (Without schedule).\n3- Other stuff that not technical , I will write about it , stuff that interest me. (Without schedule)."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html",
    "href": "posts/Writing stable diffusion from scratch/index.html",
    "title": "Writing stable diffusion from scratch",
    "section": "",
    "text": "All credit goes to www.fast.ai. All mistakes are mine. In the foundation series, I only write about part of the lecture that is related to writing stable difussion from scratch.Jeremy also talked about the big picture of the stable diffusion model. I will write about those in the big picture series of my blog posts.Almost all of the stuff in the subtitle from lectures."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#matrix-multiplication-from-foundations",
    "href": "posts/Writing stable diffusion from scratch/index.html#matrix-multiplication-from-foundations",
    "title": "Writing stable diffusion from scratch",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\nIt’s going to require some serious tenacity and a certain amount of patience,but I think you’re going to learn a lot.A lot of folks I’ve spoken to have said thatprevious iterations of this of the course is like the best course they’ve ever done, and this one’s going to be dramatically better than any previous version we’ve done of this. So hopefully you’ll find that the the hard work and patience pays off. So the goal is to get to stable diffusion from the foundations, which means we have to define what are the foundations. So I have decided to define them as follows : We’re allowed to use Python, we’re allowed to use the Python standard library. So that’s all the stuff that comes with Python by default we’re allowed to use matplotlib because I couldn’t be bothered creating my own plotting library and are allowed to use Jupyter notebooks and nbdev, which is something that creates modules from notebooks. So basically what we’re going to try to do is to yeah, rebuild everything starting from this foundation. Now to be clear, what we are allowed to use are the libraries Once we have re-implemented them correctly. And so if we if we re-implement something from NumPy or from PyTorch or whatever, we’re then allowed to use the Numpy or PyTorch or whatever version, sometimes we’ll be creating things that haven’t been created before, and that’s then going to be becoming our own library and we’re going to be calling that Library Mini AI. So we’re going to be building our own little framework as we go.\nThe foundations we’ll assume throughout this course are:\n\nPython\nmatplotlib\nThe Python standard library\nJupyter notebooks and nbdev\n\nSo, for example, here are some inputs and these inputs all come from the Python standard library, except for these two. Now, to be clear, one challenge we have is that the models we use in stable diffusion, what trained on millions of dollars worth of equipment per month, which we don’t have the time or money. So another we’re going to do is we’re going to create smaller, identical, but smaller versions of them. And so once we’ve got them working, well, then be allowed to use the big Pre-Trained versions. So that’s the basic idea. So we’re going to have to end up with our own VIE our own UNIT, our own clip encoder and, so forth. To some degree. I am assuming that you’ve completed part one of the course to some degree I will cover everything, at least briefly. But if I cover something about deep learning too fast for you to know what’s going on and you get lost, go back and watch part one or go and, you know, Google for that term for stuff that we haven’t In part one, I will go over it very thoroughly and carefully. All right. So I’m going to assume that, you know, the basic idea that which is that we’re going to need to be doing some matrix multiplication.\nSo we’re going to try to take a deep dive into matrix multiplication today and we’re going to need some input data. And I quite like working with MNIST data, MNIST is hand-written digits. It’s a classic data set they are 28 by 28 pixel grayscale images and so we can download them from this URL. So we use the path libs path object a lot. It’s got part of Python and it basically takes a string and turns it into something that you can treat as a path. For example, you can use slash to mean this file inside this subdirectory. So this is how we create a path object path objects have for example a make directory mkdir method. So I like to get everything set up, but I want to be able to rerun this so lots of times and not have it like give me errors if I run it more than once, if I read a second time, it still works. And in that case that’s because I put this exist_ok = True. How did I know that? I can say because otherwise would try to make the directory. It would already exist in a given error. How do I know what parameters I can pass to make? I just press shift tab. And so when I hit shift tab, it tells me what options there. If I press it a few times, it’ll actually puppet down at the bottom of the screen. Just to remind me I can press escape to get rid of it. Or you can just or else you can just hit tab inside and it’ll list all the things you can type as parametors. As you can see. All right. So we need to grab this URL. And so Python comes with something for doing that, which is the URL lib library that’s part of Python that has something you I will retrieve and something which I’m always a bit surprised is not widely used as people are reading the Python documentation. So you should do that a lot. So if I click on that, here is the documentation for urlretrive, I will retrieve and so I can find exactly what it can take and I can learn about exactly what it does.So I yeah, I read the documentation from the Python docs for every single method I use and I look at every single option that it takes and then I practice with it and to practice with it, I practice inside Jupyter. So if I want this import on its own, I can hit control shift hyphen and it’s going to split it into two cells and then I’ll hit ALT + Enter or Option Enter so I can create something underneath so I can type urlretrieve shift tab. And so there it all is if I’m like way down somewhere in in the notebook and I have no idea where urlretrieve comes from, I can just hit shift enter and it actually tells me exactly where it comes from. And if I want to know more about it, I can just hit question mark shift enter and it’s going to give me documentation and most of all, second question mark and it gives me the full source code and you say it’s not a lot. You know, reading the source code of Python standard library stuff is often quite revealing and you can see exactly how they do it.\nWe use MNIST (hand-written digits) as our data. It is a classic 28 by 28 pixcel data set. http://yann.lecun.com/exdb/mnist/ We can download them from GitHub URL.\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#get-data",
    "href": "posts/Writing stable diffusion from scratch/index.html#get-data",
    "title": "Writing stable diffusion from scratch",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nThat’s a great way to learn more about more about this. So in this case, I’m just going to use a very simple functionality, which is I’m going to say the URL to retrieve and the file name to save it as and again, I’m made it so I can run this multiple times. So it’s only going to do the URL retrieve if the path doesn’t exist. If already downloaded it, I don’t want it downloaded again. So I run that cell and notice that I can put exclamation mark followed by a line of bash. And it actually runs this using bash. If you’re using windows, this this won’t work. And I would very, very strongly if you’re using Windows use WSL and if you used WSL, all of these notebooks will work perfectly. So yeah, do that. All right. It on paperspace or LambdaLabs or something like that, CoLab, etc..\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\nSo I run that cell and notice that I can put exclamation mark followed by a line of bash.\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar  6 09:39 mnist.pkl.gz\n\n\nOkay, so this is a zgip file. So thankfully, Python comes with a gzip module. Python comes with quite a lot actually. And so we can open a gzip file gzip.open and we can pass in the path and we say we’re going to read it as binary as opposed to text. Okay. So this is called a context manager. It’s it’s a width clause. And what it’s going to do is it’s going to open up this gzip file. The GC object will be called F and that it runs everything inside the the block. And when it’s done it will close the file. So with blocks can do all kinds of different things.\nBut in general, with blocks that involve files, it will going to close the file automatically for you. So We can now do that. And so you can see it’s opened up the gzip file and the gzip file contains what’s called pickle objects, pickled objects, It’s basically Python objects that are being saved to disk. It’s the main way that people in pure Python save stuff and it’s part of the standard library. So this is how we load in from that file. Now the file contains a couple of tuples, so when you put a tuple on the left hand side of an equal sign, it’s quite neat. It allows us to put the first couple into two variables called x_train, y_train and the second into x_valid, y_valid.And we’ve added this trick here where you put stuff like this on the left is called D structuring and it’s a super handy way to make your code kind of clear and concise. And lots of languages support that including Python.\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\nOkay, so we’ve now got some data and so we can have a look at it. Now it’s a bit tricky because we’re not allowed to use Numpy according to our rules, but unfortunately this actually comes as Numpy, so I’ve turned it into a list. All right. So I’ve taken the first image and I’ve turned it into a list. And so we can look at a few examples of some values in that list. And here they are. So it looks like the numbers between zero and one and this is what I do, you know, when I learn about a new dataset. So when I started writing this notebook, what you see here other than the pros here is, is what I actually did when I was working with this data. This I wanted to know what it was. So I just grab a little bit of it and look at it. So I kind of got a sense now of what it is now. Interestingly, this image is 784 long list.People already have people freaking out in the comments. No numpy. Yeah, the numpy. Do you say numpy then. NumPy. Why 784 ?What is that. Well that’s because he’s a 28 by 28 images. So it’s just a flat list here of 784 long. So do I turn this 784 long thing into 28 by 28. So I want to take a list of 28 lists of 28, basically because we don’t have matrices. So how do we do that? And so we’re going to be learning a lot of cool stuff in Python here.\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\nSorry, I got to start laughing at all the stuff in that chat. The people are quite reasonably freaking out. That’s okay. We’ll get there, I promise. I hope. Otherwise I’ll embarrass myself. All right, So how do I convert a 784 long list into 28 lists? I’m going to use something called chunks. And first of all, I’ll show you what this thing does and then I show you how it works. So vals is currently a list of ten things. So I take vals and I pass it to chunks with five, it creates two lists of five is list number one of five elements. And here’s list number two of five elements. Hopefully you can say it’s doing its chunk defying this list, and this is the length of each chunk. Now, how did you do that? The way I did it is using a very, very useful thing in Python that far too many people don’t know about, just called yield. And what it does is you can see here what a loop it’s going to go through from zero up to the length of my list and it’s going to jump by five at a time. That’s going to go, in this case, 0 to 5. And then it’s going to think of this as being like return for now, it’s going to return the list from zero up to five. So it returns the first bit of the list. But yield doesn’t just return. It kind of like returns a bit and then it continues and it returns a bit more. And so specifically, what yield does is it creates an iterator and iterator is an iterator is basically something you can actually just use it that you can call next on a bunch of times.So what is iterator? Well, iter it is something that I can basically I can call next on and next basically says yield the next thing. So this should yield vals[0,5]. There it is. It did write this vals[0,5]. Now, if I run that again, it’s going to give me a different answer because it’s now up to the second part of this loop. Now it returns the last five. Okay. So this is what a iterator does. Now, if you pass an iterator to Python’s List, it runs through the entire letter, iterater it until it’s finished and creates a list of the results. And what is finished? Looks like this is what finish looks like. If you call next and get stop iteration, that means you’ve run out. And that makes sense, right? Because my loop, there’s nothing left in it. So all of that is to say we now have a way of taking a list and chunkifying it. So what if I now take my full image? Image number one chunkify it into chunks of 28 long and turn that into a list and plot it that we have successfully created an image. So that’s good. Now we are done. But there are other ways to create this iterator. And because iterate is and generators which are closely related are so important. I wanted to show you more about how to do them in Python. It’s one of these things that if you understand this, you will often find that you can throw away huge pieces of enterprise software and basically replace it with an iterator that lets you stream things one bit at a time. It doesn’t store it all in memory. It’s this really powerful thing that once I often find, once I show it to people, they suddenly go like, Oh wow, I know we’ve been using all this third party software and we could have just created a python iterator. Python comes with a whole standard library module called itertools just to make it easier to work with iterators.\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\nIn order to convert 784 to 28*28 matrix, we use yeild and chunk function to do this. chunk going through the loop and go through whole list but jump 28 at a time. yeild return and continue. It create a iterator.\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nI’ll show you one example or something from it at all, such as islice. So let’s grab our values again. These ten values. Well, that was a mistake. I should not have called this iter. Let’s just do that again. Okay, so let’s take these ten values and we can any list and turn it into an iterator by passing it to iter, which I should call it that. So I don’t override this python. That’s not a keyword. But this thing, I don’t want to override. So this is now basically something that I can call. Actually, let’s do this. I’ll show you that. I can call next on it. So if I now go next.it you can see it’s giving me each item one at a time. Okay. So that’s what converting it into an iterator does. I slice convert it into a different kind of iterator? Let’s call this isislice iterator. I write a and so you can see here what it did was it jumped stop here. Yeah. So that’s what it being better so I should query create the iterator and then call next a few times. Sorry, this is what I meant to do. It’s now only returning the first five before it calls stop iteration before it raises stop iteration. So what I does is it grabs the first and things from an iterable, something that you can iterate. Why is that interesting? Because I can pass it to list. For example. Right? And now if I pass it to list again, this iterator has now grabbed the first five things. So it’s now up to thing number six. So if I call it again, it’s the next five things. And if I call it again, then there’s nothing left. And maybe you can see we’ve actually now got this defined, but we can do it with islice. And here’s how we can do it. It’s actually pretty tricky. iter in Python or you can pass it something like a list to create an iterator or you can pass it,this is a really important word, a callable. What’s a callable? A callable is generally speaking, it’s a function. It’s something that you can put parentheses after. Could even be a class, anything you can put parentheses after. You can just think of it for now as a function. So we’ve got a pass it a function and in the second form it’s going to be called until the function returns this value here, which in this case is empty list. And we just saw that islice will return empty list when it’s done. So this here is going to keep calling this function again and again and again. And we’ve seen exactly what happens because we caught it ourselves before. There it is. Until it gets an empty list. So if we do it with 28, then we’re going to get our image again. So we’ve now got two different ways of creating exactly the same thing.If you’ve never used iterate as before, now’s a good time to pause the video and play with them. Right? So for example, you can take this here, right? And if you’ve not seen Lambdas before, they’re exactly the same as functions, but you can define them in line. So let’s, let’s replace that with a function. Okay? So now I’ve turned it into a function and then you can experiment with it. So let’s create our iterator and call F on it.\nWell,F and you can say this the first 28 and each time I do it, I’m getting another 28. Now the first few rows are all empty. But finally, look, now I’ve got some values. Call it again. See how each time I’m getting something else. This calling it again and again. And that is the values in a iterator. So that gives you a sense of like how you can use Jupyter to experiment. So what you should do is as soon as you hit something in my code that doesn’t look familiar to you, I recommend pausing the video and experimenting with that in Jupyter. And for example, iter, Most people probably have not used it at all, and certainly very few people have use this to argument form so hit shift tab a few times and now you’ve got at the bottom to the description of what it is or find that more Python iter. Yeah go to the docs. Well that’s not the right but if the docs say API wow crazy that’s terrible let’s try searching here. Yeah okay iter that’s more like it. it so now you’ve got links so it’s like okay it returns an iterated object. What’s that. Well click on it find out that that’s really important to note is that stop exception that we saw so stop iteration exception we saw next already we can find out what iterable is. And here’s an example. And as you can see, it’s using exactly the same approach that we did. But here it’s being used to read from a file. This is really cool. Here’s how to read from a file 64 bytes at a time until you get nothing processing it right so that the docs the python are quite fantastic as long as you use them. If you don’t use them, they’re not very useful at all. And I say Safer in the comments : Our local Haskell programmer appreciating this Haskell illness in Python. So that’s good. It’s not quite Haskell, I’m afraid, but it’s the closest we’re going to come. All right, here we go for time. Pretty good.\n\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\nislice\nlets grab our 10 values and learn about islice from itertools module in python.\n\nfrom itertools import islice\n\nyou can call next in it and give you next 5 items. (5 in islice). So islice only return the first five (use next to understand this).\n\nit = iter(vals)\nisit = islice(it, 5)\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#matrix-and-tensor",
    "href": "posts/Writing stable diffusion from scratch/index.html#matrix-and-tensor",
    "title": "Writing stable diffusion from scratch",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\nOkay, so now that we’ve got image, which is a list of lists and each list is 25 long, we can index into it so we can say image 20. Well, let’s do it. Image 20. Okay. Is a list of 28 numbers and then we could index into that. Okay, so we can index into it. Now normally we don’t like to do that for matrices. We would normally rather write it like this : img[20,15] Okay, So that means we’re going to have to create our own class to make that work. So to create a class in Python, you’re write Class. And then you write the name of it and then you write some really weird things. The weird things you write have two underscore is a special word And then two underscore is these things with two underscores Each side are called dunder methods, and they’re all the special magically named methods which have particular meanings to Python, and you’re just going to let them. But they’re all documented in the Python object model. dunder init. No, it’s actually terrible search. We probably maybe need to look for object model. Then also absolutely terrible. All right. So maybe try Google Python and it object model. Yeah. Finally. Okay. So what’s your manually for. Oh it’s got data or not object model. And so this is basically where all the documentation is about absolutely everything and I can click done to edit and it tells you basically this is the thing that constructs objects. So any time you want to create a class that you want to, that you want a constructor that’s going to stores and stuff. So in this case it’s going to store image. You have to define dunder init. Python’s slightly weird in that every method you have to put self here for reasons we probably don’t really need to get into right now. And then any parameters. So we’re going to be creating image passing in the thing to store the x’s they’re going to be passing in the Xs. And so here we’re just going to store it inside the self. So once I’ve got this line of code, I’ve now got something that knows how to store stuff, the x’s inside itself. So now I want to be able to call square bracket 20 comma 15. So how do we do that? Well, basically part of the data model, this is a special thing called dunder getitem. And when you call square brackets on your object, that’s what Python uses and it’s going to pass across the [20,15]. Yeah that’s indices So we’re now but basically you’re going to return this so the self.x with the first index and the second index. So let’s create that matrix class and run that And you can now see m[20,15] is the same, quick note on, you know, ways in which my code is different to everybody else’s, which it is. It’s somewhat unusual to put definitions of methods on the same line as as the the signature like this. I do a quite a lot for one liners. As I mentioned before, I find it really helps me to be able to see all the code I’m working with on the screen at once. A lot of the world’s best program has actually had that approach as well. It seems to work quite well for some people that are extremely productive. It’s not common in Python, some people are quite against it. So if you’re at work and your colleagues don’t write Python this way, you probably shouldn’t either. But if you can get away with it, I think it works quite well anyway. Okay, so now that we’ve created something that lets us index into things like this, we’re allowed to use because we were allowed to use this one feature in PyTorch. Okay, so we can now do that. And so now to create a tensor, which is basically a lot like our matrix can now pass a list into tensor to get back that tensor version of that list. Or perhaps more interestingly, we could pass in a list of lists. Maybe this gives us a name.that needs to be a list of lists just like we had before. For our image. In fact, let’s do it for our image. Let’s just pass in our image. Yeah. Okay. And so now we should be able to say tens[20,15]. Okay, so we’ve successfully reinvented that. All right. So now we can convert all of our lists into tenses. There’s a convenient way to do this, which is to use the map function in the Python standard library. So shift tab map takes function, and then some iterables, in this case one iterable, and it’s going to apply this function to each of these four things and return those four things. And so then I can put four things on the left to receive those four things. So this is going to call tensor x_train and put it in a x_train so forth. So this is converting all of these lists to tensors and storing them back in the same name. So you can see that x_train now is a tensor. So that means it has a shape property. It 50,000 images in it which each 784 long and you can find out what kind of what kind of stuff it contains by calling x_train.type() . So it contains floats. So this is the tensor class we’ll be using a lot of it. So of course you should read its documentation in I don’t love the PyTorch documentation. Some of it’s good, some of it’s not good. It’s a bit all over the place. So here’s tensor, but it’s well worth scrolling through to get a sense of like this is actually not bad. Right? It tells you how you can construct it. This is how I constructed one before passing it lists of lists. You can also pass at Numpy Array. You can change types, so on and so forth. So you know, it’s well worth reading through and like you’re not going to look at every single method it takes, but you’re kind. If you browse through it, you’ll get a general sense, right? That tensors do just about everything you can think of for numeric programing. At some point you will want to know every single one of these, or at least be aware roughly what exists. So you know what to search for in the docs. Otherwise you will end up recreating stuff from scratch, which is much, much slower than simply reading the documentation to find out it’s there. All right. So instead of instead of calling chunks or islice the thing that is roughly equivalent in a tensor is the reshape method. So reshape. So the reshape our 15,000 by 784 thing we be able to turn it into a 50000,28 by 28 tensors. So I could write here reshape to 50,000 by 28 by 28. But I kind of don’t need to because I could just put -1 here and it can figure out that that must be 50,000, because it knows that I have 50,000 by 784 items so I can figure out. So -1 means just fill this with all the rest.\n\nimg[20]\n\n[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.09375,\n 0.4453125,\n 0.86328125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.78515625,\n 0.3046875,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0]\n\n\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use this one feature in pytorch.We do it cause it more look like math than original way.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\ntens = tensor(img)\n\n\ntens[20,15]\n\ntensor(0.9883)\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\nyou could also do this imgs = x_train.reshape((50000,28,28)), -1 is another way Jeremy prefer.\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\nOkay. Now what does the word tensor mean? So there’s some very interesting history here, and I’ll try not to get too far into it because I’m a bit overenthusiastic about this stuff. I must admit. I’m very, very interested in the history of tensor programing and array programing, and it basically goes back to a language called APL. APL is a basically originally a mathematical notation that was developed in the mid to late fifties, 1950s. And at first it was used to as a notation for defining how certain new IBM systems what would work. So it was all written out in this, in this notation, it’s kind of like a replacement for mathematical notation that was designed to be more consistent and and kind of more expressive in the early sixties. So the guy who wrote made it was called Kenneth E. Iverson.In the early sixties some implementations that actually allowed this notation to be executed on a computer appeared both in notation and the executable implementations. Slightly confusingly, both called APL. APL has been in constant development ever since that time, and today is one of the world’s most powerful programing languages. And you can try it by going to try APL. And why am I mentioning it here? Because one of the things Kenneth E. Iverson did well, he studied an area of physics called tensor analysis, and as he developed APL, he basically said like, Oh, what if we took these ideas from tensor analysis and put them into a programing language? So in yeah, in APL you, you and you know have been able to for some time can basically you can define a variable and rather than saying equals which is a terrible way to define things really mathematically because that has a very different meaning most of the time in math. Instead we use Arrow to define things. We can say, okay, that’s going to be a a tensor like so, and then we can look at their contents of a and we can do things like, Oh, what if we do a*3 or a-2 and so forth. And as you can see, what it’s doing is it’s taking all the contents of this tensor and it’s multiplying them all by three or subtracting two from all of them, or perhaps more fun we could put in to be a different tensor. And we can now do things like a divided by b, and you can see it’s taking each of a and dividing by each of b. Now, this is very interesting because now we don’t have to write loops anymore. We can just express things directly. We can multiply things by scales even if they’re this is called a rank one tensor. That is to say it’s basically a method called a vector. We can take two and can divide one by the other and so forth. It’s a really powerful idea. Funnily enough, APL didn’t call them tensor even though Kenneth E. Iverson said he got this idea from tensor analysis. APL calls them arrays. NumPy, which was heavily influenced by APL, also calls them arrays. For some reason PyTorch, which very heavily influenced by APL, so by numpy doesn’t call them arrays, it calls them tensors. They’re all the same thing. They are rectangular blocks of numbers. They can be one dimensional like a vector, they can be two dimensional, like a matrix, they can be three dimensional, which is like a bunch of stacked matrices, like a batch of matrices and so forth. If you are interested in APL, which I hope you are, we have a whole APL and a array programing section on our forums and also we’ve prepared a whole set of notes on every single glyph in APL, which also covers all kinds of interesting mathematical concepts like complex direction and magnitude and all kinds of fun stuff like that. That’s all totally optional, but a lot of people who do APL say that they feel like they’ve become a much better programmer in the process. And also you’ll find here at the forums a set of 17 study sessions of an hour or two each covering the entirety of the language, every single glyph. So that’s all like where this stuff comes from. So this, this batch of 50,000 images, is what we call a rank three tensor in PyTorch and in Numpy We would call it an array with three dimensions. Those are the same thing. So what is the rank? The rank is just the number of dimensions. It’s 50,000 images of 28 high by 28 wide. So there are three dimensions. That is the rank of the tensor. So if we then pick out a particular image right, then look at its shape. We could call this a matrix. It’s a 28 by 28 tensor, or we could call it a rank two tensor vector is a rank one tensor in APL, a scalar is a rank zero tensor, and that’s the way it should be. A lot of languages in libraries don’t unfortunately think of it that way. So what is a scalar is a bit dependent on the language. Okay, so we can index into the zeroth image,20 rows and 50s colomn,get back the same number. Okay. So we can take x_train.shape, which is 50,000 by 784 and you can destructure it into N, which is the number of images and C which is the number of the 4 number of columns, for example. And we can also well, this is actually part of the standard library. So reality is mean so we can find out in y_train what’s the smallest number and what’s the maximum number. So that goes from 0 to 9. So you see here it’s not just the number zero, it’s a scalar tensor zero. They act almost the same most of the time. So here’s some example of a bit of the the y_train. So you can see these are basically this is going to be the labels, right? These are our digits and this is its shape. So this is 50,000 of these labels. Okay. And so since we’re allowed to use this in the standard library, well, it also exists in PyTorch. So that means we’re also allowed to use torch.min() and torch.max() properties. All right. So before we wrap up, we’re going to do one more thing. And I don’t know what the we would call kind of anti cheating, but according to our rules, we’re allowed to use random numbers because there is a random number generator in the Python standard library. But we’re going to do random numbers from scratch ourselves. And the reason we’re going to do that is even though according to the rules, we could be allowed to use the standard library one, it’s actually extremely instructive to build our own random number generator from scratch well, at least I think so. Let’s see what you think. So there is no way normally in software to create a random number.\n\nplt.imshow(imgs[0]);\n\n\n\n\n\nplt.imshow(imgs[3]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))\n\n\nHere we go further and broke our rule and make it even harder so we gonna do random number from scratch. We use sodu random number generator."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch/index.html#random-numbers",
    "href": "posts/Writing stable diffusion from scratch/index.html#random-numbers",
    "title": "Writing stable diffusion from scratch",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\nSo there is no way normally in software to create a random number. Unfortunately, computers, you know, add, subtract, times, logic gates, stuff like that. So how does one create random numbers? Well, you could go to the Australian National University Quantum random number generator, and this looks at the quantum fluctuations of the vacuum and provides an API which will actually hook you in and return quantum random fluctuations of the vacuum. So that’s about that’s the most random thing I’m aware of. So that would be one way to get random numbers and there’s actually an API for that. So that’s a bit of fun. You could do what Cloudflare does. Cloudflare has a huge wall full of larva lamps and it uses the pixels of a camera looking at those larva lamps to generate random numbers. Intel nowadays actually has something in its chips which you can call RDRAND, which will return random numbers on certain intel chips from 2012. All of these things are kind of slow. They can kind of get you one random number from time to time. We want some way of getting lots and lots of random numbers. And so what we do is we use something called a pseudo random number generator. A pseudo random number generator is a mathematical function that you can call lots of times, and each time you call it, it will give you a number that looks random to show you what I mean by that, I’m going to run some code and I’ve created a function which will look at the moment called rand. And if I call rand 50 times and plot it, there’s no obvious relationship between one call and the next. That’s one thing that I would expect to see from my random numbers. I would expect that each time I call rand, the numbers would look quite different to each other. The second thing is rand is meant to be returning uniformly distributed random numbers, and therefore if I call it lots and lots and lots of times and plot its, histogram, I would expect see exactly this, which is each from zero to point one. There’s a few from point one, 2.2, there’s a few 1.2, 2.3. That’s true. It’s a fairly evenly spread thing. These are the two key things I would expect to see an even distribution of random numbers and that there’s no correlation or no obvious correlation from one to the other. So we’re going to try and create a function that has these properties. We’re not going to derive it from scratch. I’m just going to tell you that we have a function here called the Wickman Hill algorithm. This is actually what Python used to use back in before Python 2.3. And the key reason we need to know about this is to understand really well the idea of random state. Random state is a global variable. It’s something which is, or at least it can be. Most of the time when we use it, we use it as a random variable and it’s just basically one or more numbers. So we’re going to start with no random state at all. I’m going to create a function called seed that we’re going to pass something to, and I just smashed the keyboard to create this number. Okay So this is my random number. You could get this from the and quantum vacuum generator or from cloudflare’s larva lamps or from your intel chips RDRAND. You know, in python land, which pretty much always is a number 42, any of those are fine. So you pass in some number or you can pass in the current tick count in nanosecond as there’s various ways of getting some random starting point. And if we pass it into seed, it’s going to do a bunch of modular divisions and create a tupple of three things, and it’s going to store them in this global state. So Rand State now contains three numbers. Okay, so why do we do that? The reason we did that is because now this function, which takes our random state, unpacks it into three things and does again a bunch of multiplications and modules and then sticks and together with various kind of weights, modulo one. So this is how you can pull out the decimal part. This returns random numbers. But the key thing I want you to understand is that we pull out the random state at the start. We do math thingies to it and then we store new random state. And so that means each time I call this, I’m going to get a different number, right? So this is a random number generator. And this is really important because lots of people in the deep learning world screw this up, including me. Sometimes, which is to remember that random number generators rely on the state. So let me show you where that will get you if you’re not careful. If we use a special thing called fork that creates a whole separate copy of this python process in one copy or.fork() returns true and in the other copy it returns false, roughly speaking. So this copy here is this. If I say this, this version here the true version is the original none copied. It’s called the parent and so on. My else here this. So this will only be called by the parent. This will only be called by the copy. It is called the child. And each one I’m calling rand. These are two different random numbers right? Wrong. Yeah. The same number. And why is that? That’s because this process here and this process here are copies of each other, and therefore they each contain the same numbers in random state. So this is something that comes up in deep learning all the time, because in deep learning we often do parallel processing, for example, to generate lots of augmented images at the same time using multiple processes fast.ai used to have a bug in fact, where we failed to correctly initialize the random number generator separately in each process.\nAnd in fact, to this day, at least as as of October 2022 torch.rand by default fails to initialize the random number generator. That’s the same number. Okay, so you got to be careful now. I have a feeling numpy gets it right. Let’s check import Numpy As an np. Okay. And so I don’t then I can’t remember which right now. Okay. NumPy also doesn’t have interesting. What about python. Right. And look at that. So python does actually remember to re initialize the random state and each fork. So you know this is something that like even if you’ve experimented in Python and you think everything’s working well in your data loader or whatever and you switch to PyTorch or numpy and now suddenly everything’s broken. So this is why we’ve spent some time re-implementing random, the random number generator from scratch, partly because it’s fun and interesting and partly because it’s important that you now understand that when you’re calling rand or any random number generator kind the default versions in numpy and PyTorch, this global state is going to be copied. So you’ve got to be a bit careful. Now I will mention our random number generator. Okay, So this is this is called percent timeout percent is a special Jupyter or Ipython function and percent time. It runs a piece of Python code this many times. So to call it ten times. Well, actually today seven loops and each one will be seven times and I’ll take the maiden standard deviation. So here I am going to generate random numbers, long chunks, and if I run that, it takes me 3 milliseconds. Like if I run it using PyTorch, this is the exact same thing in PyTorch. It’s going to take me 73 micro second. So as you can see, although we could use our version, we’re not going to because the PyTorch version is much, much faster. This is how we can create a 784 by ten. And why would we want this? That’s because this is our final layer of our neural net. Or if we’re doing a linear classifier, a linear weights want it to be 784 because that’s 28 by 28 by ten, because that’s the number of possible outputs, the number of possible digits. All right. That is it. So quite the intense lesson. I think we can all agree should keep you busy for a week. And thanks very much for joining and see you next time. Bye everybody.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\nBe carefull when you use Pytorch or Numpy\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.2706])\nIn child: tensor([0.2706])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n8.57 ms ± 368 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\n135 µs ± 54.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html",
    "href": "posts/Writing stable diffusion from scratch 2/index.html",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "",
    "text": "Important stuff you need to know after this lecture:\n1- Matrix Multiplication  2- Numba, how to compile python code to machine code  3- Frobenius Norm  4- Braodcasting Rules  5- expand_as , stride  6- unsqueeze , c[ None , : ] ,c[ : , None] 7- c[None], c[…,None] \n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#get-data",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#get-data",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar 15 09:50 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#matrix-and-tensor",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#matrix-and-tensor",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use pytorch.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#random-numbers",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#random-numbers",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.2262])\nIn child: tensor([0.2262])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n5.39 ms ± 223 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\n86.7 µs ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#matrix-multiplication",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#matrix-multiplication",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nOkay, so let’s move on with our from the foundations now. And so we were working on trying to at least get the start of a forward pass of a linear model or a simple multi-layer perceptron for MNIST going. And we had successfully created a basic tensor. We’ve got some random numbers going. So what we now need to do is we now need to be able to multiply these things together, matrix multiplication. So matrix multiplication to remind you in this case. So we’re doing MNIST, right? So we’ve got about we’re going to use a subset, let’s see. Yeah, Okay. So we’re going to create a matrix called m1, which is just the first five digits, So m1 will be the first five digits. So five rows and. Well, dot, dot dot dot dot dot. And then 780. What is it again. because it’s 28 by 28 pixels and reflect that out. So this is our first matrix and matrix multiplication, and then we’re going to multiply that by some some weights. So the weights are going to be 784 by 10 random numbers. So for every one of thes 784 pixels, each one is going to have a weight. So 784 down here, so 94 by ten. So this first column, for example, is going to tell us all the weights in order to figure out if something’s a zero. And the second column will have all the weights in deciding of the probability of something. So one, so forth, assuming we just doing a linear model. And so then we’re going to multiply these two matrices together. So when we multiply matrices together, we take row one of matrix one and we take column one of matrix two and we take each one in turn. So we take this one and we take this one, we multiply them together and then we take this one and this one and we multiply them together. And we do that for every element wise pair, and then we add them all up and that would give us the value for the very first cell that would go in here. That’s what matrix multiplication is. Okay, so let’s go ahead and create our random numbers for the weights since we’re allowed to use random number generator now and for the bias, but just use a bunch of zeros to start with. So the bias is just what we’re going to add to each one. And so for our matrix multiplication, we’re going to be doing a little mini batch. I’m going to be doing five rows of, as we discussed, five rows of so five, five images flattened out and then multiplied by this weights matrix.\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\nSo here are the shapes and one is five by seven, eight four as we saw, and m2 is seven, eight, four by ten. Okay, so keep those in mind. So here’s a handy thing. And one touch shape contains two numbers and I want to pull them out. I want to call the I’m going to think of that as I’m going to actually think of this as like a and b rather than I wanted them to. So this is like a and b, so the number of rows in a and the number of columns in b, if I say equals and one shape that will put five in ar and 784 in ac, So I’ll notice I do this a lot, this restructuring, we talked about it last week too so can do the same for m2 dot shape, put that into b rows and b columns. And so now if I write out ar,ac and br , br , you can again see the same things from the sizes. So that’s a good way to kind of give us the stuff we have to look through. So here’s our results.\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nget matrix dimsions and put them in variables to make it readable for future looping.\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\nSo here’s our results. So our resultant tensor, well, we’re multiplying, we’re multiplying together all of these seven, eight, four things and adding them up. So the resultant tensor is going to be five by ten. And then each thing in here is the result of multiplying and adding So the result here is going to start with zeros and there is this is the result and it’s going to contain ar rows, five rows and bc columns, ten columns, five coma ten. Okay, so now we have to fill that in. And so to do a matrix multiplication, so we have to first we have to go through each row one at a time and here we have that go through each row one at a time and then go through each column one at a time. And then we have to go through each pair in that row column one at a time. So it’s going to be a loop in a loop in a So here’s quick over each row, and here we’re going to loop over each column and then here we’re going to loop so each column of c, and then here we’re going to leap over each column of a, which is going to be the same as the number of rows of b, which we can see here. I say ac or br they are seven, eight, four. They’re the same. So it wouldn’t matter whether we day, ac or br, so then our result for that row and that column, we have to add onto it the product of i,k in the first matrix by k,j in the second matrix. So k, it’s going up through those seven, eight, four. And so we’re going to go across the columns and down so across the rows and down the columns, it’s going to go across the row where it goes down this column. So here is the world’s most naive, slow, uninteresting matrix multiplication. And if we run it, okay, it’s done something we have successfully hopefully successfully multiplied the matrices m1 and m2.It’s a little hard to read this, I find because because punch cards used to be 80 columns wide. We still assume screens 80 columns wide. Everything defaults to 80 wide, which is ridiculous, but you can easily change it. So if you say sit print options, you can choose your own line width. Oh, you can say it’s five by ten. We did it before. So if we change the line width, okay, that’s much easier to rate. Now we can see here the five rows and here are the ten columns for that matrix multiplication. I tend to always put this at the top of my notebooks and you can do the same thing for numpy as well. So what I’d like to do this is really important is when I’m working on code, particularly numeric code, I like to do it all step by step and Jupiter. And then what I do is once I’ve got it working is a copy all the cells that have implemented that and I paste them and then I select them all and I hit shift+m to merge. Get rid of anything that prints out stuff I don’t need. And then I put a header on the top, give it a function name, and then I select the whole lot and I hit control or f right square bracket and I’ve turned it into a function, but I still keep the stuff above it. So I can see all the step by step stuff for learning about it later. And so that’s what I’ve done here to create this function. And so this function does exactly the same things we just did, and we can see how long it takes to run by using %time. And it took about half a second, which gosh, that’s a long time to generate such a small matrix. This is just to do five MNIST digits. So that’s not going to be great. We’re going to have to speed that up. I’m actually quite surprised at how slow that is because there’s only 39,200. So, you know, if you look at the how, we’ve got a loop within a loop within a loop, it’s doing 39,200 of these. So Python. Yeah, Python. When you’re just doing python, it is it is slow. So we can’t we can’t do that. That’s why we can’t just write Python.\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\nGo through each row one at a time (5), then each column one at a time (10) and then go through each pair(784). go accross the rows , down the column multiply and add. t1[i,j] += m1[i,k] * m2[k,j]\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nDefault is 80 columns wide because of punch cards and we still do that. (Talking about legacy and network effect , haha)\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\nThis is only to show data more readable.\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\nDo this on the top of the notebook and make it easier.\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\nFor numerical programming , Jeremy recommend doing stuff line by line , check the results and dimensions and then when it works , copy all the cells and paste them after those cell and select them all and hit shift+M to merge cells get ride of everything that prints out stuff you dont need put a header on the top (def ….), select the rest of the code and hit control + ] now you have the function. Keep the same none function code above to remember what did you do and how you get there.\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n%time _=matmul(m1, m2)\n\nCPU times: user 771 ms, sys: 748 µs, total: 772 ms\nWall time: 774 ms\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#numba",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#numba",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Numba",
    "text": "Numba\nBut there is something that kind of lets this write Python we could instead use Numba.Numba is a system that takes python and turns it into basically into machine code and it’s amazingly easy to do. You can basically take a function and write and @ngit on top. And what it’s going to do is it’s going to look the first time you call this function, it’s going to compile it down to machine code and will run much more quickly. So what I’ve done here is I’ve taken the innermost loop. So just looping through and adding up all these. So I start at zero, go through and add up all those just two vectors and return it, which is called a dot product. And linear algebra, so call it dot and so Numba only works with numpy, it doesn’t work with PyTorch. So we’re just going to use arrays instead of tensers for a moment. Now have a look at this. If I try to do a dot product of one, two, three and two, three, four, it’s pretty easy to do. It took a fifth of a second, which sounds terrible, but the reason it took a fifth of a second is because that’s actually how long it took to compile this and run it. Now that it’s compiled the second time, it just has to call it it’s now 21 microseconds. And so that’s actually very fast. So with Numba we can basically make Python run at C speed. So now the important thing to recognize is if I replace this loop in Python with a called a dot which is running in machine code, then we now have one two loops running in python not three. So our 448 MS, let’s make sure if I run it, run that matmul that should be close to my t1 one. t1 is what we got before.\nAnd so when I’m refactoring or performance improving or whatever, I always like to put every step in the notebook and then test. So this test close comes from fastcore.test and it just checks. The two things are very similar. They might not be exactly the same because of floating point differences, which is fine. Okay, our matmul is working correctly, or at least it’s doing the same thing it did before. So if we now run it, it’s taking 268 micro second, versus 448 milliseconds. So it’s taking, you know, about 2000 times faster just by changing the one in my loop. So really all we’ve done is we’ve had @ngit to make it 2000 times faster, so Numba is well worth knowing about. I can make your Python code very, very fast. Okay, let’s keep making it faster. So we’re going to use stuff again, which kind of goes back APL. And a lot of people say that learning APL is the thing that’s taught them more about programing than anything else. So it’s probably worth considering learning APL And let’s just look at these various things. You got a is ten six minus four. So remember at APL, we don’t say equals, equals actually means equals. Funny enough we to say set two, we use this arrow and it’s, this is a list of ten, six, four and then b is 287. Okay. And we’re going to add them up a plus b, So what’s going on here? So it’s really important that you can think of a symbol like a as representing a tensor or an array. APL calls them arrays, pytorch call them tensors, Numpy calls them arrays. They’re the same thing. So this is a single that contains a bunch of numbers. This is a single thing that contains a bunch of numbers. This is an operation that applies to arrays or tensors. Now what it does is it works what’s called elsment-wise. It takes each pair ten and two, and that’s them together. Each pair six and eight, add them together. This is element wise addition and Fred is asking in the chat, how do you put in these symbols? If you just mouse over any of them, it will show you how to write it and the one you want is the one at the very bottom, which is the one where it says prefix. Now the prefix is the backtick character. So here it’s saying prefix hyphen gives us times. So we’ve had hyphen. So I’ve of a backtick dash b is a times b for example. So yeah, they all have shortcut keys which you learn pretty quickly. I find, and there’s a fairly consistent kind of system for those shortcut keys too. All right, So we can do the same thing in PyTorch.\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 479 ms, sys: 145 ms, total: 624 ms\nWall time: 563 ms\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 34 µs, sys: 6 µs, total: 40 µs\nWall time: 44.3 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\nThis is the test.\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n2000 time faster. We change inner most loop.\n\n%timeit -n 50 matmul(m1a,m2a)\n\n455 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nSo we can do the same thing in PyTorch. It’s a little bit more verbose. PyTorch, which is one reason I often like to do my mathematical fiddling around in APL. I can often do it with less boilerplate, which means I can spend more time thinking, you know, I can see everything on the screen at once. I don’t have to spend as much time trying to like ignore the tensor around bracket square bracket dot com, blah blah. It’s all cognitive load, which I’d rather ignore. But anyway, it does the same thing so I can say a plus b work exactly like APL. So here’s an interesting example. I can go a less than (a < b).float().mean(). So let’s try that one over here less than b. So this is a really important idea, which I think was invented by Ken Iverson, the APL guy, which is that true and false represented zero and one. And because they’re represented by zero and one, we can do things to them. We can add them up and subtract and so forth. That’s a really important idea. So in this case, I want to take the main of them, and I’m going to tell you something amazing, which is that in APL there is no function called mean. Why not? That’s because we can write the mean function, which so that’s four letters mean and we can write the mean function from scratch with four characters. I’ll show you. Here’s the whole mean function we’re going to create a function called mean, and the mean is equal to the sum of a list divided by the of the list. So this here is some divided by count. And so I have now to find a new function called mean, which calculates the mean, mean of a is less than b, there we go. And so, you know in practice, I’m not sure why people would even bother defining a function called mean because it’s just as easy to actually write it’s implementation in APL, in numpy or whatever a python. It’s going to take a lot more than four letters to implement mean. So anyway, you know, it’s a math notation and so being a math notation we can do a lot with little, which I find out folks, I can say everything going on at once anyway. Okay, so that’s how we do the same thing in pytouch. And again, you can say that the less than in both cases, operating element wise. Okay, So a is less than b is saying ten is less than two six is less than eight four is less than seven and gives us back each of those trues and faleses as zeros and onces and according to our YouTube chat, had just exploded as it should. This is why APL is. Yeah life changing."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#elementwise-ops",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#elementwise-ops",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\nElementwise addition\n\na + b\n\ntensor([12., 14.,  3.])\n\n\nCheck lecture for awesome implementation of mean.\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\nRank two tensor , aka Matrix.\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nOkay, let’s now go up to higher rank. So this here is a rank one tensor. So a rank one tensor means it’s a list of things, it’s a vector, it’s where else a rank two tensor. It’s like a list of lists. They all have to be the same length lists, or it’s like a rectangular bunch of numbers. And we call it in math, we call it a matrix. So this is how we can create a tensor containing one, two, three, four, five, six, 789. And you can see also, like, what I like to do is I want to print out the thing I just created after I created it. Two ways to do it. You can say put an enter and then write m and that’s going to do that. Or if you want to put it all in the same line, that works too. You just use a semicolon. Neither one is better than the other. They’re just different. So we could do the same thing in APL. Of course in APL it’s going to be much easier. So we’re going to define a matrix called m which is going to be a three by 3 by 3 tensor containing the numbers from 1 to 9. Okay. And there we go. That’s done it in APL, a three by three tensor containing the numbers from 1 to 9. A lot of these ideas from APL you’ll find have made their way into other programing languages. For example, if you use GO you might recognize this. This is the iota character and go uses the word iota to spell it out in a somewhat similar way. A lot of these ideas from APL have found themselves into math notation and in other languages. It’s been around since the late fifties. Okay, so here’s a bit of fun. We’re going to learn about a new thing that looks kind of crazy code for Frobenius Norm and we’ll use that from time to time as we’re doing modeling. And here’s a definition of a four Frobenius norm. It’s the sum over all of the rows and columns of a matrix, and we’re going to take each one and square it. They’re going to add them up and they’re going to take the square root.\nAnd so to implement that in pytouch is as simple as (m*m).sum().sqrt(). So this looks like a pretty complicated thing when you kind of look at it. At first it looks like a lot of squiggly business or if you said this thing here you might be like, what on earth is that? Well, now you know, it’s just a square, some square root. So again, we could do the same thing in APL. So let’s do so in APL. We want the okay, so we got a case called S.F. Now it’s interesting, Apple does this a little bit differently. So dot some by default in PyTorch sums over everything. And if you want to sum over just one dimension, you have to pass in a dimension keyword for very good reasons. APL is the opposite. It just comes across rows or just down columns. So actually we have to say sum up the flattened out version of the Matrix and say flattened out. He is comma. So his sum up the flattened out version of the Matrix. Okay, so that’s our S.F.. Oh, Oh, sorry. And the Matrix is meant to be m times m There you go. So that’s the same thing. Sum up the flattened out and by a matrix and another interesting thing about APL is it always is read right to left. There’s no such thing as operator precedence, which makes life a lot easier. Okay, then we take the square root of that. There isn’t a square root function, so we have to do to the power of 0.5 and there we go. Same thing. All right, You got the idea. Yes. A very interesting question here from Marabou or other bars for norm or absolute value. And I like answer, which is the norm, is the same as the absolute value for scalar. So in this case, you can think of it as absolute value and it’s kind of not needed because it’s being squared anyway. But yes, in this case the norm. Well, in every case for a scale, the norm is the absolute value, which is kind of a cute discovery when you realize it. So thank you for pointing that out. See the. All right. So this is just fiddling around a little bit to kind of get a sense of how these things work. So really importantly, you can index into a matrix and you’ll say rows first and then columns."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 2/index.html#matmul-with-broadcasting",
    "href": "posts/Writing stable diffusion from scratch 2/index.html#matmul-with-broadcasting",
    "title": "Writing Stable Diffusion from Scratch 2",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\nSo let’s just grab a single digit. So here’s the first digit. So its shape is it’s a 784 long vector. Okay. And remember that our weight matrix is 784 by ten. Okay. So if we say digit colon coma None dot shape, then that is a 784 by one row matrix. Okay. So there’s our matrix. And so if we then take that 784 by one and expandas m2, it’s going to be the same shape as our weight matrix. So it’s copied our image data for that digit across all of the ten vectors, representing the ten kind of linear projections we’re doing for our linear model. And so that means that we can take the digit colon comma a None so 784 by one and multiply it by the weights. And so that’s going to get us back 784 by 10 so what it’s doing, remember, is it’s basically looping through each of these 10, 784 long vectors. And for each one of them it’s multiplying it by this digit. So that’s exactly what we want to do in our matrix multiplication. So originally we had when I originally most recently I should say, we had this dot product where we were actually looping over j, which was the columns of b, So we don’t have to do that anymore because we can do it all at once by doing exactly what we just did so we can take the ith and all the columns and add a access to the end. And then just like we did here, multiply it by b and then .sum(). And so that is again exactly the same thing.\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n185 µs ± 54.5 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nThat is another metrics, multiplication, doing it using broadcasting. Now this is like a tricky to get your head around. And so if you haven’t done this kind of broadcasting before, it’s a really good time to pause the video and look carefully at each of these four cells before and understand what did I do there, Why did I do it? What am I showing you? And then experiment with trying to and so remember that we started with m1 zero, right? So just like we have here, a[i , so that’s why we’ve got, i comma colon comma None because this digit is actually m1 zero. So this is like m1 zero colon None. So this line is doing exactly the same thing as this here plus the sum. So let’s check if this matmul is the same as it used to be, yet still working and the speed of it. Okay, not bad. So 137 microseconds. So we’ve now gone from a time from 500 milliseconds to about point 1 milliseconds. Funnily enough, my MacBook Air is an m2, whereas this Mac mini is an m1 that’s a little bit slower. So my error it was a bit faster than 0.1 milliseconds. So overall we’ve got about a 5000 times speed improvement. So that is pretty exciting. And since it’s so fast now, there’s no need to use a mini batch anymore. If you remember, we used a mini batch of five images, but now we can actually use the whole dataset so fast. So now we can do the whole data set. There it is. We’ve now got 15,000 by ten, which is what we want. And so it’s taking us only 656 milliseconds now to do the whole dataset. So this is actually getting to a point now where we could start to create and train some simple models in a reasonable enough time. So that’s good news. All right. I think that’s probably a good time to take a break. We don’t have too much more of this to go, but I don’t want to keep you guys up too late. So hopefully you learned something interesting about broadcasting today. I cannot overemphasize how widely useful is in all deep learning in machine learning code. It comes up all the time. It’s basically our number one most critical kind of foundational operation. So, yeah, take your time practicing it. And also good luck with your diffusion homework from the first half of the lesson. Thanks for joining us and I’ll see you next time.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.92 s, sys: 1.61 ms, total: 1.93 s\nWall time: 2.3 s"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html",
    "href": "posts/Writing stable diffusion from scratch 3/index.html",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "",
    "text": "Things you should know and practice after this lecture :  1- Einsum  2- 2 ways you can do einsum / matmul in pytorch  3- Using GPU / Cuda  4- args and kwargs\nAll credits go to fastai and all mistakes most likely is mine. This notebook connected to pass two notebooks. So the code is here but I remove most of explanation. You can check past two posts. Enjoy learning …"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication-from-foundations",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication-from-foundations",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matrix multiplication from foundations",
    "text": "Matrix multiplication from foundations\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#get-data",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#get-data",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Get data",
    "text": "Get data\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\nurlretrieve - (read the docs!)\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar 18 22:05 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\nislice\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n<itertools.islice at 0x7fc00c934040>\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nlist(islice(it, 5))\n\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it, 5))\n\n[]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\nUse this link to learn more about iter"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matrix-and-tensor",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matrix-and-tensor",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matrix and tensor",
    "text": "Matrix and tensor\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nm = Matrix(img)\nm[20,15]\n\n0.98828125\n\n\nNow we can use pytorch.\n\nimport torch\nfrom torch import tensor\n\n\ntensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\nTensor documentation\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\nvector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\nUse destructring again. n number of images. c is full number of colums (784)\n\nn,c = x_train.shape\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\nin y_train we can find min and max of it.\n\nmin(y_train),max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#random-numbers",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#random-numbers",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Random numbers",
    "text": "Random numbers\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand(),rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.9559050644103264\nIn child: 0.9559050644103264\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.6953])\nIn child: tensor([0.6953])\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(10000)]);\n\n\n\n\n%timeit check the time of excution.\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n4.6 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\npytorch version is faster.\n\n%timeit -n 10 torch.randn(784,10)\n\n103 µs ± 42.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matrix-multiplication",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nHow long does it take to run ? Man it too much. It is o(n^3) and it is so slow\n\n%time _=matmul(m1, m2)\n\nCPU times: user 621 ms, sys: 1.34 ms, total: 623 ms\nWall time: 629 ms\n\n\n\nar*bc*ac\n\n39200"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#numba",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#numba",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Numba",
    "text": "Numba\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 363 ms, sys: 113 ms, total: 476 ms\nWall time: 445 ms\n\n\n20.0\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 19 µs, sys: 4 µs, total: 23 µs\nWall time: 26 µs\n\n\n20.0\n\n\nNow only two of our loops are running in Python, not three:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\nThis is the test.\n\nfrom fastcore.test import *\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n2000 time faster. We change inner most loop.\n\n%timeit -n 50 matmul(m1a,m2a)\n\n383 µs ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#elementwise-ops",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#elementwise-ops",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Elementwise ops",
    "text": "Elementwise ops\nTryAPL\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\nElementwise addition\n\na + b\n\ntensor([12., 14.,  3.])\n\n\nCheck lecture for awesome implementation of mean.\n\n(a < b).float().mean()\n\ntensor(0.67)\n\n\nRank two tensor , aka Matrix.\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#broadcasting",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#broadcasting",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Broadcasting",
    "text": "Broadcasting\nThe term broadcasting describes how arrays with different shapes are treated during arithmetic operations.\nFrom the Numpy Documentation:\nThe term broadcasting describes how numpy treats arrays with \ndifferent shapes during arithmetic operations. Subject to certain \nconstraints, the smaller array is “broadcast” across the larger \narray so that they have compatible shapes. Broadcasting provides a \nmeans of vectorizing array operations so that looping occurs in C\ninstead of Python. It does this without making needless copies of \ndata and usually leads to efficient algorithm implementations.\nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\nThis section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course.\n\nBroadcasting with a scalar\n\na\n\ntensor([10.,  6., -4.])\n\n\nSimplest way of broadcasting.\n\na > 0\n\ntensor([ True,  True, False])\n\n\n\na + 1\n\ntensor([11.,  7., -3.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nmultiply\n\n2*m\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\n\nBroadcasting a vector to a matrix\n\nc = tensor([10.,20,30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nc + m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt = c.expand_as(m)\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nm + t\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\n\nc.unsqueeze(0), c[None, :]\n\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))\n\n\n\nc.shape, c.unsqueeze(0).shape\n\n(torch.Size([3]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(1), c[:, None]\n\n(tensor([[10.],\n         [20.],\n         [30.]]), tensor([[10.],\n         [20.],\n         [30.]]))\n\n\n\nc.shape, c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([3, 1]))\n\n\nwe can avoid : and say c[None].\n\nc[None].shape,c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nc[:,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nm + c[:,None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\nThis adding the vector to each row.\n\nm + c[None,:]\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\n\n\nBroadcasting Rules\n\nc[None,:]\n\ntensor([[10., 20., 30.]])\n\n\n\nc[None,:].shape\n\ntorch.Size([1, 3])\n\n\n\nc[:,None]\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\n\nc[:,None].shape\n\ntorch.Size([3, 1])\n\n\n\nc[None,:] * c[:,None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[None] > c[:,None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\nm*m\n\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when It goes from right to left. - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size\nArrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#matmul-with-broadcasting",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#matmul-with-broadcasting",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\n\ndigit = m1[0]\ndigit.shape,m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\ndigit[:,None].expand_as(m2).shape\n\ntorch.Size([784, 10])\n\n\n\n(digit[:,None]*m2).shape\n\ntorch.Size([784, 10])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n231 µs ± 71.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.26 s, sys: 2.76 ms, total: 1.26 s\nWall time: 1.28 s"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#einstein-summation",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#einstein-summation",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "Einstein summation",
    "text": "Einstein summation\nSo we’re 5000 times faster than we started out. So another trick that we can use, which I’m a big fan of, is something called Einstein summation. And Einstein summation is a compact representation for representing products and sums. And this is an example of an Einstein summation. And what we’re going to do now is begin to replicate our matrix product with an Einstein summation. And believe it or not, the entire thing can be pushed down to just these characters,(ik,kj->ikj) which is pretty amazing. So let me explain what’s happening here. The arrow is separating the left hand side from the right hand side. The left hand side is the inputs. The right hand side is the output. The comma is between each inputs. So there are two inputs. The letters are just names that you’re giving to the number of rise in the number of columns. So the first matrix we’re multiplying by has i rows and k columns, the second has k rows and j columns. It’s going to go through a process which creates a new matrix that actually this is not even doing this is not yet doing the matrix multiplication. This is without the sum. This one’s going to create a new matrix that contains i rows, Well, how do we set i faces and k rows and j columns are rank three tensor. So the number of letters is going to be the rank and the rules of how this works is that if you repeat letters between input arrays,(ik,kj) so here’s my inputs (ik,kj) and we’ve got a repeated letter. It means that values along those axes will be multiplied together. So it means that each item across a row will be multiplied by each item down each column to create this i by k by j output tensor. So to remind you, our first matrix is five by 734 that’s m1. Our second matrix is 784 by ten that’s m2. So i is 5, k is 784 and J is 10. So if I do this torch.einsum then I will end up with a i k by k, it’ll be five by 784 by ten. And if you have a look, I’ve run it here on these two tensor and m1 and m2 and the shape of the result is five by 784 by ten. And what it contains is the original five rows of m1 the original ten columns of m2, and then for the other 784 that I mentioned, they all multiplied together because it’s been copied. It’s been copied between the two arguments to the einsum. And so if we now sum up that over this dimension, we get back, if we have a look, it was that we printed this somewhere. Oh, there it is. So what we get back, if we go back to the original matrix multiply, we do. We had 10.94 negative, negative point six, eight, etc. And so now with this Einstein summation version, we’ve got back exactly the same thing because what it’s done is it’s taken each of these columns by rows, multiplied them together to get this five by seven, eight, four by ten, and then add it up that 784 for each one, which is exactly what matrix multiplication does. But we’re going to use one of the two things from Einstein. The second one says if we omit a letter from the output. So the bit on the right of the arrow, it means those values will be summed. So if we remove this K, which gives us i , k and k,j goes to i,j, so we’ve removed the k entirely. That means that sum happens automatically. So if we run this, as you say, we get back again. Matrix multiplication. So Einstein summation notation is, you know, it takes some practice getting used to you, but it’s very convenient and once you get used to it, it’s actually a really nice way of thinking about what’s going on. And as we’ll see in lots of examples, often you can really simplify your code by using just a tiny little Einstein summation, and it doesn’t even have to be a sum, right? You can you don’t have to omit any letters if you’re just doing products. So maybe it’s a bit misnamed. So we can now define a matmul as simply this torch.einsum. So if we now check it, the test_close that the original result is equal to this new matmul. And yes, it is. And let’s see how the speed looks. Okay. And that was for the whole thing. So compared to 600 milliseconds. So as you can see, this is much faster than even the very fast broadcasting approach we used. So this is a pretty good trick is torch.einsum. Some okay, but of course we don’t have to do any of those things because PyTorch already knows how to do matmul. So there’s two ways we can run matmul directly. In PyTorch, you can use a special @ operator. So x_train@weights is the same as matmul train comma weights as you say, test_close or you can say torch.matmul. And interestingly, as you can see here, the speed is about the same as the einsum. So there’s no particular harm that people reason not to do an einsum. So when I say einsum, that stands for Einstein summation notation. All right, let’s go faster. Still. Currently we’re just using my CPU, but I have a GPU. It would be nice to use it. So how does a GPU work at in video? GPU and indeed pretty much all GPU use. The way they work is that they do lots and lots of things in parallel and you have to actually tell the GPU what are all the things you want to do in parallel or one a time. And so what we’re going to do is we’re going to write in Pure Python something that works like a GPU. You expect it won’t actually be in parallel, so it won’t be fast at all. But the first thing we have to do if we’re going to get something working in parallel is we have to create a function that can calculate just one thing even if a thousand other things are happening at the same time, it won’t interact with anything else. And there’s actually a very easy way to think about matrix multiplication in this way, which is what if we try to create something which, just as we’ve done here, fills in a single, the single item of the result?\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. The key rules are:\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\nOmitting a letter from the output means that values along that axis will be summed.\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\nSo far we removed 2 inner loop and we are 5k faster than original for loop. It is more cleaner code and also faster.\n\n# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntorch.einsum('ik,kj->ij', m1, m2)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n\n\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n\n\n%timeit -n 5 _=matmul(x_train, weights)\n\n30.5 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)"
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#pytorch-op",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#pytorch-op",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "pytorch op",
    "text": "pytorch op\nNow that we wrote matmul we can use pytorch matmul version. The speed is almost the same. We can use pytorch’s function or operator directly for matrix multiplication.\n\ntest_close(tr, x_train@weights, eps=1e-3)\n\n\n%timeit -n 5 _=torch.matmul(x_train, weights)\n\n32 ms ± 4.72 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nSo how do we create something that just fills in row zero column zero? Well, what we could do is we could create a new matmul where we’re going to pass in the coordinates of the place that we want to fill in. So I’m going to start by passing that zero comma zero we’ll pass at the matrices We want to multiply and we are passing a tensor that we’ve pre-filled in with zeros to put the result into. So they’re going to say, okay, the result is torch.zeros() rows by columns, cal matmul for location zero comma zero passing in those two matrices and the bunch of zeros matrix ready to put the result in. And if we call that we get the answer in cell zero zero. So here’s an implementation of that. So the implementation is first of all, we’ve been past the zero comma zero coordinates, so let’s de structure them. So hopefully you’ve been experimenting with de structuring that so important. You said all the time into i and j throw in the column, make sure that that is inside the bounds of our output matrix and we’re going to start by start at zero and loop through all of the all of the columns of a in the rows of b for i and j, just like the very innermost loop of our very first Python attempt and then at the end pop that into the output. So here’s something that fills in one piece with a grid successfully. So we could call this row by columns times each time passing in a different grid. And we could do that in parallel because none of those different locations interact with any other location. So something which can calculate a little piece of, of an output on a GPU is called a kernel. So we call this a kernel. And so now we can create something called launch kernel, we pass at the kernel. So that’s the function. So here’s an example launch kernel passing in the function and how many rows and how many columns are there in the output grid. And then give me any arguments that you need to calculate it. So in python *args just says any additional arguments that you pass are going to be put into an array called args. If you do something like C, you might have seen like variadic arguments parameters. It’s a same basic idea. So we’re going to be calling launch kernel."
  },
  {
    "objectID": "posts/Writing stable diffusion from scratch 3/index.html#cuda",
    "href": "posts/Writing stable diffusion from scratch 3/index.html#cuda",
    "title": "Writing Stable Diffusion from Scratch 3",
    "section": "CUDA",
    "text": "CUDA\nHow to use GPU instead of CPU. GPU does many more thing at the same time. CPU does not do that. We can compute each cell at the same time because none of those computation interact with other location.\nSo we’re going to be calling launch kernel. We’re going to be saying launch the kernel matmul using all the rows of a or the columns of b, and then the args which are going to be in star args are going to be m1, the first matrix, m2 the second matrix n res another touched zeros we just created. So launch kernel, it’s going to loop through the rows of a and then for each row of a loop through the columns of b and call the kernel which is matmul on that grid location and passing in m1, m2 and res. So I star args here is going to unpack that and pass them as three separate arguments. And if I run that, run all of that, you’ll see it’s done it, it’s filled in the exact same matrix. Okay. So that’s actually not fast at all. It’s not doing anything in parallel, but it’s the basic idea. So now to actually do it in parallel, we have to use something called Cuda. So Cuda is a programing model for Nvidia GPUs and to program in CUDA from Python. The easiest way currently to do that is be something called Numba. And Numba is a compiler where you’ve seen it actually already for non GPU. It’s a compiler that takes Python code and spits out, you know, compiled fast machine code. If you use its CUDA module, it’ll actually spit out GPU accelerated CUDA code. So rather than using an @njit like before, we now say @cuda.jit and it behaves a little bit differently but you’ll see that this matmul let me copy the other one over so you can compare cup it, compare it to our Python one, our Python matmul and this @cuda.jit matmul Look I think identical except for one thing. Instead of passing in the grid, there’s a special magic thing called cuda.gird() And you say how many dimensions just my grid have? And you unpack it so that’s you don’t have to. It’s just a little convenience. That Numba does for you. You don’t have to pass over the grid, it passes it over for you. So it doesn’t need this grid. Other than that, these two are identical, but the decorator is going to compile that into your GPU code. So now we need to create our output tensor just like before, and we need to do something else, which is we have to take our input matrices and our output. So our input tenses, the matrices in this case and the output tensor and we have to move them to the GPU, you I should say, copy them to the GPU.\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\nres = torch.zeros(ar, bc)\nmatmul((0,0), m1, m2, res)\nres\n\ntensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])\n\n\n*args any additional argument(s) will be put into an array called args. This is the idea.\n\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): kernel((i,j), *args, **kwargs)\n\n\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\nNow we use cuda so it will do multiply computation at the same time.\n\nfrom numba import cuda\n\n\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\nSo cuda.to_device() copies a tensor to the GPU. And so we’ve got three things getting copied to the GPU here and therefore we store the three things over here. Another way I could have written this is I could have said map, which I kind of quite like doing a function which is cuda.to_device to each of these arguments and this would be the same thing. This is going to call CUDA dot device on x_train and put it in here on weights and put it in here and an r and put it in rg. That’s a slightly more convenient way to do it. Okay, so we’ve got our 50,000 by ten output. That’s just all zeros. Of course, that’s just how we created it. And now we’re going to try and fill it in. There is a there’s a particular detail that you don’t have to worry about too much, which is in CUDA They don’t just have a grid, but there’s also a concept of blocks and there’s something we call here TPP, which is threads per block. This is just a detail of the kind of programing model you don’t have to worry about too much. You can just basically copy this. And what it’s going to do is it’s going to call each grid item in parallel and with a number of different processes, basically. So this is just the code which turns the grid into blocks. And so you don’t have to worry too much about the details of that. You just always run it.\nDecorator compile it to GPU code.\n\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n\n\n!pip install numba\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (0.56.4)\nRequirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.9/dist-packages (from numba) (1.22.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba) (63.4.3)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba) (0.39.1)\n\n\n\nfrom numba import cuda\nr = np.zeros(tr.shape)\nm1g,m2g,rg = cuda.to_device(x_train),cuda.to_device(weights),cuda.to_device(r)\n\n\nr.shape\n\n(50000, 10)\n\n\n\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(3125, 1)\n\n\nOkay. And so now how do you call the equivalent of launch kernal? it’s it’s a slightly weird way to do it, but it works fine. You call matmul, but because matmul has cuda.jit, it’s got a special thing, which is you have to put something in square brackets afterwards, which is you have to tell it how many blocks per grid. That’s just the result from the previous cell and how many threads per block in each of the two dimensions. So again, you can just copy and paste this from my version, but then you pass in the three arguments to the function. This will be a, c, and c, and this. Okay, this is, this is how you launch a kernel. So this will launch the kernel matmul on the GPU. You at the end of it, rg is going to get filled in. It’s gone. It’s on the GPU, which is not much good to us so we don’t have to copy it back to the CPU, which is called the host copy to host to a run that and it’s done and test_close shows us that result is similar to our original results. So it seems to be working. So that’s great. So I see Sylvor on the YouTube chat is finding that it’s not working on his Mac. That’s right. So this will only work on it in NVIDIA CPU as basically all of the GPU, nearly all the CPU stuff we look at only works on video.\n\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n\nMac GPUs are gradually starting to get a little bit of support from machine learning libraries, but it’s taking quite a while. It’s been, you know, it’s got quite a way to go. As I say this at least towards the end of 2022, if this works for you and later on that’s yeah, that’s great. Okay, so let’s time how fast that is. Okay, so that was 3.6 1 milliseconds. And so if we compare that to the PyTorch matmul on CPU, that was 15 milliseconds. So that’s great. So it’s faster still. So how much faster? Oh, by the way, we can actually go faster than that, which is we can use the exact same code we had from the PyTorch up. But here’s a trick. If you just take your tensor and write .cuda after it, it copies it over to the GPU. If it’s on a if it’s on a Nvidia GPUs, you do the same for weights.cuda. So these are two cuda versions and now I can do the whole thing. And this will actually run on the GPU and then to copy it back to the host, you just say .cpu(). So if we look to see how fast that is, 458 ms .So yeah, that is somebody you just pointed out that I wrote the wrong thing here 1e-3. Okay, so how much faster is that? Well full 458 microseconds original on the whole data set was 663 microseconds. So compared to our broadcast version, we are another 1000 times faster. So overall, this version here, compared to our original version, which was here, the difference in performance is 5 million x , So when you say people say, Yeah, Python can be pretty slow, it can be better to run the stuff on the GPU if possible. We’re not talking about a 20% change, we’re talking about a 5 million x change. So that’s a big deal. And so that’s why you need to be running stuff on the GPU. All right. Some folks on YT are wondering how on earth I’m running cuda when I’m on a mac and given it sets localhost here, that’s because I’m using something called SSH tunneling, which we might get to sometime. I suspect my life coding from the previous course might have covered that already, but this is basically you can use a Jupyter notebook that’s running anywhere in the world from your own machine using something called SSH Tunneling, which is a good thing to look up a OK when a person asks if Einstein summation borrows anything from APL. Oh, yes, it does, actually. So it’s kind of the other way around. Actually. APL borrows it from Einstein notation. I don’t know if you remember I mentioned that Iverson, when he developed APL was heavily influenced by tensor analysis. And so this Einstein notation is very heavily used there. If you’ll notice a key thing that happens in Einstein notation is there’s no loop. You know, there isn’t this kind of sigma, you know, i from here to here and then you put the i inside the function that you’re summing up, everything’s implicit and APL takes that a very long way and, and J takes it even further, which is what Iverson developed after APL and this kind of general idea of of removing the index is very important in APL and it’s become very important in numpy PyTorch TensorFlow and so forth. All right. So finally we know how to multiply matrices. Congratulations. So let’s practice that. That’s practice what we’ve learned. So we’re going to go to zero two main shift to practice this. And so we’re going to try to exercise our kind of tensor manipulation operation muscles in this section. And the key actually endpoint for this is the homework. And so what you need to be doing is getting yourself to a point that you could implement something like this, but for a different algorithm, why do we care about this?\n\n%%timeit -n 10\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n\n10.9 ms ± 3.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nm1c,m2c = x_train.cuda(),weights.cuda()\n\n\nr=(m1c@m2c).cpu()\n\n\n%timeit -n 10 r=(m1c@m2c).cpu()\n\n2.27 ms ± 288 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "",
    "text": "What you should know and practice after this lecture: 1- Easily plot matrix that is not easily plottable  2- Broadcasting roles  3- Creating sample data  4- meanshift algorithm  5- You can use peresentify to draw on screen  6- Animation"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#create-data",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#create-data",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "Create data",
    "text": "Create data\n\nn_clusters=6\nn_samples =250\n\nTo generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.\n\ncentroids = torch.rand(n_clusters, 2)*70-35\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\nSo how does sample work? Well, we’re passing in the centroid, and so what we want is we’re going to get back. So each of those centroid contains an X in a Y. So multivariate normal is just like normal. It’s going to give you back normally distributed data, but more than one item. That’s why it’s multivariate. And so we passed in two means a main for X and a mean for our Y. And so that’s the mean that we’re going to get. And our standard deviation is going to be five. Why do we use torch.diag(tensor([5.,5.])))? That’s because we’re saying that because that for multivariate normal distributions, there’s not just one standard deviation. Each column that you get back, there could also be a connection between columns. The columns might not be independent. So you actually need so it’s called a covariance matrix, not just to make, not just a variance. We discussed that a little bit more in lesson 9B if you’re interested in learning more about that. Okay, So this is something that’s going to give us back random columns of data with this mean and this standard deviation.\nAnd this is the number of samples that we want and this is coming from PyTorch. So PyTorch has a whole bunch of different distributions that you can use, which can be very handy. So there is our data. Okay. So remember, for sample clustering, we we don’t know the different colors and we don’t know where the X is. That’s kind of our job is to figure that out. We might just briefly also look at how to plot. So in this case, we want to plot the X s and we want to plot the data so it looks like this. So what I do is I look through each centroid and I grab that centroid samples and they’re just all done in order. So I grab it from in_samples: to (i+1)n_samples, and then I create a scatterplot with the samples on them. And what I’ve done is I’ve created an axis here and you’ll see y later that we can also pass one in. But I’m not passing one it. And so we create a plot and an axis. And so in that matplotlib, you can keep plotting things on the same axis. So then I plot on the centroid a big x, which is black, and then a smaller x, which is what is that magenta? And so that’s how I get these X’s. So that’s how plot data works. Okay, so how do we create something now that starts with all the dots and returns where the X is are ? We’re going to use a particular algorithm, particular clustering algorithm called meanshift.\n\ndef sample(m): return MultivariateNormal(m, torch.diag(tensor([5.,5.]))).sample((n_samples,))\n\n\nslices = [sample(c) for c in centroids]\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\nBelow we can see each centroid marked w/ X, and the coloring associated to each respective cluster.\n\ndef plot_data(centroids, data, n_samples, ax=None):\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#mean-shift",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#mean-shift",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "Mean shift",
    "text": "Mean shift\nMost people that have come across clustering algorithms have learnt about k-means. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a bandwidth to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.\nThe algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the Gaussian kernel of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the bandwidth, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step\nThis will iteratively push points that are close together even closer until they are next to each other.\nAnd and meanshift is a nice clustering approach because you don’t have to say how many clusters there are. So it’s not that often that you’re actually going to know how many clusters there are. So we don’t have to say quite a few things, like the very popular K means required to say how many in step. You just have to pass them in quite a bandwidth, which we’ll learn about, which can actually be chosen automatically. And it can also handle clusters of any shape so they don’t have to be bold shaped like that. But they are here. They can be kind of like L-shaped or ellipse shaped or whatever. And so here’s what’s going to happen. We’re going to pick some point. So let’s say we pick that point just there. Okay? And so what we now do is we go through each data point, so we pick the first one, and so we then find the distance between that point and every other point. Okay. So we’re going to have to say what is the distance between that point and that point? And point and that point and that point and that point and also the ones further away, that point and that point. And you do it for every single point compared to the one that we’re currently looking at. Okay. So we get all of those as a big list. And now what we’re going to do is we’re going to take a weighted average of all of those points. Now That’s not interesting without the weighting. If we just take our average of all of the points and how far away they are, we’re going to end up somewhere here, right? This is the average of all the points. But the key is that we’re going to take an average and find the right spot. The key is we need to find an average that is weighted by how far away things are.\nSo, for example, this one over here is a very long way away from our point of interest. And so it should have a very low weight and the weighted average where else this point here, which is very close, should have a very high weight in our weighted average. So What we do is we create weights for every point compared to the one that we’re currently interested in using a what’s called a Gaussian kernel that we’ll look at. But the key thing to know is that points that are further away from our point of interest, which is this one, are going to have lower weights. That’s what we mean, that they’re penalized. The rate at which weights for a zero is determined by this thing that we set at the start called the bandwidth. And that’s going to be the standard deviation of our Gaussian. So we take an average of all the points in the dataset, a weighted average weighted by how far away they are. So for our point of interest, right, the this point is going to get a big weight. This point is going to get a big weight. This point is going to get a big weight. That point is going to get a tiny weight.\nThat point is going to get an even tiny weight. So it’s mainly going to be a weighted average of these points at a nearby. And the weighted average of those points, I would guess, is going to be somewhere around about here. Right. And would have a similar thing for the weighted average of the points near this one. That’s going to probably be somewhere around about here or maybe over here. And so it’s going to move all of these points in closer. It’s almost like a gravity right. They’re kind of going to be moved like closer and closer in towards this kind of gravitational center. And then these ones will go towards their own gravitational center and so forth. Okay.\n\nmidp = data.mean(0)\nmidp\n\ntensor([ 9.222, 11.604])\n\n\n\nplot_data([midp]*6, data, n_samples)\n\n\n\n\nSo let’s take a look at it. All right. So what’s the gaussian kernel? This is the gaussian kernel, which was a sign in the original March for science back in the days when the idea of not following scientists was considered socially unacceptable. We used to have a March for these things, if you remember. So this is this is not normal. So this is the definition of the gaussian kernel, which is also known as the normal distribution. This is the shape of it. So you’ve seen it before. And here is that formula copied directly off the science match sign. Okay, here we see the square root, two pi, etc..\nSo here’s the definition of the gaussian kernel, which you may remember from high school… This person at the science march certainly remembered!\n\nOkay. And bw is the standard deviation. Now what does that look like? It’s very helpful to have something that we can very quickly plot any function that doesn’t come with matplotlib , but it’s very easy to write one. Just say, oh, let’s as X, let’s use all the numbers from 0 to 10, a hundred of them spaced evenly. That’s what linspace Does. it linearly spaced 100 numbers in this range. That’s going to be our Xs. So plot those Xs and plot F of X is the Ys. So here’s a very nice little plot_func, we want. And here it is. And as you can see here, we’ve now got something where if you are this like very close to the point of interest, you’re going to get a very high weight. And if you’re a long way away from the point of interest, you’ll get a very low weight. So that’s the key thing that we wanted to remember is something that penalizes further away points more. Now, you’ll notice here I’ve managed to plot this function for a bandwidth of 2.5, and the way I did that was using this special thing from functools(functools.partial), . Now, the first thing to point out here is that very often drives me crazy. I see people trying to find out what something is in Jupiter, and the way they do it is they’ll scroll up to the top of the notebook and search through the imports and try to find it. That is the dumb way to do it. The smart way to do it is just to type it and press shift enter and it’ll tell you where it comes from and you can get its help with Question Mark and you can get it also source code with two question marks. Okay, So just type it to find out where it comes from. Okay. So this is as Sylver mentioned in the chat, also known as carrying or partial function application. This creates a new function. So let’s just grab it. We create a new function. And this function F is is the function Gaussian, but it’s going to automatically pass. BW equals 2.5. So this is a partially applied function.\n\ndef gaussian(d, bw): return torch.exp(-0.5*((d/bw))**2) / (bw*math.sqrt(2*math.pi))\n\n\ndef plot_func(f):\n    x = torch.linspace(0,10,100)\n    plt.plot(x, f(x))\n\n\nplot_func(partial(gaussian, bw=2.5))\n\n\n\n\nSo I could type f of four f(tensor(4.0)), for example, that’s going to be a tensor. There we go. And you can see that’s exactly what this is got to for across. Yep, about .44. So we use partial function application all the time. It’s a very, very, very important tool. Without it, for example, plotting this function would have been more complicated with it. It was trivially easy. I guess the alternative, like one alternative which would be fine but slightly more clunky, would be we could create a little function in line so we could have said, Oh, plot a function. Then I’m going to define right now, which is called lamb, which is lambda X, which is Gaussian and of X with a bandwidth of 2.5. You could do that too. You know, it’s it’s fine, but, but yeah, partials I think are a bit neater, a bit less to think about.\nThey often produce some nature and clearer code. Okay. Why did we decide to make the bandwidth 2.5 as a as a rule of thumb, choose a bandwidth which covers about a third of the data. So if we kind of found ourselves somewhere over here, write a bandwith which covers about a third of the data would be enough to cover two clusters ish. So it would be kind of like this big. So somewhere in the middle there. So that’s the basic idea. Yeah. So but you can play around with bandwidth and get different amounts of clusters. I should mention, like often when you see something that’s kind of on the complicated side, like a Gaussian, you can often simplify things. I think most implementations and write ups I’ve seen talk about using Gaussians, but if you look at the shape of it, it looks a lot like this shape. So this is a triangular weighting which is just using clamp_min So it’s just using a linear with clamp_min And yeah, it occurred to me that we could probably use this just as well. So I did find it.\nI decided to define this triangular weighting and then we can try both anyway. So I will start with we’re going to use the Gaussian version. All right. So we’re going to be move literally moving all the points towards the kind of center of gravity.\n\nf = partial(gaussian,bw=2.5)\n\n\nf(tensor(4.0))\n\ntensor(0.044)\n\n\n\npartial\n\nfunctools.partial\n\n\n\nplot_func(lambda x : gaussian(x,bw=2.5))\n\n\n\n\nIn our implementation, we choose the bandwidth to be 2.5.\nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n\ndef tri(d, i): return (-d+i).clamp_min(0)/i\n\n\nplot_func(partial(tri, i=8))\n\n\n\n\nSo we don’t want to mess up our original data so we clone it. It’s a PyTorch thing is .clone (data.clone()), it’s very handy. And so Big X is our matrix of data. I mean, it’s actually a That’s right. Matrix of data. Yeah. And then little x will be our first point. And it’s pretty common to use big X a capital letters for matrices. So this is our data. This is the first point.\nOkay. So there it is. We’re going to start at 26.2, 26.3. So 26.2, 26.3. So somewhere up here, so little x, its shape is just it’s a rank one tensor of shape two. Big X is a rank two tensor of 1500 data points by two, the X and Y. And if we call x[None], that would add a unit axis to that. And the reason I’m going to show you that is because we want to find the distance from little x to everything in Big X and the way we do a distance is with minus, but you wouldn’t be able to go, you wouldn’t be able to go X minus big X and get the right actually to you get the right answer. Let’s think about that X shape. Oh, we’ve got that already. I know actually that is going to work isn’t it? So, yes. All right. So you can see why we’ve got these two versions here. If we do x[None], we’ve got something of shape. One comma, two. Now we can subtract that from something, a shape 1500 comma two, because the twos match up because they’re the same and the 1500 and the one matches up because we remember our Numpy roles, everything matches up to a unit axis. So it’s going to copy this matrix across every of this matrix and it works.\nBut you remember there’s a special trick which is if you’ve got two shapes of different lengths, we can use the shorter length and it’s going to add unit axes to the front to make it as long as necessary. So we actually don’t need the x[None]. We can just use little x and it works because it’s going to say, is this compatible with this? Well, the last axis, remember we go right to left the last axis matches the second last axis, Oh, it doesn’t exist. So we pretend that there’s a unit axis, and so it’s going to do exactly the same thing as this. So if you have not studied the broadcasting from last week carefully, that might not have made a lot of sense to you. And so definitely at this point, you might want to pause the video and go back and reread the NumPy broadcasting rules from last time and practice them because that’s what we just did. We use numpy broadcasting rules and we’re going to be doing this dozens more times throughout the rest of the course and many more times, in fact, in this lesson.\n\nX = data.clone()\nx = data[0]\n\n\nx\n\ntensor([26.204, 26.349])\n\n\n\nx.shape,X.shape,x[None].shape\n\n(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))\n\n\n\nx-X\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        ...,\n        [-4.568, 17.025],\n        [-3.151, 22.389],\n        [-4.964, 21.040]])\n\n\n\n(x[None]-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\n\n(x-X)[:8]\n\ntensor([[ 0.000,  0.000],\n        [ 0.513, -3.865],\n        [-4.227, -2.345],\n        [ 0.557, -3.685],\n        [-5.033, -3.745],\n        [-4.073, -0.638],\n        [-3.415, -5.601],\n        [-1.920, -5.686]])\n\n\nHi, Dan, The thing I’m using to write on the screen is called presentify.It’s this thing here. It’s very cool. And a graphics tablet graphics tablets are quite cheap nowadays. Oh, excuse me. I use a cheap Chinese equivalent of a webcam on tablet. All right. Hi, everybody. Welcome back. So we had got to the point where we had managed to get the distance between our first point x and all of the other points in the data. And so just looking at the first eight of them here. So the very first instance is of course zero on the X axis and zero on the Y axis because it is the first point. The other thing is that because we the way we created the clusters is they’re all kind of next to each other in the list. So these are all in the first cluster. So none of them are too far away from each other. So now that we’ve got all the distances, it’s easy enough to, well, not the distances on X and Y, it’s easy enough to get the distance, the kind of Euclidean distance, so we can just square their difference and sum and square root. And actually maybe this is a good time to talk about norms and to talk about what we just did there. Um, so we’ve got all these data points. So here’s one of our data points and here’s the other one of our data points, and there’s some, you know, distance across the X axis and there’s some distance along the Y axis. So we could call that change in X and change in Y.\nAnd one way to think about this distance then is it’s this distance here. So to calculate that we couldn’t use Pythagoras, so a squared plus b squared equals C squared or in our case so this would be c, a, and b, so, so in our case it would be the square root of the change in X squared plus the change in Y squared. And rather than saying square root, we could say to the power of a half another way of saying the same thing. But there’s a different way we could find the distance. We could first go along here and then go up here. And so that one would be change in X, if you like, to the one plus change in Y to the one to the power of one one. Yeah, I got a slightly odd way for reasons you’ll see in a moment. It’s just this otherwise, in general, if we’ve got a whole list of numbers, we can add them up. Let’s say they’re some list V, we can add them up, we can do each one to the power of some number alpha and take that sum to the one over alpha. And this thing here is called a norm. So you might have remember we came across that last week and we come across it again this week. They basically come up, I don’t know, they might end up coming up every week.\nThey come up all the time, particularly because the two norm, which we could write like this or we could write like this or we could write like this, they’re all the two norm this is just saying it’s this equation for alpha equals two And Stefano is pointing out we should actually have an absolute value. I’m not going to worry about that. We’re just doing real numbers. So I keep things simple. Oh, I guess first higher than one. Now you’re probably right for something like three. Yeah, I guess we do need an absolute value there. That’s a good point because okay, we could have this one. And so the distance actually has to be the absolute value. So the change in x is the absolute value of that distance. Yes. Thank you, Stefano. Okay. So we’ll have the absolute value. Okay. So the two, norm, is what happens when every calls to and we would call this in this case, we would call this the Euclidean distance. But actually where it comes up more often is when you’re doing like a lost function.\nSo the mean squared error is just while the root means squared error, I should say, is just the two norm. Where else the mean absolute error is the one norm. And these are also known as L2 and L1. And remember what we saw in that paper last week. We saw it in this form. There’s a two up here which is where they got rid of the square root again. So would have just been a change in x squared plus change in Y squared. And now we don’t even need the parentheses. Okay, so all of this is to say that for, you know, this comes up all the time because we’re very, very often interested in distances and errors and things like that. I’m trying to think I don’t feel like I’ve ever seen anything other than one or two. So although it is a general concept, I don’t think we’re going to see probably things other than one or two in this course. I’d be excited if we do, that would be kind of cool. So here we’re taking the Euclidean distance, which is the two on. So this has got eight things in it because we’ve summed it over dimension one. So here’s your first homework is to rewrite using torch.einsum, you won’t be able to get rid of the x minus x. You’ll still need to have that in there.\nBut when you’ve got a multiply followed by a sum, now you want to get rid of the square root. Either you should be able to get rid of the multiply in the sum by doing it in a single torch.einsum. So we’re summing up over the first dimension, which is this dimension. So in other words, with summing up the X in the Y axis, okay, so now we can get the, the weights by passing those distance is into our gaussian. And so as we would expect, the biggest weights, it gets up 0.16. So the closest one is itself, it’s going to be at a big weight. These other ones get reasonable weights and the ones that are in totally different clusters have weights small enough that at three significant figures they appear to be zero. Okay, so we’ve got our weights. So there the weights are 1500 long vector and of course our original data is 1500 by two, the X and the Y for each one. So we now want a weighted average. We want this data, we want it’s average weighted by this. So normally an average is the sum of your data divided by the count. That’s a normal average weighted average item in your data. It’s let’s put some i’s around here. Just to be more clear, each item in your data is going to have a different weight. And so you multiply each one by the weights. And so rather than dividing by n, which is just the sum of ones, we would divide by the sum of weights. So is an important concept to be familiar with. Weighted averages. So we need to multiply every one of these x says by this.\n\n# rewrite using torch.einsum\ndist = ((x-X)**2).sum(1).sqrt()\ndist[:8]\n\ntensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])\n\n\n\nweight = gaussian(dist, 2.5)\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\nOkay, so can we say weights times X? No. All right. Why didn’t that work? So remember, we go right to left. So first of all, it’s going to say, let’s look at the two and multiply that by the 15. Are they compatible? Things are compatible if they’re equal or if at least one of them is one. These are not equal and they’re not one, so they’re not compatible. That’s why it says the size of a tensor a, must match. Now, when it says match, it doesn’t mean they have to be the same. One of them can be one. Okay. That’s what it means to match. They’re either or. One of them is one. So that doesn’t work. On the other hand, what if this was 1500 comma one? If it was 1500 comma one, then they would match because the one and the two match because one of them’s a unit axis and the 1500 and the 1500 match because they had the same. So that’s what we’re going to do because that would then copy this to every one of these, which is what we want. We want weights for each of these (x,y) tuples. So to add the trailing unit axis, we say every row and a trailing unit axis.(weight[:,None]*X) So that’s what that shape looks like. So we can then multiply that by x and as you can see, it’s now weighting each of them. And so each of these x’s and y is down the bottom, they’re all zero. So we can sum that up and then divide by the sum of weights. So let’s now write a function that puts all this together so you can see this really important way of like to me, the only way that makes sense to do a particularly scientific numerical programing.\nI actually do all my programing this way, but particularly scientific numerical programing is write it all out step by step, check every piece, have it all that documented for you and for others, and then copy the cells, merge them together and indent them to indent its control+right+spare bracket and put a function header on top. So here’s all those things we just did. And now, rather than just grabbing the first x, we enumerate through all of them. So that’s the distance we had before. That’s the weight we had before. There’s the product we had before. And then finally some across the rows divide by the sum of the weights. So that’s going to calculate for is It's going to move. So it's actually changing Capital X, so it's changing the is thing and capital X so that it’s now the weighted sum. Oh, actually sorry, the weighted average of all of the other data weighted by how far it is away. So that’s going to do a single step. So the main shift update is extremely straightforward, which is clone the data, iterate a few times and do the update. So if we run it, take 600 milliseconds. And what I’ve done is I’ve plotted the centroid moved by two pixels or two one up two pixels, two units so that you can see them and so you can see the dots is where our data is. And they’re dots now because every single data point is on top of each other on a cluster. And so you can see they are now in the correct spots. So it is successfully clustered our data.\n\nweight\n\ntensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])\n\n\n\nweight[:,None].shape\n\ntorch.Size([1500, 1])\n\n\n\nweight[:,None]*X\n\ntensor([[    4.182,     4.205],\n        [    1.215,     1.429],\n        [    0.749,     0.706],\n        ...,\n        [    0.000,     0.000],\n        [    0.000,     0.000],\n        [    0.000,     0.000]])\n\n\n\ndef one_update(X):\n    for i, x in enumerate(X):\n        dist = torch.sqrt(((x-X)**2).sum(1))\n#         weight = gaussian(dist, 2.5)\n        weight = tri(dist, 8)\n        X[i] = (weight[:,None]*X).sum(0)/weight.sum()\n\n\ndef meanshift(data):\n    X = data.clone()\n    for it in range(5): one_update(X)\n    return X\n\n\n%time X=meanshift(data)\n\nCPU times: user 1.3 s, sys: 13.4 ms, total: 1.32 s\nWall time: 1.44 s\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\nSo that’s great news. And so we could test out our hypothesis. Could we use triangular just as well as we could have used Gaussian. So control slash comments and on comments, yeah, we got exactly the same results. So that’s good. It’s really important to know these keyboard shortcuts hit H to get a list of them. Some things that are really important don’t have keyboard shortcuts. So if you click help edit keyboard shortcuts. This list of all the things Jupyter can do and you can add keyboard shortcuts to things that don’t have them. So for example, I always add keyboard shortcuts to run all cells above and run all cells below. As you can see, I type Q and then A for above and Q and then B for below. All right. Now that was kind of boring in a way, because it did five steps, but we just saw the result. What did it look like? One step at a time. This isn’t just fun. It’s really important to be able to see things happening one step at a time because there are so many algorithms we do which are like updating weights or updating data, you know? So for stable diffusion, for example, very likely to want to show, you know, your incrementally denoising and so forth. So in my opinion, it’s important to know how to do animations. And I found the documentation for this unnecessarily complicated because it’s a lot of it’s about how to make them performant. But most of the time we probably don’t care too much about that. So I want to show you a little trick, a simple way to create animations without any trouble. So that matplotlib animation has something called FuncAnimation. That’s what we’re going to use to create an animation. You have to create a function and the function you’re going to be calling FuncAnimation passing in the name of that function and saying how many times to run it. And that’s what this frames the argument that says run this function this many times and then create an animation that that basically contains the result of that with a 500 millisecond interval between each one. So what’s this do one going to do to create one frame of animation? We will call our one_update.\nHere it is one_update, right? We’re going to call this that’s going to update our access and then we’re going to have an access which we’ve created here. So we’re going to clear whatever was on the plot before and plot our new data on that access. And then the only other thing you need to do is that the very first time it calls it, we want to plot it before running and d is going to be passed automatically the frame number. So for the zeroth frame, we’re going to not do the update, but it’s going to plot the data as it is already. I guess another way we could have done that would have been just to say if d then do the update the update, I suppose that should work too. Maybe it’s even simpler. Let’s see if I just break it. Okay So we’re going to clone our data. We’re going to create our figure in our subplots vertical FuncAnimation calling do_one 5 times, and then we’re going to display the animation. And so let’s see, so HTML takes some HTML and displays it and to_jshtml(), creates some HTML.\nSo that’s why it’s created. This HTML includes JavaScript. And so I click run one, two, three, four, five. That’s the five steps. So if I click loop, you’ll see them running again and again. Fantastic. So that’s how easy it is to create a matplotlib animation. So hopefully now you can use that to play around with some fun stable fusion animations as well. You don’t just have to use to to_jshtml. You can also create Oopsie Daisy. You can also create movies. For example. So you can call to_html5_video would be another option. And you can save an animation as a movie file. So this okay, all these different options for that, but hopefully that’s enough to get you started. So for your homework, I would like you when you create your k means or whatever, to try to create your own animation or create an animation of some stable diffusion thing that you’re playing with. So don’t forget this important ax.chear().without the ax.chear(), it prints it on top of the last one, which sometimes is what you want To be fair. But in this case, it’s not what I wanted. All right, So kind of slow half a second for not that much data, I’m sure would be nice. It was faster. Well, the good news is we can GPU accelerate it. The bad news is it’s not going to GPU You accelerate that Well, because of this loop, this is looping 1500 times. If we so looping is not going to run on the GPU."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#animation",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#animation",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "Animation",
    "text": "Animation\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\ndef do_one(d):\n    if d: one_update(X)\n    ax.clear()\n    plot_data(centroids+2, X, n_samples, ax=ax)\n\n\n# create your own animation\nX = data.clone()\nfig,ax = plt.subplots()\nani = FuncAnimation(fig, do_one, frames=5, interval=500, repeat=False)\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nSo the best we could do with this would be to move all this to the GPU. Now the problem is that calling something the GPU 1500 times from Python is a really bad idea because there’s this kind of huge communication overhead of this of flow of control and data switching back between the CPU and the GPU. It’s the kernel launching overhead. It’s bad news. So you don’t want to have a really big fast python loop that inside it calls cuda code. GPU code. So we need to make all of this run without the loop, which we could do with broadcasting. So let’s roll up our sleeves and try to get the broadcast version of this working. So generally speaking, the way we tend to do things with broadcasting on a GPU is we create batches or mini batches. So to create batches or mini batches, we don’t just call them batches. Nowadays, we create a batch size. So let’s say we’re going to do a batch size of five, so we’re going to do five at a time. All right, so how do we do five at a time? This is only doing one at a time. How do we do five at a time last before it’s final data and this time little x for our testing. So I’ve got to do everything ahead of time. Little tests as we always do. This is not now X[0] anymore, but it’s X colon bs (X[:bs]), so it’s the first five. This is now the first five items. Okay, so little x is now a five by two metrics. This is how mini batch the first five items as before. Our data itself is 1500 by two. All right. So we need a distance calculation.\nBut previously our distance calculation, previously a distance calculation only worked if Little x was a single number and it returned just the distance is from that to everything in Big X. But we need something that’s actually going to be return a Matrix right. We’ve got let’s say we’ve got five by two in little x and then in big X we’ve got something much bigger not to scale, obviously we’ve got 1500 by two. And what is the distance between these two things? Well, if you think about it, there’s going to be a distance between item one and item one, but there’s also going to be a distance between item one, item two, and there’s going to be a distance between let’s use a different color for the next one, item two and item one, right? So the output of this is actually going to be a matrix. The distances are actually going to give us a matrix where I mean, it doesn’t matter which way around, we do what we can decide, but if we it this way around for each of the five things in the mini batch, there will be 1500 distances. The distance between every one. So we’re going to need to do a broadcasting to do this calculation. So this is a function that we’re going to create and it’s going to create this, as you can see, five by 1500 output. But let’s say how we get it. So can we do X minus x? No, we can’t. Why is that? That’s because big X is 1500 by two and little x is five by two.\nSo it’s going to look at remember our roles right to left these compatible? Yes they are They’re the same these compatible. No, they’re not. Okay. Because they’re different. So that’s not possible to do What if though we want it to What if we insert in big X and axis at the start here and in little x we add an axis in the middle here then now these are compatible because you’ve got they’re the same because I should use arrows really? They are compatible because one of them is a one. And these are compatible because one of them is a one as well. So they are all compatible. And what it’s going to do is it’s going to do the subtraction between these directly and it’s going to copy this across all 1500 rows. It will copy it. This is going to be copied and then this across five rows, and then this will be copied across these 1500 rows because what broadcasting does, it’s not really copying, but it’s effectively copying. And so that gives us it can now subtract them and that gives us what we wanted, which is five by 1500 and then also by two because there’s both the x and the y. So that’s why this works. That’s what this is doing here. It’s taking the subtraction, it’s squaring them, and then summing over that last shortest axis, summing over the X and the Y squids and then take square root. I don’t know why as it touched that square root, we could just put dot square root at the end. But same, same. In fact, it’s worth mentioning that. So most things that can do on tensors, you can either write torch. as a function or you can write it as a method. Generally speaking, both should be fine. Not everything, but most things work in both ways. Okay, so now we’ve got this matrix, which is five by 1500. And the nice thing is that our Gaussian kernel doesn’t actually have to be changed to get the weights, believe it or not. And the reason for that is now how do we get the source code? I could move back up there or I can just type Gaussian question mark, question mark and see it. And the nice thing is that this is just this is a scalar, so it broadcasts over anything and then this is also just a scalar. So this is all going to work fine without any fiddling around. Okay, so now we’ve got a 5, 1500 a weight. So that’s the weight for each of the five things. There are mini batch each of the 1500 things, each of them as compared to. And then we’ve got the shape of the data itself, X.shape, which is the 1500 points. So now we want to apply each one of these weights to each of these columns. So we need to add a unit access to the end set at a unit, access to the end, we could say colon, comma, colon, common, none, but dot, dot, dot means all of the axes up until however many you need. So in this case, the last one comma None[…,None]."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 4/index.html#gpu-batched-algorithm",
    "href": "posts/Writing Stable Diffusion from Scratch 4/index.html#gpu-batched-algorithm",
    "title": "Writing Stable Diffusion from Scratch 4",
    "section": "GPU batched algorithm",
    "text": "GPU batched algorithm\nTo truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.\n\nbs=5\nX = data.clone()\nx = X[:bs]\nx.shape,X.shape\n\n(torch.Size([5, 2]), torch.Size([1500, 2]))\n\n\n\ndef dist_b(a,b): return (((a[None]-b[:,None])**2).sum(2)).sqrt()\n\n\ndist_b(X, x)\n\ntensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],\n        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],\n        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],\n        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],\n        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])\n\n\n\ndist_b(X, x).shape\n\ntorch.Size([5, 1500])\n\n\n\nX[None,:].shape, x[:,None].shape, (X[None,:]-x[:,None]).shape\n\n(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))\n\n\n\nweight = gaussian(dist_b(X, x), 2)\nweight\n\ntensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],\n        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],\n        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],\n        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])\n\n\n\nweight.shape,X.shape\n\n(torch.Size([5, 1500]), torch.Size([1500, 2]))\n\n\n\nweight[...,None].shape, X[None].shape\n\n(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\nSo this is going to add an access to the end. So this is going to turn this is going to turn weight dot shape from five comma 1500 to 5 comma 1500 from a one. And this is going to add an access to the start. Remember, it’s the same as X[None] = X[None,:,:]. And so let’s check our rules left, right to left. These are compatible because one of them is one. These are compatible because they’re both the same. And these are compatible because one of them is one. Okay? So it’s going to be copying each weight across to each of the X and Y, which is what we want. We want to we want to weight both of those components and it’s going to copy each of the 1500 points sorry, each of the point five times, because we do in fact want to wait every one of the five things now, mini batches, the separate set of weights for each of them. So that sounds perfect. So that’s how I think through these calculations. Okay. So we can now do that multiplication, which is going to give us something of five by 1500 by two, because we end up with the maximum of our ranks. And then we sum up over those 1500 points and that’s going to give us now five new data points. Now, something that you might notice here is that we’ve got a product and a sum\n\nnum = (weight[...,None]*X[None]).sum(1)\nnum.shape\n\ntorch.Size([5, 2])\n\n\n\nnum\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.217],\n        [231.302, 234.155]])\n\n\nAnd when you say a product and a sum that tells you maybe we should use einsum. So in this case, we’ve got our weight, we’ve got five by 1500. So let’s call those i and j As for the five and 1500 we’ve got, the X is 1500 by two. Now we want to take the product of that and that so wanted to use the same name for this row. So he use j again. And then k is the number of rows, that’s the two. And then we want to end up with ik. So einsum, exactly the same result. That’s great. But you might recognize this. That’s exactly the einsum Something we had just before when we were doing matrix multiplication. Oh, that is a matrix multiplication. We’ve just re-invented matrix multiplication using this rather nifty. So we could also just use that. And so, you know, again, this is like what I was playing around with this morning as I started to look at this and I was thinking like, Oh, you know, can we simplify this? I don’t like this kind of like messing around of axes and summing over dimensions and whatnot. And so it’s nice to get things down to Einstein or better still, get down to matrix multipliers. It’s just clearer, you know, it’s stuff that we recognize because we use them all the time they all work performance would be pretty similar. I suspect. Okay, so now that we’ve got that, we then need to do our sum and we’ve got our five points. This is our five denominators. So we’ve got our numerator that we calculated up here for our weighted for our weighted average. The denominator is just the sum of the weights, remember.\n\ntorch.einsum('ij,jk->ik', weight, X)\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\nweight@X\n\ntensor([[367.870, 386.231],\n        [518.332, 588.680],\n        [329.665, 330.782],\n        [527.617, 598.218],\n        [231.302, 234.155]])\n\n\n\ndiv = weight.sum(1, keepdim=True)\ndiv.shape\n\ntorch.Size([5, 1])\n\n\nAnd so numerator, divided by denominator is our answer. So again, we’ve gone through every we’ve checked out all the dimensions all along the way. So nothing’s going to surprise us. Don’t try and write a function like this. Just bang from scratch. Right. You’re going to drive yourself crazy. Instead, do it step by step. So here’s our meanshift algorithm, clone the data, go through five iterations, and now go from 0 to n and batch size at a time. So Python has something called slices so we can create a slice of X starting at one up to i + batch size. Right. Unless it’s gone past, in which case use n. And so then we’re just copying and pasting each of the lines of code that we had before. Actually had us copy the cells and merge them. Of course I don’t actually copy and paste because it’s slow and boring and there’s my final step to create the new X[s]. And so notice here s is not a single thing. It’s a slice of things you might not have seen slice before, but this is just internally what Python’s doing when he is :, And it’s very convenient when you to use the same slice multiple times. Okay, so let’s do that using Cuda. I would run it first without cuda, but I mean, I’ve done all the steps before, so it should be fine so puppet on the GPU and run meanshift and let’s see how long that takes. It takes one millisecond and previously without GPU, it took 400 milliseconds. And you know, the other thing we should probably think about doing is looking at other batch sizes as well because now we’re looping over batches, right? So if we make the batch size bigger that for loop, it’s going to do less looping. So what if we make that 16? Will that be any faster?\n\nnum/div\n\ntensor([[26.376, 27.692],\n        [26.101, 29.643],\n        [28.892, 28.990],\n        [26.071, 29.559],\n        [29.323, 29.685]])\n\n\n\ndef meanshift(data, bs=500):\n    n = len(data)\n    X = data.clone()\n    for it in range(5):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs,n))\n            weight = gaussian(dist_b(X, X[s]), 2.5)\n#             weight = tri(dist_b(X, X[s]), 8)\n            div = weight.sum(1, keepdim=True)\n            X[s] = weight@X/div\n    return X\n\nAlthough each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.\nOh, I see. Thank you. People on YouTube pointing out that I’m passing batch size, so I actually need to put it here. All right. So if we used a batch size of five, I wonder is missing. Oh, look at that. I’ve totally made it slow now. And in 57 milliseconds. Haha. Okay. 64, All right. Finally, that makes much more sense. Okay, so the bigger, bigger is better. And I guess we could actually do all 5000 at once. Probably nice. All right. Thank you YouTube friends, for solving that bizarre mystery. Okay. All right. So that’s pretty great. I mean, you know, to say that we can you optimize a meanshift like actually google for this to see if it’s been done before. And it’s the kind of thing that people, like write papers about. So I think it’s great that we can do it so easily with PyTorch. And it’s the kind of thing that previously had been considered, you know, a very challenging academic problem to solve. So maybe you can do something similar with some of these. Now, I haven’t told you what these are. So part of the homework is to go read about them and learn about them. dbscan, funnily enough, actually is an algorithm that I accidentally invented and then discovered a year later had already been invented.\nLSH comes up all the time, so that’s great. And in fact I have a strong feeling and I’ve been thinking about this for a while, that something like LSH could be used to speed this whole thing up a lot. Because if you think about it and again, maybe already this already exists, I don’t know. But if you think about it, when we did that distance calculation, the vast majority of the the weights or nearly zero. And so it seems pointless to create that big you know kind of eventually 1500 by 1500 matrix. That’s like it’d be much better if we just found the ones that were like pretty close by and just took their average. And so you want an optimized nearest neighbors, basically. And so this is an example of something that can give you a, an, a kind of a fast nearest neighbors algorithm or, you know, there are things like. kd trees and trees and stuff like that. So if you want to, like have a bonus bonus, invent a new meanshift algorithm which picks only the closest points to avoid the quadratic time. All right. So not very often you get an assignment, which is to invent a new meanshift algorithm, I guess a super, super bonus. Super, super bonus. Publish a paper that describes it. All right. You definitely get four points. If you do that, we’ll give you a number of points equal to the impact factor of the journal. You get it published in. Okay. So what I want to do now is move on to calculus, which for some of us may not be our favorite topic. Yeah, that’s funny. I found out that I in some version here already, I didn’t notice. Okay. Or is ahead of his time. That guy. Let’s talk about calculus. If you’re not super comfortable with derivatives and what they are and why we care. called The Essence of Calculus, which I strongly recommend watching. It’s just a pleasure, actually, to watch, as is everything that is on 3blue1brown.\n\ndata = data.cuda()\n\n\nX = meanshift(data).cpu()\n\n\n%timeit -n 5 _=meanshift(data, 1250).cpu()\n\n6.06 ms ± 746 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nplot_data(centroids+2, X, n_samples)\n\n\n\n\nHomework: implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.\nBonus: Implement it in APL too!\nSuper bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.\nSuper super bonus: Publish a paper that describes it :D\nWhere do we start. So the good news is just like you don’t have to know much linear algebra at all, you basically just need to know about matrix multiplication. You also don’t need to know much calculus at all. Just derivatives. So let’s think about like what derivatives are. So I’m going to borrow actually the same starting point that when 3blue1brown uses one of their videos is to consider a car, and we’re going to see how far away from home it is at various time points. Okay. So after a minute, let’s say after a second, it’s traveled five meters and then after 2 seconds, it’s traveled ten meters. Okay. And after 3 seconds, you can probably guess it’s traveled 15 meters. So there’s this concept here of a I got it the wrong way around. Obviously. So time, distance. Okay. So there’s this concept of Yeah, of like location. It’s like how far how far if you traveled at a particular point in time. So we can look at one of these points and find out how far that car has gone. We could also take two points and we can say, where did it start at the start of those two points and where did it finish at the end of those two points. And we can say between those two points, how much time passed and how far did they travel? In 2 seconds. They traveled ten meters. So we could now also say, all right, well, the slope of something, let’s rise over, run. You’ll see.ten meters in 2 seconds. And notice we don’t just divide the numbers. We also divide the units. We get five meters per second. So this here is now changed the dimensions entirely. We’re now not looking at distance, but we’re looking at speed or velocity. And it’s equal to rise over run. It’s equal to the rate of change. And what it says really is as time the X-axis goes up by one second, what happens to the distance in meters as one second passes? How does the number of meters change?\nAnd so maybe these aren’t points at all. Maybe there’s a function that it’s a continuum of points, and so you can do that for the function. So the function is a function of time. Distance is a function of time. And so we could say, what’s the slope of that function? And we can get the slope from point A to point B using over run. So from T one to T two the amount of distance, that’s the amount of time that’s passed is T2 minus T1. That’s how much time has passed that say this is t one, this is t two and the distance that they’ve traveled while they’ve moved from wherever they are at the end to wherever they were at the start. So that’s the change in distance, divided by the change in time, Change in distance, divided by change in time. Okay, So that’s why. So another way. Now the thing is, when we talk about calculus, we talk about finding a slope, but we talk about finding a slope of something that’s more often more tricky than this, right? We have slopes of things that look more like this and we say, what’s this slope absent Terrible attempt at drawing? Let’s maybe put it over here because I’m left handed. What’s this slope now? What does it mean to have like the idea of a velocity at an exact moment in time? It doesn’t mean anything, you know, at an exact moment in time, you’re just like it’s frozen. Right? What’s happening exactly now? But what you can do is you can say, well, what’s the change in time between a bit before a point and bit after a point? And what’s the change in distance between a bit before our point and a bit after our point? And so you can do the same kind of rise over run the thing, right? But you can make that distance between T2 and T1 smaller and smaller and smaller. So let’s rewrite this in a slightly different way. Let’s call the denominator the distance between T1 plus a little bit I’ll call it d, it’s that minus T1. So this is T2 = T1+d, right?\nIt’s T1 pluss a little bit. So we say oh his T1. Let’s add a little bit and notice that we when we write it this well let’s actually, let’s do the rest of it. So now f52 becomes f of t one plus a little bit and this is the same. And now notice here that t one plus t minus t one. We can delete all that because it just comes out to d. So this is another way of calculating the slope of our function. And as you get smaller and smaller and smaller, we’re kind of getting a triangle that’s tinier and tinier and tinier and it still makes sense it still that some time has passed and the car has moved, right? But it’s just smaller and smaller amounts of time. Now, if you did calculus at that college or at school, you might have done all this stuff messing around with limits, Epsilon Delta and blah, blah, blah. I’ve got really good news. It turns out you can actually just think of this d as a really small number where d is the difference if, uh, it’s. And so when we calculate the slope, we can write it in a slightly different way as the change in dY divided by the change in dX, this here is the change in dY, and this here is the change in dX. And so in other words, this here is a very small number. A very small number, and this here is the result in the function of changing by that very small number. And this way of thinking about calculus is known as the calculus of in infinitesimal. and it’s how Leibniz its originally developed it. And it’s been turned into a whole theory nowadays. And the reason I talk about it here is because when we do calculus, you’ll see me doing stuff all the time where. I act like the dx is a really small number. And when I was at school I was told I wasn’t allowed to do that. I’ve since learned that it’s totally fine, do that.\nSo, for example, next lesson, we’re going to be looking at the chain role, which looks like this. The dy/dx equals to dy/du * du/xd And I’m just going to say, Oh, these two small numbers can cancel out. And that’s why obviously they’re the same thing and that’s all going to work out nicely. So what do you know? What would be very helpful would be if before the next lesson, if you’re not totally up to date with your, you know, remembering all the stuff you did in high school about calculus is watch the 3blue1brown Course, we are not going to be looking. I don’t think at all that integration. So you don’t have to worry about that. Also we are not going to on the whole be doing any derivatives by hand. So for example, there are rules such as to why the dy/dx, if y equals x squared is 2x, these kind of rules, you’re not really going to have to learn because PyTorch is going to do them all for you. The one that we care about is going to be the chain role that we’re going to learn about that next time. Okay. I hope I don’t get beaten to a bloody pulp the next time I walk into a mathematician’s conference, I suspect I might. But hopefully I get away with this. I think it’s safe. We’ll see how we go. So thanks, everybody very much for joining me and really look forward to seeing you next time where we’re going to do backpropagation from scratch. We’ve already learned to multiply matrices, so once we’ve got backpropagation as well, we’ll be ready to train a neural network. All right. Thanks So."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 5/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 5/index.html",
    "title": "Writing Stable Diffusion from Scratch 5",
    "section": "",
    "text": "After This you should be comfortable with following :  1- Chain Rule  2- Basic linear NN architecture  3- SGD concept  4- Coding the model in python and pytorch\nSo now we’re going to take a look at notebook number three in the normal repo course22p2, and we’re going to be looking at the forward and backward passes of a simple multi-layer perceptron, a neural network. The initial stuff up here is just importing things and just settings and stuff that they’re just copying and pasting some stuff from previous notebooks around paths and parameters and stuff like that. So we’ll skip over this now. So will often be kind of copying and pasting stuff from one notebook to another. That’s kind first cell get things set up and have also loading in our data for MNIST as tensors. Okay, so we to start with, need to create the basic architecture of our neural network. And I did mention at the start of the course that we will briefly review everything that we need to cover. So we should briefly review what basic neural networks are and why they are what they are. So to start with, let’s consider a linear model. Oops, that’s not how I do it,let’s take the most simple example possible, which is we’re going to pick a single pixel from from our MNIST pictures. And so that will be our x and for our, for our y values. Then we’ll have some lost function of how good is this model? Sorry, not some lost function that’s even simpler for our y value. We’re going to be looking at how likely is it that this is, say, the number three based on the value of this one pixel? So the pixel, its value will be x and the probability of being the number three we’ll call y and if we just have a linear model, then it’s going to look like this. And so in this case, it’s it’s saying that the brighter this pixel is, the more likely it is that it’s the number three. And so there’s a few problems with this. The first one, obviously, is that as a linear model, it’s very limiting because maybe, you know, we actually are trying to draw something that looks more like this. So how would you do that? Well, there’s actually a neat trick we can use to do that. What we could do is, well, let’s first of all, talk about something we can’t do, something we can’t do is to add a bunch of additional lines. So consider what happens if we say, okay, well, let’s add a few different lines.\nSo let’s also add this line. So what would be the sum of our two lines? Well, the answer is, of course, that the sum of the two lines will itself be a line. So it’s not going to help us at all match the actual curve that we want. So here’s the trick. Instead, we could create a line like this that actually we could get this line. And now consider what happens if we add this original line with this new what’s not a line, right? It’s it’s two line segments. So what we would get is this everything to the left of this point is going to not be changed. If I add these two lines together, because this is zero all the way and everything to the right of it is going to be reduced. It looks like they’ve got similar slopes, so we might end up with instead. So this would all disappear here and instead we would end up with something like this. And then we could do that again, right? We could add an additional line that looks a bit like that. So it would go, but this time it could go even further out here and it could be something like this. Say, so what if we added that? Well, again, at the point underneath here, it’s always zero, so it won’t do anything at all. But after that it’s going to make it even more negatively sloped. And if you can see using this approach, we could add up lots of these rectified lines. These lines are truncated zero, and we could create any shape we want with enough of them. And these lines are very easy to create because actually all we need to do is to create just a regular line, just to get a regular line. Right. Which we can move up, down, left, right, change its angle, whatever, and then just say if it’s greater than zero, truncate it to zero, or we could do the opposite because through a line going the opposite direction, it has less than zero, we could say truncated to zero, and that would get rid of as we want. This whole section here and make it flat. Okay. So these are rectified lines and so we can sum up a bunch of these together to basically match any arbitrary curve.\n(Bahman note )This is amazing , the is intuition of a theory that we can create or mimic any funtion with neural net. Like creating anything fron linear model. (I do not remember the paper or the creator / he is very famous , he said something like this in his paper but did not try for more layers and years after that when people did that new era of AI started) . (Bahman note )\nSo let’s start by doing that. Oh, the other thing we should mention, of course, is that we’re going to have not just one pixel, but we’re going to have lots of pixels. So to start with, the kind of most, you know, slightly, you know, the only slightly less simple approach, we could have something where we’ve got, you know, pixel number one and pixel number two, we’re looking at two different pixels to see how likely they are to be the number three. And so that would allow us to draw more complex shapes that have some kind of surface between them. Okay. And then we can do exactly the same thing is to create these surfaces.\nWe can add up lots of these rectified lines together, but now they’re going to be kind of rectified planes, but it’s going to be exactly the same thing. We’re going to be adding together a bunch of lines, each one of which is truncated at zero. Okay. So that’s the quick review. And so to do that, we’ll start out by just defining a few variables. So n is the number of training examples. m Is the number of pixels. c is the number of possible values of our digits. And so here they are, (50000, 784, tensor(10)). Okay. So what we do is to is we basically decide ahead of time how many of these lines segment thing is to add up. And so the number that we create in a layer is called the number of hidden nodes or activations. So we call that nh, So let’s just arbitrarily decide on creating 50 of those. So in order to create lots of lines which where they’re going to truncate at zero, we can do a matrix multiplication. So with the matrix multiplication, we’re going to have something where we’ve got by 500000 rows by 784 column. Yeah, by 784 columns. And we’re going to multiply that by something with 784 rows and ten columns and why is that? Well, that’s because if we take this very first line of this first vector here, write one of 784 values there. The pixel values of the first image. Okay, So this is our first image. And so they’re each going to each of those 700 different values, but we multiply it by each of these 784 values and in the first column, the zero index column. And that’s going to give us a number in our output. So our output is going to be And so that result will multiply those together and add them up and that result is going to end up over here in this first. So and so each of these columns is going to eventually represent if this if this was a this is a linear model in this case, this is just the example of doing a linear model, each of these cells is going to represent the probability. So this first column will be the probability of being zero, and the second column will be the probability of one. The third column will be the probability of being a two and so forth. So that’s why we’re going to have these ten columns, each one allowing us to write the 784 inputs. Now of course, we’re going to do something bit more tricky than that, which is actually we’re going to have a 74 by 50 input going into a 784 by 50 output to create the 50 hidden layers. Then we’re going to truncate those at zero and then multiply that by a 50 by 10 to create an output. So we do it in two steps. So and the way it sgd works is we start with just this is our weight matrix here and this is our data, this is our outputs. The way it works is that this weight matrix is initially filled with random values. So called this contains our pixel values. This contains the results. So w is going to start with random values. So here’s our weight matrix. It’s going to have, as we discussed, 50,000 by 50 random values and it’s not enough just to multiply. We also have to add. So that’s what makes it a linear function. So we call those the biases, the things we add. We can just start those zeros, so we’ll need one for each output. So 50 of those and so that’ll be layer one. And then as we just mentioned, layer two will be a matrix that goes from 50 hidden. And now I’m going to do something totally cheating. To simplify some of the calculations for the calculus, I’m only going to create one output."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 5/index.html#the-forward-and-backward-passes",
    "href": "posts/Writing Stable Diffusion from Scratch 5/index.html#the-forward-and-backward-passes",
    "title": "Writing Stable Diffusion from Scratch 5",
    "section": "The forward and backward passes",
    "text": "The forward and backward passes\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nfrom pathlib import Path\nfrom torch import tensor\nfrom fastcore.test import test_close\ntorch.manual_seed(42)\n\nmpl.rcParams['image.cmap'] = 'gray'\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Mar 27 04:43 mnist.pkl.gz\n\n\n!ls -l data\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 5/index.html#foundations-version",
    "href": "posts/Writing Stable Diffusion from Scratch 5/index.html#foundations-version",
    "title": "Writing Stable Diffusion from Scratch 5",
    "section": "Foundations version",
    "text": "Foundations version\n\nBasic architecture\n\nn,m = x_train.shape\nc = y_train.max()+1\nn,m,c\n\n(50000, 784, tensor(10))\n\n\n\n# num hidden\nnh = 50\n\nWell, I’m going to create one output. That’s because I’m not going to use cross entropy just yet. Instead, I’m going to use MSE. So actually I’m going to create one output, which will literally just be what do i think it is? from 0 to 10. And so then we’re going to compare those to the actual. So these will be our y predicted to only use a little hat for that and we’re going to compare that to our actuals. And yeah, in this very hacky approach, let’s say we predict over here the number nine and the actual is the number two and we’ll compare those together using MSE, which will be a stupid way to do it because it’s saying that nine is further away from being true than two, nine is farther away from true than it is from four in terms of how correct it is, which is which is not what we want at all. But this is what we’re going to do just to simplify our starting point. So that’s why we’re going to have a single output for this weight matrix in a single output for this bias. So linear, let’s create a function for putting extra linear layer with these weights in these biases. So it’s a matrix multiply and an add. All right. So we can now try it. So if we multiply we doing x_valid this time? So just to clarify x_valid is 10,000 by 784. So if we put x_valid through our weights and biases with a linear layer, we end up with a 10,000 by 50. So 10,050 long hidden activations, they’re not quite ready yet because we have to put them through relu. And so we’re going to clamp at zero, so anything under zero will become zero. And so here’s what it looks like when we go through the linear layer and then the relu here. And you can see he has a tensor with a bunch of things, some of which are zero, or they’re positive. And so that’s the result of this match, this matrix multiplication. Okay. So to create a basic MLP multi-layer perceptron from scratch, we will take our mini batch of X’s x, b is an x match. We will create our first layers output with a linear and then we will put that through over here and then that will go through the second linear. So the first one uses the w1, b1. Okay, these ones and the second one uses the w2 to b2. And so we’ve now got a simple model and as we hoped, when we pass in the validation set, we get back 10000 digits. So that’s a good start. Okay, so let’s use our ridiculous lost function of MSE. So our results is 10,000 by one and our y_valid is just a vector. Now what’s going to happen if I do res minus y-valid?\n\nw1 = torch.randn(m,nh)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)\nb2 = torch.zeros(1)\n\n\ndef lin(x, w, b): return x@w + b\n\n\nt = lin(x_valid, w1, b1)\nt.shape\n\ntorch.Size([10000, 50])\n\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = relu(t)\nt\n\ntensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n        ...,\n        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])\n\n\n\ndef model(xb):\n    l1 = lin(xb, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\n\nres = model(x_valid)\nres.shape\n\ntorch.Size([10000, 1])\n\n\n\n\nLoss function: MSE\nSo before you continue in the video, have a think about that. What’s going to happen if I do res minus y_valid by thinking about the numpy broadcasting rules we’ve lent? Okay, let’s try it. Oh terrible. We’ve ended up with a 10,000 by 10,000 matrix. So 100 million points now we would expect an MSA to contain a thousand points. Why does that happen? The reason it happened is because we have to start out at the last dimension and go right to left and we compare to the 10,000 to the one and say, are they compatible? And the answer is, that’s right. Alexi And the chats got it right, broadcasting roles. So the answer is that this one will be broadcast over these 10,000. So this pair here will give us the next one and we’ll also move here to the next one. Oh, there is no next one. What happens now? If you remember the rules in search, say, unit access for us. So I now have 10,000 by one. So that means each of the 10,000 outputs from here is will end up being broadcast across the 10,000 rows here. So that means that will end up for each of those 10,000. We’ll have another 10,000. So we’ll end up with a 10,000 by 10,000 output. So that’s not what we want. So how could we fix that? Well, what we really would want, would we want this to be If that was 10,000 comma one, then we’d compare these two right to left and they’re both one. So those those match and there’s nothing to broadcast because they’re the same. And then we’ll go to the next one, 10000 to 10000 those match. So that just go element wise for those and we’d end up with exactly what we want it to hand out with 10,000 results. Or alternatively we could remove this dimension and then again, same thing we’re going to add right to left compatible 10,000. So they will get element wise operation. So in this case I got rid of the trailing comma one. It’s a couple of ways you could do that. One is just to say, okay, grep every row and the zeroth column of res and that’s going to turn it from a 10,000 by one into a 10,000.\nOr alternatively we can say dot squeeze. Now dot squeeze removes all trailing unit vectors and possibly also prefix unit vectors. I can’t quite recall. I guess we should try. So let’s say res None comma calling, comma None, q.shape. Okay. So if I go q.squeeze. shape. Okay. So all the unit vectors get removed. Sorry, all the unit dimensions get removed I should say. Okay. So now that we’ve got a way to remove that axis that we didn’t want, we can use it. And if we do that subtraction now we get 10,000 just like we wanted it. So now let’s get our training and validation wise, we’ll turn them into floats because we’re using MSI. So let’s calculate our predictions for the training set, which is 50,000 by one. And so if we create an MSE function that just does what we just said we wanted, so it does a subtraction and then squares it and then takes the mean that the MSE. So there we go. We now have a loss function being applied to our training set. Okay, now we need gradients. So as we briefly discussed last time, gradients slopes and in fact maybe it would even be easier to look at last time. So this is last times notebook. And so we saw how the gradient at this point is the slope here and so it’s the as we discussed rise over run now so that means as we increase in this case time by one the distance increases by how much that’s what the slope is. So why is this interesting? The reason it’s interesting is because let’s consider our neural network. Our neural network is some function that takes two things two groups of things. It contains a matrix of our inputs, and it contains our weight matrix. And we want to and let’s assume we’re also putting it through a loss function. So let’s say, well, I guess we can be explicit about that. So we could say we then take the result of that and we put it through some lost function.\nSo these are the predictions and we compare it to our actual dependent variable. So that’s our neural net and that’s our last function. Okay. So if we can get the derivative of the loss with respect to, let’s say, one particular weight, so let’s say weight, and I’m like number zero, what is that doing? Well, it’s saying as I increase, increase the weight by a little bit, what happens to the loss? And if it says, oh, well, that would make the loss go down, then obviously I want to increase the weight by a little bit. And if it says, Oh, it makes the loss go up, then obviously I want to do the opposite. So that derivative of the loss with respect to the weights each one of those tells us how to change the weights. And so to remind you, we then change each weight by that derivative times a little bit and subtract it from the original weights. And we do that a bunch of times, and that’s called SGD. Now, there’s something interesting going on here, which is that in this case there’s a single input and a single output, and so the derivative is a single number at any point. It’s the speed. In this case, the vehicle is going. But consider a more complex function like, say, this one. Now, in this case, there’s one output, but there’s two inputs. And so if we want to take the derivative of this function, then we actually need to say, well, what happens if we increase x by a little bit? And also what happens if we increase y by a little bit? And in each case what happens to x? And so in that case, the derivative is actually going to contain two numbers, right? It’s going to contain the derivative of x with respect to y and it’s going to contain the derivative of z with respect to x. What happens if we change each of these two numbers? So, for example, these could be, as we discussed, two different weights in our neural network and Z could be our loss. For example, now we’ve got actually 784 inputs, right? So we would actually have 784 of these. So we don’t normally write them all like that. We would just say, yes, this little squiggly symbol to say the derivative of the loss across all of them with respect to all of the weights. Okay. And that’s just saying that there’s a whole bunch of them. It’s a shorthand way of writing this. Okay. So it gets more complicated still, though, because think about what happens if, for example, you were in the first layer where we’ve got a weight matrix that’s going to end up giving us 50 outputs, right? So for every image, we’re going to have 784 inputs to our function and we’re going to have 50 outputs to our function.\nAnd so in that case, I can’t even draw it right because like for every even if I had two inputs and outputs, then as I increase my first input, I’d actually need to say, how does that change both of the two outputs? And as I change my second input, how does that change both of my two outputs? So for the full thing, you’re actually going to end up with a matrix of derivatives. It basically says for every input that you change by a little bit, how much does it change every output of that function? So you’re going to end up with a matrix. So that’s what we’re going to be doing, is we’re going to be calculating these derivatives, but rather than being single numbers, they’re going to actually contain matrices with a row for every input and a column for every output and a single cell in that matrix. Well, tell us, as I change this input by a little bit, how does it change this output Now? Eventually we will end up with a single number for every input, and that’s because our loss in the end is going to be a single number. And this is like a a requirement that you’ll find when you try to use SGD is that your loss has to be a single number. And so we generally get it by either doing the the sum or a mean or something like that. But as you’ll see on the way there, we’re going to have to be dealing with these matrix of derivatives. So I just want to mention, as I might have said before, I can’t even remember there is this paper that Terence Power and I wrote a while ago, which goes through all this, and it basically assumes that you only know high school calculus. And if you don’t checkout Khan Academy, but then it describes matrix calculus in those terms. So it’s going to explain to you. Exactly. And it works through lots and lots of examples. So, for example, as it mentions here, when you have this matrix of derivatives, we call that a Jacobean matrix. So like there’s all these like words, it doesn’t matter too much if you know them or not, but it’s convenient to be able to talk about, you know, the matrix of all of the derivatives. If somebody just says this Jacobean, it’s a little convenient. It’s a bit, a little bit easier than saying the matrix of all of the derivatives where all of the rows are things that are, are all the inputs and all the columns of the outputs. So yeah, if you want to really understand, get to a point where papers are easier to read particular, it’s quite useful to know this notation and, and, and definitions of words. https://explained.ai/matrix-calculus/\nYou can certainly get away without it. It’s just something to consider. Okay, so we need to be able to calculate derivatives at least of a single variable. And I am not going to worry too much about that because that is something you do in high school math and b, because your computer can do it for you and so you can do it symbolically using something called sympy, which is really great. So if you create two symbols called x and y, you can say please differentiate x squared with respect to x. And if you do that sympy will tell you the answer is 2x. If you say differentiate three x squared plus nine with respect to x, sympy will tell you that six x and a lot of you probably will have used Wolfram Alpha that does something very similar. I kind of quite like this because I can quickly do it inside my notebook and included in my prose. So I think sympy is pretty cool. So you know, basically yeah. That if you, you know, you can quickly calculate derivatives on a computer. Having said that, I do want to talk about why the derivative of three x squared plus nine equals six x, because that’s going to be very important. So three x squared plus nine. So we’re going to start with the the information that that derivative of a to the b with respect to a equals ba , So for example, the derivative of x squared with respect to x equals 2x. So that’s just something I’m hoping you remember from high school or refresh your memory is in Khan Academy also. So there that is there. So what we could now do is we could rewrite this derivative as 3u plus nine and then we’ll write u equals x squared. Okay, Now this is getting easier. The derivative of two things being added together is simply the sum of the derivatives. Oh, I forgot. B minus one of the exponent. Thank you. So B eight the power of b minus one. That’s what it should be, which would be two x to the power of one and the one is not needed. Thank you for fixing that. All right. So we just some of them up so we get the derivative of 3u is actually just well, it’s going to be the derivative of that plus the derivative of that. Now the derivative of any constant with respect to a variable is zero. Because if I change something an input, it doesn’t change the constant, it’s always nine. So that’s going to end up as zero. And so we’re going to end up with the dy/du equals something plus zero. And the derivative of 3u with respect to u is just three, because it’s just a line that’s its slope. Okay, But that’s not dy/dx Well, the cool thing is that dy/dx is actually just equal to dy/du  du/dx. So I’ll explain why in a moment. But for now then let’s recognize we’ve got du/dx = 2x, we know that one 2x so we can now multiply these two bits together and we will end up with two X times three, which is six x, which is what sympy they told us. So fantastic. Okay, this is something we need to know really well, and it’s called the chain rule and it’s best to understand it intuitively.\nSo to understand it intuitively, we’re going to take a look at an interactive animation. So I found this nice interactive animation on this page here https://webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html\nAnd the idea here that we’ve got a wheel spinning around and each time it spins round, this is x going up. Okay. So at the moment there’s some change in x,dx over a period of time. All right. Now this wheel is eight times bigger than this wheel. So each time this goes round, once, if we connect the two together, this wheel would be going round four times faster because the difference between the multiple between eight and two is four. Maybe I’ll bring this up to here. So now that the this wheel is has got twice as big a circumference as the other wheel, each time this goes around, once this is going around two times. So the change in u each time X goes round, once the change in u will be two. So that’s what du/dx is saying. The change in u for each change in x is two. Now we could make this interesting by connecting this wheel to this well. Now this wheel is twice as small as this wheel. So now we can see that again. Each time this spins round, once this spins round twice because this has twice the circumference of this. So therefore dy/du equals two. But now that means every time this goes round, once this goes round twice every time this one goes round, once this one goes round twice. So therefore every time this one goes round once this one goes round four times So to dy/dx equals four. So you can see here how the two well how the, du/dx has to be multiplied with the dy/du to you to get the total. So this is what’s going on in the chain role and this is what you want to be thinking about is this idea that you’ve got one function that is kind of this intermediary. And so you have to multiply the two impacts to get the impact of the X. Wheel, on the y Wheel, we’ll help you find that useful. I find this personally, I find this intuition quite useful. So why do we care about this? Well, the reason we care about this is because we want to calculate the gradient of our MSE applied to our model. And so our inputs are going through a linear, they’re going through a relu here, they’re going through another linear, and then they’re going through an MSE. So there’s four different steps going on. And so we’re going to have to combine those all together. And so we can do that with a chain rule. So if our steps are that lost function is the so we’ve got the lsot function, which is some function of the predictions and the actuals. And then we’ve got we’ve got the second layer is a function of actually let’s say let’s call this the output of the second layer slightly. We have notation, but hopefully it’s not too bad. It’s going to be a function of the relu of the activations and relu activation are a function of the first layer and the first layer is a function of the inputs. Oh, and of course this also has weights and biases. So we’re basically going to have to calculate the derivative of that. Okay. But then remember that this is itself a function. So then we need to multiply that derivative by the derivative of that, but that’s also a function. So we have to multiply that derivative by this, but that’s also a function. So we have to multiply that derivative by this. So that’s going to be our approach. We’re going to start at the end, we’re going to take its derivative and then we’re going to gradually keep multiplying as we go each step through. And this is called back propagation. So back propagation sounds pretty fancy, but it’s actually just using the chain rule. Gosh, I didn’t spell that very well.it’s just using the chain rule. And as you’ll see, it’s also just taking advantage of a computational trick of memorizing some things on the way. And in our chat, Stephen made a very good point about understanding nonlinear functions in this case, which is just a consider that the wheels could be growing and shrinking all the time as they’re moving, but you’re still going to have the same compound effect, which I really like that. Thank you, Sylvar There’s also a question in the chat about why is this column comma zero being placed in the function given that we can do it outside the function? Well, the point is we want an MSE function that will apply to any output, not that we’re not using it once we want it to work any time. So we haven’t actually modified preds or anything like that or y_train. So we want this to be able apply to anything without us having to like preprocessor. It is basically the idea here. Okay, so let’s take a look at the basic idea. So he is going to do a forward pass in a backward pass. So the forward passes where we calculate the loss.\nSo the loss is, oh, I’ve got an error here that should be diff. Yeah, we go. So the loss is going to be the output of our neural net minus our target squared. Then take the mean, Okay. And then our output is going to be the output of the second linear layer. The second linear layer’s input will be the relu, the relu input will be the first layer.\nIt’s going to take our input, put it through a linear layer, put that through a relu, put that through a linear layer and calculate the MSE. Okay, That bit hopefully is pretty straightforward. So\nwhat about the backward pass? So the backward pass, what I’m going to do and you’ll see why in a moment is I’m going to store the gradients of each layer. So for example, the gradients of the loss with respect to its inputs in the layer itself. So I’m going to create a new attribute, I could call it anything I like, and it’s going to call it (.g) .So I’ve got to create, a new attribute called out.g, which is going to contain the gradients. You don’t have to do it this way, but as you’ll see, it turns out pretty convenient. So that’s just going to be two times the difference because we’ve got different squared, right? So that’s just the derivative. And then we have taken the mean here. So we have to do the same thing here divided by the input shape. And so that’s, that’s those gradients, that’s good. And now what we need to do is multiply by the gradients of the previous layer. So what are the gradients of a linear layer? I’ve created a function for that here. So the gradient is a linear layer. We’re going to need to know the weights of the layer. We’re going to need to know the biases of the layer. And then we’re also going to know the input to the linear layer, because that’s the thing that’s actually being manipulated in here. And then we also going to need the output because we have to multiply by the gradients because we’ve got the chain role. So again, we’re going to store the gradients of our input like inp.g . So this would be the gradients of our output with respect to the input (inp.g), and that’s simply the weights because the weights. (w.t). So a matrix multiplier is just a whole bunch of linear functions. So each one slope is just its weight, but you have to multiply it by the gradient of the outputs because of the chain role and then the gradient of the outputs. With respect to the weights (w.g) is going to be the input times the output summed up. I’ll talk more about that in a moment. The derivatives of the bias is very straightforward. It’s the gradients of the output added together because the bias is just a constant value. So for the chain role, we simply just use output times one, which is output. So for this one here again, we have to do the same thing we’ve been doing before, which is multiply by the output gradients because of the chain rule and then we’ve got the input weights. So every single one of those has to be multiplied by the outputs. And so that’s why we have to do an unsqueese minus one. So what I’m going to do now is I’m going to show you how I would experiment with this code in order to understand it, and I would encourage you to do the same thing. It’s a little harder to do this one cell by cell because we kind of want to put it all into this function like this. So we need a way to explore the calculations interactively and the way we do that is by using the Python debugger.\nHere is how you let me see a few ways to do this. Here’s one way to use the Python debugger. The Python debugger is called PDB. So if you say PDB dot set trace in your code, then that tells the debugger to stop execution. When it reaches this line, it sets a breakpoint. So if I call forward and backward, you can see here it stopped and the interactive Python debugger I PDB has popped up with an arrow pointing at the line of code. It’s about to run. And at this point there’s a whole range of things we can do to find out what they are. We page for help understanding how to use the Python debugger. It’s one of the most powerful things I think you can do to improve your coding. So one of the most useful things you can do is to print something. You see all these single letter things. They’re just shortcuts. But in a debugger you want to be able to do things quickly instead of typing print, just type p. So for example, let’s take a look at the shape of the input. So I type p for print input shape. So I’ve got a 50,000 by 50 input to the last layer that makes sense. These are the hidden activations coming in to the last layer for every one of our images. What about the output gradients? And there’s that as well. And actually a little trick you can ignore that you don’t have to use the p at all if your variable name is not the same as any of these commands. So I could have just typed out.g.shape, get the same thing. Okay. So you can also put in expressions so let’s have a look at the shape of this. So the output of this is let’s see if it makes sense. We’ve got the input 50,000 by 50. We put a new axis on the end unsqueeze minus one is the same as doing dot is indexing it with dot dot dot comma None sets but a new axis at the end. So that would have become And then the out.g.unsqueeze. We’re putting in the first dimension, so we’re going to have 50,000 by 50 by one times 50,000 by one by one. And so we’re going to end we’re going to end up getting this broadcasting happening over these last two dimensions, which is why we end up with 50,000 by 50 by one. And then with summing up, this makes sense, right? We want to sum up over all of the inputs, each image is individually contributing to the derivative. And so we want to add them all up to find their total impact. Because remember the sum of a bunch of the derivative of the sum of functions is the sum of the derivatives of the functions. So we can just some of them up. Now, this is one of these situations where if you see a times and a sum and unsqueeze, it’s not a bad idea to think about Einstein summation notation. Maybe there’s a way to simplify this. So first of all, let’s just see how we can do some more stuff in the debugger. I’m going to continue so just continue running.\nSo press c for continue and it keeps running until it comes back again to the same spot. And the reason we’ve come to the same spot twice is because Lin_grad is called two times. So we would expect that the second time we’re going to get a different bunch of inputs and outputs. And so I can print out a couple of the inputs and output gradient. So now, yes, So this is the first layer going into the second layer. So that’s exactly what we expect to find out What called this function, you just type w is where am I? And so you can see here, where am I? Oh, forward and backward was called. See the arrow that called lin_grad the second time and now we’re here in w.g equals. If we want to find out what actually ends up being equal to. I can press n for to say go to the next line and so now we’ve moved from line 5 to line 6, the instruction point is now looking at line six. So I could now print out, for example, w.g.shape, and that’s the shape our weights. One person on the chat has pointed out that you can use breakpoint instead of this import PDB business. Unfortunately, the breakpoint keyword doesn’t currently work in Jupyter or in IPython, so we actually can’t. Sadly. That’s why I’m doing it the old fashioned way. So this way maybe they’ll fix the bug at some point. But for now we have to type all this. Okay, so those are a few things to know about, but I would definitely suggest looking up a Python PDB tutorial to become very familiar with this incredibly powerful tool because it really is so very handy. So if I just press continue again, it keeps running all the way to the end and it’s now finished running forward and backward. https://realpython.com/python-debugging-pdb/\n(Of course, mse is not a suitable loss function for multi-class classification; we’ll use a better loss function soon. We’ll use mse for now to keep things simple.)\n\nres.shape,y_valid.shape\n\n(torch.Size([10000, 1]), torch.Size([10000]))\n\n\n\n(res-y_valid).shape\n\ntorch.Size([10000, 10000])\n\n\nWe need to get rid of that trailing (,1), in order to use mse.\n\nres[:,0].shape\n\ntorch.Size([10000])\n\n\n\nres.squeeze().shape\n\ntorch.Size([10000])\n\n\n\n(res[:,0]-y_valid).shape\n\ntorch.Size([10000])\n\n\n\ny_train,y_valid = y_train.float(),y_valid.float()\n\npreds = model(x_train)\npreds.shape\n\ntorch.Size([50000, 1])\n\n\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\n\nmse(preds, y_train)\n\ntensor(4308.76)\n\n\n\n\nGradients and backward pass\n\nfrom sympy import symbols,diff\nx,y = symbols('x y')\ndiff(x**2, x)\n\n2*x\n\n\n\ndiff(3*x**2+9, x)\n\n6*x\n\n\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g = out.g.sum(0)\n\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1>0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\nSo when it’s finished, we would find that there will now be, for example, w1.g, because this is this is the gradients that it just calculated and there would also be a x_train.g and so forth. Okay, so let’s see if we can simplify this a little bit. So I would be inclined to take these out and give them their own variable names just to make life a bit easier. It would have been better if I’d actually done this before the debugging, so it be a bit easier to type. So let’s set i and o equal to input and output .g.unsqueeze. And actually let’s get rid of the that we just call it. So i will be input. All right. So we’ve got here actually now let’s put the end sequences back. I changed my mind. So. So just by. Oh, okay. So we’ll get rid of our breakpoint and double check that we’ve got our gradients. Okay. And I guess before we re run out, we should probably set those to zero. So what I would do here to try things out is I put my breakpoint there and then I would try things. So let’s go next. And so I realize here that what we’re actually doing is we’re basically doing exactly the same thing as einsum. Some would say. So I could test that out by trying an einsum. Right. Because I’ve just got this is being replicated. And then I’m summing over that dimension because that’s the multiplication that I’m doing. So I’m basically multiplying the first dimension of and then summing over that dimension. So I could try running that and oh, it works. So that’s interesting. I’d be easier if I just read this shaped bit long, isn’t it, dot shape. Okay, so it’s a 50 by one. Oh, and I’ve got zeros because I did exchange that zero. That was silly. Let’s try that again. That should be done. Gradient.zero. Okay, so let’s try doing an einsum. And there we go. That seems to be working. That’s pretty cool. So we’re we’ve multiply at this repeating index. So we were just multiplying the first dimensions together and then summing over them. So there’s no i here. Now, that’s not quite the same thing as a matrix multiplication, but we could turn it into the same thing as more metaphor application just by swapping in i,and j so that the other way around that way would have j, i comma , i k we can swap into dimensions very easily. That’s what’s called the transpose. So that would become a matrix multiplication if we just use the transpose. And in numpy the transpose is the capital T attribute. So here is exactly the same thing. Using a matrix multiply and a transpose. And let’s check. Yeah, that’s the same thing as well. Okay, cool. So that tells us that now we’ve checked in our debugger that we can actually replace all this with a matrix. Multiply. We don’t need that anymore. Let’s see if it works. That does. All right. x_trained.g. Okay. So hopefully that’s convinced you that the debugger is a really handy thing for playing around with numeric programing ideas or coding in general. And so I think now is a good time to take a break. So let’s take a eight minute break and I’ll see you back here after a seven minute break. I’ll see you back here in. 7 minutes. Thank you.\nAll right. I’m back. What have I missed? Hmm? Okay. Welcome back. So we’ve calculated our derivatives, and we want to test them. Luckily, PyTorch already has derivatives implemented, so I’ve got a totally cheat. And. And is PyTorch to calculate the same derivatives. So don’t worry about how this works yet, because I’m actually going to be doing all this from scratch anyway. For now, I’m just going to run it all through PyTorch and check that that derivatives are the same as ours. And they are. So we’re on the right track. Okay. So this is all pretty clunky. I think we can all agree and obviously it’s clunkier than what we do in PyTorch.\n\nforward_and_backward(x_train, y_train)\n\n\n# Save for testing against later\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n\nWe cheat a little bit and use PyTorch autograd to check our results.\n\ndef mkgrad(x): return x.clone().requires_grad_(True)\nptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))\n\n\ndef forward(inp, targ):\n    l1 = lin(inp, w12, b12)\n    l2 = relu(l1)\n    out = lin(l2, w22, b22)\n    return mse(out, targ)\n\n\nloss = forward(xt2, y_train)\nloss.backward()\n\n\nfor a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)\n\nSo how do we simplify things? That’s a really cool refactoring that we can do. So what we’re going to do is we’re going to create a whole class for each of our functions, for the regular function and for the linear function. So the the way that we’re going to do this is we’re going to create a dunder call. What is dunder call do let me show you. So if I create a class and we’re just going to set that to print, hello. So if I create an instance of that class and then I call it as if it was a function oops, missing the dunder bit here, call it as if it’s a function. It says hi. So in other words, you know, everything can be changed in Python. You can change how a class behaves, you can make it look like a function. And to do that you simply define dunder call your passing it an argument like so. Okay, so that’s what dunder call does. It just says it’s just a little bit of syntax, sugary kind of stuff to say. I want to be able to treat it as if it’s a function without any method at all. You can still do it the method way you could have done this. I don’t know why you’d want to, but you can. But because it’s got this special magic named dunder call, you don’t have to write that time to dunder call at all. So here, if we create an instance of the earlier class, we can treat it as a function. And what it’s going to do is it’s going to take its input and do the relu on it. But if you look back at the forward and backward, there’s something very interesting about the backward pass, which is that it has to know about, for example, this intermediate gets passed over here, this intermediate calculation gets passed over here because of the chain role. We’re going to need some of the intermediate calculations and not just because the chain rule, but because of actually how the derivatives are calculated. So we need to actually store each of the layer intermediate calculations.\nAnd so that’s why value doesn’t just calculate and return the output, but it’s also stores its output and it also stores its input. So that way then when we call backward, we know how to calculate that we set the inputs gradient because remember we stored the input. So we can do that right and it’s going to just be oh input greater than zero dot float. Right? So that’s the definition. Okay. Of the derivative of a relu and then chain rule. So that’s how we can calculate the forward pass and the backward pass for relu. And we’re not going to have to then store all this intermediate stuff separately. It’s going to happen automatically so we can do the same thing for a linear layer. Now linear layer needs some additional state weights and biases. relu doesn’t, right? So there’s no in it. So when you create a linear layer, we have to say what are its weights, what are its biases? We store them away. And then when we call it when the forward pass, just like before we store the input. So that’s exactly the same line here. And just like before we calculate the output and store it and then return it. And this time of course we just call Lin and then for the backward pass, it’s the same thing. Okay, so the input gradients we calculate just like before, oh .t brackets is exactly the same with a little t as big T is as a property. So that’s the same thing. That’s just the transpose. Calculate the gradients of the weights again with a chain rule and the bias, just like we do that before and they’re all being stored in the appropriate places. And then for MSE, we can do the same thing. We don’t just calculate the MSE, but we also store it and also now the MSE. And it just needs two things an input and a target. So we’ll store those as well so that in the backward pass we can calculate its gradient of the input as being two times the difference. And there it all is. Okay, so our model now it’s much easier to define. We can just create a bunch of layers, linear one,, w1,b1 relu, linear ,w2,b2, and then we can store an instance of the MSE. So this is not calling MSE, it’s creating an instance of the MSE class, and this is an instance of the Lin class.\nThis is an instance of relu class. So that is being stored. So then when we call the model, we pass it, our inputs in our target. We go through each layer, set x equal to the result of calling that layer and then pass that to the loss. So there’s something kind of interesting here that you might have noticed, which is that Something interesting here is that we don’t have two separate functions inside and inside our model, the lost function being applied to a separate neural net. But we’ve actually integrated the lost function directly into the neural net, into the model, see how the loss is being calculated inside the model. Now, that’s neither better nor worse than having it separately. It’s just different. And generally a lot of hugging faced stuff does it this way. They actually put the loss inside the forward Most stuff in FastAI and a lot of other libraries does it separately, which is the loss is a whole separate function and the model only returns the result of putting it through the layers. So for this model we’re going to actually do the loss function inside model. So for backward, we just do each thing so self.loss.backward . So that self taught losses the MSE object. So that’s going to call backward, right? And it’s stored when it was called here it was storing remember the inputs, the targets, the outputs so it can calculate the backward and then we go through each layer is in reverse. Right. This is back propagation backwards, reversed, calling backward on each one. So that’s pretty interesting. I think. So now we can calculate the model, we can calculate the loss we can call backward, and then we can check that each of the gradients that we stored earlier equal to each of our new gradients\nOkay. So Williams asked a very good question that if you do puts put the loss inside here, how on earth do you actually get predictions? So generally what happens is in practice, huggingface models do something like that. So say self.preds equals x and then they’ll say self.final_loss = self.loss(x,targ) that and then return self.final_loss. And that way I guess you don’t even need that last bit. Well anyway, this is what they do so I’ll leave it there. And so that way you can kind of check like model.preds for example. So it’ll be something like that. Or alternatively, you can return not just the loss, but both as a dictionary, stuff like that. So a few different ways you could do it actually. Now think about it. I think that’s what they do is they actually return both as a dictionary. So it would be, it’d be like return dictionary loss equals that , preds = x .Something like that, I guess is what they would do. Anyway. There’s a few different ways to do it. Okay. So hopefully you can see that this is really making it nice and easy for us to do our forward pass and our backward paths. Without all of this manual fiddling around. Every class now can be totally separately considered and can be combined. However want. We could create layers so you could try creating a bigger neural net if you want to, but we can refactor it more. So basically as a rule of thumb, when you see repeated like self.inp = inp…. That’s a sign can refactor things. And so what we can do is a simple refactoring this to create a new class called module and modules gonna do those things. You said it’s going to store the inputs and it’s going to call something called self.forward in order to create our self.out because remember, that was one of the things we had again and again and again, self.out, return it. And so now that’s going to be a thing called forward which actually in this it doesn’t do anything because the whole purpose of this module is to be inherited. When we call backward, it’s going to call self.backward passing in self.out because notice all of our backwards always wanted to get hold of self.out because we need it for the chain role. So let’s pass that in and pass in those arguments that we start earlier."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 5/index.html#refactor-model",
    "href": "posts/Writing Stable Diffusion from Scratch 5/index.html#refactor-model",
    "title": "Writing Stable Diffusion from Scratch 5",
    "section": "Refactor model",
    "text": "Refactor model\n\nLayers as classes\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\n\n\nModule.forward()\nAnd so *args means take all of the arguments regardless whether it’s zero one, two or more, and put them into a list. And then that’s what happens when it’s inside the actual signature. And then when you call a function using star, it says take this list and expand them into separate calling backward with each one separately. So now for relu, you look how much simpler it is. Let’s copy the old relu and the new relu. So the old relu you had to do all this storing stuff manually and it had a little self.stuff as well. But now we can get rid of all of that and just implement forward because that’s the thing that’s been called and that’s the thing that we need to implement. And so now the forward is relu just as the one thing we want, which also makes the code much cleaner and more understandable. Ditto for backward. It just does. The one thing we want. So that’s nice. Now we still have to multiply it by two after the chain rule manually, but same thing for linear, same thing for MSE. So these all look a lot nicer. And one thing to point out here is that there’s often opportunities to manually speed things up when you create custom order grad functions in PyTorch. And here’s an example. Look, this calculation is being done twice. It seems like a waste, doesn’t it so at the cost of some memory, we could instead store that calculation as diff. Right. And I guess we’d have to store it fur use it later. So I don’t need to be self.diff if and the cost of that memory, we could now remove this redundant calculation because we’ve done it once before already and stored it and just use it directly. And this is something that you can often do in neural nets. So there’s this compromise between storing things, the memory use of that and then the computational speed up of not having to recalculate it. This is something we come across a lot. And so now we can call it in the same way, create a model passing in all of those layers. So you can see with our model, we’re just so the model hasn’t changed at this point. The definition was up here. We just passed in The weights for the layers calculate the loss called backward and look. It’s the same right? Okay. So thankfully, PyTorch has written all this for us. And remember, according to rules of our game, once we’ve reimplemented it, we’re allowed to use PyTorch as version. So PyTorch calls their version and nn.Module. And so it’s exactly the same you inherit from an nn.Module. So if we want to create a linear layer just like this one, rather than inheriting our module, we will inherit from that module, but everything’s exactly the same. So we create our, we can create our random numbers. So in this case, rather than passing in the already randomized weights, we’re actually going to generate the random weights ourselves and the zeroed biases and. Then here’s our linear layer, which you could also use Lin for that.\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n\n\nclass Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n\n\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n    def forward(self, inp): return inp@self.w + self.b\n    def bwd(self, out, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse(Module):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.01)\ntest_close(ig, x_train.g, eps=0.01)\n\nAnd of course,\nso define our forward. And why don’t we need to define backward? Because PyTorch already knows the derivatives of all of the functions in PyTorch and it knows how to use the chain rule so we don’t have to do backward at all. It will actually do that entirely for us, which is very cool. So we only need forward. We don’t need backward.\nSo let’s create a model that uses that nn.Moddule. Otherwise it’s exactly the same as before. And now we’re going to use PyTorch as MSE loss because we’ve already implemented ourselves. It’s very common to use torch.nn.functional as capital P. This is where lots of these handy functions live, including MSE loss. And so now you know why we need the colon comma None because you saw the problem if we don’t have it. And so the model call backward. And remember, we stored our gradients in something called (.g) PyTorch stores them in something called (.grad), but it’s doing exactly the same thing. So there is the exact same values. All right. So let’s see if there’s any questions. Not yet. Okay. All right. If anybody in the class has any questions or comments about any of this, let me know. Remember to upvote questions that you’re interested in.\nSo we’ve we’ve created a matrix multiplication from scratch.\nWe’ve created linear layers,\nwe’ve created a complete backprop system of modules.\nWe can now calculate both the forward pass and the backward pass for linear layers and relus so we can create a multi-layer perceptron.\nSo we’re now up to a point where we can train a model.\nSo let’s do that mini batch training Notebook number 4.\n\n\nAutograd\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Linear(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in,n_out).requires_grad_()\n        self.b = torch.zeros(n_out).requires_grad_()\n    def forward(self, inp): return inp@self.w + self.b\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return F.mse_loss(x, targ[:,None])\n\n\nmodel = Model(m, nh, 1)\nloss = model(x_train, y_train)\nloss.backward()\n\n\nl0 = model.layers[0]\nl0.b.grad\n\ntensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 6/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 6/index.html",
    "title": "Writing Stable Diffusion from Scratch 6",
    "section": "",
    "text": "What you should learn and practice after this :  1- log , e rules 2- what is softmax 3- 4- Train a simple linear model from scratch\nSo we’re now up to a point where we can train a model. So let’s do that mini batch training Notebook number four. So same first cell as before. We won’t go through it. The cells also the same as before, so we won’t go through it is the same model that we had before, so we won’t go through it. So I just rerunning all that to see. Okay,\nso the first thing we should do, I think, is to improve our loss function. So it’s not total rubbish anymore. So if you watch part one, you might recall that there are some Excel notebooks. One of those Excel notebooks is entropy example. Okay, so this is what we looked at. So just to remind you, what we’re doing now is which we’re saying, okay, rather than rather than outputting a single number for each image, we’re going to instead output ten numbers for each image. And so that’s going to be a one hot encoded set of it’ll be like 1000, etc.. And so then that’s going to be well, actually the outputs wont be 1000, they’ll be basically probabilities won’t they. So it’ll be like .99 comma, you know, 3.01 etc. and the targets will be one hot encoded. So if it’s the digit zero, for example, it might be 10000 that dot dot for all the ten possibilities. And so to see, you know, how good is it. So in this case, it’s very good. It had 0.99 probability prediction that it’s zero. And indeed it is, because this is this is the 100 coded version. And so the way we implement that is we don’t even need to actually do the one hot encoding, thanks to some tricks, we can actually just directly store the integer, but we can treat it as if it’s one hot encoded so we can just store the actual target zero as an integer. So the way we do that is we say, for example, for a single output, oh, it could be cat, let’s say cat, dog, plain fish building the neural net spits out a bunch of outputs. What we do for softmax is we go e to of the power of each of those outputs. We sum up all of those e of the power offs. So here’s the e of the power of each of those outputs is the sum of them. And then we divide one each one by the sum. So we divide each one by the sum that gives us a softMax’s. And then for the lost function, we then compare the softMax’s to the one encoded version. So let’s say it was a dog that’s going to have a one for dog and zero everywhere else. And then softMax, this is from this nice blog post here. This is the calculation, some of the ones and zeros. So each of the ones and zeros multiplied by the log of the probabilities. So here is the log probability times the actuals. And since the actual is either zero or one and only one of them is going to be a one, we’re only going to end up with one value here. And so if we add them up, it’s all zero except for one of them. So that’s a cross entropy. So in the special case where the outputs one had encoded, then during the one hot encoded multiplied by the log, softmax is actually identical to simply saying, Oh, dog is in this row, let’s just look it up directly and take its log soft max.\nWe can just index directly into it. So it’s exactly the same thing. So that’s just review. So if you haven’t seen that before, then yeah, go and watch the part one video where we went into that in a lot more detail. Okay, So here’s our softmax calculation. It’s e to the power of each output divided by the sum of them all, we can use sigma notation to say exactly the same thing. And as you can see, Jupyter notebook lets us use latex. If you haven’t used latex before, it’s actually surprisingly easy to learn. You just put dollar signs around your equations like this and your equations backslash is going to be kind of like your functions if you like, and curly parentheses actually, Curly are used to kind of fit for arguments. So you can see here, here is e to the power of and then underscore as used just subscript. So this is x subscript i and power of is used for super scripts. So his dots you can see here it is dots so it’s actually yeah learning like tech is easier than you might expect. It can be quite convenient for writing these functions when you want to. So anyway, that’s what soft Max is as we’ll see in a moment. Well, actually, as you’ve already seen in cross entropy, we don’t really want soft Maxs. We want log of soft Macs. So a log of softMaxs is here it is. So we’ve got x.exp divided by x.exp.sum and we’re going to sum up over the last dimension and then we actually want to keep that dimension so that when we do the divided by, we want a trailing unit axis for exactly the same reason we saw when we did our MSE last function. So if you sum with keepdim = true, that leaves the unit axis in in that last position. So we don’t have to put it back to avoid that horrible out of product issue. So this is the equivalent of this and then .log. So that’s log of soft Maxs. So there is the log of the soft Maxs that are predictions. Now in terms of high school math, you may have forgotten, but you definitely are going to want to know a Key piece that in that list of things is is log and exponent rules.\nSo check out Khan Academy or similar if you’ve forgotten them. But a quick reminder is, for example, the one we mentioned here log of A over B equals log of A minus log of B and equivalently log of A times B equals log of A plus log of B. And these are very handy because, for example, division can take a long time, multiply, can create really big numbers and have lots of floating point error. Being able to replace these things with pluses and minuses is very handy indeed. In fact, I used to give people an interview question which I did a lot of stuff with school and math school actually only has a some function for group by clauses. And I used to ask people how you would deal with calculate a compound interest column where the answer is basically that you have to say because compound interest is taking products. So it has be the sum of the log of the column and then e of the power of all that. So it’s like all kinds of little places that these things coming in handy, but they come in to neural nets all the time. So we’re going to take advantage of that because we’ve got a divided by it’s being logged and also rather handily. We’re going to have therefore the log of x.exp.log minus the log of this, but expand log opposites. So that is going to end up being x minus. So log soft max is just x minus, all this logged. And here it is, all this logged. So that’s nice. So here’s our simplified version. Okay, Now there’s another very cool trick, which is one of these things I figured out myself and then discovered other people have known it for years. So not my trick, but it’s always to rediscover things. The trick is what’s written here. Let me explain what’s going on. This piece here, the log of this sum right this sum here we’ve got x.exp.sum. Now x could be some pretty big numbers. And e of the power of that’s going to be really big numbers. And e of the power of things creating really big numbers. Well, really big numbers. There’s much less precision in your computers floating point handling the further, you get away from zero, basically. So we don’t want really big numbers, particularly because we’re going to be taking derivatives. And so if you’re in an area that’s not very precise as far as 13 point math is concerned, then the derivatives are going to be a disaster. They might even be zero because you’ve got two numbers that the computer can’t even recognize is different.\nSo this is bad, but there’s a nice trick we can do to make it a lot better. What we can do is we can calculate the max of the max of X, right. And we’ll call that A. And so then rather than doing the log of the sum of e of x i, we’re instead going to define A as being the maximum of all of our X values. It’s our biggest number. Now, if we then subtract that from every number, that means none of the numbers are going to be big by definition because we have subtracted it from all of them. Now the problem is that’s given us a different result. Right? But if you think about it, let’s expand this sum. It’s e to the power of x one , say plus e of the power of X to plus e of the power of x three and so forth. Okay, Now we just subtracted a from our exponents, which is we’re now wrong. But I’ve got good news, I’ve got good news and bad is. The bad news is that you’ve got more high school math to remember, which is exponent rules. So X to the a plus b equals x to the a, times x to the b, and x to the b minus b equals x to the a divided by x to the b, And to convince yourself that’s true, consider for example, two to the power of two three. What is that what you’ve got to do?\nThe power of two is just two times two and to the power of two plus three. Well, it’s two times. Two times it is to the power of five. So you’ve got two to the power of two, you’ve got two of them here and you’ve got another three of them here. So we’re just adding up the number to get the total index so we can take advantage of this here and say like, Oh well this is equal to a to the e to the x1 over e to the a plus e to the x2 over e to the a … And this is a common denominator. So we can put all that together why do we do all that. Because if we now multiply that all by e of the a these would cancel out and we get the thing we originally wanted. So that means we simply have to multiply this by that. And this gives us exactly the same thing as we had before.\nBut with critically, this no longer ever going to be a giant number, so this might seem a bit weird. We’re doing extra calculations. It’s not a simplification, it’s a complexification, but it’s one that’s going to make it easier for our floating point unit. So that’s our trick. It’s rather than doing log of this sum, what we actually do is log of e of times the sum of e to the X minus a, And since we’ve got a log of a product that’s just a log, that’s just the sum of the logs and log of e to the a so it’s a plus that so this here is called\nthe log sum x trick people are pointing out. Thank you for that of course should have been inside the log you got this go sticking it on the outside like a crazy person. Yeah. Yeah, That’s what I meant to say. Let’s check if you’ve got any questions. Any questions yet? Okay, so here is the log.call it a (m in notebook) But anyway, so we find the maximum on the last dimension. And then here is the m plus that exact thing. Okay, so that’s just another way of doing that. Okay, So that’s the log, sum exp. So now we can rewrite log soft max as x minus log sum exp, and we’re not going to use our evasion because PyTorch already has one, so we’re just use pytorches.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 6/index.html#initial-setup",
    "href": "posts/Writing Stable Diffusion from Scratch 6/index.html#initial-setup",
    "title": "Writing Stable Diffusion from Scratch 6",
    "section": "Initial setup",
    "text": "Initial setup\n\nData\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = Model(m, nh, 10)\npred = model(x_train)\npred.shape\n\ntorch.Size([50000, 10])\n\n\n\n\nCross entropy loss\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}\\]\nor more concisely:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}\\]\nIn practice, we will need the log of the softmax when we calculate the loss.\n\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\n\nlog_softmax(pred)\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)\n\n\nNote that the formula\n\\[\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)\\]\ngives a simplification when we compute the log softmax:\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\n\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\\]\nwhere a is the maximum of the \\(x_{j}\\).\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nThis way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\ntest_close(logsumexp(pred), pred.logsumexp(-1))\nsm_pred = log_softmax(pred)\nsm_pred\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)\n\n\nThe cross entropy loss for some target \\(x\\) and some prediction \\(p(x)\\) is given by:\n\\[ -\\sum x\\, \\log p(x) \\]\nBut since our \\(x\\)s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as \\(-\\log(p_{i})\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\ny_train[:3]\n\ntensor([5, 0, 4])\n\n\n\nsm_pred[0,5],sm_pred[1,0],sm_pred[2,4]\n\n(tensor(-2.20, grad_fn=<SelectBackward0>),\n tensor(-2.37, grad_fn=<SelectBackward0>),\n tensor(-2.36, grad_fn=<SelectBackward0>))\n\n\n\nsm_pred[[0,1,2], y_train[:3]]\n\ntensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)\n\n\n\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\n\nloss = nll(sm_pred, y_train)\nloss\n\ntensor(2.30, grad_fn=<NegBackward0>)\n\n\nAnd if we check, we yeah, we go, it’s our results. And so then as we’ve discussed, the cross entropy loss is the sum of the outputs times the log probabalities. And as we discussed, our outputs are one hot and coded or actually they’re just the integers better still. So what we can do is we can I guess I should make that more clear. Actually the just the integer indices so we can simply rewrite that as negative log of the target. So that’s what we had in our Excel. And so how do we do that in PyTorch? So this is quite interesting. There’s A lot of cool things you can do with array indexing in PyTorch in Numpy. So basically they use the same approaches. Let’s take a look. Here is the first three actual values in y_train. They’re five zero and four. Now what we want to do is we want to find in our soft max predictions, we want to get five the fifth prediction in the zeroth row, the zeroth prediction in the first row and the fourth in the index two row. So these are the numbers that we want. This is going to be what we add up for the first few rows of last function. So how do we do that all in one go? Well, here’s a cool trick. See here, I’ve got 0,1,2. If we index using a two lists, we can put here 0, 1,2, and for the second list we can put y_train[:3]. So it’s going to be it’s going to be zero comma 5, 1 comma zero, 2 comma four, which is, as you say, exactly the same thing. So therefore, this is actually giving us what we need for the cross entropy loss. So if we take range of our targets first dimension or zero index dimension, which is all this is and the target and then take the negative of that dot mean, that gives us our cross entropy loss, which is pretty neat in my opinion. All right. So pytorch calls this negative log likelihood loss and that’s all it is. And so if we take the negative log likelihood and so get negative likelihood and we pass that to , the log soft max, then we get the loss. And this particular combination in PyTorch called F.across_entropy. So let’s check. Yep. F.across_entropy gives us exactly the same thing. So that’s cool. So we have now re-implemented cross entropy loss and there’s a lot of confusing things going on there. A lot. And so this is one of those where you should pause the video and go back and look at each step and think not just like, what is it doing, but why is it doing it? And also try in lots of different values yourself to see if you can see what’s going on and then put this aside and test yourself by reimplementing log soft max and nll_loss and cross entropy yourself and compare them to pytouches values. And so that’s the piece of homework for you for this week\nThen use PyTorch’s implementation.\n\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n\nIn PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.\n\ntest_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 6/index.html#basic-training-loop",
    "href": "posts/Writing Stable Diffusion from Scratch 6/index.html#basic-training-loop",
    "title": "Writing Stable Diffusion from Scratch 6",
    "section": "Basic training loop",
    "text": "Basic training loop\nSo now that we’ve got that we can actually create a training loop. So let’s set our last function to be cross entropy. Let’s create a batch size of 64. And so here’s our first mini batch. Okay, so xb is the x mini batch. It’s going to be from zero up to 64 from our training set, so we can now calculate our predictions. So that’s 64 by ten. So for each of the 64 images in the mini batch, we have ten probability one for each digit. And our y(s) is just a let’s print those out. So there’s our first 64 target values. So these are the actual digits. And so our loss function. So we’re going to start with a bad loss because it’s entirely random at this point. Okay. So for each of the predictions\nwe made, so those are our predictions. And so remember, those predictions are a 64 by ten. What did we predict? So for each one of these 64 rows, we have to go in and see where is the highest number. So if we go through here, we can go through each one. Here’s a 0.123 So the highest number is this one. So you got to find the index of the highest number. The function to find the index of the highest number is called argmax. And here it is three. And I guess we could have also written this probably as preds.argmax. Normally you can do them either way. I actually prefer normally to do it this way. Yeah. There’s the same thing. Okay. And the reason we want this is because we want to be at a complete accuracy. We don’t need it for the actual neural net, but we just like to be able to see how we’re going because it’s like it’s a metric. It’s something that we use for understanding. So we take the argmax, we compare it to the actual. So that’s going to give us a bunch of bools. If you turn those into floats, they’ll be ones and zeros. The mean of those flights is the accuracy. So our current accuracy, not surprisingly, is around 10%. It’s 9% because it’s random. That’s what you would expect. Let’s train our first neural net. So we’ll set a learning rate, which are a few epochs. So we’re going to go through each epoch and we’re going to go through from zero up to n. That’s the 50,000 training rows and skipping by 64 the batch size each time. And so we’re going to create a slice that starts at i So starting at zero and goes up 64 unless we’re just got to n And so then we will slice into our training set for the x and for the y to get an x and y batches. We will then calculate our predictions, our loss function, and do it backward.\nSo the way I did this originally was I had all of these in in separate cells and I just typed in, you know, i equals zero and then went through one cell at a time calculating each one until they all worked. And so then I can put them in a loop. Okay. So once we’ve got done backward, we can then with torch.no_grad go through each layer and if that’s a layer that has weights, will update them to the existing weights minus the gradients times of learning and then zero out. So the weights and biases for the gradients, the gradients of the weights and biases this underscore means do it in place. So that sets this to zero. So if I run that up, it’s going to run all of them. I guess I skipped. So there we go. It’s finished. So you can see that our accuracy on the training sets have been unfair. But 23 epochs is 97%. So we now have a digit recognizer trains pretty quickly and it’s not terrible at all. So that’s a pretty good starting point. All right. So what we’re going to do next time is we’re going to refactor this training loop to make it dramatically dramatic training, dramatically simple step by step, until eventually we will get it down to where is it. So we’ll get it down to something much, much shorter. And then we’re going to add a validation set to it and a multi processing data loader. And then yeah, we’ll be in a pretty good position, I think, to to start training some more interesting models. All right. Hopefully you found that useful and led some interesting things. And so what I’d really like you to do is at this point, now that you’ve kind of like got all these key basic pieces in place, is to really try to recreate them without picking as much as possible. So, you know, recreate your matrix, multiply, recreate those forward and backward passes, recreate something that steps through layers and even see if you can like recreate the idea of the dot forward and the dot backward. Make sure it’s all in your head really clearly so that you fully understand what’s going on. You know, at the very least, if you don’t have time for that, because that’s a big job, you could pick out a smaller part of that, the piece that you’re more interested in, or you could just go through and look really at these notebooks. So if you go to kernel restart and clear output, it’ll delete all the outputs and try to think like what are the shapes of things? Can you guess what they are, can you check them? And so forth. Okay, Thanks everybody. I hope you have a great week and I will see you next time by.\nBasically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better\n\nloss_func = F.cross_entropy\n\n\nbs=50                  # batch size\n\nxb = x_train[0:bs]     # a mini-batch from x\npreds = model(xb)      # predictions\npreds[0], preds.shape\n\n(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n torch.Size([50, 10]))\n\n\n\nyb = y_train[0:bs]\nyb\n\ntensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,\n        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])\n\n\n\nloss_func(preds, yb)\n\ntensor(2.30, grad_fn=<NllLossBackward0>)\n\n\n\npreds.argmax(dim=1)\n\ntensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3,\n        9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n:::\n\naccuracy(preds, yb)\n\ntensor(0.08)\n\n\n\nlr = 0.5   # learning rate\nepochs = 3 # how many epochs to train for\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')\n:::\n\nxb,yb = x_train[:bs],y_train[:bs]\npreds = model(xb)\nreport(loss_func(preds, yb), preds, yb)\n\n2.30, 0.08\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad   * lr\n                    l.weight.grad.zero_()\n                    l.bias  .grad.zero_()\n    report(loss, preds, yb)\n\n0.12, 0.98\n0.12, 0.94\n0.08, 0.96"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 7/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 7/index.html",
    "title": "Writing Stable Diffusion from Scratch 7",
    "section": "",
    "text": "All credits goes to fast.ai  All mistakes are mine.  I have to put code from previous lessons scence they are connected.\nYou should know and practice following after this blog post : 1- Refactor pervious code to make it cleaner  2- Know how nn module works in pytorch  3- how setattr works ?  4- repr ?  5- yeild from ?  6- supper() and object ?  7- reduce, map ?  8- optimizer  9- learning rate ?  10- Sampler  11- Collate function  12- Multi processing data loader\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=2}\n:::"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 7/index.html#initial-setup",
    "href": "posts/Writing Stable Diffusion from Scratch 7/index.html#initial-setup",
    "title": "Writing Stable Diffusion from Scratch 7",
    "section": "Initial setup",
    "text": "Initial setup\n\nData\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        \n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = Model(m, nh, 10)\npred = model(x_train)\npred.shape\n\ntorch.Size([50000, 10])\n\n\n\n\nCross entropy loss\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}\\]\nor more concisely:\n\\[\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}\\]\nIn practice, we will need the log of the softmax when we calculate the loss.\n\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\n\nlog_softmax(pred)\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)\n\n\nNote that the formula\n\\[\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)\\]\ngives a simplification when we compute the log softmax:\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\n\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\\]\nwhere a is the maximum of the \\(x_{j}\\).\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nThis way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\ntest_close(logsumexp(pred), pred.logsumexp(-1))\nsm_pred = log_softmax(pred)\nsm_pred\n\ntensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n        ...,\n        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)\n\n\nThe cross entropy loss for some target \\(x\\) and some prediction \\(p(x)\\) is given by:\n\\[ -\\sum x\\, \\log p(x) \\]\nBut since our \\(x\\)s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as \\(-\\log(p_{i})\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\ny_train[:3]\n\ntensor([5, 0, 4])\n\n\n\nsm_pred[0,5],sm_pred[1,0],sm_pred[2,4]\n\n(tensor(-2.20, grad_fn=<SelectBackward0>),\n tensor(-2.37, grad_fn=<SelectBackward0>),\n tensor(-2.36, grad_fn=<SelectBackward0>))\n\n\n\nsm_pred[[0,1,2], y_train[:3]]\n\ntensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)\n\n\n\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\n\nloss = nll(sm_pred, y_train)\nloss\n\ntensor(2.30, grad_fn=<NegBackward0>)\n\n\nThen use PyTorch’s implementation.\n\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n\nIn PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.\n\ntest_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 7/index.html#basic-training-loop",
    "href": "posts/Writing Stable Diffusion from Scratch 7/index.html#basic-training-loop",
    "title": "Writing Stable Diffusion from Scratch 7",
    "section": "Basic training loop",
    "text": "Basic training loop\nBasically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better\n\nloss_func = F.cross_entropy\n\n\nbs=50                  # batch size\n\nxb = x_train[0:bs]     # a mini-batch from x\npreds = model(xb)      # predictions\npreds[0], preds.shape\n\n(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n torch.Size([50, 10]))\n\n\n\nyb = y_train[0:bs]\nyb\n\ntensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,\n        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])\n\n\n\nloss_func(preds, yb)\n\ntensor(2.30, grad_fn=<NllLossBackward0>)\n\n\n\npreds.argmax(dim=1)\n\ntensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3,\n        9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=29}\ndef accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n:::\n\naccuracy(preds, yb)\n\ntensor(0.08)\n\n\n\nlr = 0.5   # learning rate\nepochs = 3 # how many epochs to train for\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=32}\ndef report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')\n:::\n\nxb,yb = x_train[:bs],y_train[:bs]\npreds = model(xb)\nreport(loss_func(preds, yb), preds, yb)\n\n2.30, 0.08\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad   * lr\n                    l.weight.grad.zero_()\n                    l.bias  .grad.zero_()\n    report(loss, preds, yb)\n\n0.11, 0.96\n0.13, 0.96\n0.10, 0.96"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 7/index.html#using-parameters-and-optim",
    "href": "posts/Writing Stable Diffusion from Scratch 7/index.html#using-parameters-and-optim",
    "title": "Writing Stable Diffusion from Scratch 7",
    "section": "Using parameters and optim",
    "text": "Using parameters and optim\nAnd so we’re now going to I’m going to show you something that’s part of pytorch and then going to show you how to build it, and then you’ll see why this is really useful. So PyTorch has a sub Module starting nn. And in there there’s something called the Module class. Now we can we don’t normally use it this way, but I just want to show you how it works. We can create an instance of it in the usual way we create instances of classes and then we can assign things to attributes of that module. So for example, it’s assign a linear layer to it. And if we now print out that, you’ll see it says, Oh, this is a module containing something called f00, which is a linear layer. But here’s something quite tricky. This module we can say, show me all of the named children of that module and it says, Oh, this one code foo and it’s a linear layer. And we can say, Oh, show me all of the parameters of this module. And it says, Oh, okay, sure. There’s two of them. There’s this four by three tensor, that’s the weights and there’s this four long vector. That’s the biases. And so somehow just by creating this module and assigning this to it, it’s automatically tracked what’s in this module and what are its parameters. That’s pretty neat. So we’re going to see both how and why it does that. I’m just going to point out, by the way, why did I add list here? If I just said m1.named_children(), it just prints out generate an object which is not very helpful and that’s because this is a kind of iterator called a generator, and it’s something which is going to only produce the contents of this when I actually do something with it, such as list them out.\nSo just popping a list around a generator is one way to like run the generator and get its output. That little trick when you want to look inside a generator.\nOkay, so now, as I said, we don’t normally use it this way. What we normally do is we create our own class. So, for example, we create our own multilayer perception and we inherit it. We inherit from a nn.Module. And so then in init, this is the thing that constructs an object of the class. This is the special magic method that does that well, say, okay, well, how many inputs are there to this multilayer perceptron? How many hidden activations and how many output activations are there? So just be one hidden layer. And then here we can do just like we did up here, where we assign things as attributes. We can do that in this constructor, so we create an l1 attribute, which is a linear layer from number into number. Hidden l2 is a linear layer from number hidden number out, and we’ll also create a relu. And so when we call that module(call), we can take the input that we get and run the linear layer and then run the relu here and then run the l2. And so I can create one of these, as you say, and I can have a look and see like, Oh, here’s the attribute l1 and there it is, like I said, and I can say print out the model and the model knows all the stuff that’s in it. And I can go through each of the named children and print out the name and the layer. Now, of course, if you’re a member, although you can use call, we actually showed how we can refactor things using forward such that it would automatically kind of do the things necessary to make all the, you know, or automatic gradient stuff work correctly.\nAnd so in practice we’re actually not going to do it call we would do forward. So this is an example of creating a custom PyTorch module. And the key thing to recognize is that it knows what are the attributes you added to it, and it also knows what all the parameters.\nSo if I go through the parameters and print out the shapes, you can see I’ve got my linear layers, weights first, my first linear layers weights, my first linear layers biases second linear layers weights, second linear layers biases. And this is because we set nh, the number of hidden to 50.\nSo why is that interesting? Well, because now I don’t have to write all this anymore. Going through layers and having to make sure that they’ve all been put into a list where you’ve just been able to add them as attributes and they’re automatically going to appear as parameters. So we can just say, go through each parameter and update it based on the gradient and the learning rate. And furthermore, you can actually just go model.zero_grad and it’ll zero out all of the gradients. So that’s really made our code quite a lot nicer and quite a lot more flexible, which is cool. we do\nSo let’s check that this still works. There we go. So just to clarify with if I called report on this before I ran it, as you would expect, the accuracy is about 8% with about 10% less and the loss is pretty high. And so after I run this fit this model, the accuracy goes up and the loss goes down. So basically it’s all of this is exactly the same as before. The only thing I’ve changed are these two lines of code, so that’s a really useful refactoring. So what how on earth did this happen? How did it know what the parameters and layers are? Automatically it used a trick called dunder setattr. and we’re going to create our own and nn.module.\n\nParameters\n\nm1 = nn.Module()\nm1.foo = nn.Linear(3,4)\nm1\n\nModule(\n  (foo): Linear(in_features=3, out_features=4, bias=True)\n)\n\n\n\nlist(m1.named_children())\n\n[('foo', Linear(in_features=3, out_features=4, bias=True))]\n\n\n\nm1.named_children()\n\n<generator object Module.named_children at 0x7f64aaeec190>\n\n\n\nlist(m1.parameters())\n\n[Parameter containing:\n tensor([[ 0.57,  0.43, -0.30],\n         [ 0.13, -0.32, -0.24],\n         [ 0.51,  0.04,  0.22],\n         [ 0.13, -0.17, -0.24]], requires_grad=True), Parameter containing:\n tensor([-0.01, -0.51, -0.39,  0.56], requires_grad=True)]\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x): return self.l2(self.relu(self.l1(x)))\n\n\nmodel = MLP(m, nh, 10)\nmodel.l1\n\nLinear(in_features=784, out_features=50, bias=True)\n\n\n\nmodel\n\nMLP(\n  (l1): Linear(in_features=784, out_features=50, bias=True)\n  (l2): Linear(in_features=50, out_features=10, bias=True)\n  (relu): ReLU()\n)\n\n\n\nfor name,l in model.named_children(): print(f\"{name}: {l}\")\n\nl1: Linear(in_features=784, out_features=50, bias=True)\nl2: Linear(in_features=50, out_features=10, bias=True)\nrelu: ReLU()\n\n\n\nfor p in model.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\ndef fit():\n    for epoch in range(epochs):\n        for i in range(0, n, bs):\n            s = slice(i, min(n,i+bs))\n            xb,yb = x_train[s],y_train[s]\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters(): p -= p.grad * lr\n                model.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\n\n0.19, 0.96\n0.11, 0.96\n0.04, 1.00\n\n\nBehind the scenes, PyTorch overrides the __setattr__ function in nn.Module so that the submodules you define are properly registered as parameters of the model.\nNow. So if there was no such thing as an nn.module, here’s how would build it. And so let’s actually build it and also add some things to it. So in init, we would have to create a dictionary for our named children. This is going to contain a list dictionary of all of the layers. Okay. And then just like before, we’ll create a couple of linear layers, right? And then what we’re going to do is going to define this special magic thing that Python has setattr. And this is called automatically by Python if you have it, every time you set an attribute such as here or here and it’s going to be past the name of the attribute, the key and the value is the actual thing on the right hand side of the equals sign. Now, generally speaking, things that start with an underscore where we use for either private stuff. So we check that it doesn’t start with an underscore. And if it doesn’t start with an underscore, setattr will put this value into the modules dictionary with this key and then call Python’s the normal python setattr, try to make sure it just actually does the attribute setting. So super is how you call whatever is in the the superclass, the base class. So another useful thing to know about is how do we how does how does it do this nifty thing where you can just type the name and it kind of lists out all this information about it.\nThat’s a special thing called repr. So here, repr will just have it return a stringified version of the module’s dictionary. And then here we’ve got parameters. How did parameters work? So how did this thing work? Well, we can go through each of those modules, go through each value. So the values of the modules is all the actual layers and then go through each of the parameters in each module and yield p. So that’s going to, that’s going to create an iterator. If you remember when we looked at iterates for all the parameters, So let’s try it so we can create one of these modules.\nthere they are now just mentioned something that’s optional, kind of like advanced Python that a lot of people don’t know about, which is there’s no need to loop through a list or a generator or I guess I say look for an iterator and yield. There’s actually a shortcut, which is you can just say yield from and then give it the iterator.\nAnd so with that we can get this all down to one line of code and it’ll do exactly the same thing. So that’s basically saying yield one at a time. Everything in here, that’s what yield from does. So there’s a little advanced python thing, totally optional. But if you’re interested I think it can be kind of neat.\n\nclass MyModule:\n    def __init__(self, n_in, nh, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n\n    def __setattr__(self,k,v):\n        if not k.startswith(\"_\"): self._modules[k] = v\n        super().__setattr__(k,v)\n\n    def __repr__(self): return f'{self._modules}'\n    \n    def parameters(self):\n        for l in self._modules.values(): yield from l.parameters()\n\n\nmdl = MyModule(m,nh,10)\nmdl\n\n{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}\n\n\n\nfor p in mdl.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\nSo we’ve now learned how to create our own implementation of an nn.module and therefore we are now allowed to use pytouches nn.module. So that’s good news. So how would we do using the PyTorch and nn.module? How would we create the model that we started with, which is where we had this self,layers because we want to somehow register all of these all at once? That’s not going to happen based on the code we just wrote. So to do that, let’s have a look. We can so let’s make a list of the layers we want. And so we’ll create again, a subclass of nn.module. Make sure you call the super classes in it first(super()init) and we’re just store list of layers. And then to tell PyTorch about all those layers, we basically have to loop through them and call add_module and say what the name of the module is and what the module is. And again, because should probably should have used forward to here in the first place and you can say this is now done exactly the same thing. Okay. So if you’ve used a sequential model before, you’ll see or you can say that we’re on the path to creating a sequential model.\nOkay. So Gonash asked an interesting question, which is what on earth is super calling? Because we actually in fact, we don’t even need the parentheses here. We actually don’t have a base class. That’s because if you don’t put any parentheses or if you put empty parentheses, it’s actually a shortcut for writing that. And so Python has stuff in object which does, you know, all the normal object, things like storing your attributes so that you can get them back later. So that’s what’s happening there. Okay.\nSo this is a little bit awkward is to have to store the list and then enumerate and call add_module. So now that we’ve implemented that from scratch, we can use PyTorch is version, which is they’ve just got something called ModuleList that just does that for you. Okay. So if you use ModuleList and pass that list of layers, it will just go ahead and register them all those modules for you. So here’s something called sequential model. So this is just like nn.sequential now. So if I create it passing in the layers, there you go. You can see that’s my module containing my module list with my layers. And so to know why I never used forward for these things, it’s silly because it doesn’t add a terribly in this stage. But anyhow, okay, so call fit. And there we go. Okay, so, so in forward here, I just go through each layer and I set the result of that equal to calling that layer on the previous result and then pass and return it at the end.\n\n\nRegistering modules\n\nfrom functools import reduce\n\nWe can use the original layers approach, but we have to register the modules.\n\nlayers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n\nNow there’s little another way of doing this, which I think is kind of fun. It’s not like shorter or anything at this stage. I just wanted to show an example of something that you see quite a lot in machine learning code, which is the use of reduce.\nThis implementation here is exactly the same as this thing here. So let me explain how it works. A lot reduced. So reduced is a very common kind of like fundamental or computer science concept reductions. This is something that does a reduction.\nAnd what a reduction is??\nis it’s something that says start with the third parameter, some initial value. So we’re going to start with x, the thing with being passed and then loop through a sequence. So look through each of our layers and then for each layer, call some function. Here is our function and the function is going to get passed. First time around, it’ll be past the initial value and the first thing in your list. So your first layer and x. So it’s just going to call the layer function on x the second time around to take the output of that and passes that in as a second as the first parameter and passes in the second layer. So then the second time this goes through, it’s going to be calling the second layer on the result of the first layer and so forth, and that’s what a reduction is. And so you might see reduce, you’ll certainly see it talked about quite a lot in in papers and books and you might sometimes also see it in code. It’s a very general concept. And so here’s how you can implement a sequential model using reduce. So there’s no explicit loop there, although it’s still happening internally.\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers\n        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n\n    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)\n\n\nmodel = Model(layers)\nmodel\n\nModel(\n  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n  (layer_1): ReLU()\n  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nmodel(xb).shape\n\ntorch.Size([50, 10])\n\n\n\n\nnn.ModuleList\nnn.ModuleList does this for us.\n\nclass SequentialModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        \n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nmodel = SequentialModel(layers)\nmodel\n\nSequentialModel(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=50, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=50, out_features=10, bias=True)\n  )\n)\n\n\n\nfit()\n\n0.12, 0.96\n0.11, 0.96\n0.07, 0.98\n\n\nAll right. So now that we’ve re implemented sequential, we can just go ahead and use PyTorch as version.there’s an nn.Sequential we can pass in our layers and we can fit, not surprisingly, we can see the model. So yeah, it looks very similar to the one we built ourselves. All right.\n\n\nnn.Sequential\nnn.Sequential is a convenient class which does the same as the above:\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.16, 0.94\n0.13, 0.96\n0.08, 0.96\n\n\n(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\nmodel\n\nSequential(\n  (0): Linear(in_features=784, out_features=50, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\nSo this thing of looping through parameters and updating our parameters based on gradients and aligning right and then zeroing them is very common.\nSo common that there is something that does at all for us and that’s called an optimizer. It’s the stuff in Optim. So let’s create our own optimizer. And as you can see, it’s just going to do the two things we just saw.\n1-It’s going to go through each of the parameters and update them using the gradient in the lending rate.\n2-And there’s also zero grad which will go through each parameter and set their gradients to zero.\nIf you used .data like it’s just a way of avoiding having to say touch.no_grid basically.\nOkay, so in Optimizer we’re going to pass at the parameters that we want to optimize. I’m going to pass at the learning, right? And we’re just going to store them away. And since the parameters might be a generator, we’ll call list to turn them into a list. So we are going to create our optimizer, pass it in the model. parameters which have been automatically constructed for us by an nn.module. And so here’s our new loop. Now, we don’t have to do any of the stuff manually. We can just say opt.step. So that’s going to call this and opt.zero_grad and that’s going to call this. There it is. So we’ve now built our own SGD optimizer from scratch.\nSo I think this is really interesting right? Like these things which seem like they must be big and complicated once we have this nice structure in place, you know, an SGD to optimize, it doesn’t take much code at all. And so it’s all very transparent, simple clear. If you’re having trouble using complex library code that you’ve found elsewhere, you know, this can be a really good approach is to actually just go all the way back and move as you know, as many of these abstractions as you can and like run everything by hand to see exactly what’s going on. It can be really freeing to see that you can do all this anyway, since PyTorch has this for us In torch.optim. It’s got a optim.SGD. And just like our version, you pass in the parameters and you pass in the learning, right? So you really see it is just the same. So let’s define something called get model that’s going to return the model, the sequential model and the optimizer for it. So if we go model, comma opt equals get model, and then we can call the lost function to see where it’s starting. And so then we can write our training loop again, go through each epoch, go through each starting point for our for our batches, grab the slice, slice into our x and y in the training set to get a predictions, calculate our loss to the backward pass, to the optimizer, step to the zero gradient and print out how you’re going at the end of each one. And then we go,\n\n\noptim\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= p.grad * self.lr\n\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nopt = Optimizer(model.parameters())\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.18, 0.94\n0.13, 0.96\n0.11, 0.94\n\n\nPyTorch already provides this exact functionality in optim.SGD (it also handles stuff like momentum, which we’ll look at later)\n\nfrom torch import optim\n\n\ndef get_model():\n    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n    return model, optim.SGD(model.parameters(), lr=lr)\n\n\nmodel,opt = get_model()\nloss_func(model(xb), yb)\n\ntensor(2.33, grad_fn=<NllLossBackward0>)\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(n,i+bs))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.12, 0.98\n0.09, 0.98\n0.07, 0.98"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 7/index.html#dataset-and-dataloader",
    "href": "posts/Writing Stable Diffusion from Scratch 7/index.html#dataset-and-dataloader",
    "title": "Writing Stable Diffusion from Scratch 7",
    "section": "Dataset and DataLoader",
    "text": "Dataset and DataLoader\nAll right, so let’s keep making this simpler. This don’t say much code. So one thing we could do is we could replace these lines of code with one line of code by using something, call the dataset class.\nSo the dataset class is just something that we’re going to pass in our independent and dependent variable. Well, store them away as self.x and self.y. Why we’ll have something. So if you if you define len, then that’s the thing that allows the len function to work. So the length of the dataset would just be the length of the independent variables.\nAnd then getitem is a thing that will be called automatically any time you use square brackets in Python. So that task is going to call this function passing in the indices you want. So when we grab some items from our dataset, we’re going to return a couple of the x values and the y values.\nSo then we’ll be able to do this. So let’s create a data set using this tiny little tree line class. it’s going to be a dataset containing the x and y training, and they’ll create another dataset containing the x and y valid. And those two datasets will call train_ds and valid_ds. So let’s check the length of those data sets should be the same as the length of the xs and they are. And so now we can do exactly what we hope we could do. We can say xb,yb equals train_ds and passing some slice. So that’s going to give us back our check The shapes are correct. It should be five by 28, by 28. And the y is should just be five. And so here they are, the xs and the y’s. So that’s nice. We’ve created a dataset from scratch and again, it’s not complicated at all. And if you look at the actual PyTorch source code, this is basically your data sets do so let’s try it. We call get_model() And so now we’ve replaced our dataset line with this one and as usual it still runs. And so this is what I do when I’m writing code is I try to like always make sure that my starting code works as I refactor. And so you can see all the steps. And so somebody reading my code can then see exactly like, why am I building everything I’m building? How does it all fit in? Say that it still works and I can also keep it clear in my own head. So I think this is a really nice way of implementing libraries as well. All right. So now we’re going to replace these two lines of code with this one line of code. So we’re going to create something called a data loader.\n\nDataset\nIt’s clunky to iterate through minibatches of x and y values separately:\n    xb = x_train[s]\n    yb = y_train[s]\nInstead, let’s do these two steps together, by introducing a Dataset class:\n    xb,yb = train_ds[s]\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=68}\nclass Dataset():\n    def __init__(self, x, y): self.x,self.y = x,y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i): return self.x[i],self.y[i]\n:::\n\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nassert len(train_ds)==len(x_train)\nassert len(valid_ds)==len(x_valid)\n\n\nxb,yb = train_ds[0:5]\nassert xb.shape==(5,28*28)\nassert yb.shape==(5,)\nxb,yb\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))\n\n\n\nmodel,opt = get_model()\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        xb,yb = train_ds[i:min(n,i+bs)]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\n0.17, 0.96\n0.11, 0.94\n0.09, 0.96\n\n\n\n\nDataLoader\nSo now we’re going to replace these two lines of code with this one line of code. So we’re going to create something called a data loader. And a data loader is something that’s just going to do this. Okay. So we need to create an iterator. So an iterator is a class that has a iter method. When you say for in in Python behind the scenes, it’s actually calling iter to get a special object, which it can then loop through using yield. So it’s basically getting this thing that you can iterate through using the yield. So a data loader is something that’s going to have a data set and a batch size because we’re going to go through the batches and grab one batch at a time. So we have to store away the data set in the batch size. And so when we when we call the for loop, it’s going to code iter. We’re going to want to do exactly what we saw before, go through the range just like we did before, and then yield that bit of the data set. And that’s all. So that’s a data letter. So we can now create a train data loader and a validator loader from our train data set and validator set. And so now we can, if you remember the way you can create one thing out of an iterator so you don’t need to use a for loop, you can just say iter. And that will also code and data. Next, we’ll just grab one value from it. So here we will run this and you can see we’ve now just confirmed wave xb is a 50 by 784. And why yb, there it is. And then we can check what it looks like. So let’s grab the first element of our x batch, make it 28 by 28. And there it is. So now that we’ve got a date loader again, we can grab our model and we can simplify our fit function to just go for xb,yb and train_dl.\nPreviously, our loop iterated over batches (xb, yb) like this:\nfor i in range(0, n, bs):\n    xb,yb = train_ds[i:min(n,i+bs)]\n    ...\nLet’s make our loop much cleaner, using a data loader:\nfor xb,yb in train_dl:\n    ...\n\nclass DataLoader():\n    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n    def __iter__(self):\n        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\n\n\nxb,yb = next(iter(valid_dl))\nxb.shape\n\ntorch.Size([50, 784])\n\n\n\nyb\n\ntensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7,\n        6, 8, 9, 0, 3, 8, 3, 7, 7, 8, 4])\n\n\n\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\nmodel,opt = get_model()\n\n\ndef fit():\n    for epoch in range(epochs):\n        for xb,yb in train_dl:\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.11, 0.96\n0.09, 0.96\n0.06, 0.96\n\n\n(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))\n\n\n\n\nRandom sampling\nSo this is getting nice and small, don’t you think? And it still works the same way. Okay, so this is really cool. And now that it’s nice and concise, we can start adding features to it. So one feature I think we should add is that our training set each time we go through it, it should be in a different order. It should be randomized the order. So instead of always just going through these indexes in order, we want some way to say use random indexes. So the way we can do that is create a class called sampler. And what sampler is going to do here is if we create a sampler without shuffle, without randomizing it, it’s going to simply return all the numbers from zero up to n in order and it’ll be an iterator.\nSo the way we can do that is create a class called sampler. And what sampler is going to do here is if we create a sampler without shuffle, without randomizing it, it’s going to simply return all the numbers from zero up to end in order and it’ll be an iterator. So this is done data, but if I do want to, then it will randomly shuffle them. So here you can see I’ve created a sampler without shuffle. So if I then make an iterator from that and print a few things in the iterator, you can see it’s just printing out the indexes. It’s going to want, or I can do exactly the same thing as we learned earlier in the course using Isolés, we can grab the first five. So here’s the first five things from a sampler when it’s not shuffled. So as you can see, these are just indexes. So we could add shuffle equals true. And now that’s going to call random dot shuffle, which just randomly permits them. And now if I do the same thing, I’ve got random indexes. If my source data. So why is that useful? Well, what we can now do is create something called a batch sampler, and the batch sampler is going to do is it’s going to basically do this isolates thing for us. So we’re going to say, okay, pass in a sampler, set something that generates indices and pass in a batch size. And remember, we’ve looked at chunking before. It’s going to chunk that iterator by that batch size. And so if I now say, all right, please take our sampler and create batches of four, as you can see here, it’s creating batches of four indices at a time. So rather than just looping through them in order, I can now loop through this batch sampler. So we’re going to change our data loader so that now it’s going to take some batch sampler and it’s going to look through the batch sampler that’s going to give us indices and then we’re going to get that data set item from that batch for everything in that batch.\nSo that’s going to give us a list and then we have to stack all of the (x es) and all of the (y eys) together into tensers. So I’ve created something here called Collate function and we’re going to default that to this little function here, which is going to grab our batch, pull out the (x es) and (y eys) separately, and then stack them up into tensors. So this is called our collate function. Okay, So if we put all that together, we can create a training sampler, which is a batch sampler over the training set with shuffle True, a validation sampler will be a batch sampler over the validation set with shuffle false. And so then we can pass that into this data loader class, The training data set and the training sampler and the COLLATE function, which we don’t really need because we’re just using the default one. So I guess we can just get rid of that. And so now here we go. We can do exactly the same thing as before. xb,yb =next(iter(valid_dl)) this time we use the validator loader, check the shapes. And so now check. That still works. And it does. So this is how PyTorch is actual data loaders work. This is the this is all the pieces they have. They have samplers, they have batch samplers, they have a collation function and they have data letters. So remember that what I want you to be doing for your homework is experimenting with these carefully to see exactly what each thing is taking in. Okay, so here is asking on the chat what is this collate thing doing?\nOkay, so collate function and it defaults to collate. What does it do? Well, let’s see. Let’s go through each of these steps. Okay, so we need so we’ve got a batch sampler. So let’s do just the valid sampler. No fit didn’t work. We have to look at that to. Okay, so the batch sampler, here it is. So we’re going to go through each thing in the batch sampler. So let’s just grab one thing from the batch sampler. Okay? So the output with the batch sampler will be next. It’s a okay, so here’s what the batch sampler contains. All right. Just the first 50 digits, not surprisingly, because this is a validation sampler. If we did a training sampler, that would be randomized. Yeah, they are. Okay, so then what we then do is we go self.ds[i] for i and b, so let’s copy that copy paste. And so rather than self.ds[i] will just say train_ds[i] for i in o ,Sorry. Training. Okay. So what it’s created here is a list of tuples of tensors, I think. Let’s have a look. So let’s have a look. So we’ll call this a whatever. So p zero. Okay, is a tuple. It’s got the x and the y independent, independent variable. So that’s not what we want. What we want is something that we can live through. We want to get batches. So what the collation function is going to do is it’s going to take all of our x’s and all of our y’s and collect them into two tensors, one tensor x’s and one tensor y’s. So the way it does that is it. First of all, calls zip. So zip is a very, very commonly used python function. It’s got nothing to do with the compression program zip. But instead what it does is it effectively allows us to like transpose things so that now, as you can see, we’ve got all of the second elements, index one elements all together and all of the index zero elements together. And so then we can stack those all up together and that gives us our y`s for our batch. So that’s what collate it does. So the collate function is used an awful lot in in PyTorch increasingly nowadays where hackingface stuff uses it a lot and so we’ll be using it a lot as well. And basically it’s a thing that allows us to customize how the data that we get back from our dataset set. Once it’s been kind of generating a list of things from the dataset, how do we put it together into some into a bunch of things that our model can take as inputs, because that’s really what we want here. So that’s what the collection function does. All right. So let’s try this again. And the reason these are wrong, it’ll be something to do with a report. It’s a accuracy. So something about the accuracy is not printing out correctly. Let’s see if we can figure it out. So probably does it down here. Yeah, it’d be something to do with that shuffling. If we tend to shuffle off, probably find out our work. Yes, it does. Okay, sir, my shuffling bracket. Hmm. Oh, I see why this should be preds. Better.So when we run our model and we call fit(), we get the same results. One trick I was just going to mention normally in init the way we we very, very often want to grab all the stuff that we’ve been passed as parameters and store it away like so this is the wrong way around. Like so this is something that I do so often that fast core has a quick little shortcut for it. Just called Store at Trust all attributes. And so if you just put that in your init, then you just need one line of code and it does exactly the same thing. So there’s a little shortcut as you see. And so you’ll see that quite a bit. All right, let’s have a seven minute break and see you back here very soon.\nWe want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized.\n\nimport random\n\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n    def __iter__(self):\n        res = list(range(self.n))\n        if self.shuffle: random.shuffle(res)\n        return iter(res)\n\n\nfrom itertools import islice\n\n\nss = Sampler(train_ds)\n\n\nit = iter(ss)\nfor o in range(5): print(next(it))\n\n0\n1\n2\n3\n4\n\n\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nss = Sampler(train_ds, shuffle=True)\nlist(islice(ss, 5))\n\n[28659, 39049, 23211, 13983, 38058]\n\n\n\nimport fastcore.all as fc\n\n\nclass BatchSampler():\n    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n\n\nbatchs = BatchSampler(ss, 4)\nlist(islice(batchs, 5))\n\n[[7445, 36933, 36891, 13229],\n [47783, 46860, 1239, 20962],\n [8646, 29897, 9202, 31355],\n [48398, 35167, 44700, 27769],\n [7834, 22128, 40411, 5830]]\n\n\n\ndef collate(b):\n    xs,ys = zip(*b)\n    return torch.stack(xs),torch.stack(ys)\n\n\nclass DataLoader():\n    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)\n\n\ntrain_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\nvalid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp)\nvalid_dl = DataLoader(valid_ds, batchs=valid_samp)\n\n\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\nmodel,opt = get_model()\n\n\nfit()\n\n0.33, 0.12\n0.22, 0.06\n0.02, 0.04\n\n\n\n\nMultiprocessing DataLoader\nAnd we’re going to look at a multi processing data loader and then we’ll have nearly finished this notebook.\nAll right. So you send it because wants to know how it implements store_attr. You can always look it up. Right. So if we could go look at here is the docs and there’s always a link to the source. Yeah, go. You can create a version that’s a lot less code than this. This is dealing with a few extra things like slots and store_args and stuff. So basically the trick is to see who’s calling you using some rather internal ish stuff on code. All right, let’s keep going. So we’ve seen how to create a data loader and sampling from it. The PyTorch data loader works exactly like this, but it uses a lot more code because it implements multi processing. And so multi processing means that the actual this thing here, that code can be run in multiple processes, that can be run in parallel for multiple items. So this code, for example, might be opening up a jpeg, rotating it, flipping it, etc.. Right. So because remember, this is just calling the getitem for a data set. So that could be doing a lot of work for each item and we’re doing it for every item in the batch. So we’d love to do those all in parallel. So I’ll show you a very quick and dirty way that basically does the job. So Python has a multiprocessor library. It doesn’t work particularly well with PyTorch tensors, so PyTorch has created an exact implementation of it. So it’s identical API wise, but it does work well with tensors. So this is basically what a script the multi processing. So this is not quite shading because multi processing is in the standard library and this is API equivalent. So I’m going to say we’re allowed to do that. So as we’ve discussed, you know, when we call square brackets on a class, it’s actually identical to calling the getitem function on on the object. So you can see here, if we say give me items three, six, eight and one, it’s the same as calling getitem passing in three, six, eight and one.\nNow why does this matter??\nWell I’ll show you why it matters because we’re going to be able to use map and explain why we want to use map the moment map is a really important concept. You might have heard of MapReduce. So we’ve already talked about reductions and what those are maps are kind of the other key piece map is something which takes a sequence and calls a function on every element of that sequence.\nSo imagine we had a couple of batches of indices three and six and eight and one. Then we’re going to call getitem on each of those batches. So that’s what map does. map calls this function on every element of the sequence. And so that’s going to give us the same stuff. But now this, same as this, but now batched into two batches. Now why do we want to do that? Because multi processing has something called Pool where you can tell it. How many workers do you want to read and how many processes you want to run? And it then has a map which works just like the python normal python map, but it runs this function in parallel over the items from this iterator. So this is how we can create a multi processing data loader. So here we are creating our data loader and again, we don’t actually need to pass in the collate function because we using the default one. So if we say n_workers equals two and then create that if we say next, see how it’s taking a moment took a moment because it was firing off those two workers in the background. So the first batch actually comes out more slowly. But the reason that we would use a multi processing data loader is if this is doing a lot of work, we want it to run in parallel. And even though the first item might come out a bit slower, once those processes are fired up, it’s going to be faster to run. So this is yeah, this is a really simplified multi processing data loader because this needs to be super, super efficient. PyTorch has lots more code than this to make it much more efficient. But the idea is this and this is actually a perfectly good way of experimenting or building your own data loader to make things work exactly how you want. So now that we’ve really implemented all this from PyTorch, let’s just grab the PyTorch. And as you can see, they’re exactly the same data laoder. They don’t have one thing called sampler that you pass shuffle to. They have two separate classes called sequential sampling random sampler. I don’t know why they do it that way. It’s a bit more work to me, but same idea. And they’ve got that sampler. And so it’s exactly the same idea. That training sampler is a batch sampler with a random sampler. The validation sampler is a batch sampler with a sequential sampler passing in batch sizes. And so we can now pass those samplers to the data loader. This is now the PyTorch data letter. And just like ours, it also takes a collate function. Okay. And it works cool. So that’s as you can see, it’s it’s doing exactly the same stuff that ours is doing with exactly the same API. And it’s got some shortcuts, as I’m sure you’ve noticed when you’ve used data loaders.\nSo, for example, calling batch sampler is very going to be very, very common. So you can actually just pass the batch size directly to a data loader and it will then auto create the batch samples for you so you don’t have to pass in batch sampler at all. Instead, you can just say sampler and it will automatically wrap that in the batch sampler for you. That does exactly the same thing. And in fact, because it’s so common to create a random sampler or a sequential sampler for a data set, you don’t have to do that manually. You can just pass in shuffle equals true or shuffle equals false to the data loader. And that does again, exactly the same thing. There it is.\nNow, something that is very interesting is that when you think about it, the batch sampler and the collation function are things which are taking the result of the sampler looping through them and then collating them together. But what we could do is actually because our datasets know how to grab multiple indices at once, we can actually just use the batch sampler as a sampler. We don’t actually have to look through them and collate them because they’re basically instantly collated. They come pre collated. So this is a trick which actually huggingface stuff can use as I won’t be saying it again. So this is an important thing to understand is how come we can pass a batch sample to the sampler. What’s it doing?\nAnd so rather than trying to look through the PyTorch code, I suggest going back to our non multi processing pure Python code to see exactly how that would work, because it’s a really nifty trick for things that you can grab multiple things from at once and it can save a whole lot of time. It can make your code a lot faster. Okay, so now that we’ve got all that nicely implemented,\n\nimport torch.multiprocessing as mp\nfrom fastcore.basics import store_attr\n\n\ntrain_ds[[3,6,8,1]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1, 1, 0]))\n\n\n\ntrain_ds.__getitem__([3,6,8,1])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1, 1, 0]))\n\n\n\nfor o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n\n\n\nclass DataLoader():\n    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n    def __iter__(self):\n        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\nit = iter(train_dl)\n\n\nxb,yb = next(it)\nxb.shape,yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\n\nPyTorch DataLoader\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=106}\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n:::\n\ntrain_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\nvalid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)\n\n\ntrain_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)\n\n\nmodel,opt = get_model()\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.10, 0.06\n0.10, 0.04\n0.27, 0.06\n\n\n(tensor(0.04, grad_fn=<NllLossBackward0>), tensor(0.98))\n\n\nPyTorch can auto-generate the BatchSampler for us:\n\ntrain_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)\n\nPyTorch can also generate the Sequential/RandomSamplers too:\n\ntrain_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\nvalid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)\n\n\nmodel,opt = get_model()\nfit()\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n0.21, 0.14\n0.15, 0.16\n0.05, 0.10\n\n\n(tensor(0.06, grad_fn=<NllLossBackward0>), tensor(0.98))\n\n\nOur dataset actually already knows how to sample a batch of indices all at once:\n\ntrain_ds[[4,6,7]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([9, 1, 3]))\n\n\n…that means that we can actually skip the batch_sampler and collate_fn entirely:\n\ntrain_dl = DataLoader(train_ds, sampler=train_samp)\nvalid_dl = DataLoader(valid_ds, sampler=valid_samp)\n\n\nxb,yb = next(iter(train_dl))\nxb.shape,yb.shape\n\n(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 7/index.html#validation",
    "href": "posts/Writing Stable Diffusion from Scratch 7/index.html#validation",
    "title": "Writing Stable Diffusion from Scratch 7",
    "section": "Validation",
    "text": "Validation\nwe should now add a validation set and there’s not really too much to talk here. We’ll just take our fit function. And this is exactly the same code that we had before. And then we’re just going to add something which goes through the validation set and gets the predictions and sums up the losses and accuracies and from time to time prints out the loss and accuracy. And so get deals we will implement by using the PyTorch data loader now. And so now our whole process will be get deals passing in the training and validation data set that is set for our validation data loader.\n???I’m doubling the batch size because\nit doesn’t have to do back propagation,???\nso it should use about half as much memory. So I can use a bigger batch size, get our model and then call this fit. And now it’s printing out the loss and accuracy on the validation set. So finally we actually know how we’re doing, which is that we’re getting 97% accuracy on the validation set, and that’s on the whole thing, not just on the last batch. So that’s cool. We’ve now implemented a proper working, sensible training loop. It’s still, you know, a bit more code than I would like, but it’s not bad. And every line of code in there and every line of code it’s calling, it’s all stuff that we have built ourselves, reimplemented ourselves. So we know what’s going on. And that means it’s going to be much easier for us to create anything we can think of. We don’t have to rely on other people’s code, so hopefully you’re as excited about that as I am because it really opens up a whole world for us.\nYou always should also have a validation set, in order to identify if you are overfitting.\nWe will calculate and print the validation loss at the end of each epoch.\n(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=116}\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc,count = 0.,0.,0\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred,yb).item()*n\n                tot_acc  += accuracy (pred,yb).item()*n\n        print(epoch, tot_loss/count, tot_acc/count)\n    return tot_loss/count, tot_acc/count\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=117}\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n:::\nNow, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:\n\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\nmodel,opt = get_model()\n\n\n%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)\n\n0 0.14236383258365096 0.958100004196167\n1 0.12564025239087642 0.9632000041007995\n2 0.13069150418043138 0.9645000052452087\n3 0.10988456704188138 0.9670000064373017\n4 0.11636368061415851 0.9678000068664551\nCPU times: user 6.57 s, sys: 25.2 ms, total: 6.59 s\nWall time: 6.66 s"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 9/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 9/index.html",
    "title": "Writing Stable Diffusion from Scratch 9",
    "section": "",
    "text": "All credits goes to fast.ai All mistakes are mine.\nYou should know and practice following after this blog post :  1- Callbacks , callable class  2- Partial  3- Lambda  4- dunder thingies\nSo we’ve kind of got nearly got all of our infrastructure in place before we do this. Some pieces of python, which not everybody knows and I want to kind of talk about and kind of computer science concepts I want to talk about. So that’s what our six foundations is about. So this whole section is just going to tell. It is going to talk about some stuff in Python that you might not have come across before, or maybe it’s a review for some of you as well. And it’s all stuff we’re going to be using basically in the next notebook.\nSo that’s why I want to talk to cover it. So we’re going to be creating a learner class. So a learner class is going to be a very general purpose training loop, which we can get to to do anything that we want it to do. And we’re going to be creating things called callbacks to make that happen. And so therefore we’re going to spend a few moments talking about what are callbacks, how are they used in in computer science, how are they implemented? Look at some examples. They come up a lot.\nPerhaps the most common place that you see callbacks in software is for doing events of events from some graphical user interface. So the main graphical user interface library in Jupyter notebooks is called ipywidgets, and we can create a widget like a pattern . And when we display it, it shows me a button and at the moment it doesn’t do anything. If I click on it. What we can do though, is we can add and onclick callback to it,we’re going to pass it a function which is called when you click it. So to find that function. So I’m going to say w.on_click(f) is going to assign the f function to the on_click callback. Now, if I click this, there you go. It’s doing it. Now, what does that mean?\nWell, a callback is simply a callable that you’ve provided. So remember, a callable is a more general version of a function. So in this case, it is a function that you’ve provided that will be called back to when something happens. So in this case, so something that’s happening is that they’re clicking a button. So this is how we are defining and using a callback as a GUI event. So basically everything in ipywidgets, if you want to create your own graphical user interfaces for Jupyter, you can do it with ipy widgets and by using these callbacks. So these particular kinds of callbacks are called events, but it’s just a callback. All right, so that’s somebody else’s callback."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 9/index.html#callbacks",
    "href": "posts/Writing Stable Diffusion from Scratch 9/index.html#callbacks",
    "title": "Writing Stable Diffusion from Scratch 9",
    "section": "Callbacks",
    "text": "Callbacks\n\nCallbacks as GUI events\n\nimport ipywidgets as widgets\n\nFrom the ipywidget docs:\n\nthe button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked\n\n\nw = widgets.Button(description='Click me')\n\n\nw\n\n\n\n\n\ndef f(o): print('hi')\n\n\nw.on_click(f)\n\nNB: When callbacks are used in this way they are often called “events”.\nLet’s create our own callback. So let’s say we’ve some very slow calculation, and so it takes a very long time to add up the numbers 0 to 5 squared because we sleep for a second after each one. So let’s run our slow calculation. Still running. Oh, how’s it going? Come on, finish our calculation.\nThere we go. The answer is 30. Now, for a slow calculation like that, such as training, a model, it’s a slow calculation. It’ll be nice to do things like, I don’t know, you know, print out loss from time to time or show a progress bar or whatever. So generally, for those kinds of things, we would like to define a callback that is called at the end of each epoch or batch or every few seconds or something like that.\n\n\nCreating your own callback\n\nfrom time import sleep\n\n\ndef slow_calculation():\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n    return res\n\n\nslow_calculation()\n\n30\n\n\nSo here’s how we can modify our calculation routine such that you can optionally pass it a callback. And so all of these codes are the same, except we’ve added this one line of code that says if there’s a callback, then call it and pass in what what we’re up to. So then we could create our callback function. So this is just like we created a our callback function f let’s create a show_progress callback function that’s going to tell us how far we’ve got. So now if we call slow calculation passing in our callback, you can say it’s going to call this function at the end of each step.\nSo here we’ve created our own callback so there’s nothing special about a callback. Like it doesn’t require its own like syntax. It’s not a new concept, it’s just an idea really, which is the idea of passing in a function which some other function will call at particular times, such as at the end of a step or such as when you click a button. So that’s what we mean by callbacks.\nWe don’t have to define the function ahead of time. We could define the function at the same time that we call the slow calculation by using Lambda. So as we’ve discussed before, Lambda just defines a function, but it doesn’t give it a name. So here’s a function that takes one parameter and prints out exactly the same thing as before. So here’s the same way as doing it, but using a lambda, we could make it more sophisticated now and rather than always saying also we finished epoc, whatever we could have let you pass in an exclamation and we print that out. And so in this case, we could now have our lambda call that function. And so one of the things that we can do now is to again, we can create a function that returns a function. And so we could create a make_ _show_progress function where you pass in the exclamation mark. We could then create in this no need to give it a name. it’s just return it directly. We can return a function that calls that exclamation. So here we are passing in Nice, and that’s exactly the same as doing something like what we’ve done before. We could say instead of using a lambda we can create in a function like this. So here is now a function that returns, a function that says exactly the same thing. Okay, so one way with the lambda when we’re allowed to lambda and one of the reasons I wanted to show you that is so we can do exactly the same thing using partial. So with partial, it’s going to do exactly the same thing as this kind of makes show progress. It’s going to call, show progress and pass. Okay. I guess so is again an example of a function returning a function. And so this is a function that calls show progress, passing in this as the first parameter. And Again, it does exactly the same thing. Okay. So where you get we tend to use partial a lot. So that’s certainly something worth spending time practicing. Now, as we’ve discussed, Python doesn’t care about types in particular, and there’s nothing about any of this that requires cb to be a function.\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n        if cb: cb(i)\n    return res\n\n\ndef show_progress(epoch): print(f\"Awesome! We've finished epoch {epoch}!\")\n\n\nslow_calculation(show_progress)\n\nAwesome! We've finished epoch 0!\nAwesome! We've finished epoch 1!\nAwesome! We've finished epoch 2!\nAwesome! We've finished epoch 3!\nAwesome! We've finished epoch 4!\n\n\n30\n\n\n\n\nLambdas and partials\n\nslow_calculation(lambda o: print(f\"Awesome! We've finished epoch {o}!\"))\n\nAwesome! We've finished epoch 0!\nAwesome! We've finished epoch 1!\nAwesome! We've finished epoch 2!\nAwesome! We've finished epoch 3!\nAwesome! We've finished epoch 4!\n\n\n30\n\n\n\ndef show_progress(exclamation, epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n\n\nslow_calculation(lambda o: show_progress(\"OK I guess\", o))\n\nOK I guess! We've finished epoch 0!\nOK I guess! We've finished epoch 1!\nOK I guess! We've finished epoch 2!\nOK I guess! We've finished epoch 3!\nOK I guess! We've finished epoch 4!\n\n\n30\n\n\n\ndef make_show_progress(exclamation):\n    def _inner(epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n    return _inner\n\n\nslow_calculation(make_show_progress(\"Nice!\"))\n\nNice!! We've finished epoch 0!\nNice!! We've finished epoch 1!\nNice!! We've finished epoch 2!\nNice!! We've finished epoch 3!\nNice!! We've finished epoch 4!\n\n\n30\n\n\n\nfrom functools import partial\n\n\nslow_calculation(partial(show_progress, \"OK I guess\"))\n\nOK I guess! We've finished epoch 0!\nOK I guess! We've finished epoch 1!\nOK I guess! We've finished epoch 2!\nOK I guess! We've finished epoch 3!\nOK I guess! We've finished epoch 4!\n\n\n30\n\n\n\nf2 = partial(show_progress, \"OK I guess\")\n\nIt just happens to be it just has to be a callable.\nA callable is something that that you can that you can call. And so as we’ve discussed another way of creating a callable is defining to__call__.\nSo here’s a class and this is going to work exactly the same as our make show progress thing but now as a class so there’s a init which store the explanation and call the prints and so now we’re creating a object which is callable and does exactly the same thing\nso these are all like fundamental ideas that I want you to get really comfortable with the idea of call , dunder things in general, partials, classes because they come up all the time in PyTorch code and, and in the code we’ll be writing and, in fact, pretty much all frameworks. So it’s really important to feel comfortable with them. And remember, you don’t have to rely on the resources we’re providing, you know, if there are certain things here that are very new to you, you know, Google around for some tutorials, so ask for help in the forums, for finding things and so forth.\n\n\nCallbacks as callable classes\n\nclass ProgressShowingCallback():\n    def __init__(self, exclamation=\"Awesome\"): self.exclamation = exclamation\n    def __call__(self, epoch): print(f\"{self.exclamation}! We've finished epoch {epoch}!\")\n\n\ncb = ProgressShowingCallback(\"Just super\")\n\n\nslow_calculation(cb)\n\nJust super! We've finished epoch 0!\nJust super! We've finished epoch 1!\nJust super! We've finished epoch 2!\nJust super! We've finished epoch 3!\nJust super! We've finished epoch 4!\n\n\n30\n\n\n\n\nMultiple callback funcs; *args and **kwargs\nAnd then I’m just going to briefly recover something I’ve mentioned before, which is *args, **kwargs because again, they come up a lot. I just want to show you how they work. So if we create a function that has *args and **kwargs, nothing else, and I’m just going to this function, just print them now, I’m going to call the function, I’m going to pass three, I’m going to pass a and I’m going to pass thing one equals.(f(3, ‘a’, thing1=“hello”)) Hello. Now, these are past what we would say by position. We haven’t got a block equals. They’re just stuck. They’re things that are passed by position are placed in *args if you have one. It doesn’t have to be called args, you can call it anything you like but in the star bit.\nAnd so you can see here that args is a tuple containing the positionally path documents.\nAnd then kwargs is a dictionary containing the name arguments. So that is all that *args and **kwargs does. And as I say, there’s nothing special about these names. I call this a I’ll call this b, okay. And it’ll do exactly the same thing.\ndef f(*a, **b): print(f”args: {a}; kwargs: {b}“)\nOkay, so this comes up a lot. And so it’s it’s important to remember that this is literally all that they’re doing. And then on the other hand, let’s say we had a function which takes a couple of let’s try that print. I actually just put them directly a, b, c, okay. We can also, rather than just using them as parameters, we can also use some of them when calling something. So let’s say I create something called args again. It doesn’t have to be called args called, which contains one comma two. And I create something called kwargs that contains a dictionary\nargs = [1,2] kwargs = {‘c’:3}\nG and I can pass in star args,star star kwargs. And that’s going to take this one two and pass them as individual arguments for positionally. And it’s going to take the {‘c’:3} and pass that as a named argument. c equals three. And there it is. Okay, so they’re kind of two linked but different ways that use star and star star.\n\ndef f(*a, **b): print(f\"args: {a}; kwargs: {b}\")\n\n\nf(3, 'a', thing1=\"hello\")\n\nargs: (3, 'a'); kwargs: {'thing1': 'hello'}\n\n\n\ndef g(a,b,c=0): print(a,b,c)\n\n\nargs = [1,2]\nkwargs = {'c':3}\ng(*args, **kwargs)\n\n1 2 3\n\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb: cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb: cb.after_calc(i, val=res)\n    return res\n\nOkay. Now here’s a slightly different way of doing callbacks, which I really like in this case. I’ve now passing in a callback that’s not callable, but instead it’s going to have a method called before_calc and another method called after_calc. And I’m so now my callback is going to be a class containing a before_calc and after_calc method. And so if I run that, you can see that there it goes.\nOkay. And so this is printing before and after every step by call, calling before_calc and after_calc. So callback actually doesn’t have to be a callable. It doesn’t have to be a function. A callback could be something that contains methods. So we could have a version of this which actually, as you can see here, it’s going to pass int after_calc, both the epoch number and the value it’s up to. But by using star args and star star kwargs I can just safely ignore them if I don’t want them. Right? So it’s just going to chew them up and not complain.\nIf I didn’t have those here, it won’t work because it got passed in value equals and there’s nothing here looking for val equals that doesn’t like that. So this is one good use star args and star star kwargs eat up arguments You don’t want.\nOr we could use the argument. So let’s actually use epoch and Val and print them out and there it is. So this is a more sophisticated callback that’s giving us status as we go.\n\nclass PrintStepCallback():\n    def before_calc(self, *args, **kwargs): print(f\"About to start\")\n    def after_calc (self, *args, **kwargs): print(f\"Done step\")\n\n\nslow_calculation(PrintStepCallback())\n\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\nAbout to start\nDone step\n\n\n30\n\n\n\nclass PrintStatusCallback():\n    def __init__(self): pass\n    def before_calc(self, epoch, **kwargs): print(f\"About to start: {epoch}\")\n    def after_calc (self, epoch, val, **kwargs): print(f\"After {epoch}: {val}\")\n\n\nslow_calculation(PrintStatusCallback())\n\nAbout to start: 0\nAfter 0: 0\nAbout to start: 1\nAfter 1: 1\nAbout to start: 2\nAfter 2: 5\nAbout to start: 3\nAfter 3: 14\nAbout to start: 4\nAfter 4: 30\n\n\n30\n\n\n\n\nModifying behavior\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb and hasattr(cb,'before_calc'): cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb and hasattr(cb,'after_calc'):\n            if cb.after_calc(i, res):\n                print(\"stopping early\")\n                break\n    return res\n\n\nclass PrintAfterCallback():\n    def after_calc (self, epoch, val):\n        print(f\"After {epoch}: {val}\")\n        if val>10: return True\n\n\nslow_calculation(PrintAfterCallback())\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 5\nAfter 3: 14\nstopping early\n\n\n14\n\n\n\nclass SlowCalculator():\n    def __init__(self, cb=None): self.cb,self.res = cb,0\n    \n    def callback(self, cb_name, *args):\n        if not self.cb: return\n        cb = getattr(self.cb,cb_name, None)\n        if cb: return cb(self, *args)\n\n    def calc(self):\n        for i in range(5):\n            self.callback('before_calc', i)\n            self.res += i*i\n            sleep(1)\n            if self.callback('after_calc', i):\n                print(\"stopping early\")\n                break\n\n\nclass ModifyingCallback():\n    def after_calc (self, calc, epoch):\n        print(f\"After {epoch}: {calc.res}\")\n        if calc.res>10: return True\n        if calc.res<3: calc.res = calc.res*2\n\n\ncalculator = SlowCalculator(ModifyingCallback())\n\n\ncalculator.calc()\ncalculator.res\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 6\nAfter 3: 15\nstopping early\n\n\n15"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 9/index.html#dunder__-thingies",
    "href": "posts/Writing Stable Diffusion from Scratch 9/index.html#dunder__-thingies",
    "title": "Writing Stable Diffusion from Scratch 9",
    "section": "__dunder__ thingies",
    "text": "__dunder__ thingies\nAnything that looks like __this__ is, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call __init__. These are defined as part of the python data model.\nFor instance, if python sees +, then it will call the special method __add__. If you try to display an object in Jupyter (or lots of other places in Python) it will call __repr__.\nOkay, So finally, let’s just review this idea dunder, which we’ve mentioned before, but just to, to really nail this home, anything that looks like this underscore, underscore something, underscore, underscore something is special. And basically it could be that Python has to find that special thing or PyTorch has to find that special thing or numpy as to find that special thing. But this special these are called under methods. And some of them are defined as part of the Python data model.\nAnd so if you go to the Python documentation, it’ll tell you about these various different his repr which we used earlier is init that we used earlier. So they’re all here. PyTorch has some of its own, numpy has some of its own.\nSo for example, if python says plus what it actually does is it calls add. So if we want to create something that’s not very good at adding things, it actually also always adds point. I want to it that I can say sloppy at a one plus floppy at a two equals 3.01. So plus here is actually calling add. So if you’re not familiar with this, click on this data model link and read about these specific one two, three, four, five, six, seven, eight, nine, ten, 11 methods because we’ll be using all of these in the course.\nSo I’ll try to revise them when we can. But I’m generally going to assume that, you know, a particularly interesting one is getattr and getitem. We’ve seen setattr already get across just the opposite. Take a look at this. Here’s a class. It just contains two attributes a, b, that are set one and two. So create that an object of that class a.b equals two because I set b to two. Okay. Now when you say dot B, that’s just in texture. Good. Basically in Python, what it’s actually calling behind the scenes is getattr, it calls, getattr on the object. And so this one here is the same getattr a comma b which hopefully\n\nclass SloppyAdder():\n    def __init__(self,o): self.o=o\n    def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01)\n    def __repr__(self): return str(self.o)\n\n\na = SloppyAdder(1)\nb = SloppyAdder(2)\na+b\n\n3.01\n\n\nSpecial methods you should probably know about (see data model link above) are:\n\n__getitem__\n__getattr__\n__setattr__\n__del__\n__init__\n__new__\n__enter__\n__exit__\n__len__\n__repr__\n__str__\n\n\n__getattr__ and getattr\n\nclass A: a,b=1,2\n\n\na = A()\n\n\na.b\n\n2\n\n\n\ngetattr(a, 'b')\n\n2\n\n\nI’ll actually that’ll be yes our calls get a b and this can kind of be fun because you could call, getattr a comma and either b or a randomly . So if I run this 21112 as you can say, it’s random. So yeah, Python such a dynamic language, you can even set it up so it literally don’t know what attributes are going to be called. Now getattr behind the scenes. It’s actually calling something called getattr and by default it’ll use the version in the object based class. So here’s something just like a it’s got a and b defined, but I’ve also got time getattr defined and so getattr It’s only called for stuff that hasn’t been defined yet and it’ll pass in the key of the the name of the attribute. So generally speaking, if the first character is an underscore, it’s going to be private or special. So That’s going to raise an attribute error. Otherwise I’m going to steal it and return hello from k. So if I go b.athat’s defined so it gives me one. If I go b.foo, that’s not defined. So calls getattr and I get back hello from foo. And so this gets used a lot in both fastai code and also huggingface code to you know often make it more convenient to access things. So that’s yeah that’s how we getattr function and getattr method work.\n\ngetattr(a, 'b' if random.random()>0.5 else 'a')\n\n1\n\n\n\nclass B:\n    a,b=1,2\n    def __getattr__(self, k):\n        if k[0]=='_': raise AttributeError(k)\n        return f'Hello from {k}'\n\n\nb = B()\n\n\nb.a\n\n1\n\n\n\nb.foo\n\n'Hello from foo'\n\n\nOkay so I went over that pretty quickly since I know for quite a few folks this will be all review, but I know for folks who haven’t seen any of this, this is a lot to cover. So I’m hoping that you all kind of go back over this, revise it slowly, experiment with it, and look up some additional resources and ask on the forum and stuff. That’s not clear. Remember, everybody has parts of the course that’s really easy for them and parts of the course that are completely unfamiliar for them. And so if this particular part of the course is completely unfamiliar to you, it’s not because this is harder or going to be more difficult or whatever. It’s just so happens that this is a bit that you’re less familiar with. Or maybe this stuff about calculus in the last lesson was a bit that you’re less familiar with. There isn’t really anything, particularly in the course, that’s more difficult than other parts. It’s just that, you know, based on whether you happen to have that background.\nAnd so yeah, if you spend a few hours studying and practicing, you know, you’ll be able to pick up these things and yeah, so don’t stress if there are things that you don’t get right away, just take the time. And if you Yeah, if you do get lost, please ask because people are very keen to help. If you’ve tried asking on the forum, hopefully you noticed that people are really keen to help. All right. So I think this has been a pretty successful lesson. We’ve we’ve got to a point where we’ve got a pretty nicely optimized training loop. We actually understand exactly what data load is and data sets do. We’ve got an optimizer. We’ve been playing with hugging face data sets and we’ve got those working really smoothly. So we really feel like we’re in a pretty good position to to write our generic learner training loop and then we can start building and experimenting with lots of models. So look forward to seeing you next. Time to doing that together. Okay."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 8/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 8/index.html",
    "title": "Writing Stable Diffusion from Scratch 8",
    "section": "",
    "text": "Things you will should know and practice after reading this :  1- Getting data from dataloader of huggingface dataset  2- @inplace in python  3- plotting\nYou need to install stuff in colab so the notebook works. I have some extra installation in compare to Jeremy original notebook. For more infor check this link in fastai forum: https://forums.fast.ai/t/is-there-any-workaround-to-use-mini-ai-in-colab/104732\nLet`s learn how to use huggingface datasets.\nSo one thing that we’re going to want to be able to do now that we’ve got a training loop is to grab data and there’s a really fantastic library of datasets available on huggingface nowadays. And so let’s look at how we use those datasets now that we know how to bring things into data loader and stuff, so that now we can use the entire world of huggingface datasets with our code.So you need to pip install datasets. And once you’ve pip install datasets, you have to say from datasets import and you can import a few things. Just these two things now like dataset like Dataset Builder, and we’re going to look at a dataset called fashion MNIST. And so the way things tend to work with hacking faces is something called the Hugging Face Hub, which has models and it has datasets, amongst other things. And generally you’ll give them a name and you can then say, in this case, load a dataset builder for fashion MNIST. Now a dataset builder is just basically something which has some metadata about about this dataset. So the dataset builder has a dot info and the dot info has a dot description. And here’s a description of this. And as you can see again, we’ve got 28 by 28 to grayscale, so it’s going to be very familiar to us because this is just like MNIST. And again, we’ve got ten categories and again we’ve got 60,000 training examples and again we’ve got 10,000 test examples. So this is this is code, as it says, direct drop in replacement for MNIST.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=8}\n:::\nAnd so the dataset builder also will tell us what are what’s in this dataset. And so huggingface stuff generally uses dictionaries rather than tuples. So there’s going to be an image of type image and there’s going to be a label of type class label. There’s ten classes and these are the names of the classes. So it’s quite nice that in huggingface datasets, you know, we can kind of get this information directly. It also tells us if there are some recommended training test splits, we can find out those as well. So this is the size of the training split and the number of examples. So now that we’re ready to start playing it with that, we can load the dataset. Okay, so this is a different train load dataset builder versus load dataset. So this will actually download it, cache it, and here it is, and it creates a dataset dictionary. So a dataset dictionary, if you’ve used fastai is basically just like what we call the datasets class they call the dataset dict class. So it’s a dictionary that contains, in this case, a train and a test item. And those are datasets. And these datasets are very much like the datasets that we created in the previous notebook. So we can now grab the training and test items from that dictionary and just pop them into variables. And so we can now have a look at the zero index thing in training. And just like we were promised, it contains an image and a label. So as you can see, we’re not getting tuples anymore. We’re getting dictionaries containing the x and the y in this case image and label. So I’m going to get pretty writing image and label and strings all the time, so I’m just going to store them as x and y. So x is going to be the string image and y will be the string label. I guess the other way I could have done that would have been to say x comma, y equals that probably a bit neater because it’s coming straight from the features. And if you, if you iterate into a dictionary you get back its, its keys, that’s why that works. So anyway, I’ve done it manually here, which is a bit sad, but there you go. Okay, so we can now grab the from train[0][x], which we’ve already seen. We can grab the x by the image and there it is. It’s the image where you could grab the first five images in the first five labels, for example. And there they are. Now, we already know what the names of the classes are, so we could now see what these map two by grabbing those features. So there they are. So This is a special huggingface class, which most libraries have something including fast ai, that works like this. There’s something called int to string, which is going to take these and convert them to these. So if I call it on our y batch, you’ll see we’ve got first is ankle boots."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 8/index.html#hugging-face-datasets",
    "href": "posts/Writing Stable Diffusion from Scratch 8/index.html#hugging-face-datasets",
    "title": "Writing Stable Diffusion from Scratch 8",
    "section": "Hugging Face Datasets",
    "text": "Hugging Face Datasets\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\n\n\n\n\n\n\n\n\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n\nds_builder.info.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\nds_builder.info.splits\n\n{'train': SplitInfo(name='train', num_bytes=31296655, num_examples=60000, shard_lengths=None, dataset_name=None),\n 'test': SplitInfo(name='test', num_bytes=5233818, num_examples=10000, shard_lengths=None, dataset_name=None)}\n\n\n\ndsd = load_dataset(name)\ndsd\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48. Subsequent calls will reuse this data.\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\n\ntrain,test = dsd['train'],dsd['test']\ntrain[0]\n\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F7D5C52E6E0>,\n 'label': 9}\n\n\n\nx,y = ds_builder.info.features\n\n\nx,y\n\n('image', 'label')\n\n\n\nx,y = 'image','label'\nimg = train[0][x]\nimg\n\n\n\n\nThat is indeed an ankle boot. They might have a couple of t shirts and a dress. Okay, so how do we use this to train a model? Well, we’re going to need a data loader, and we want a data loader that for now we’re going to just, like, return it before it’s going to return. Well, actually, we’re going to do something different. We’re going to have our collate function is actually going to return a dictionary. Actually, this is pretty common for huggingface stuff.\nAnd PyTorch doesn’t mind, it’s happy for you to return a dictionary from a collation function. So rather than returning a tuple of the stacked up actually this looks very familiar. This looks a lot like the thing that goes through the dataset for each one and stacks them up just like we did in the previous notebook. So We’re doing all all in one step here in our collate function. And then again, exactly the same thing. Go through our batch grab the y and this is just stacking them up with the integers so we don’t have to call stack. And so we’re now going to have the image and label bits in our dictionary. So if we create a data loader using that collation function, grab one batch so we can go a batch x.shape. It’s a 16 by one by 28 by 28. And our y if a batch here, here it is. So the thing to notice here is that we haven’t done any transforms or anything or written own dataset class or anything. We’re actually putting all the work directly in the collation function. So this is like a really nice way to skip all the kind of abstractions of your framework if you want to is you can just do all of your work and collate functions so it’s going to pass you each item. So it’s going to you’re going to get the batch directly and it’s going to go through each item. And so here we’re saying, okay, grab the x key from that dictionary, convert it to a tensor, and then do that for everything in the batch and then stack them all together. So this is yeah, this is like can be quite a nice way to do things if you want to do things just very manually without having to think too much about, you know, a framework, particularly if you’re doing really custom stuff, this can be quite helpful.\n\nxb = train[:5][x]\nyb = train[:5][y]\nyb\n\n[9, 0, 0, 3, 0]\n\n\n\nfeaty = train.features[y]\nfeaty\n\nClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\n\n\nfeaty.int2str(yb)\n\n['Ankle boot',\n 'T - shirt / top',\n 'T - shirt / top',\n 'Dress',\n 'T - shirt / top']\n\n\n\ntrain['label'][:5]\n\n[9, 0, 0, 3, 0]\n\n\n\ndef collate_fn(b):\n    return {x:torch.stack([TF.to_tensor(o[x]) for o in b]),\n            y:tensor([o[y] for o in b])}\n\n\ndl = DataLoader(train, collate_fn=collate_fn, batch_size=16)\nb = next(iter(dl))\nb[x].shape,b[y]\n\n(torch.Size([16, 1, 28, 28]),\n tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))\n\n\nHaving said that, huggingface data sets absolutely lets you avoid doing everything in collate function, which if we want to create really simple applications, that’s where we’re going to eventually went ahead. So we can do this using a transform instead. And so the way we do that is we create a function. It’s going to take our batch, it’s going to replace the x in our batch with the tensor version of each of those ???? images, and I’m not even stacking them or anything. And then we’re going to return that batch. And so huggingface datasets has something called with_transform, and that’s going to take your huggingface dataset. it’s going to apply this function to every element and it doesn’t run that all now. It’s going to basically run when it behind the scenes, when it calls getitem, it will call this function on the fly. So in other words, this could have data augmentation, which can be random or whatever, because it’s going to be rerun time. You grab an item, it’s not cached or anything like that. So other than that, this data set has exactly the API, same API as any other data set.\nIt has a length it has a getitem so you can pass it to a data loader. And so PyTorch already knows how to collate dictionaries of tensors. So we’ve got a dictionary of tensors now. So that means we don’t need a collate function anymore. I can create a data loader from this without a collate function. As you can see. And so this is to give you the exactly the same thing as before about without having to create a custom collate function. Now, even this is a bit more code than I want having to return. This seems a bit silly, but the reason I had to do this is because huggingfaced datasets expects the with transform function to return to the the new version of the of the data. So I wanted to be able to write it like this transform in place and just say the change I want to make and have it automatically return that. So if I call, if I create this function, that’s exactly the same as a previous one that doesn’t have return.\n\ndef transforms(b):\n    b[x] = [TF.to_tensor(o) for o in b[x]]\n    return b\n\n\ntds = train.with_transform(transforms)\ndl = DataLoader(tds, batch_size=16)\nb = next(iter(dl))\nb[x].shape,b[y]\n\n(torch.Size([16, 1, 28, 28]),\n tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))\n\n\n\ndef _transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\nHow would I turn this into something which does return the result? So here’s an interesting trick.\nWe could take that function, pass it to another function to create a new function, which is the a version of this in-place function that returns the result. And the way I do that is by creating a function called inplace. It takes a function, it returns a function. The function it returns is one that calls my original function and then returns the result. So this is the function. This is a function generating function, and it’s modifying an in-place function to become function that returns that a new version of that data. And so this is a function this function is passed to this function which returns a function. And here it is. So here’s the version that huggingface you’ll be able to use so I can now pass that to with_transform. And it does exactly the same thing. So this is very, very common in Python.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=27}\ndef inplace(f):\n    def _f(b):\n        f(b)\n        return b\n    return _f\n:::\n\ntransformi = inplace(_transformi)\n\n\nr = train.with_transform(transformi)[0]\nr[x].shape,r[y]\n\n(torch.Size([784]), 9)\n\n\nIt’s so common that line of code can be entirely removed and replaced with this little token. If you have a function and put that at the start, you can then put that before a function. And what it says is:\ntake this whole function, pass it to this function and replace it with a result. So this is exactly the same as the combination of this and this.\nAnd when we do it this way, this kind of little syntax, sugar is called a decorator. So there’s nothing, nothing magic about decorators. It’s literally, literally identical to this. Well, I guess the only difference is we don’t end up with this unnecessary intermediate underscore version, but the result is exactly the same. And therefore, I can create a transformed dataset by using this. And there we go. It’s all working fine. Yeah. So, I mean, none of this is particularly necessary, but what we’re doing is we’re just kind of like saying, you know, the pieces that we can make and put in place to make this stuff as easy as possible, and we don’t have to think about things too much. All right, Now, with all this, we can basically make things pretty automatic. And the way we can make things pretty automatic is we’re going to use a cool thing in Python code itemgetter.An itemgetter is a function that returns a function. So hopefully you’re getting used to this idea. Now, this creates a function that gets the a , c items from a dictionary or something that looks like a dictionary. So here’s a dictionary, it contains Keys, a, b, and c, So this function will take a dictionary and return the a and c values. And as you can see, it has done exactly that. Explain why this is useful in a moment. I just wanted to briefly mention what did I mean when I said something that looks like a dictionary? I mean, this is a dictionary. Okay, That looks like a dictionary, but it python doesn’t care about what type things actually are. It only cares about what they look like. And remember that when we call something with square brackets, when we index into something behind the scenes, it’s just call getitem so we could create our own class and it’s getitem gets the key and it’s just going to manually return one if k equals a, two if k is b or 3 otherwise. And look, that class also works just fine with an itemgetter.\nThe reason this is interesting is because like a lot of people write Python as if it’s like C++ or Java or something. They write as stiff as if it’s this kind of statically thing. But I really wanted to point out that it’s an extremely dynamic language and there’s a lot more flexibility than you might have realized anyway. That’s a little aside. So what we can do is think about a batch, for example, where we’ve got these two dictionaries. Okay, so PyTorch comes with a default collation function called not surprisingly, default_collate. So that’s part of PyTorch. And what default_collate does with dictionaries is it simply takes the matching keys and then grabs their values and stacks them together. And so that’s why if I called default_collate a is now one, three, b is now two four. That’s actually what happened before when we created this data loader is that used default_collate function which does that. It also works on things that are tuples, not dictionaries, which is what most of you would have used before.\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\ntdsf = train.with_transform(transformi)\nr = tdsf[0]\nr[x].shape,r[y]\n\n(torch.Size([784]), 9)\n\n\n\nd = dict(a=1,b=2,c=3)\nig = itemgetter('a','c')\nig(d)\n\n(1, 3)\n\n\n\nclass D:\n    def __getitem__(self, k): return 1 if k=='a' else 2 if k=='b' else 3\n\n\nd = D()\nig(d)\n\n(1, 3)\n\n\n\nlist(tdsf.features)\n\n['image', 'label']\n\n\n\nbatch = dict(a=[1],b=[2]), dict(a=[3],b=[4])\ndefault_collate(batch)\n\n{'a': [tensor([1, 3])], 'b': [tensor([2, 4])]}\n\n\nAnd what we can do there for is we could create something called collate_dict, which is something which is going to take a dataset and it’s going to create a itemgetter function for the features in that data set, which in this case is image and label. So this is a function which will get the image and label items. And so we’re not going to return a function and that function is simply going to call our itemgetter on default_collate. And what this is going to do is it’s going to take a dictionary and collate it into a tuple just like we did up here. So if we run that so we’re not going to call data loader on our transform dataset passing in. And remember, this is a function that attends a function. So it’s a collation function for this dataset and there it is. So now this looks a lot like what we had in our previous notebook. This is not returning a dictionary, that it’s returning a tuple. So this is a really important idea for particularly for working with hugging face data sets is that they tend to do things with dictionaries and most things in the pytouch world tend to work with tuples. So you can just use this now to convert anything that takes that returns dictionaries into something that provides tuples by passing it as a collation function to your data letter.\nSo remember, you know the thing you want to be doing this this week is, is doing things like import pdb, pdb.set.trace(), dataset,right, put breakpoints, step through, see what’s happening, you know, not just here, but also even more importantly, doing it inside, the innermost inner function. So then you can see as I do wrong, that oh, today set underscore, trace. So then we can see exactly what’s going on. Print out b, less the code and I could step into it and look, I’m now inside the default function, which is inside pytouch. And so I can now see exactly how that works. There it all is. So it’s going to go through and this code is going to look very familiar because we’ve implemented all this ourselves, except it’s being careful that it works for lots of different types of things. Dictionaries, numpy, arrays, so on and so forth.\nso the first thing I want to do, oh, actually something I do want to mention here. This is so useful. We want to be able to use it in all of our notebooks. So rather than copying and pasting this every time, it would be really nice to create a python module that contains this definition. So we’ve created a library called nbdev. It’s really a whole system called nbdev, which does exactly that. It creates modules. You can use from your notebooks and the way you do it is you use this special thing we call comment directives, which is hash pipe export. So you put this at the top of a cell and it says do something special for this. So what this does is it says, put this into a python module for me, please export it to a python module. What python module is it going to put it in. Well, if you go all the way to the top, you tell it what default export module to create. So it’s going to create a module called datasets. So what I do at the very end of this module is I’ve got this line that says import nbdev, nbdev.nbdev_export(). And what that’s going to do for me is create a library, a Python library. I’m going to have a dataset.py in it and we’ll see everything that we export it. Here it is collate_dict. It will appear this for me. And so what that means is now in the future, in my notebooks, I will be able to import collate_dict from that from my datasets. Now you might wonder how does it know to call it Mini AI? What’s mini AI Well an nbdev, you create a setting start any file, right? You say what the name of your library is. So we’re going to be using this quite a lot now because we’re getting to the point where we’re starting to implement stuff that didn’t exist before. So previously most of this stuff was pretty much all the stuff we’ve created. I’ve said like, Oh, that already exists in PyTorch, so we don’t need it, we just use pytorch. But we’re now getting to a point where we’re starting to create stuff that doesn’t anywhere and we’ve created it ourselves and so therefore we want to be able to use it again. So during the rest of this course we’re going to be building together a library called miniai. That’s going to be our framework, our version of something like fastai, maybe it’s something like what fastai 3 will end up being. Well, see, So that’s what’s going on here too. So we’re going to be using once I start using miniai, I’ll show you exactly how to install this. But that’s what this export is. And so you might have noticed I also had an export on this inplace thing and I also had it on my necessary import statements. Okay. And we want to be able to see what this dataset looks like.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=37}\ndef collate_dict(ds):\n    get = itemgetter(*ds.features)\n    def _f(b): return get(default_collate(b))\n    return _f\n:::\n\ndlf = DataLoader(tdsf, batch_size=4, collate_fn=collate_dict(tdsf))\nxb,yb = next(iter(dlf))\nxb.shape,yb\n\n(torch.Size([4, 784]), tensor([9, 0, 0, 3]))"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 8/index.html#plotting-images",
    "href": "posts/Writing Stable Diffusion from Scratch 8/index.html#plotting-images",
    "title": "Writing Stable Diffusion from Scratch 8",
    "section": "Plotting images",
    "text": "Plotting images\nSo I thought it now is a good time to talk a bit about plotting because how to visualize things well is really important. And again, the idea is we know we’re not allowed to use fastai plotting library, so we got to learn how to do everything ourselves.\nSo here’s the basic way to plot such an image using matplotlib so we can create a batch, grab the x part of it, grab the very first thing in that and imshow and it show an image. And here it is. There is our ankle boot. So let’s start to think about what stuff we might create, which we can export to make this a bit easier. So let’s create something called show_image, which basically does imshow, but we’re going to do a few extra things. We will make sure that it’s in the correct access order. We will make sure it’s not uncleared character that’s on the CPU here. If it’s not a numpy array will convert it to a numpy array will be get a pass in an existing access, which we’ll talk about soon. If we want to, we’ll be able to set a title if we want to. And also this thing here removes all this ugly zero five blah blah, blah access because we’re showing an image. We don’t want any of that. So if we try that, you can see there you go. We’ve also been able to say what size we want. The image there at all is. Now here’s something interesting. When I say help, the help shows the things that I implemented, but it also shows a whole lot more things. How did that magic thing happen? And you can see they work because his fixed size, which I didn’t add all Oh, sorry, I did that. Well, okay, that’s a bad example. Anyway, these other ones all work as well. So how did that happen?\nWell, the trick is that I added **kwargs here:\nAnd kwargs says, You can pass as many any other arguments as you like that aren’t listed and they’ll all be put into a dictionary with this name and then when I call iamshow. I pass that entire dictionary.  here means as separate arguments. And that’s how come it works. And then how come doesn’t know how come it knows what help to provide. The reason why is that fastcore has a special thing called delegate’s, which is a decorator. So now you know what a decorator is and you tell it. What is it that you’re going to be passing kwargs to? I’m going to be passing it to you, iamshow. And then it automatically creates the documentation correctly to show you what kwargs can do.\n\nb = next(iter(dl))\nxb = b['image']\nimg = xb[0]\nplt.imshow(img[0]);\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=40}\n@fc.delegates(plt.Axes.imshow)\ndef show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n    \"Show a PIL or PyTorch image on `ax`.\"\n    if fc.hasattrs(im, ('cpu','permute','detach')):\n        im = im.detach().cpu()\n        if len(im.shape)==3 and im.shape[0]<5: im=im.permute(1,2,0)\n    elif not isinstance(im,np.ndarray): im=np.array(im)\n    if im.shape[-1]==1: im=im[...,0]\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.set_xticks([]) \n    ax.set_yticks([]) \n    if noframe: ax.axis('off')\n    return ax\n:::\n\nhelp(show_image)\n\nHelp on function show_image in module __main__:\n\nshow_image(im, ax=None, figsize=None, title=None, noframe=True, *, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, interpolation_stage=None, filternorm=True, filterrad=4.0, resample=None, url=None, data=None)\n    Show a PIL or PyTorch image on `ax`.\n\n\n\n\nshow_image(img, figsize=(2,2));\n\n\n\n\n\nfig,axs = plt.subplots(1,2)\nshow_image(img, axs[0])\nshow_image(xb[1], axs[1]);\n\n\n\n\nSo this is a really helpful way of being able to kind of extend existing functions like iamshow and still get all of their functionality and all of their documentation and at your right. So delegates is one of the most useful things we have in fast core, in my opinion. So we’re going to export that. So now we can use show image anytime you want, which is nice. Something that’s really helpful to know about matplotlib is how to create subplots. So for example, what happens if you want to plot two images next to each other. So in matplotlib subplots creates multiple plots and you pass it number of rows and the number of columns. So this here has, as you see, one row and two columns and it returns axes. Now, what it calls axes is what it refers to as the individual plots. So if we now call show image on the first image passing in Axes zero, it’s going to get that here. Right Then we call iamshow that means put the image on this subplot that I call it a subplot. Unfortunately, they call it an axis. Put it on this axis. So that’s how come we’re able to show an image, one image on the first axis and then show a second image on the second axis, by which we mean subplot. And there’s are two images. So that’s pretty handy. So I’ve decided to add some additional functionality to subplots. So therefore, I use delegates on subplots because I’m adding functionality to it and I’m going to be taking kwargs and passing it through to subplots. And the main thing I wanted to do is to automatically create an appropriate figure size by just finding out. You tell us what image size you want. And I also want to be able to add a title for the whole set of subplots. And so there it is. And then I also want to show you that in it automatically, if we want to create documentation for us as well for our library and here is the documentation. So as you can see here for the stuff I’ve added, it’s telling me exactly what each of these parameters are, that type, the defaults and information about each one and that is automatically coming from these little comets, these we call these documents. This is all automatic stuff done by fast core and native. And so you might have noticed when you look at Fastai library documentation, it always has all this info. So that’s that’s that’s why you don’t actually have to show doc it automatically added to your documentation for you.\nI’m just showing you here what it’s going to end up looking like. And you can see that it’s worked with delegates. It’s put all the extra stuff from delegates in here as well, and they all stood out here as well. So anyway, subplots. So let’s create a three by three set of plots and we’ll grab the first images. And so now we can go through each of the subplots. Now it returns it as a three by three, basically a list of three lists of three items. So I flatten them all out into a single list. So I go through each of those subplots and go through each image and show each image on each axis. And so here’s a quick way to quickly show them all. As you can see, it’s a little bit ugly here, so we’ll keep on adding more useful, plotting functionality. So here’s something that again. It calls our subplots delegates to it, but we’re going to be able to say, for example, how many subplots do we want? And it’ll automatically calculate the rows in the columns and it’s going to remove the axes for any ones that we’re not actually using. And so here we got that. So that’s what get grids going to let us do. So we’re getting quite close. And so finally, why don’t we just create a single thing called show images that’s going to get our grid and it’s going to go through our images optionally with lists of titles and show each one.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=44}\n@fc.delegates(plt.subplots, keep=True)\ndef subplots(\n    nrows:int=1, # Number of rows in returned axes grid\n    ncols:int=1, # Number of columns in returned axes grid\n    figsize:tuple=None, # Width, height in inches of the returned figure\n    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n    suptitle:str=None, # Title to be set to returned figure\n    **kwargs\n): # fig and axs\n    \"A figure and set of subplots to display images of `imsize` inches\"\n    if figsize is None: figsize=(ncols*imsize, nrows*imsize)\n    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if suptitle is not None: fig.suptitle(suptitle)\n    if nrows*ncols==1: ax = np.array([ax])\n    return fig,ax\n:::\n\nfig,axs = subplots(3,3, imsize=1)\nimgs = xb[:8]\nfor ax,img in zip(axs.flat,imgs): show_image(img, ax)\n\n\n\n\nAnd we can use that here. You can see where you have successfully got all of our labeled images. And so we yeah, I think all this stuff for the plotting is pretty useful. So as you might have noticed, they were all exported. So in our datasets.py, we’ve got our get_grid, we’ve got our subplots, we’ve got our show_image. So that’s going to make life easier for us now since we have to create everything from scratch. We have created all of those things. So as I mentioned at the very end, we have this one line of code to run. And so just to show you, if I remove miniai data sets,it’s all empty. And then I run this line of code and now it’s back, as you can see, and it tells you it’s auto generated. All right, So we are nearly at the point where we can build our learner. And once we’ve built learner, we’re going to be able to really dive deep into training and studying models.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=48}\n@fc.delegates(subplots)\ndef get_grid(\n    n:int, # Number of axes\n    nrows:int=None, # Number of rows, defaulting to `int(math.sqrt(n))`\n    ncols:int=None, # Number of columns, defaulting to `ceil(n/rows)`\n    title:str=None, # If passed, title set to the figure\n    weight:str='bold', # Title font weight\n    size:int=14, # Title font size\n    **kwargs,\n): # fig and axs\n    \"Return a grid of `n` axes, `rows` by `cols`\"\n    if nrows: ncols = ncols or int(np.floor(n/nrows))\n    elif ncols: nrows = nrows or int(np.ceil(n/ncols))\n    else:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.floor(n/nrows))\n    fig,axs = subplots(nrows, ncols, **kwargs)\n    for i in range(n, nrows*ncols): axs.flat[i].set_axis_off()\n    if title is not None: fig.suptitle(title, weight=weight, size=size)\n    return fig,axs\n:::\n\nfig,axs = get_grid(8, nrows=3, imsize=1)\nfor ax,img in zip(axs.flat,imgs): show_image(img, ax)\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=50}\n@fc.delegates(subplots)\ndef show_images(ims:list, # Images to show\n                nrows:int|None=None, # Number of rows in grid\n                ncols:int|None=None, # Number of columns in grid (auto-calculated if None)\n                titles:list|None=None, # Optional list of titles for each image\n                **kwargs):\n    \"Show all images `ims` as subplots with `rows` using `titles`\"\n    axs = get_grid(len(ims), nrows, ncols, **kwargs)[1].flat\n    for im,t,ax in zip_longest(ims, titles or [], axs): show_image(im, ax=ax, title=t)\n:::\n\nyb = b['label']\nlbls = yb[:8]\n\n\nnames = \"Top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot\".split()\ntitles = itemgetter(*lbls)(names)\n' '.join(titles)\n\n'Boot Top Top Dress Top Pullover Sneaker Pullover'\n\n\n\nshow_images(imgs, imsize=1.7, titles=titles)\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=54}\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls[:2]\n\n    @classmethod\n    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n        f = collate_dict(dd['train'])\n        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f))\n:::"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 10/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 10/index.html",
    "title": "Writing Stable Diffusion from Scratch 10",
    "section": "",
    "text": "All credits goes to fast.ai All mistakes are mine. You should know and practice following after this blog post : 1- Convolution in concept and in coding\nHi, all, and welcome to lesson 15. And what we’re going to endeavor to do today is to create a convolutional auto encoder. And in the process, we will see why doing that well is a tricky thing to do. And time permitting, we will begin to work on a framework, a deep learning framework to make life a lot easier. Not sure how far we’ll get on that today. Time wise, let’s see how we go and get straight into it. So, okay, so today let’s start by talking before we can create a convolutional auto encoder we need to talk about convolutions and and what are they and what are they for.\nBroadly speaking, convolutions are something that allows us to to tell our neural network a little bit about the structure of the problem. That’s going to make it a lot easier for it to solve the problem. And in particular, the structure of our problem is we’re doing things with images. Images are laid out on a grid, a 2D grid with black and white, or a 3 to 4 color or a 44 color video or whatever. And so we would say, you know, there’s a relationship between the pixels going across the pixels going down. They tend to be similar to each other. Differences in those pixels across those dimensions tend to have meaning . Patterns of pixels that appear in different places often represent the same thing. So, for example, a cat in the top left is still a cat, even if it’s in the bottom right. These kinds of this kind of prior information is something that is naturally captured by a convolutional neural network, something that is convolutions. Generally speaking, this is a good thing because it means that we will be able to use less parameters and less computation, and because more of that information about the problem solving is kind of encoded directly into our architecture.Maybe I should plug my other life her. Let me do that as well as how to get enough light. Okay. Let me say yes, there are other architectures that don’t encode that prior information as strongly, such as a multilayer perceptron, which we’ve been looking at so far, or a Transformers network, which we haven’t looked at yet.\nThose kinds of architectures could potentially give us what they do, give us more flexibility and given enough time, compute and data, they could potentially find things that maybe CNN’s would struggle to find. So we’re not always going to use convolutional neural networks, but they’re pretty good starting point and certainly something important to understand. They’re not just used for images. We can also take advantage of one dimensional convolutions for language based tasks. For instance, the convolutions come up a lot. So in this notebook, one thing you’ll notice that might be of interest is we are importing stuff from miniai now. Now miniai is this little library that we’re starting to create and we’re creating nbdev. So we’ve got a miniai training and a mini A.I. data sets. And so if we look, for example, at the Datasets notebook, it starts with something that says that the default export module is called datasets, and some of the cells have a export directive on them. And at the very bottom we had something that called nbdev export. Now what that’s going to do is it’s going to create a file called dataset.py Just here, datasets.py why and it contains that’s those cells that we exported. And why does it why is it called miniai.datasets? That’s because everything miniai is stored in settings.mini and there’s something here. So create a library lib_name called miniai. I you can’t use this library until you install it. Now we haven’t uploaded it to 2 the pipi installable package from the public server, but you can actually install a local directory as if it’s a python module that you’ve kind of installed from the internet. And to do that you say pip install in the usual way, but you say minus e is set to editable and that means set up the current directory as a python module. Well, current directory actually any directory you like. I just put dot to be in the current directory and so you’ll see that’s going to go ahead and actually install all my library. And so after I’ve done that I can now import things from that library, as you say. Okay, so this is just the same as before. We’re going to grab Our MNIST data set and we’re going to create a convolutional neural network on it. So before we do that, we’re going to talk about what are convolutions. And one of my favorite descriptions of convolutions comes from the student in I think was our very first course, Matt Kleinsmith, who wrote this really nice medium article, CNN’s from Different Viewpoints. https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c\nwhich I’m going to steal from. And here’s the basic idea. I say that this is our image. It’s a three by three image with nine pixels labeled from A-J as capital letters.Now a convolution uses something called a kernel, and a kernel is just another tensor. In this case, it’s a two by two matrix again. So it’s this one. We’re going to have Alpha, beta, gamma Delta as our four values in this convolution now in this kernel. Now one thing I mentioned I can’t remember I’ve said this before is the Greek letters are things that you want to be able to I think I have mentioned this. You want to be able to pronounce them. So if you don’t know how to read these and say what these names are, make sure you head over to Wikipedia or whatever and learn the names of all the Greek letters so that you can because they come up all the time. Okay, so what happens when we apply a convolution with this two by two kernel, two this three by three image? I mean, it doesn’t have to be an image. It’s in this case it’s just a rank two tensor, but it might represent an image. What happens is we take the kernel and we overlay it over the first. They don’t two by two separate like so. And specifically what we do is we match color the color. So the output of this first two by two overlay would be alpha times A plus, beta times B plus gamma times D plus delta times E, and that would yield some value P And that’s going to end up in the top left of a two by two output. So the top right of the two by two output, we’re going to slide. It’s like a slide in window. We’re going to slide our kernel over to here and apply each of our coefficients to these respectively colored squares. And then ditto for the bottom left and then ditto for the bottom right. So we end up with this equation. P, as we discussed, is Alpha A plus, beta B plus eight plus delta E plus some bias term. Q So the top right as you can say, it’s just alpha in this case times B And so we’re just multiplying them together and adding them up. Multiply together at the map, multiplied together and add them up. So we’re basically you can imagine that we’re basically flattening these out into rank one tensors into vectors. And then doing a dot product would be one way of thinking about what’s happening as we slide this kernel over these windows. And so this is called a convolution. So let’s try and create a convolution. So for example, let’s grab our training images and take a look at one and let’s create a three by three kernel. So remember, a kernel is just we’ve already got all appears has a lot of times in computer science and math. We’ve already seen the term kernel to mean a piece of code that we run on a year across lots of parallel kind of virtual devices or potentially in a grid. There’s a similar idea here. We’ve got a computation which is in this case kind of this dot product or something like a dot product, okay, sliding over occurring lots of times over a grid. But it’s yeah, it’s a bit different. So that’s kind of another use of the word kernel."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 10/index.html#creating-the-cnn",
    "href": "posts/Writing Stable Diffusion from Scratch 10/index.html#creating-the-cnn",
    "title": "Writing Stable Diffusion from Scratch 10",
    "section": "Creating the CNN",
    "text": "Creating the CNN\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n:::\nRefactoring parts of your neural networks like this makes it much less likely you’ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\n\nsimple_cnn = nn.Sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,16),           #2x2\n    conv(16,10, act=False), #1x1\n    nn.Flatten(),\n)\n\nYou now if you’ve got a mac, you can use a device called Apple. If you’ve got an Apple Silicon Mac, you’ve got a device called NPS, which is going to use the know Macs GPU here or if you’ve got it in video, you can use Cuda, which will use your and video creators, you know, ten times or more, possibly much more faster than a mac. So you definitely want to use in video if you can, but if you just running it on a laptop or whatever, you can use amps. So basically you’re going to know what device to use. Do we want to use Coda or amps? You can check if you can check torchlight back backhands that is available to see if you’re running on a mac with amps you can check\ntorch.backends.mps.is_available()\n.It is available to see if you’ve got an in video GPU, in which case you’ve got CUDA. And if you’ve got neither, of course you’ll have to use the CPU to do computation. So I’ve created a little function to device which takes a tensor or a dictionary or a list of tensors or whatever, and a device to move it to. And it just goes through and moves everything onto that device. Or if it’s a dictionary, a dictionary of things, value has moved onto that device that has a handy little function. And so we can create a custom collate function which calls the PyTorch default collation function and then puts those tensors onto our device. And so with that, we’ve now got enough to run train this neural net on the GPU. You we created this get_dls function in the last lesson. So we’re going to use that passing in the datasets that we just created and our default collation function, we’re going to create our optimizer using our CNN’s parameters and then we call fit(), now fit for a member that we also created in our last lesson and it’s done. Yeah, it’s really, it’s a lot less code than last time I ran it. I don’t know if I’ve changed something weird. Let’s say now. There we go.\nI must have done something weird. Okay, so I then what I did then was I reduced the learning rate by a factor of four and ran it again.\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\n\nx_imgs = x_train.view(-1,1,28,28)\nxv_imgs = x_valid.view(-1,1,28,28)\ntrain_ds,valid_ds = Dataset(x_imgs, y_train),Dataset(xv_imgs, y_valid)\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n:::\n\nfrom torch import optim\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 2.2963203880310057 0.10639999992847443\n1 0.35034381108283996 0.8865999997138977\n2 0.2884100193500519 0.9101999996185303\n3 0.3795499787807465 0.8783999994277955\n4 0.14102031185626984 0.957599999332428\n\n\nAnd eventually, yeah, I got to a fairly similar accuracy to what we did on our multi on our MLP. So yeah, we’ve got a convolutional network working. I think that’s pretty encouraging and it’s nice that to train it, we didn’t have to write much code right? We were able to use code that we had already built. We were able to use the dataset class that we made to get_dls function that we made and the fit function that we made. And you know, because those things are written in a fairly general way, they work just as well for conv net as they did for an MLP. Nothing had to change. So that was nice. Notice we had to take the model and put it on the device as well. So that will go through and basically put all of the tensors that are in that model onto the MPS or CUDA device, if appropriate. So if we’ve got a batch size of 64 and as we do one channel channel, height width. So normally this is referred to as NCW, so n generally when you see N in a in a paper or whatever, in this way it’s referring to the batch size and being the number that’s the mnemonic, the number of items in the batch C is the number of channels height by width, and W. TensorFlow doesn’t use that TensorFlow uses an NHWC So we generally call these that channels last since channels are at the and this one we normally call channels first. Now of course it’s not actually channels first, it’s actually channel second, but we ignore the batch bit\nin some models, particularly some more modern models. It turns out the channels last is faster. So PyTorch has recently added support for channels last. And so you’ll see that being used more and more as well.\nYeah, we go. All right. So a couple of comments and questions from our chat. The first is Sam Watkins pointing out that where we’ve actually had a bit of a win here, which is that the number of parameters now CNN is is pretty small by comparison. So the of in the MLP version, the number of parameters is equal to basically the size of this matrix. Right. So M times and NH, plus the number in this, which will be an H times ten. And you know something that at some point we probably should do is actually create something that allows us to automatically calculate the number of parameters. And I’m ignoring the bias there. Of course, let’s say what would be a good way to do that? Maybe np.product. Yeah. Okay. So what we could do, what we could do is just calculate this automatically by doing a little list comprehension here.\nSo there’s the number of parameters across all of the different layers. So both bias and weights. And then we could, I guess, just well, we could just use well, let’s use pytouch so we could turn that into a tensor and sum it up. So that’s the number in our MLP. And then the number in our simple CNN. So that’s pretty cool. We’ve gone down from 40000 to 5000 and got about the same number there. Oh, thank you, Jonathan. Jonathan’s reminding me that there’s a better way than np.product shape, which is just to say I dot number of elements num of element. Very nice.\nNow, one person asked a very good question, which is I thought convolutional neural networks can handle any sized image and actually know this convolutional network cannot handle any sized image. This convolutional neural network only handles images that once they go through these tried to convs end up with a one by one because otherwise you can’t dot Flatten it and end up with 16 by ten. So we will learn how to create comv nets that can handle any sized input. But there’s nothing particularly about a net that necessitates that it has to be any sized input that it can handle. Okay.\nSo just let’s briefly finish this section off by talking about this. Yeah. That this well, particularly on to talk about the idea of receptive field, consider this. Yeah. One input Channel four output. Channel three by three kernal. Right. So that’s just here just to show you what we’re doing here conv one. Well actually so simple. See it in simple CNN. This is the model we created. Remember, is like a sequential model containing sequential models because that’s how our current function worked. So simple. CNN zero is our first layer. It contains both conv and a relu so I simple CNN zero zero is the actual conv. So if we grab that whole conf one, it’s a four by one, by three by three. So number of outputs, number of input channels and height by weight for that kernal and then it’s got its bias as well. So that’s how we could kind of deconstruct what’s going on with our weight matrices or our parameters inside a convolution. Now I’m going to switch over to Excel.\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\n0 0.10183437039852142 0.9675999994277954\n1 0.10473719484806061 0.9674999994277954\n2 0.10602079322338104 0.9673999995231628\n3 0.09614700574874878 0.9709999995231628\n4 0.09545752574205399 0.9696999994277954\n\n\n\nUnderstanding Convolution Arithmetic\nIn an input of size 64x1x28x28 the axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order (aka “channels-last”). Channels-last is faster for many models, so recently it’s become more common to see this as an option in PyTorch too.\nWe have 1 input channel, 4 output channels, and a 3×3 kernel.\n\nsimple_cnn[0][0]\n\nConv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n\n\nconv1 = simple_cnn[0][0]\nconv1.weight.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\n\nconv1.bias.shape\n\ntorch.Size([4])\n\n\nThe receptive field is the area of an image that is involved in the calculation of a layer. conv-example.xlsx shows the calculation of two stride-2 convolutional layers using an MNIST digit. Here’s what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.\n\nThe blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Click trace precedents again:\n\nIn this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the receptive field\nThe deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer."
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 10/index.html#color-images",
    "href": "posts/Writing Stable Diffusion from Scratch 10/index.html#color-images",
    "title": "Writing Stable Diffusion from Scratch 10",
    "section": "Color Images",
    "text": "Color Images\nA colour picture is a rank-3 tensor:\n\nfrom torchvision.io import read_image"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "",
    "text": "All credits goes to fast.ai All mistakes are mine.  You should know and practice following after this blog post : 1- Making a flexible learner  2- Start to build our framework\nThis is where we take a halt and we say, okay, let’s build up a framework that we can use to rapidly try things and understand when things are working and when things aren’t working. So we’re going to start creating a learner. So what is learner? It’s basically the idea is this learner is going to be something that we build which will allow us to try like anything that we can imagine very quickly and we will build that on top of that learner, things that will allow us to introspect what’s going on inside a model will allow us to do multi process CUDA to go fast. It will allow us to add things like data augmentation. It will allow us to try a wide variety of architectures quickly and so forth. So that’s going to be the idea and of course we’re going to create it from scratch.\nAnd so let’s start with fashion MNIST before and let’s create a dataloader class, which is going to look a bit like what we had before, where we’re just going to pass in. This is just couldn’t be simpler, right? We’re just going to pass in two dataloaders and store them away and I’m going to create a class method from dataset dictionary. And what that’s going to do is it’s going to call dataloader on each of the dataset dictionary items with our batch side batch size and instantiate our class. So if you haven’t seen classmethod before, it’s what allows us to say dataloaders dot something. In order to construct this, we’re going to put this in in it just as well, but we’ll be building more complex data loaded things later. So I thought we might start by kind of getting the basic structure right. So this is all pretty much the same as what we’ve had before, not doing anything on the device here because that’s we know that didn’t really work. Okay. Oh, this is an old thing. You don’t need to cuda anymore. So we’re going to use to_device. So here’s a, here’s an example of a very simple learner that fits on one screen, and this is basically going to replace our fit function. So a learner is going to be something that is going to train or learn a particular model using a particular set of data load as a particular loss function. But some particular learning rate and some particular optimizer or some particular optimization function. Now normally, you know, most people would often kind of store each of these away separately by writing like self.model equals model, blah, blah, blah. Right. And as I think we’ve talked about before, that’s, you know, that kind of huge amounts of boilerplate. It just it’s more stuff that you can get wrong and it’s more stuff to mean that you have to read to understand the code and yeah, don’t like that kind of repetition. So instead we just call fc.store_attr() to do that all in one line. Okay. So that basic idea with a class is to think what’s the information it’s going to need. So you pass that, all the constructor store away and then our fit function is going be got the basic stuff that we have for keeping track of accuracy. So this is only work for stuff that’s a classification where we can use accuracy, put the model on our device, create the optimizer store, how many epochs we’re going through. Then for each epoch we’ll call the one epoch function and the one epoch function. We’re going to either do train or evaluation. So we pass in true if we’re training and false if we’re evaluating. And they’re basically almost the same.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=2}\n:::"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#learner",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#learner",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "Learner",
    "text": "Learner\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n\n\n\n\n\n\n\n\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48. Subsequent calls will reuse this data.\n\n\n\n\n\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\nbs = 1024\ntds = dsd.with_transform(transformi)\n\n\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\n(torch.Size([1024, 784]), tensor([5, 4, 9, 4, 3, 0, 6, 5, 7, 6]))\n\n\nWe basically set the model to training or not. We then decide whether to use the validation set or the training set based on whether we’re training. And then we go through each batch in the dataloader and call one batch. And one batch is then the thing which is going to put our batch onto the device, call our model, call our loss function. And then if we’re training, then do our backward step, our optimizer step in zero gradient, and then finally calculate our metrics or stats. And so here’s where we calculate our metrics. So that’s basically what we have there. So let’s go back to using an MLP we call FIT. And the way it goes. This is an era here pointed out by Kevin thank you self.model.to that to one thing I guess we could try now is we think that maybe we can use more than one process so let’s try that. again and let’s check htop. Oh so fast. I didn’t even see. Hey guys, you can see all 4 cpu’s being used once. Bang, it’s done. Okay, so that’s pretty great. Let’s see how fast it looks here. Bump, bump. All right. Lovely. Okay, so that’s a good sign.\nWe’ve got a learner that can fit things, but it’s not very flexible. It’s not going to help us, for example, with our auto encoder, because there’s no way of like just, you know, changing which things are used for predicting with or for coupling with. We can’t use it for anything except things that involve accuracy with a binary classification. Sorry, a right, sorry. Yeah. A multiclass classification. It’s not flexible at all, but it’s a start. And so I wanted to basically this all on one screen so you can see what the basic learner looks like. All right, so how do we do things other than multiclass accuracy?\nI decided to create a metric class and basically a metric class. It’s a something where we are going to define subclasses of it that calculate particular metrics. So for example, here I’ve got a subclass of a metric called accuracy. So if you haven’t done subclasses before, you can basically think of this as saying, Please copy and paste all the code from here into here for me. But the bit that says def calc, replace it with this version. So in fact this would be identical to copying and pasting this whole thing, typing accuracy here and replacing the definition calc with that. That’s what is happening here when we do subclasses. So it’s basically and pasting all that code in there. For us, it’s actually more powerful than that. There’s more we can do with that. But in this case, this is all that’s happening with this subclasses and that’s this is called setting all these that that’s\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.xb,self.yb = to_device(self.batch)\n        self.preds = self.model(self.xb)\n        self.loss = self.loss_func(self.preds, self.yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n        with torch.no_grad(): self.calc_stats()\n\n    def calc_stats(self):\n        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n        self.accs.append(acc)\n        n = len(self.xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\n    def one_epoch(self, train):\n        self.model.training = train\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(dl): self.one_batch()\n        n = sum(self.ns)\n        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n    \n    def fit(self, n_epochs):\n        self.accs,self.losses,self.ns = [],[],[]\n        self.model.to(def_device)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        for self.epoch in range(n_epochs):\n            self.one_epoch(True)\n            with torch.no_grad(): self.one_epoch(False)\n\n\nm,nh = 28*28,50\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2)\nlearn.fit(1)\n\n0 True 1.17530390625 0.5987\n0 False 1.1203112723214286 0.6135857142857143"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#basic-callbacks-learner",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#basic-callbacks-learner",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "Basic Callbacks Learner",
    "text": "Basic Callbacks Learner\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=13}\nclass CancelFitException(Exception): pass\nclass CancelBatchException(Exception): pass\nclass CancelEpochException(Exception): pass\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=14}\nclass Callback(): order = 0\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=15}\ndef run_cbs(cbs, method_nm, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, method_nm, None)\n        if method is not None: method(learn)\n:::\n\nclass CompletionCB(Callback):\n    def before_fit(self, learn): self.count = 0\n    def after_batch(self, learn): self.count += 1\n    def after_fit(self, learn): print(f'Completed {self.count} batches')\n\n\ncbs = [CompletionCB()]\nrun_cbs(cbs, 'before_fit')\nrun_cbs(cbs, 'after_batch')\nrun_cbs(cbs, 'after_fit')\n\nCompleted 1 batches\n\n\n\nclass Learner():\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.preds = self.model(self.batch[0])\n        self.loss = self.loss_func(self.preds, self.batch[1])\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n\n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        try:\n            self.callback('before_epoch')\n            for self.iter,self.batch in enumerate(self.dl):\n                try:\n                    self.callback('before_batch')\n                    self.one_batch()\n                    self.callback('after_batch')\n                except CancelBatchException: pass\n            self.callback('after_epoch')\n        except CancelEpochException: pass\n    \n    def fit(self, n_epochs):\n        self.n_epochs = n_epochs\n        self.epochs = range(n_epochs)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        try:\n            self.callback('before_fit')\n            for self.epoch in self.epochs:\n                self.one_epoch(True)\n                self.one_epoch(False)\n            self.callback('after_fit')\n        except CancelFitException: pass\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n\n\nm,nh = 28*28,50\ndef get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nmodel = get_model()\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[CompletionCB()])\nlearn.fit(1)\n\nCompleted 64 batches\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=21}\nclass SingleBatchCB(Callback):\n    order = 1\n    def after_batch(self, learn): raise CancelFitException()\n:::\n\nlearn = Learner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=[SingleBatchCB(), CompletionCB()])\nlearn.fit(1)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#metrics",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#metrics",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "Metrics",
    "text": "Metrics\nso the accuracy metric is here and then this is kind of our really basic metric which is we’re going to use for just for loss. And so what happens is we’re going to let’s, for example, create an accuracy metric object. We’re basically to add in mini batches of data, right? So for example, here’s a mini batches of inputs and predictions. Here’s another mini batch of inputs and predictions, and then we’re going to call dot value and it will calculate the accuracy. Now the value is a neat little thing. It doesn’t require parentheses after it because it’s called a property. And so a property is something that just calculates automatically without putting having to put parentheses. That’s what a property is, what property getter anyway. And so they look like this, you give it a name. And so we are going to be each time we call ADD, we are going to be storing that input and that target and also the number of items in the mini batch. Optionally for now, that’s just always going to be one. And you can see here that we then call dot Calc, which is going to call the accuracy_calc. So just see how they equal and then we’re going to append to the list of values that calculation. And we’re also going to append to the list of ends in this case, just one. And so then to calculate the value, we just do that. So that’s all that’s happening for accuracy. And then we can do for loss. We can just use metric directly because metric will just calculate the average of whatever it’s passed. So we can say, oh, add the number 0.6, so the target’s optional and saying this is a mini batch of size And then add the value 0.9 with a mini batch size of two and then get the value. And as you can see, that’s exactly the same as the weighted average of 0.6 and point nine with weights of 32 and two. So created a metric class. And so that’s something that we can use to create any metric we like just by overriding calc. Or we could create totally things from scratch as long as they have an add and a value. Okay,\nso we’re now going to change our learner and what we’re going to do is we’re going to keep the same basic structure. So it’s going to be fit. It’s going to go through each epoch. It’s going to call one epoch passing and true and false. As for training and validation, one epoch is going to go through each batch in the data oader and call one batch. One batch is going to the prediction get loss and if it’s training, it’s going to do that backward step and zero grade. But there’s a few other things going on. So let’s take a look. Well, actually, let’s just look at it in use first. So when we use it, we’re going to be creating a learner with the model dataloaders, last function learning rate and some callbacks, which we’ll learn about in a moment and we call fit.\n\nclass Metric:\n    def __init__(self): self.reset()\n    def reset(self): self.vals,self.ns = [],[]\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals)*ns).sum()/ns.sum()\n    def calc(self, inps, targs): return inps\n\n\nclass Accuracy(Metric):\n    def calc(self, inps, targs): return (inps==targs).float().mean()\n\n\nacc = Accuracy()\nacc.add(tensor([0, 1, 2, 0, 1, 2]), tensor([0, 1, 1, 2, 1, 0]))\nacc.add(tensor([1, 1, 2, 0, 1]), tensor([0, 1, 1, 2, 1]))\nacc.value\n\ntensor(0.45)\n\n\n\nloss = Metric()\nloss.add(0.6, n=32)\nloss.add(0.9, n=2)\nloss.value, round((0.6*32+0.9*2)/(32+2), 2)\n\n(tensor(0.62), 0.62)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#some-callbacks",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#some-callbacks",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "Some callbacks",
    "text": "Some callbacks\npip install torcheval\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=27}\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n:::\n\nmetric = MulticlassAccuracy()\nmetric.update(tensor([0, 2, 1, 3]), tensor([0, 1, 2, 3]))\nmetric.compute()\n\ntensor(0.50)\n\n\n\nmetric.reset()\nmetric.compute()\n\ntensor(nan)\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=30}\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    return x.detach().cpu()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=31}\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=32}\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)\n:::\n\nmodel = get_model()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics])\nlearn.fit(1)\n\n{'accuracy': '0.602', 'loss': '1.183', 'epoch': 0, 'train': 'train'}\n{'accuracy': '0.700', 'loss': '0.847', 'epoch': 0, 'train': 'eval'}"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#flexible-learner",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#flexible-learner",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "Flexible learner",
    "text": "Flexible learner\n\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @contextmanager\n    def cb_ctx(self, nm):\n        try:\n            self.callback(f'before_{nm}')\n            yield\n            self.callback(f'after_{nm}')\n        except globals()[f'Cancel{nm.title()}Exception']: pass\n        finally: self.callback(f'cleanup_{nm}')\n                \n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        with self.cb_ctx('epoch'):\n            for self.iter,self.batch in enumerate(self.dl):\n                with self.cb_ctx('batch'):\n                    self.predict()\n                    self.get_loss()\n                    if self.training:\n                        self.backward()\n                        self.step()\n                        self.zero_grad()\n    \n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n            with self.cb_ctx('fit'):\n                for self.epoch in self.epochs:\n                    if train: self.one_epoch(True)\n                    if valid: torch.no_grad()(self.one_epoch)(False)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=35}\nclass TrainCB(Callback):\n    def __init__(self, n_inp=1): self.n_inp = n_inp\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n    def backward(self, learn): learn.loss.backward()\n    def step(self, learn): learn.opt.step()\n    def zero_grad(self, learn): learn.opt.zero_grad()\n:::\nNB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.\nAnd it’s going to do our thing. And look, we’re going to have charts and stuff. All right? So the basic idea is going to look very similar. So we’re going to call fit. So when we construct it, we’re going to be passing in exactly the same things as before. But we’ve got one extra thing callbacks, which we’ll see in a moment, store the attributes as before, and we’re going to be doing some stuff with the callbacks. So when we call Fit for this number of epochs, we’re going to store away how many epochs we’re going to do. We’re also going to store away the actual range that we’re going to loop through as soft epochs. So here’s that looping through so epochs, we’re going to create the optimizer using the optimizer function and the parameters, and then we’re going to call _fit. Now, I want to know if is underscore fit, why didn’t we just copy and paste so this into here Why why do this? It’s because we’ve created a special decorator with callbacks. What does that do?\nSo it’s up here with callbacks. With callbacks is a class. It’s going to just store one thing, which is the name. In this case, the name is fit. And what it’s going to do is Now, this is a decorator, right? So when we call it remember, decorators get past a function. So it’s going to get past whole function. And that’s going to be called if that’s a __call. Remember, is what happens when a class is treated, an object is treated as if it’s a function. So it’s going to get past this function. So this function is underscore fit. And so what we want to do is we want to return a different function. It’s going to, cause, call the function that we were asked to call using the arguments and key documents we were asked to use. But before it calls that function, it’s going to call a special method called callback passing in the string before in this case before underscore, fit. After it’s completed, it’s going to call that method called callback and passing the string after underscore fit, and it’s going to wrap the whole thing in a try except. BLOCK And it’s going to be looking for an exception called Cancel Fit Exception. And if it gets one, it’s not going to complain. So let me explain what’s going on with all of those things.\nLet’s look at example of a callback. Let’s change this DeviceCB So for example, here is a callback called DeviceCB Device callback and before FIT will be called automatically before that underscore fit method is called and it’s going to put the model onto our device CUDA or MPS if we have one. Otherwise it would just be on GPU. So what’s going to happen here? So it’s going to call, we’re going to call fit. It’s going to go through these lines of code. It’s going to call, underscore, a fit. Underscore fit is not this function underscore fit is this function with F is this function. So it’s going to call our learn act callback passing in before underscore, fit and callback is defined here. What’s callback going to do? It’s going to be past the string before underscore it. It’s going to then go through each of our callbacks sorted based on their order. And you can see here callback can have an order and it’s going to look at that callback and try to get an attribute called before underscore fit and it will find one. And so then it’s going to call that method. Now, if that method doesn’t exist, it doesn’t appear at all, then getattr will return this instead. Identity is a function just here. This is an identity function. All it does is whatever arguments it gets passed, it returns them. If it’s not past any arguments, it just returns. So there’s a lot of python going on here and that is why we did that.\nFoundations lesson. And so for people who haven’t done a lot of this python, there’s going to be a lot of stuff to experiment with and learn about. And so do ask on the forums if any of these bits get confusing. But the best way to learn about these things is to open up this Jupyter Notebook and try and create really simple versions of things. Right? So for example, let’s try identity. Identity. How exactly does identity work? I can call it, and it gets nothing. I can call it with one spec one I could call it with. I get back, I got it with a yes, fully tested this, call it with a one and get a one. And how is it doing that exactly. So remember we can add a break point and this is be a great time to really test your debugging skills. Okay so remember in our debugger we can hit H to find out what the commands are, but you really should do a tutorial on the debugger if you’re not familiar with it. And then we can step through each one. So I can now print args and it’s actually a trick which I like is that args is actually a command funnily enough, which I’ll just tell you the arguments to any function, regardless of what they’re called, which is kind of nice. And so then we can step through by pressing and, and after this we can check like, okay, what is X now and what is args now, right? So remember it really experiment with these things. So anyway, we’re going to talk about this a lot more in the next lesson. But before that, if you’re not familiar with try except blocks, you know, spend some time practicing them. If you’re not familiar with decorators, well, we’ve seen them before, So go back and look at them again really carefully. If you’re not familiar with the debugger practice with that, if you haven’t spent much time with get at, try remind yourself about that. So try to get yourself really familiar and comfortable as much as possible with the pieces, because if you’re not comfortable with the pieces and the way put the pieces together is going to be confusing. There’s actually something in education in kind of the theory of education called cognitive load theory. And The theory of cognitive basically cognitive load theory says if you’re trying to learn something, but your cognitive load is really high because of all lots of other things going on at the same time, you’re not going to learn it. So it’s going to be hard for you to learn this framework that we’re building. If you have too much cognitive load of like, what the hell’s a decorator or what they getattr or what to sort of do or what’s possible, you know, all these things. Now, I actually spent quite a bit of time trying to make this as simple as possible, but but also as flexible as it needs to be for the rest of the course. And this is this is this is as simple as I could get it. So these are kind of things that you actually have to learn. But in doing so, you’re going to be able to write some really, you know, powerful and general code yourself. So hopefully you’ll find this a really valuable and mind expanding exercise in in bringing high level software engineering skills to your data science work. Okay. So with that, this looks like a good place to leave it and look forward to seeing you next time.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=36}\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])\n:::\n\nmodel = get_model()\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.596\n      1.167\n      0\n      train\n    \n    \n      0.729\n      0.794\n      0\n      eval"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#updated-versions-since-the-lesson",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#updated-versions-since-the-lesson",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "Updated versions since the lesson",
    "text": "Updated versions since the lesson\nAfter the lesson we noticed that contextlib.context_manager has a surprising “feature” which doesn’t let us raise an exception before the yield. Therefore we’ve replaced the context manager with a decorator in this updated version of Learner. We have also added a few more callbacks in one_epoch().\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=39}\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=40}\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n:::\n\nmodel = get_model()\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.616\n      1.168\n      0\n      train\n    \n    \n      0.719\n      0.789\n      0\n      eval"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#trainlearner-and-momentumlearner",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#trainlearner-and-momentumlearner",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "TrainLearner and MomentumLearner",
    "text": "TrainLearner and MomentumLearner\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=42}\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=43}\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n:::\n\n# NB: No TrainCB\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.676\n      0.936\n      0\n      train\n    \n    \n      0.791\n      0.585\n      0\n      eval"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 11/index.html#lrfindercb",
    "href": "posts/Writing Stable Diffusion from Scratch 11/index.html#lrfindercb",
    "title": "Writing Stable Diffusion from Scratch 11",
    "section": "LRFinderCB",
    "text": "LRFinderCB\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss < self.min: self.min = loss\n        if loss > self.min*3: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n\n\nlrfind = LRFinderCB()\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs)\nlearn.fit(1)\nplt.plot(lrfind.lrs, lrfind.losses)\nplt.xscale('log')\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=47}\nfrom torch.optim.lr_scheduler import ExponentialLR\n:::\nExponentialLR\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=48}\nclass LRFinderCB(Callback):\n    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.sched = ExponentialLR(learn.opt, self.gamma)\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss < self.min: self.min = loss\n        if math.isnan(loss) or (loss > self.min*self.max_mult):\n            raise CancelFitException()\n        self.sched.step()\n\n    def cleanup_fit(self, learn):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n:::\n\ncbs = [DeviceCB()]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-5, cbs=cbs)\nlearn.fit(3, cbs=LRFinderCB())\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=50}\n@fc.patch\ndef lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))\n:::\nlr_find was added in lesson 18. It’s just a shorter way of using LRFinderCB.\n\nMomentumLearner(get_model(), dls, F.cross_entropy, cbs=cbs).lr_find()"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "",
    "text": "All credits goes to fast.ai  All mistakes are mine. You should know and practice following after this blog post :  1- Way better Learner  2- Learner finder\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#learner",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#learner",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "Learner",
    "text": "Learner\nHi there, and welcome to lesson 16, where we are working on building our first flexible deep learning framework, the learner. And I’ve got some very good news, which is that I have thought of a way of doing it a little bit more gradually and simply actually than last time. So that should that should make things a bit easier. So we’re going to take it a bit more step by step. So we’re working in the 9 learner notebook today and we’ve seen already this, this basic callbacks learner And so the idea is that this learner, all right, this is why we saw no, we’ve seen so far this learner, which wasn’t flexible at all, but it had all the basic pieces, which is we’ve got a fit method. We had coding that we can only calculate accuracy and average loss. We’re had coding, we’re putting things on a default device, hard coding, a single learning rate. But the basic idea is here we go through each epoch and call one epoch to to train or evaluate depending on this flag, and then we loop through each batch and the dataloader and one batch is going to grab the x and y parts of the batch, call the model, call the loss function. And if we’re training, do the backward pass and then print out well, calculate the statistics for our accuracy and then at the end of the epoch, print that out. So it wasn’t very flexible, but it did do something. So that’s good.\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n\n\n\n\n\n\n\n\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48. Subsequent calls will reuse this data.\n\n\n\n\n\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\n\nbs = 1024\ntds = dsd.with_transform(transformi)\n\n\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\n(torch.Size([1024, 784]), tensor([5, 4, 9, 4, 3, 0, 6, 5, 7, 6]))\n\n\nSo what we’re going to do now is we’re going to do as an intermediate step, we’re going to look at a but I’m calling a basic callback, learner, And it actually has nearly all the functionality of the full thing, the way we’re going to after we look at this basic callback learner, we’re then going to after creating some callbacks and metrics, we’re going to look at something called the Flexible learner So let’s go step by step. So the basic callbacks,learner looks very similar to the previous. learner It, it’s got a fit function which is going to go through each epoch calling one epoch with training on and then training off, and then one epoch. We’ll go through each batch and call one batch and one batch will call the model the loss function. And if we’re training, it will do the backward step. So that’s all pretty similar. But there’s a few more things going on here. For example, if we have a look at fit, you’ll see that after creating the optimizer, so called self.opt_func. So opt func here defaults to SGD. So we instantiate an object passing in our models parameters and the requested learning rate. And then before we start looping through one epoch at a time. Now we’ve set epochs here. We first of all call self.callback and passing in before fit. Now what does that do? Self.callback is here and it takes a method name. So in this case it’s before fit and it calls a function called run callbacks. It passes in a list of our callbacks and the method name in this case before fit. So Run callbacks is something that’s going to go for each callback and it’s going to sort them in order of their order attribute. And so there’s a base class of callbacks which has an order of zero. So our callbacks are all going to have the same order of zero in which you ask otherwise. So here’s an example of a callback. So before we look at how callbacks work, let’s just let’s just run a callback so we can create a ridiculously simple callback code completion callback, which before we start fitting a new model it will set its count attribute to zero , after each batch it will increment that after completing fitting process, it will print out how many batches we’ve done. So before we even train the model, we could just run manually before fit ,after_batch and ,after_fit using this run_cbs. And you can see it’s ended up saying completed one batches. So what did that do? So it went through each of the cbs. In this list there’s only one. So it’s going to look at the one CB and it’s going to try to use getattr to find an attribute with this name, which is before_fit. So if we try that manually. So this is the kind of thing I want you to do if you find anything difficult to understand is do it all manually. So create a callback, set it cbs zero, just like you’re doing in a loop, right? And then find out what happens if we call this and pass in this and you’ll say it’s returned a method. And then what happens to that method? It gets called. So let’s try calling it there. Yeah. So that’s what happened when we call the before_fit, which doesn’t do anything very interesting. But if we then call after_batch and then we call after_fit, there it is. Right. So yeah, make sure you don’t just run code willy nilly, but understand it by experimenting with it. And I don’t always experiment with it myself in these classes. Often I’m leaving that to you, but sometimes I’m trying to give you a sense of how I would experiment with code if I was learning it. So then having done that, I would then go ahead and delete those cells. But you can say I’m using this interactive notebook environment to to explore and learn and understand. And so now we’ve got and if I haven’t created a simple example of something to make it really easy to understand, you should do that right? Then just use what I’ve already created or what somebody else has already created. So we’ve now got something that works totally independently. We can see how it works. This is what a callback does. So a callback is something which we’ll look at a class, a callback as a class where you can define one or more of before, after fit, before, after batch, and before after epoch. So it’s going to go through and run all the callbacks that have a before fit method before we start fitting. Then it’ll go through each epoch and call one epoch with training and one epoch with evaluation. And then when that’s all done, it will call after it callbacks and one epoch well before it starts on enumerating through the batches. It will call before epoch, and when it’s done, it will call after_epoch. The other thing you’ll notice is that there’s a try, except immediately before every before method and immediately after every after method there’s a try and there’s an except and each one has a different thing to look for. Cancel fit exception, cancel a epoch exception and cancel that exception. So here’s the bit which goes through each batch calls before batch processes, the batch calls after batch, and if there’s an exception that says type cancel batch exception, it gets ignored. So what’s that for? So the reason we have this is that any of our callbacks code call could raise any one of these three exceptions to say, I don’t want to do this batch, please. So maybe I’ll look an example of that in a moment so we can now train with this. So let’s call create a little get model function that creates a sequential model with just some linear layers. And then we’ll probably didn’t run all those and then we’ll call fit. And it’s not telling us anything interesting because the only callback we added was the completion callback. That’s fine. It’s, it’s training, it’s doing something and we now have a train model, just didn’t print out any metrics. And I think that we don’t have any callbacks for that. But that’s the basic idea. So we could create a maybe we could call it a single batch callback, which after batch, after a single batch, it raises a cancel fit the exception. So that’s a pretty I mean, I suppose that could be kind of useful actually, if you want to just run one batch tomorrow to make sure it works. So we could try that. Yeah. So now we’re going to add to our list of callbacks. The single batch callback. Let’s try it. And in fact, you know, we probably want this. Let’s just have a think here. Oh, that’s fine.\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.xb,self.yb = to_device(self.batch)\n        self.preds = self.model(self.xb)\n        self.loss = self.loss_func(self.preds, self.yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n        with torch.no_grad(): self.calc_stats()\n\n    def calc_stats(self):\n        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n        self.accs.append(acc)\n        n = len(self.xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\n    def one_epoch(self, train):\n        self.model.training = train\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(dl): self.one_batch()\n        n = sum(self.ns)\n        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n    \n    def fit(self, n_epochs):\n        self.accs,self.losses,self.ns = [],[],[]\n        self.model.to(def_device)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        for self.epoch in range(n_epochs):\n            self.one_epoch(True)\n            with torch.no_grad(): self.one_epoch(False)\n\n\nm,nh = 28*28,50\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2)\nlearn.fit(1)\n\n0 True 1.17530390625 0.5987\n0 False 1.1203112723214286 0.6135857142857143"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#basic-callbacks-learner",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#basic-callbacks-learner",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "Basic Callbacks Learner",
    "text": "Basic Callbacks Learner\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass CancelFitException(Exception): pass\nclass CancelBatchException(Exception): pass\nclass CancelEpochException(Exception): pass\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Callback(): order = 0\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef run_cbs(cbs, method_nm, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, method_nm, None)\n        if method is not None: method(learn)\n:::\n\nclass CompletionCB(Callback):\n    def before_fit(self, learn): self.count = 0\n    def after_batch(self, learn): self.count += 1\n    def after_fit(self, learn): print(f'Completed {self.count} batches')\n\n\ncbs = [CompletionCB()]\nrun_cbs(cbs, 'before_fit')\nrun_cbs(cbs, 'after_batch')\nrun_cbs(cbs, 'after_fit')\n\nCompleted 1 batches\n\n\n\nclass Learner():\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.preds = self.model(self.batch[0])\n        self.loss = self.loss_func(self.preds, self.batch[1])\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n\n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        try:\n            self.callback('before_epoch')\n            for self.iter,self.batch in enumerate(self.dl):\n                try:\n                    self.callback('before_batch')\n                    self.one_batch()\n                    self.callback('after_batch')\n                except CancelBatchException: pass\n            self.callback('after_epoch')\n        except CancelEpochException: pass\n    \n    def fit(self, n_epochs):\n        self.n_epochs = n_epochs\n        self.epochs = range(n_epochs)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        try:\n            self.callback('before_fit')\n            for self.epoch in self.epochs:\n                self.one_epoch(True)\n                self.one_epoch(False)\n            self.callback('after_fit')\n        except CancelFitException: pass\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n\n\nm,nh = 28*28,50\ndef get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\n\nmodel = get_model()\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[CompletionCB()])\nlearn.fit(1)\n\nCompleted 64 batches\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass SingleBatchCB(Callback):\n    order = 1\n    def after_batch(self, learn): raise CancelFitException()\n:::\n\nlearn = Learner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=[SingleBatchCB(), CompletionCB()])\nlearn.fit(1)\n\nLet’s run it. There we go. So it ran and nothing happened. And the reason Nothing happened is because this canceled before this ran so we could make this run second, by setting its order to be higher. And we could say just order equals one because the default order is zero. And we saw in order of the order attribute, it didn’t work either. What did I do wrong that cancels fit? Oh, that’s interesting. It’s the after_fit here. Yes, actually, let’s just cancel a epock exception. Yeah, we go that way to run the final fit. Yeah. Yeah. So it did one batch for the the training and one batch for the evaluation. So it’s a total of two batches. So remember, callbacks are not a special magic part of like the Python language or anything. It’s just a name we use to refer to these functions or classes or quotables. More accurately, that we that we pass into something that will then call back to that callable at particular times. And I think these are kind of interesting kinds of callbacks because these callbacks have multiple methods in them. So as each method of callback is each class with all those methods of callback, I don’t know. I tend to think of the class with all the methods in as a single callback. I’m not sure if we have great nomenclature for this. All right. So let’s actually try to get this doing something more interesting by not modifying the learner at all, but just by adding callbacks because that’s the great hope of callbacks. Right?\nSo it would be very nice if it told us the accuracy and the loss. So to do that, it would be great to have a class that can keep track of a metric. So I’ve created here a metric class, and maybe before we look at it, we’ll see how it works. You could create, for example, an accuracy metric by defining the calculation necessary to calculate the accuracy metric, which is the mean of how often the inputs equal the targets. And the idea is you could then create an accuracy metric object. You could add a batch of inputs and targets and add another batch of inputs and targets and get the value. And then you would get the point four or five accuracy or another way you could do it would be just to create a metric which simply text gets the weighted average, for example, of your loss. So you could add point six as the loss with a batch size of 32.9 as a loss and a batch size of two. And then that’s going to give us a weighted average loss of 0.62, which is equal to this weighted average calculation. So that’s like one way we could kind of make it easy to calculate metrics. So here’s the class. Basically, we’re going to keep track of all of the actual values that we’re averaging and the number in each many batch. And so when you add a mini batch, we call calculate which, for example, for accuracy, remember, this is going to override the parent classes, calculate that does the calculation here, and then we’ll add that to our list of values. We will add to our list of batch sizes. The current batch size. And then when you calculate the value, we will calculate the weighted sum. That’s right. The weighted mean ,weighted average. Now notice that here value, I didn’t have to put parentheses after it, and that’s because it’s a property. I think we’ve seen this before. So just to remind you, property just means you don’t have to put parentheses after it to get it to get the calculation to happen. All right. So just let me know if anybody’s got any questions up to here. Of course. So we now need some way to use this metric in a callback to actually print out first thing I’m going to do, though, is going to create one more one useful metric first,"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#metrics",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#metrics",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "Metrics",
    "text": "Metrics\n\nclass Metric:\n    def __init__(self): self.reset()\n    def reset(self): self.vals,self.ns = [],[]\n    def add(self, inp, targ=None, n=1):\n        self.last = self.calc(inp, targ)\n        self.vals.append(self.last)\n        self.ns.append(n)\n    @property\n    def value(self):\n        ns = tensor(self.ns)\n        return (tensor(self.vals)*ns).sum()/ns.sum()\n    def calc(self, inps, targs): return inps\n\n\nclass Accuracy(Metric):\n    def calc(self, inps, targs): return (inps==targs).float().mean()\n\n\nacc = Accuracy()\nacc.add(tensor([0, 1, 2, 0, 1, 2]), tensor([0, 1, 1, 2, 1, 0]))\nacc.add(tensor([1, 1, 2, 0, 1]), tensor([0, 1, 1, 2, 1]))\nacc.value\n\ntensor(0.45)\n\n\n\nloss = Metric()\nloss.add(0.6, n=32)\nloss.add(0.9, n=2)\nloss.value, round((0.6*32+0.9*2)/(32+2), 2)\n\n(tensor(0.62), 0.62)"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#some-callbacks",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#some-callbacks",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "Some callbacks",
    "text": "Some callbacks\nso now let’s do our metrics. Now, of course we couldn’t use metrics until we built them by hand. The good news is we don’t have to write every single metric now by hand because they already exist in a fairly new project called torcheval, which is an official PyTorch project. And so torcheval is something that gives us actually I came across it after I tried it biometric class, but it actually looks pretty similar to the one that I built earlier. So you can install it with PIP. I’m not sure if it’s on Conda yet, but it probably will be soon. By the time you see the video, I think it’s pure python anyway, so it doesn’t matter how you install it. And yeah, it has a pretty similar, pretty similar approach where you call dot update any call dot compute. So the slightly different names, but they’re basically super similar to the thing that we just built. But there’s a nice good list of metrics to pick from. So because we’ve already built our own, now that means we’re allowed to use this so we can import the multiclass accuracy metric. And the mean metric. And just to show you, they look very, very similar. If we call multiclass accuracy and we can pass in a mini batch of inputs and targets and compute. And that all works nicely now.In fact, it’s exactly the same as what I wrote. We both added this thing called Reset, which basically, well, resets it and so obviously we’re going to be wanting to do that properly at the start of each epoch. And so if you reset it and then try to compute, you’ll get Nan because you can’t get accuracy accuracy’s meaningless when you don’t have any data yet. Okay, So let’s create a metrics callback so we can print out our metrics. I’ve got some ideas to improve this, which maybe I’ll do this week, but here’s a basic working version slightly hacky, but it’s not too bad. So generally speaking, one thing I noticed actually is I don’t know if this is considered a bug, but a lot of the metrics didn’t seem to work correctly and torcheval When I had tensors that were on the paper and had requires grad, so I created a little to_cpu function, which I think is very useful, and that’s just going to detach the So Detach takes the tensor and removes all the gradient history, the computation history used to calculate a gradient and puts it on the CPU. Better do the same for dictionaries of tenses, lists of tenses and tuples of tenses. So our metrics callback basically here’s how we’re going to use it. So let’s run it. So here we are creating a metrics callback object and saying we want to create a metric called accuracy. That’s what’s going to print out. And this is the metric that object we’re going to use to calculate accuracy. And so then we just passed that in as one of our callbacks.\nAnd so you can see what it’s going to do is it’s going to print out the epoch number, whether it’s training or evaluating. So training set or validation set, and it’ll print out our metrics and our current status. Actually, we can simplify that. We don’t need to print those bits because it’s all in the dictionary. Now let’s do that. Yeah, okay. So let’s take a look at how this works.\nSo we are going to be creating for the callback. We’re going to be passing in the names and object metric objects for the metrics to track and print. So here it is. Here **metric. So since start, stop before and as a little shortcut, I decided that it might be nice if you didn’t want to write accuracy equals you could just remove that and run it. And if you do that then it will give it a name and address. Use the same name as the class. And so that’s why you can either pass in *ms will be a tuple. Well, I mean, it’s going to be pulled out, so it’s just passing a list of positional arguments. It should be turned into a tuple or you can pass in name arguments, sort of be turned into a dictionary. If you pass in positional arguments, then I’m going to turn them into named arguments in the dictionary by just grabbing the name from their type. So that’s where this comes from. That’s all that’s going on here. Just a little shortcut, bit of convenience. I was told that, okay. And this is yeah, this is a bit I think I can simplify a little bit, but I’m just adding manually an additional metric, which is I’m going to call the loss, and that’s just going to be , the weighted average of the losses. So before we start fitting, we we’re going to actually tell the learner that we are the metrics callback. And so you’ll see later where we’re going to actually use this before each epoch. We will reset all of our metrics. After each epoch, we will create a dictionary of the keys and values, which are the actual strings that we want to print out and we will call log, which for now we’re just print them. And then after each batch, this is the key thing. We’re going to actually grab the input and target. We’re going to put them on the CPU and then we’re going to go through each of our metrics and call that update. So remember the update in the metric is the thing that actually says, here’s a batch of data, right? So we’re passing in the batch of data, which is the predictions and the targets, and then we do the same thing for our special loss metric passing in the actual loss and the size of any batch. And so that’s how we’re able to get this. Yeah, this actual running on the NVIDIA here and showing our metrics and obviously there’s lots of room to improve how this is displayed. But all the information we needed here and it’s just a case of changing that function. Okay. So that’s our kind of like intermediate complexity learner. We can make it more sophisticated, but it’s still exactly so it’s still going to fit in a single screen of so this is kind of my goal here was to keep everything in a single screen of code. This first bit is exactly the same as before, but you’ll see that the one epoch can fit and batch has gone from, let’s say, about what it was before. It’s gone from quite a lot of code, all this to much less code. And the trick to doing that is I decided to use a\ncontext manager. We’re going to learn more about context managers in the next notebook. But basically originally last week I was saying, I’m going to do this as a decorator, but I realized a context managers better. Basically what we’re going to do is we’re going to call our before and after callbacks in a try, except BLOCK And to say that we want to use the callbacks in the try and except BLOCK we’re going to use a with statement. So in Python, a statement says everything in that block, call our context manager before and after it. Now, there’s a few ways to do that, but one really easy one is using this context manager, decorator and everything up and to the up tool. The yield statement is called before your code where it says yield it, then calls your code and then everything after the yield is called after your code. So in this case it’s going to be try self dot, callback before name where name is fit and then it will call for self dot, epoch, etc. because that’s where the yield is and then it’ll call self-talk callback after fit. Except okay. And now we need to grab the cancel fit exception. So all of the variables that you have in Python all live inside a special dictionary called globals. So this dictionary contains all of your variables, so I can just look up in that dictionary. The variable called cancel Fit with a capital F exception. So this is except cancel fit exception. So this is exactly the same then as this code. Except the nice thing is now I only have to write it once rather than at least three times. Now I’m really going to want more of them. So, you know, I tend to think it’s worth yeah,\nI tend to think it’s worth refactoring a code When you have duplicate code, particularly here, we had the same code three times, so that’s going to be more of a maintenance headache. We probably don’t want to add callbacks to more things later. So by putting it into a context manager just once, I think we’re going to reduce our maintenance burden by redo, because I’ve had a similar thing in Fastai for some years now and it’s been quite convenient. So that’s what this context managers about. Yeah, don’t think that the code is exactly the same. So we create our optimizer and then with our callback context Manager for fit go through each epoch one epoch set it to training or non-trading mode based on the argument we pass in perhaps a training or validation set based on the argument we in. And then using the context manager for epoch go through each batch in the dataloader and then for each batch in the data loader using the batch context.\nNow this is where something gets quite interesting. We call predict get lost and if we’re training backward step and zero grad but previously we actually called self dot model etc. self dot lost function, etc. Williams saying that the notebooks in the repos a bit behind that you pulled it before class because I committed just before class it it’s always worth pulling when class starts. So we go through each batch and code before batch do the batch. That’s our slow version. Right? What are we doing? Oh, yes. I’m going to be over here. Okay. I’m back where we are. Yes. So previously we were calling. Yeah, calling, calling. The model, calling the lost function, calling , backward, up, step up to zero grad. But now we are calling instead. Self-Talk predicts, self dot get lost, self dot backward. And how on earth is that working? Because they are not defined here at all. And so the reason I’ve decided to do this is it gives us a lot of flexibility.\npip install torcheval\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n:::\n\nmetric = MulticlassAccuracy()\nmetric.update(tensor([0, 2, 1, 3]), tensor([0, 1, 2, 3]))\nmetric.compute()\n\ntensor(0.50)\n\n\n\nmetric.reset()\nmetric.compute()\n\ntensor(nan)\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    return x.detach().cpu()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass MetricsCB(Callback):\n    def __init__(self, *ms, **metrics):\n        for o in ms: metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n\n    def _log(self, d): print(d)\n    def before_fit(self, learn): learn.metrics = self\n    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = 'train' if learn.model.training else 'eval'\n        self._log(log)\n\n    def after_batch(self, learn):\n        x,y,*_ = to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n        self.loss.update(to_cpu(learn.loss), weight=len(x))\n:::\na very simple one, just two lines of code called the device_callback. And that is something which is going to allow us to use CUDA or or the Apple GPU or whatever without the complications we had before of, you know, how do we have multiple processes in our data loader and also use our device and not have everything fall over. So the way we could do it is we could say before it put the model onto the default device and before each batch is run, put that batch onto the device because look what happened in in the\nthis is really, really important in the learner. Absolutely. Everything is put inside self dot, which means it’s all modifiable. So we go for self dot iteration, number, comma, self self dot, the batch itself. And you’ve been writing the data letter and then we call one batch, but before that we call the callback so we can modify this. Now, how does the callback get access to the learner? Well, what actually happens is we go through each of our callbacks and put set an attribute called learn equal to the learner. And so that means in the callback itself, we can say self.learn, don’t learn that model. And actually we could make this a bit better. I think so make it like maybe you don’t want to use a default device. So this is where I would be inclined to add a constructor and set device and we could default it to the default device. Of course, and then we could use that instead and that would give us a bit more flexibility. So if you wanted to try it on some different device, then you could. I think that might be a slight improvement. Okay, so there’s a callback we can use to put things on cuda to, and we could check that it works by. Just quickly going back to our outline here and maybe if a single batch CB and replace it with device_cb Yeah, it still works. So that’s a good sign.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): fc.store_attr()\n    def before_fit(self, learn):\n        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)\n:::\n\nmodel = get_model()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics])\nlearn.fit(1)\n\n{'accuracy': '0.602', 'loss': '1.183', 'epoch': 0, 'train': 'train'}\n{'accuracy': '0.700', 'loss': '0.847', 'epoch': 0, 'train': 'eval'}"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#flexible-learner",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#flexible-learner",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "Flexible learner",
    "text": "Flexible learner\nAnd so the reason I’ve decided to do this is it gives us a lot of flexibility. We can now actually create our own way of doing predict, get lost, backward step and zero grad in different situations. We’re going to see some of those situations. So what happens if we call self dot predict and it doesn’t exist? Well, it doesn’t necessarily cause an error. What actually happens is it calls a special magic method in Python called getattr. That’s been seen before. And what I’m doing here is I’m saying, okay, well if it’s one of these special five things, don’t raise an attribute error, which is this is the default thing. It does. But instead create a callback. Oh, actually, I should say call self dot, callback, passing in that name. So it’s actually going to call self dot, callback, quote, predict and self-talk callback is exactly the same as before. And so what that means now is to make this work exactly the same as it did before. I need a callback which does these five things, and here it is, I’m going to call it train callback. So here are the five things predict get lost backwards step and zero grade. So they’re here. Predict get loss backward step and zero grad. Okay. So they’re almost exactly the same as what they looked like in our intermediate learner. Except now I just need to have self dot learn in front of everything because we remember this is a callback. It’s not the learner. And so for a callback, the callback can access the learner using self dot learned So self dot learned operates prets self dot learned model passing and self dot learned what batch and just the independent variables did over the loss calls the last function backward step zero grid. So that’s at this point. This isn’t doing anything that I wasn’t doing before. But the nice thing is now, if you want to use huggingface, accelerate or you want something that works on huggingface, data styles, dictionary things or whatever, you can actually change exactly how it behaves by just call it passing by creating a callback to training. And if you want everything except one thing to be the same, you can inherit from train. CB So this is I’ve, I’ve not tried this before, I haven’t seen this done anywhere else. So it’s a bit of an experiment. So I wanted to hear how you go with it. And then finally I thought it’d be nice to have a progress bar. So let’s create a progress callback. And the progress bar is going to show on a current loss and going to put create a plot of it. So I’m going to use a project that we created called Fast Progress, mainly created by the Wonderful Solver. And basically fast progress is, yeah, a very nice way to create a very flexible progress bars. So let me show you what it looks like first. So let’s get the model and train and as you can see it actually in real time updates the graph and everything, they go, Yeah, that’s pretty cool. So that’s the that’s the progress bar, the metrics callback, the device callback and the training callback all in action. So before we fit, we actually have to set self dot learn epochs. Now that might look a little bit weird, but self dot learned on epochs is the thing that we loop through for self dot epoch in so we can change that. So it’s not just a normal range, but instead it is a progress bar around the range. We can then check. Remember I told you that the learner is going to have the metrics attribute applied. We can then say, Oh, if the learner has a metrics attribute, then let’s replace the underscore log method there with ours and our one instead will write to the progress bar. Now this is pretty simple. It looks very similar to before, but we could easily replace this, for example, with something that creates an optimal table, which is another thing. Fast progress, so or other stuff like that. So you can see we can modify.\n\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @contextmanager\n    def cb_ctx(self, nm):\n        try:\n            self.callback(f'before_{nm}')\n            yield\n            self.callback(f'after_{nm}')\n        except globals()[f'Cancel{nm.title()}Exception']: pass\n        finally: self.callback(f'cleanup_{nm}')\n                \n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        with self.cb_ctx('epoch'):\n            for self.iter,self.batch in enumerate(self.dl):\n                with self.cb_ctx('batch'):\n                    self.predict()\n                    self.get_loss()\n                    if self.training:\n                        self.backward()\n                        self.step()\n                        self.zero_grad()\n    \n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n            with self.cb_ctx('fit'):\n                for self.epoch in self.epochs:\n                    if train: self.one_epoch(True)\n                    if valid: torch.no_grad()(self.one_epoch)(False)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass TrainCB(Callback):\n    def __init__(self, n_inp=1): self.n_inp = n_inp\n    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n    def backward(self, learn): learn.loss.backward()\n    def step(self, learn): learn.opt.step()\n    def zero_grad(self, learn): learn.opt.zero_grad()\n:::\nNB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.\nAnd it’s going to do our thing. And look, we’re going to have charts and stuff. All right? So the basic idea is going to look very similar. So we’re going to call fit. So when we construct it, we’re going to be passing in exactly the same things as before. But we’ve got one extra thing callbacks, which we’ll see in a moment, store the attributes as before, and we’re going to be doing some stuff with the callbacks. So when we call Fit for this number of epochs, we’re going to store away how many epochs we’re going to do. We’re also going to store away the actual range that we’re going to loop through as soft epochs. So here’s that looping through so epochs, we’re going to create the optimizer using the optimizer function and the parameters, and then we’re going to call _fit. Now, I want to know if is underscore fit, why didn’t we just copy and paste so this into here Why why do this? It’s because we’ve created a special decorator with callbacks. What does that do?\nSo it’s up here with callbacks. With callbacks is a class. It’s going to just store one thing, which is the name. In this case, the name is fit. And what it’s going to do is Now, this is a decorator, right? So when we call it remember, decorators get past a function. So it’s going to get past whole function. And that’s going to be called if that’s a __call. Remember, is what happens when a class is treated, an object is treated as if it’s a function. So it’s going to get past this function. So this function is underscore fit. And so what we want to do is we want to return a different function. It’s going to, cause, call the function that we were asked to call using the arguments and key documents we were asked to use. But before it calls that function, it’s going to call a special method called callback passing in the string before in this case before underscore, fit. After it’s completed, it’s going to call that method called callback and passing the string after underscore fit, and it’s going to wrap the whole thing in a try except. BLOCK And it’s going to be looking for an exception called Cancel Fit Exception. And if it gets one, it’s not going to complain. So let me explain what’s going on with all of those things.\nLet’s look at example of a callback. Let’s change this DeviceCB So for example, here is a callback called DeviceCB Device callback and before FIT will be called automatically before that underscore fit method is called and it’s going to put the model onto our device CUDA or MPS if we have one. Otherwise it would just be on GPU. So what’s going to happen here? So it’s going to call, we’re going to call fit. It’s going to go through these lines of code. It’s going to call, underscore, a fit. Underscore fit is not this function underscore fit is this function with F is this function. So it’s going to call our learn act callback passing in before underscore, fit and callback is defined here. What’s callback going to do? It’s going to be past the string before underscore it. It’s going to then go through each of our callbacks sorted based on their order. And you can see here callback can have an order and it’s going to look at that callback and try to get an attribute called before underscore fit and it will find one. And so then it’s going to call that method. Now, if that method doesn’t exist, it doesn’t appear at all, then getattr will return this instead. Identity is a function just here. This is an identity function. All it does is whatever arguments it gets passed, it returns them. If it’s not past any arguments, it just returns. So there’s a lot of python going on here and that is why we did that.\nFoundations lesson. And so for people who haven’t done a lot of this python, there’s going to be a lot of stuff to experiment with and learn about. And so do ask on the forums if any of these bits get confusing. But the best way to learn about these things is to open up this Jupyter Notebook and try and create really simple versions of things. Right? So for example, let’s try identity. Identity. How exactly does identity work? I can call it, and it gets nothing. I can call it with one spec one I could call it with. I get back, I got it with a yes, fully tested this, call it with a one and get a one. And how is it doing that exactly. So remember we can add a break point and this is be a great time to really test your debugging skills. Okay so remember in our debugger we can hit H to find out what the commands are, but you really should do a tutorial on the debugger if you’re not familiar with it. And then we can step through each one. So I can now print args and it’s actually a trick which I like is that args is actually a command funnily enough, which I’ll just tell you the arguments to any function, regardless of what they’re called, which is kind of nice. And so then we can step through by pressing and, and after this we can check like, okay, what is X now and what is args now, right? So remember it really experiment with these things. So anyway, we’re going to talk about this a lot more in the next lesson. But before that, if you’re not familiar with try except blocks, you know, spend some time practicing them. If you’re not familiar with decorators, well, we’ve seen them before, So go back and look at them again really carefully. If you’re not familiar with the debugger practice with that, if you haven’t spent much time with get at, try remind yourself about that. So try to get yourself really familiar and comfortable as much as possible with the pieces, because if you’re not comfortable with the pieces and the way put the pieces together is going to be confusing. There’s actually something in education in kind of the theory of education called cognitive load theory. And The theory of cognitive basically cognitive load theory says if you’re trying to learn something, but your cognitive load is really high because of all lots of other things going on at the same time, you’re not going to learn it. So it’s going to be hard for you to learn this framework that we’re building. If you have too much cognitive load of like, what the hell’s a decorator or what they getattr or what to sort of do or what’s possible, you know, all these things. Now, I actually spent quite a bit of time trying to make this as simple as possible, but but also as flexible as it needs to be for the rest of the course. And this is this is this is as simple as I could get it. So these are kind of things that you actually have to learn. But in doing so, you’re going to be able to write some really, you know, powerful and general code yourself. So hopefully you’ll find this a really valuable and mind expanding exercise in in bringing high level software engineering skills to your data science work. Okay. So with that, this looks like a good place to leave it and look forward to seeing you next time.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])\n:::\n\nmodel = get_model()\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.596\n      1.167\n      0\n      train\n    \n    \n      0.729\n      0.794\n      0\n      eval"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#updated-versions-since-the-lesson",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#updated-versions-since-the-lesson",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "Updated versions since the lesson",
    "text": "Updated versions since the lesson\nHi there, and welcome to lesson 16, where we are working on building our first flexible deep learning framework, the learner. And I’ve got some very good news, which is that I have thought of a way of doing it a little bit more gradually and simply actually than last time. So that should that should make things a bit easier. So we’re going to take it a bit more step by step. So we’re working in the 9 learner notebook today and we’ve seen already this, this basic callbacks learner And so the idea is that this learner, all right, this is why we saw no, we’ve seen so far this learner, which wasn’t flexible at all, but it had all the basic pieces, which is we’ve got a fit method. We had coding that we can only calculate accuracy and average loss. We’re had coding, we’re putting things on a default device, hard coding, a single learning rate. But the basic idea is here we go through each epoch and call one epoch to to train or evaluate depending on this flag, and then we loop through each batch and the dataloader and one batch is going to grab the x and y parts of the batch, call the model, call the loss function. And if we’re training, do the backward pass and then print out well, calculate the statistics for our accuracy and then at the end of the epoch, print that out. So it wasn’t very flexible, but it did do something. So that’s good. So what we’re going to do now is we’re going to do as an intermediate step, we’re going to look at a but I’m calling a basic callback, learner, And it actually has nearly all the functionality of the full thing, the way we’re going to after we look at this basic callback learner, we’re then going to after creating some callbacks and metrics, we’re going to look at something called the Flexible learner So let’s go step by step. So the basic callbacks,learner looks very similar to the previous. learner It, it’s got a fit function which is going to go through each epoch calling one epoch with training on and then training off, and then one epoch. We’ll go through each batch and call one batch and one batch will call the model the loss function. And if we’re training, it will do the backward step. So that’s all pretty similar. But there’s a few more things going on here. For example, if we have a look at fit, you’ll see that after creating the optimizer, so called self.opt_func. So opt func here defaults to SGD. So we instantiate an object passing in our models parameters and the requested learning rate. And then before we start looping through one epoch at a time. Now we’ve set epochs here. We first of all call self.callback and passing in before fit. Now what does that do? Self.callback is here and it takes a method name. So in this case it’s before fit and it calls a function called run callbacks. It passes in a list of our callbacks and the method name in this case before fit. So Run callbacks is something that’s going to go for each callback and it’s going to sort them in order of their order attribute. And so there’s a base class of callbacks which has an order of zero. So our callbacks are all going to have the same order of zero in which you ask otherwise. So here’s an example of a callback. So before we look at how callbacks work, let’s just let’s just run a callback so we can create a ridiculously simple callback code completion callback, which before we start fitting a new\nAfter the lesson we noticed that contextlib.context_manager has a surprising “feature” which doesn’t let us raise an exception before the yield. Therefore we’ve replaced the context manager with a decorator in this updated version of Learner. We have also added a few more callbacks in one_epoch().\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass Learner():\n    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n        cbs = fc.L(cbs)\n        fc.store_attr()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            self.backward()\n            self.callback('after_backward')\n            self.step()\n            self.callback('after_step')\n            self.zero_grad()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs: self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None: lr = self.lr\n            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n            self._fit(train, valid)\n        finally:\n            for cb in cbs: self.cbs.remove(cb)\n\n    def __getattr__(self, name):\n        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n        raise AttributeError(name)\n\n    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n    \n    @property\n    def training(self): return self.model.training\n:::\n\nmodel = get_model()\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.616\n      1.168\n      0\n      train\n    \n    \n      0.719\n      0.789\n      0\n      eval"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#trainlearner-and-momentumlearner",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#trainlearner-and-momentumlearner",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "TrainLearner and MomentumLearner",
    "text": "TrainLearner and MomentumLearner\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n:::\n\n# NB: No TrainCB\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.676\n      0.936\n      0\n      train\n    \n    \n      0.791\n      0.585\n      0\n      eval"
  },
  {
    "objectID": "posts/Writing Stable Diffusion from Scratch 12/index.html#lrfindercb",
    "href": "posts/Writing Stable Diffusion from Scratch 12/index.html#lrfindercb",
    "title": "Writing Stable Diffusion from Scratch 12",
    "section": "LRFinderCB",
    "text": "LRFinderCB\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss < self.min: self.min = loss\n        if loss > self.min*3: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult\n\n\nlrfind = LRFinderCB()\ncbs = [DeviceCB(), lrfind]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs)\nlearn.fit(1)\nplt.plot(lrfind.lrs, lrfind.losses)\nplt.xscale('log')\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom torch.optim.lr_scheduler import ExponentialLR\n:::\nExponentialLR\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass LRFinderCB(Callback):\n    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n    \n    def before_fit(self, learn):\n        self.sched = ExponentialLR(learn.opt, self.gamma)\n        self.lrs,self.losses = [],[]\n        self.min = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        loss = to_cpu(learn.loss)\n        self.losses.append(loss)\n        if loss < self.min: self.min = loss\n        if math.isnan(loss) or (loss > self.min*self.max_mult):\n            raise CancelFitException()\n        self.sched.step()\n\n    def cleanup_fit(self, learn):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n:::\n\ncbs = [DeviceCB()]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-5, cbs=cbs)\nlearn.fit(3, cbs=LRFinderCB())\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n@fc.patch\ndef lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))\n:::\nlr_find was added in lesson 18. It’s just a shorter way of using LRFinderCB.\n\nMomentumLearner(get_model(), dls, F.cross_entropy, cbs=cbs).lr_find()"
  }
]