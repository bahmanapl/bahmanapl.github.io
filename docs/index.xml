<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Bahman Sadeghi</title>
<link>https://bahmansadeghi.com/index.html</link>
<atom:link href="https://bahmansadeghi.com/index.xml" rel="self" type="application/rss+xml"/>
<description>This is Bahman Sadeghi's personal Blog.</description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Mon, 03 Apr 2023 19:30:00 GMT</lastBuildDate>
<item>
  <title>Writing Stable Diffusion from Scratch 11</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index.html</link>
  <description><![CDATA[ 



<p>All credits goes to fast.ai All mistakes are mine. <br> You should know and practice following after this blog post :<br> 1- Making a flexible learner <br> 2- Start to build our framework</p>
<div class="cell" data-outputid="00df2c41-036a-4965-b148-cd98664eff2b" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uqq git<span class="op" style="color: #5E5E5E;">+</span>https:<span class="op" style="color: #5E5E5E;">//</span>github.com<span class="op" style="color: #5E5E5E;">/</span>fastai<span class="op" style="color: #5E5E5E;">/</span>course22p2</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Preparing metadata (setup.py) ... done
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 9.2 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.4/158.4 kB 9.3 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.9/934.9 kB 7.5 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 2.3 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 47.8 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 9.3 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 7.6 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 10.7 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 4.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 11.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 34.3 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.9/87.9 kB 11.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 12.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 26.1 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 14.4 MB/s eta 0:00:00
  Building wheel for miniai (setup.py) ... done</code></pre>
</div>
</div>
<p>This is where we take a halt and we say, okay, let’s build up a framework that we can use to rapidly try things and understand when things are working and when things aren’t working. So we’re going to start creating a learner. So what is learner? It’s basically the idea is this learner is going to be something that we build which will allow us to try like anything that we can imagine very quickly and we will build that on top of that learner, things that will allow us to introspect what’s going on inside a model will allow us to do multi process CUDA to go fast. It will allow us to add things like data augmentation. It will allow us to try a wide variety of architectures quickly and so forth. So that’s going to be the idea and of course we’re going to create it from scratch.</p>
<p>And so let’s start with fashion MNIST before and let’s create a dataloader class, which is going to look a bit like what we had before, where we’re just going to pass in. This is just couldn’t be simpler, right? We’re just going to pass in two dataloaders and store them away and I’m going to create a class method from dataset dictionary. And what that’s going to do is it’s going to call dataloader on each of the dataset dictionary items with our batch side batch size and instantiate our class. So if you haven’t seen classmethod before, it’s what allows us to say dataloaders dot something. In order to construct this, we’re going to put this in in it just as well, but we’ll be building more complex data loaded things later. So I thought we might start by kind of getting the basic structure right. So this is all pretty much the same as what we’ve had before, not doing anything on the device here because that’s we know that didn’t really work. Okay. Oh, this is an old thing. You don’t need to cuda anymore. So we’re going to use to_device. So here’s a, here’s an example of a very simple learner that fits on one screen, and this is basically going to replace our fit function. So a learner is going to be something that is going to train or learn a particular model using a particular set of data load as a particular loss function. But some particular learning rate and some particular optimizer or some particular optimization function. Now normally, you know, most people would often kind of store each of these away separately by writing like self.model equals model, blah, blah, blah. Right. And as I think we’ve talked about before, that’s, you know, that kind of huge amounts of boilerplate. It just it’s more stuff that you can get wrong and it’s more stuff to mean that you have to read to understand the code and yeah, don’t like that kind of repetition. So instead we just call fc.store_attr() to do that all in one line. Okay. So that basic idea with a class is to think what’s the information it’s going to need. So you pass that, all the constructor store away and then our fit function is going be got the basic stuff that we have for keeping track of accuracy. So this is only work for stuff that’s a classification where we can use accuracy, put the model on our device, create the optimizer store, how many epochs we’re going through. Then for each epoch we’ll call the one epoch function and the one epoch function. We’re going to either do train or evaluation. So we pass in true if we’re training and false if we’re evaluating. And they’re basically almost the same.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=2}</p>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">import</span> math,torch,matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">import</span> fastcore.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">as</span> fc</span>
<span id="cb3-3"><span class="im" style="color: #00769E;">from</span> collections.abc <span class="im" style="color: #00769E;">import</span> Mapping</span>
<span id="cb3-4"><span class="im" style="color: #00769E;">from</span> operator <span class="im" style="color: #00769E;">import</span> attrgetter</span>
<span id="cb3-5"><span class="im" style="color: #00769E;">from</span> functools <span class="im" style="color: #00769E;">import</span> partial</span>
<span id="cb3-6"><span class="im" style="color: #00769E;">from</span> copy <span class="im" style="color: #00769E;">import</span> copy</span>
<span id="cb3-7"></span>
<span id="cb3-8"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> optim</span>
<span id="cb3-9"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="im" style="color: #00769E;">from</span> miniai.conv <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb3-12"></span>
<span id="cb3-13"><span class="im" style="color: #00769E;">from</span> fastprogress <span class="im" style="color: #00769E;">import</span> progress_bar,master_bar</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">import</span> matplotlib <span class="im" style="color: #00769E;">as</span> mpl</span>
<span id="cb4-2"><span class="im" style="color: #00769E;">import</span> torchvision.transforms.functional <span class="im" style="color: #00769E;">as</span> TF</span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> contextlib <span class="im" style="color: #00769E;">import</span> contextmanager</span>
<span id="cb4-4"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn,tensor</span>
<span id="cb4-5"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset,load_dataset_builder</span>
<span id="cb4-6"><span class="im" style="color: #00769E;">from</span> miniai.datasets <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb4-7"><span class="im" style="color: #00769E;">from</span> miniai.conv <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb4-8"><span class="im" style="color: #00769E;">import</span> logging</span>
<span id="cb4-9"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb5-2">torch.manual_seed(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb5-3">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">logging.disable(logging.WARNING)</span></code></pre></div>
</div>
<section id="learner" class="level2">
<h2 class="anchored" data-anchor-id="learner">Learner</h2>
<div class="cell" data-outputid="1e22eda1-1dd0-42b0-8396-b656bb01b759" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">x,y <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'image'</span>,<span class="st" style="color: #20794D;">'label'</span></span>
<span id="cb7-2">name <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"fashion_mnist"</span></span>
<span id="cb7-3">dsd <span class="op" style="color: #5E5E5E;">=</span> load_dataset(name)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"87ea1f4ad25c4ec3b1e2458d3e171e4d"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"0a896e8a82ae4a58b51685aec49b85ed"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"82fb1a2d44ce4a898e93925d71f31520"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading and preparing dataset fashion_mnist/fashion_mnist to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"06779bd845574d069e32c884f450f31a"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"85bb2bb7d3444b558ed39ad5e438e59b"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"c421687ed7824bfe9bf357d808474106"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"e98eb3486bb74e888b6f3585f207cc98"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"f0f89d98287c4c079dde3036968c7a8e"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"dc480f90b1254d62bb0c477957887d81"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"b830314d176045f29f23e793d273b6a8"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"04a346300c2a4d38bea6bc4d5ea5b77c"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48. Subsequent calls will reuse this data.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"e4389b9d1261482da526b19b7e9eb153"}
</script>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="at" style="color: #657422;">@inplace</span></span>
<span id="cb10-2"><span class="kw" style="color: #003B4F;">def</span> transformi(b): b[x] <span class="op" style="color: #5E5E5E;">=</span> [torch.flatten(TF.to_tensor(o)) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> b[x]]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">bs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1024</span></span>
<span id="cb11-2">tds <span class="op" style="color: #5E5E5E;">=</span> dsd.with_transform(transformi)</span></code></pre></div>
</div>
<div class="cell" data-outputid="a6a79404-888b-40b9-d51a-89e03eda3a97" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">dls <span class="op" style="color: #5E5E5E;">=</span> DataLoaders.from_dd(tds, bs, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb12-2">dt <span class="op" style="color: #5E5E5E;">=</span> dls.train</span>
<span id="cb12-3">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dt))</span>
<span id="cb12-4">xb.shape,yb[:<span class="dv" style="color: #AD0000;">10</span>]</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(torch.Size([1024, 784]), tensor([5, 4, 9, 4, 3, 0, 6, 5, 7, 6]))</code></pre>
</div>
</div>
<p>We basically set the model to training or not. We then decide whether to use the validation set or the training set based on whether we’re training. And then we go through each batch in the dataloader and call one batch. And one batch is then the thing which is going to put our batch onto the device, call our model, call our loss function. And then if we’re training, then do our backward step, our optimizer step in zero gradient, and then finally calculate our metrics or stats. And so here’s where we calculate our metrics. So that’s basically what we have there. So let’s go back to using an MLP we call FIT. And the way it goes. This is an era here pointed out by Kevin thank you self.model.to that to one thing I guess we could try now is we think that maybe we can use more than one process so let’s try that. again and let’s check htop. Oh so fast. I didn’t even see. Hey guys, you can see all 4 cpu’s being used once. Bang, it’s done. Okay, so that’s pretty great. Let’s see how fast it looks here. Bump, bump. All right. Lovely. Okay, so that’s a good sign.</p>
<p>We’ve got a learner that can fit things, but it’s not very flexible. It’s not going to help us, for example, with our auto encoder, because there’s no way of like just, you know, changing which things are used for predicting with or for coupling with. We can’t use it for anything except things that involve accuracy with a binary classification. Sorry, a right, sorry. Yeah. A multiclass classification. It’s not flexible at all, but it’s a start. And so I wanted to basically this all on one screen so you can see what the basic learner looks like. All right, so how do we do things other than multiclass accuracy?</p>
<p>I decided to create a metric class and basically a metric class. It’s a something where we are going to define subclasses of it that calculate particular metrics. So for example, here I’ve got a subclass of a metric called accuracy. So if you haven’t done subclasses before, you can basically think of this as saying, Please copy and paste all the code from here into here for me. But the bit that says def calc, replace it with this version. So in fact this would be identical to copying and pasting this whole thing, typing accuracy here and replacing the definition calc with that. That’s what is happening here when we do subclasses. So it’s basically and pasting all that code in there. For us, it’s actually more powerful than that. There’s more we can do with that. But in this case, this is all that’s happening with this subclasses and that’s this is called setting all these that that’s</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;">class</span> Learner:</span>
<span id="cb15-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model, dls, loss_func, lr, opt_func<span class="op" style="color: #5E5E5E;">=</span>optim.SGD): fc.store_attr()</span>
<span id="cb15-3"></span>
<span id="cb15-4">    <span class="kw" style="color: #003B4F;">def</span> one_batch(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb15-5">        <span class="va" style="color: #111111;">self</span>.xb,<span class="va" style="color: #111111;">self</span>.yb <span class="op" style="color: #5E5E5E;">=</span> to_device(<span class="va" style="color: #111111;">self</span>.batch)</span>
<span id="cb15-6">        <span class="va" style="color: #111111;">self</span>.preds <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.model(<span class="va" style="color: #111111;">self</span>.xb)</span>
<span id="cb15-7">        <span class="va" style="color: #111111;">self</span>.loss <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.loss_func(<span class="va" style="color: #111111;">self</span>.preds, <span class="va" style="color: #111111;">self</span>.yb)</span>
<span id="cb15-8">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.model.training:</span>
<span id="cb15-9">            <span class="va" style="color: #111111;">self</span>.loss.backward()</span>
<span id="cb15-10">            <span class="va" style="color: #111111;">self</span>.opt.step()</span>
<span id="cb15-11">            <span class="va" style="color: #111111;">self</span>.opt.zero_grad()</span>
<span id="cb15-12">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad(): <span class="va" style="color: #111111;">self</span>.calc_stats()</span>
<span id="cb15-13"></span>
<span id="cb15-14">    <span class="kw" style="color: #003B4F;">def</span> calc_stats(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb15-15">        acc <span class="op" style="color: #5E5E5E;">=</span> (<span class="va" style="color: #111111;">self</span>.preds.argmax(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)<span class="op" style="color: #5E5E5E;">==</span><span class="va" style="color: #111111;">self</span>.yb).<span class="bu" style="color: null;">float</span>().<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb15-16">        <span class="va" style="color: #111111;">self</span>.accs.append(acc)</span>
<span id="cb15-17">        n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.xb)</span>
<span id="cb15-18">        <span class="va" style="color: #111111;">self</span>.losses.append(<span class="va" style="color: #111111;">self</span>.loss<span class="op" style="color: #5E5E5E;">*</span>n)</span>
<span id="cb15-19">        <span class="va" style="color: #111111;">self</span>.ns.append(n)</span>
<span id="cb15-20"></span>
<span id="cb15-21">    <span class="kw" style="color: #003B4F;">def</span> one_epoch(<span class="va" style="color: #111111;">self</span>, train):</span>
<span id="cb15-22">        <span class="va" style="color: #111111;">self</span>.model.training <span class="op" style="color: #5E5E5E;">=</span> train</span>
<span id="cb15-23">        dl <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.dls.train <span class="cf" style="color: #003B4F;">if</span> train <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.dls.valid</span>
<span id="cb15-24">        <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.num,<span class="va" style="color: #111111;">self</span>.batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(dl): <span class="va" style="color: #111111;">self</span>.one_batch()</span>
<span id="cb15-25">        n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">sum</span>(<span class="va" style="color: #111111;">self</span>.ns)</span>
<span id="cb15-26">        <span class="bu" style="color: null;">print</span>(<span class="va" style="color: #111111;">self</span>.epoch, <span class="va" style="color: #111111;">self</span>.model.training, <span class="bu" style="color: null;">sum</span>(<span class="va" style="color: #111111;">self</span>.losses).item()<span class="op" style="color: #5E5E5E;">/</span>n, <span class="bu" style="color: null;">sum</span>(<span class="va" style="color: #111111;">self</span>.accs).item()<span class="op" style="color: #5E5E5E;">/</span>n)</span>
<span id="cb15-27">    </span>
<span id="cb15-28">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, n_epochs):</span>
<span id="cb15-29">        <span class="va" style="color: #111111;">self</span>.accs,<span class="va" style="color: #111111;">self</span>.losses,<span class="va" style="color: #111111;">self</span>.ns <span class="op" style="color: #5E5E5E;">=</span> [],[],[]</span>
<span id="cb15-30">        <span class="va" style="color: #111111;">self</span>.model.to(def_device)</span>
<span id="cb15-31">        <span class="va" style="color: #111111;">self</span>.opt <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.opt_func(<span class="va" style="color: #111111;">self</span>.model.parameters(), <span class="va" style="color: #111111;">self</span>.lr)</span>
<span id="cb15-32">        <span class="va" style="color: #111111;">self</span>.n_epochs <span class="op" style="color: #5E5E5E;">=</span> n_epochs</span>
<span id="cb15-33">        <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n_epochs):</span>
<span id="cb15-34">            <span class="va" style="color: #111111;">self</span>.one_epoch(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb15-35">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad(): <span class="va" style="color: #111111;">self</span>.one_epoch(<span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">m,nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">28</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">50</span></span>
<span id="cb16-2">model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</div>
<div class="cell" data-outputid="bf502c1f-0d34-46fd-e2f4-b7176a153479" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(model, dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>)</span>
<span id="cb17-2">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 True 1.17530390625 0.5987
0 False 1.1203112723214286 0.6135857142857143</code></pre>
</div>
</div>
</section>
<section id="basic-callbacks-learner" class="level2">
<h2 class="anchored" data-anchor-id="basic-callbacks-learner">Basic Callbacks Learner</h2>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=13}</p>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="kw" style="color: #003B4F;">class</span> CancelFitException(<span class="pp" style="color: #AD0000;">Exception</span>): <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb19-2"><span class="kw" style="color: #003B4F;">class</span> CancelBatchException(<span class="pp" style="color: #AD0000;">Exception</span>): <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb19-3"><span class="kw" style="color: #003B4F;">class</span> CancelEpochException(<span class="pp" style="color: #AD0000;">Exception</span>): <span class="cf" style="color: #003B4F;">pass</span></span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=14}</p>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;">class</span> Callback(): order <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=15}</p>
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;">def</span> run_cbs(cbs, method_nm, learn<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb21-2">    <span class="cf" style="color: #003B4F;">for</span> cb <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">sorted</span>(cbs, key<span class="op" style="color: #5E5E5E;">=</span>attrgetter(<span class="st" style="color: #20794D;">'order'</span>)):</span>
<span id="cb21-3">        method <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">getattr</span>(cb, method_nm, <span class="va" style="color: #111111;">None</span>)</span>
<span id="cb21-4">        <span class="cf" style="color: #003B4F;">if</span> method <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>: method(learn)</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="kw" style="color: #003B4F;">class</span> CompletionCB(Callback):</span>
<span id="cb22-2">    <span class="kw" style="color: #003B4F;">def</span> before_fit(<span class="va" style="color: #111111;">self</span>, learn): <span class="va" style="color: #111111;">self</span>.count <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb22-3">    <span class="kw" style="color: #003B4F;">def</span> after_batch(<span class="va" style="color: #111111;">self</span>, learn): <span class="va" style="color: #111111;">self</span>.count <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb22-4">    <span class="kw" style="color: #003B4F;">def</span> after_fit(<span class="va" style="color: #111111;">self</span>, learn): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'Completed </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>count<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> batches'</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="760d14db-d8b7-4d99-f1db-fd9e07cd5ef9" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">cbs <span class="op" style="color: #5E5E5E;">=</span> [CompletionCB()]</span>
<span id="cb23-2">run_cbs(cbs, <span class="st" style="color: #20794D;">'before_fit'</span>)</span>
<span id="cb23-3">run_cbs(cbs, <span class="st" style="color: #20794D;">'after_batch'</span>)</span>
<span id="cb23-4">run_cbs(cbs, <span class="st" style="color: #20794D;">'after_fit'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Completed 1 batches</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="kw" style="color: #003B4F;">class</span> Learner():</span>
<span id="cb25-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model, dls, loss_func, lr, cbs, opt_func<span class="op" style="color: #5E5E5E;">=</span>optim.SGD): fc.store_attr()</span>
<span id="cb25-3"></span>
<span id="cb25-4">    <span class="kw" style="color: #003B4F;">def</span> one_batch(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb25-5">        <span class="va" style="color: #111111;">self</span>.preds <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.model(<span class="va" style="color: #111111;">self</span>.batch[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb25-6">        <span class="va" style="color: #111111;">self</span>.loss <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.loss_func(<span class="va" style="color: #111111;">self</span>.preds, <span class="va" style="color: #111111;">self</span>.batch[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb25-7">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.model.training:</span>
<span id="cb25-8">            <span class="va" style="color: #111111;">self</span>.loss.backward()</span>
<span id="cb25-9">            <span class="va" style="color: #111111;">self</span>.opt.step()</span>
<span id="cb25-10">            <span class="va" style="color: #111111;">self</span>.opt.zero_grad()</span>
<span id="cb25-11"></span>
<span id="cb25-12">    <span class="kw" style="color: #003B4F;">def</span> one_epoch(<span class="va" style="color: #111111;">self</span>, train):</span>
<span id="cb25-13">        <span class="va" style="color: #111111;">self</span>.model.train(train)</span>
<span id="cb25-14">        <span class="va" style="color: #111111;">self</span>.dl <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.dls.train <span class="cf" style="color: #003B4F;">if</span> train <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.dls.valid</span>
<span id="cb25-15">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb25-16">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'before_epoch'</span>)</span>
<span id="cb25-17">            <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">iter</span>,<span class="va" style="color: #111111;">self</span>.batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.dl):</span>
<span id="cb25-18">                <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb25-19">                    <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'before_batch'</span>)</span>
<span id="cb25-20">                    <span class="va" style="color: #111111;">self</span>.one_batch()</span>
<span id="cb25-21">                    <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_batch'</span>)</span>
<span id="cb25-22">                <span class="cf" style="color: #003B4F;">except</span> CancelBatchException: <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb25-23">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_epoch'</span>)</span>
<span id="cb25-24">        <span class="cf" style="color: #003B4F;">except</span> CancelEpochException: <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb25-25">    </span>
<span id="cb25-26">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, n_epochs):</span>
<span id="cb25-27">        <span class="va" style="color: #111111;">self</span>.n_epochs <span class="op" style="color: #5E5E5E;">=</span> n_epochs</span>
<span id="cb25-28">        <span class="va" style="color: #111111;">self</span>.epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">range</span>(n_epochs)</span>
<span id="cb25-29">        <span class="va" style="color: #111111;">self</span>.opt <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.opt_func(<span class="va" style="color: #111111;">self</span>.model.parameters(), <span class="va" style="color: #111111;">self</span>.lr)</span>
<span id="cb25-30">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb25-31">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'before_fit'</span>)</span>
<span id="cb25-32">            <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.epoch <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.epochs:</span>
<span id="cb25-33">                <span class="va" style="color: #111111;">self</span>.one_epoch(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb25-34">                <span class="va" style="color: #111111;">self</span>.one_epoch(<span class="va" style="color: #111111;">False</span>)</span>
<span id="cb25-35">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_fit'</span>)</span>
<span id="cb25-36">        <span class="cf" style="color: #003B4F;">except</span> CancelFitException: <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb25-37"></span>
<span id="cb25-38">    <span class="kw" style="color: #003B4F;">def</span> callback(<span class="va" style="color: #111111;">self</span>, method_nm): run_cbs(<span class="va" style="color: #111111;">self</span>.cbs, method_nm, <span class="va" style="color: #111111;">self</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">m,nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">28</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">50</span></span>
<span id="cb26-2"><span class="kw" style="color: #003B4F;">def</span> get_model(): <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</div>
<div class="cell" data-outputid="3282da4e-da5d-4fc9-f35a-cf152c584c8d" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">model <span class="op" style="color: #5E5E5E;">=</span> get_model()</span>
<span id="cb27-2">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(model, dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>[CompletionCB()])</span>
<span id="cb27-3">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Completed 64 batches</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=21}</p>
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="kw" style="color: #003B4F;">class</span> SingleBatchCB(Callback):</span>
<span id="cb29-2">    order <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb29-3">    <span class="kw" style="color: #003B4F;">def</span> after_batch(<span class="va" style="color: #111111;">self</span>, learn): <span class="cf" style="color: #003B4F;">raise</span> CancelFitException()</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(get_model(), dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>[SingleBatchCB(), CompletionCB()])</span>
<span id="cb30-2">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="metrics" class="level2">
<h2 class="anchored" data-anchor-id="metrics">Metrics</h2>
<p>so the accuracy metric is here and then this is kind of our really basic metric which is we’re going to use for just for loss. And so what happens is we’re going to let’s, for example, create an accuracy metric object. We’re basically to add in mini batches of data, right? So for example, here’s a mini batches of inputs and predictions. Here’s another mini batch of inputs and predictions, and then we’re going to call dot value and it will calculate the accuracy. Now the value is a neat little thing. It doesn’t require parentheses after it because it’s called a property. And so a property is something that just calculates automatically without putting having to put parentheses. That’s what a property is, what property getter anyway. And so they look like this, you give it a name. And so we are going to be each time we call ADD, we are going to be storing that input and that target and also the number of items in the mini batch. Optionally for now, that’s just always going to be one. And you can see here that we then call dot Calc, which is going to call the accuracy_calc. So just see how they equal and then we’re going to append to the list of values that calculation. And we’re also going to append to the list of ends in this case, just one. And so then to calculate the value, we just do that. So that’s all that’s happening for accuracy. And then we can do for loss. We can just use metric directly because metric will just calculate the average of whatever it’s passed. So we can say, oh, add the number 0.6, so the target’s optional and saying this is a mini batch of size And then add the value 0.9 with a mini batch size of two and then get the value. And as you can see, that’s exactly the same as the weighted average of 0.6 and point nine with weights of 32 and two. So created a metric class. And so that’s something that we can use to create any metric we like just by overriding calc. Or we could create totally things from scratch as long as they have an add and a value. Okay,</p>
<p>so we’re now going to change our learner and what we’re going to do is we’re going to keep the same basic structure. So it’s going to be fit. It’s going to go through each epoch. It’s going to call one epoch passing and true and false. As for training and validation, one epoch is going to go through each batch in the data oader and call one batch. One batch is going to the prediction get loss and if it’s training, it’s going to do that backward step and zero grade. But there’s a few other things going on. So let’s take a look. Well, actually, let’s just look at it in use first. So when we use it, we’re going to be creating a learner with the model dataloaders, last function learning rate and some callbacks, which we’ll learn about in a moment and we call fit.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="kw" style="color: #003B4F;">class</span> Metric:</span>
<span id="cb31-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.reset()</span>
<span id="cb31-3">    <span class="kw" style="color: #003B4F;">def</span> reset(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.vals,<span class="va" style="color: #111111;">self</span>.ns <span class="op" style="color: #5E5E5E;">=</span> [],[]</span>
<span id="cb31-4">    <span class="kw" style="color: #003B4F;">def</span> add(<span class="va" style="color: #111111;">self</span>, inp, targ<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb31-5">        <span class="va" style="color: #111111;">self</span>.last <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.calc(inp, targ)</span>
<span id="cb31-6">        <span class="va" style="color: #111111;">self</span>.vals.append(<span class="va" style="color: #111111;">self</span>.last)</span>
<span id="cb31-7">        <span class="va" style="color: #111111;">self</span>.ns.append(n)</span>
<span id="cb31-8">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb31-9">    <span class="kw" style="color: #003B4F;">def</span> value(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb31-10">        ns <span class="op" style="color: #5E5E5E;">=</span> tensor(<span class="va" style="color: #111111;">self</span>.ns)</span>
<span id="cb31-11">        <span class="cf" style="color: #003B4F;">return</span> (tensor(<span class="va" style="color: #111111;">self</span>.vals)<span class="op" style="color: #5E5E5E;">*</span>ns).<span class="bu" style="color: null;">sum</span>()<span class="op" style="color: #5E5E5E;">/</span>ns.<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb31-12">    <span class="kw" style="color: #003B4F;">def</span> calc(<span class="va" style="color: #111111;">self</span>, inps, targs): <span class="cf" style="color: #003B4F;">return</span> inps</span></code></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;">class</span> Accuracy(Metric):</span>
<span id="cb32-2">    <span class="kw" style="color: #003B4F;">def</span> calc(<span class="va" style="color: #111111;">self</span>, inps, targs): <span class="cf" style="color: #003B4F;">return</span> (inps<span class="op" style="color: #5E5E5E;">==</span>targs).<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
</div>
<div class="cell" data-outputid="b8f582be-f9da-4931-be5f-2220b9203751" data-execution_count="25">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">acc <span class="op" style="color: #5E5E5E;">=</span> Accuracy()</span>
<span id="cb33-2">acc.add(tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>]), tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>]))</span>
<span id="cb33-3">acc.add(tensor([<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>]), tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>]))</span>
<span id="cb33-4">acc.value</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor(0.45)</code></pre>
</div>
</div>
<div class="cell" data-outputid="19e775e9-b0d5-435f-cdbd-75d22c556d0a" data-execution_count="26">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">loss <span class="op" style="color: #5E5E5E;">=</span> Metric()</span>
<span id="cb35-2">loss.add(<span class="fl" style="color: #AD0000;">0.6</span>, n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>)</span>
<span id="cb35-3">loss.add(<span class="fl" style="color: #AD0000;">0.9</span>, n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb35-4">loss.value, <span class="bu" style="color: null;">round</span>((<span class="fl" style="color: #AD0000;">0.6</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">32</span><span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">0.9</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>)<span class="op" style="color: #5E5E5E;">/</span>(<span class="dv" style="color: #AD0000;">32</span><span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span>), <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>(tensor(0.62), 0.62)</code></pre>
</div>
</div>
</section>
<section id="some-callbacks" class="level2">
<h2 class="anchored" data-anchor-id="some-callbacks">Some callbacks</h2>
<pre><code>pip install torcheval</code></pre>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=27}</p>
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="im" style="color: #00769E;">from</span> torcheval.metrics <span class="im" style="color: #00769E;">import</span> MulticlassAccuracy,Mean</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="ddc8e9c8-5dc1-4cf8-c077-cb3f1840b10e" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">metric <span class="op" style="color: #5E5E5E;">=</span> MulticlassAccuracy()</span>
<span id="cb39-2">metric.update(tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">3</span>]), tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>]))</span>
<span id="cb39-3">metric.compute()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor(0.50)</code></pre>
</div>
</div>
<div class="cell" data-outputid="7060a084-7aa3-4614-b364-64c5b128c312" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">metric.reset()</span>
<span id="cb41-2">metric.compute()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>tensor(nan)</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=30}</p>
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;">def</span> to_cpu(x):</span>
<span id="cb43-2">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(x, Mapping): <span class="cf" style="color: #003B4F;">return</span> {k:to_cpu(v) <span class="cf" style="color: #003B4F;">for</span> k,v <span class="kw" style="color: #003B4F;">in</span> x.items()}</span>
<span id="cb43-3">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(x, <span class="bu" style="color: null;">list</span>): <span class="cf" style="color: #003B4F;">return</span> [to_cpu(o) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> x]</span>
<span id="cb43-4">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(x, <span class="bu" style="color: null;">tuple</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">tuple</span>(to_cpu(<span class="bu" style="color: null;">list</span>(x)))</span>
<span id="cb43-5">    <span class="cf" style="color: #003B4F;">return</span> x.detach().cpu()</span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=31}</p>
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="kw" style="color: #003B4F;">class</span> MetricsCB(Callback):</span>
<span id="cb44-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>ms, <span class="op" style="color: #5E5E5E;">**</span>metrics):</span>
<span id="cb44-3">        <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> ms: metrics[<span class="bu" style="color: null;">type</span>(o).<span class="va" style="color: #111111;">__name__</span>] <span class="op" style="color: #5E5E5E;">=</span> o</span>
<span id="cb44-4">        <span class="va" style="color: #111111;">self</span>.metrics <span class="op" style="color: #5E5E5E;">=</span> metrics</span>
<span id="cb44-5">        <span class="va" style="color: #111111;">self</span>.all_metrics <span class="op" style="color: #5E5E5E;">=</span> copy(metrics)</span>
<span id="cb44-6">        <span class="va" style="color: #111111;">self</span>.all_metrics[<span class="st" style="color: #20794D;">'loss'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.loss <span class="op" style="color: #5E5E5E;">=</span> Mean()</span>
<span id="cb44-7"></span>
<span id="cb44-8">    <span class="kw" style="color: #003B4F;">def</span> _log(<span class="va" style="color: #111111;">self</span>, d): <span class="bu" style="color: null;">print</span>(d)</span>
<span id="cb44-9">    <span class="kw" style="color: #003B4F;">def</span> before_fit(<span class="va" style="color: #111111;">self</span>, learn): learn.metrics <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span></span>
<span id="cb44-10">    <span class="kw" style="color: #003B4F;">def</span> before_epoch(<span class="va" style="color: #111111;">self</span>, learn): [o.reset() <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.all_metrics.values()]</span>
<span id="cb44-11"></span>
<span id="cb44-12">    <span class="kw" style="color: #003B4F;">def</span> after_epoch(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb44-13">        log <span class="op" style="color: #5E5E5E;">=</span> {k:<span class="ss" style="color: #20794D;">f'</span><span class="sc" style="color: #5E5E5E;">{</span>v<span class="sc" style="color: #5E5E5E;">.</span>compute()<span class="sc" style="color: #5E5E5E;">:.3f}</span><span class="ss" style="color: #20794D;">'</span> <span class="cf" style="color: #003B4F;">for</span> k,v <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.all_metrics.items()}</span>
<span id="cb44-14">        log[<span class="st" style="color: #20794D;">'epoch'</span>] <span class="op" style="color: #5E5E5E;">=</span> learn.epoch</span>
<span id="cb44-15">        log[<span class="st" style="color: #20794D;">'train'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'train'</span> <span class="cf" style="color: #003B4F;">if</span> learn.model.training <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'eval'</span></span>
<span id="cb44-16">        <span class="va" style="color: #111111;">self</span>._log(log)</span>
<span id="cb44-17"></span>
<span id="cb44-18">    <span class="kw" style="color: #003B4F;">def</span> after_batch(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb44-19">        x,y,<span class="op" style="color: #5E5E5E;">*</span>_ <span class="op" style="color: #5E5E5E;">=</span> to_cpu(learn.batch)</span>
<span id="cb44-20">        <span class="cf" style="color: #003B4F;">for</span> m <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.metrics.values(): m.update(to_cpu(learn.preds), y)</span>
<span id="cb44-21">        <span class="va" style="color: #111111;">self</span>.loss.update(to_cpu(learn.loss), weight<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(x))</span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=32}</p>
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="kw" style="color: #003B4F;">class</span> DeviceCB(Callback):</span>
<span id="cb45-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, device<span class="op" style="color: #5E5E5E;">=</span>def_device): fc.store_attr()</span>
<span id="cb45-3">    <span class="kw" style="color: #003B4F;">def</span> before_fit(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb45-4">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">hasattr</span>(learn.model, <span class="st" style="color: #20794D;">'to'</span>): learn.model.to(<span class="va" style="color: #111111;">self</span>.device)</span>
<span id="cb45-5">    <span class="kw" style="color: #003B4F;">def</span> before_batch(<span class="va" style="color: #111111;">self</span>, learn): learn.batch <span class="op" style="color: #5E5E5E;">=</span> to_device(learn.batch, device<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.device)</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="9fdab7ac-0675-4b43-e1a6-8b279e9b23e6" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">model <span class="op" style="color: #5E5E5E;">=</span> get_model()</span>
<span id="cb46-2">metrics <span class="op" style="color: #5E5E5E;">=</span> MetricsCB(accuracy<span class="op" style="color: #5E5E5E;">=</span>MulticlassAccuracy())</span>
<span id="cb46-3">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(model, dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>[DeviceCB(), metrics])</span>
<span id="cb46-4">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'accuracy': '0.602', 'loss': '1.183', 'epoch': 0, 'train': 'train'}
{'accuracy': '0.700', 'loss': '0.847', 'epoch': 0, 'train': 'eval'}</code></pre>
</div>
</div>
</section>
<section id="flexible-learner" class="level2">
<h2 class="anchored" data-anchor-id="flexible-learner">Flexible learner</h2>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;">class</span> Learner():</span>
<span id="cb48-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model, dls<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">0</span>,), loss_func<span class="op" style="color: #5E5E5E;">=</span>F.mse_loss, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>, cbs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, opt_func<span class="op" style="color: #5E5E5E;">=</span>optim.SGD):</span>
<span id="cb48-3">        cbs <span class="op" style="color: #5E5E5E;">=</span> fc.L(cbs)</span>
<span id="cb48-4">        fc.store_attr()</span>
<span id="cb48-5"></span>
<span id="cb48-6">    <span class="at" style="color: #657422;">@contextmanager</span></span>
<span id="cb48-7">    <span class="kw" style="color: #003B4F;">def</span> cb_ctx(<span class="va" style="color: #111111;">self</span>, nm):</span>
<span id="cb48-8">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb48-9">            <span class="va" style="color: #111111;">self</span>.callback(<span class="ss" style="color: #20794D;">f'before_</span><span class="sc" style="color: #5E5E5E;">{</span>nm<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb48-10">            <span class="cf" style="color: #003B4F;">yield</span></span>
<span id="cb48-11">            <span class="va" style="color: #111111;">self</span>.callback(<span class="ss" style="color: #20794D;">f'after_</span><span class="sc" style="color: #5E5E5E;">{</span>nm<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb48-12">        <span class="cf" style="color: #003B4F;">except</span> <span class="bu" style="color: null;">globals</span>()[<span class="ss" style="color: #20794D;">f'Cancel</span><span class="sc" style="color: #5E5E5E;">{</span>nm<span class="sc" style="color: #5E5E5E;">.</span>title()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">Exception'</span>]: <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb48-13">        <span class="cf" style="color: #003B4F;">finally</span>: <span class="va" style="color: #111111;">self</span>.callback(<span class="ss" style="color: #20794D;">f'cleanup_</span><span class="sc" style="color: #5E5E5E;">{</span>nm<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb48-14">                </span>
<span id="cb48-15">    <span class="kw" style="color: #003B4F;">def</span> one_epoch(<span class="va" style="color: #111111;">self</span>, train):</span>
<span id="cb48-16">        <span class="va" style="color: #111111;">self</span>.model.train(train)</span>
<span id="cb48-17">        <span class="va" style="color: #111111;">self</span>.dl <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.dls.train <span class="cf" style="color: #003B4F;">if</span> train <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.dls.valid</span>
<span id="cb48-18">        <span class="cf" style="color: #003B4F;">with</span> <span class="va" style="color: #111111;">self</span>.cb_ctx(<span class="st" style="color: #20794D;">'epoch'</span>):</span>
<span id="cb48-19">            <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">iter</span>,<span class="va" style="color: #111111;">self</span>.batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.dl):</span>
<span id="cb48-20">                <span class="cf" style="color: #003B4F;">with</span> <span class="va" style="color: #111111;">self</span>.cb_ctx(<span class="st" style="color: #20794D;">'batch'</span>):</span>
<span id="cb48-21">                    <span class="va" style="color: #111111;">self</span>.predict()</span>
<span id="cb48-22">                    <span class="va" style="color: #111111;">self</span>.get_loss()</span>
<span id="cb48-23">                    <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.training:</span>
<span id="cb48-24">                        <span class="va" style="color: #111111;">self</span>.backward()</span>
<span id="cb48-25">                        <span class="va" style="color: #111111;">self</span>.step()</span>
<span id="cb48-26">                        <span class="va" style="color: #111111;">self</span>.zero_grad()</span>
<span id="cb48-27">    </span>
<span id="cb48-28">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, n_epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, valid<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, cbs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb48-29">        cbs <span class="op" style="color: #5E5E5E;">=</span> fc.L(cbs)</span>
<span id="cb48-30">        <span class="co" style="color: #5E5E5E;"># `add_cb` and `rm_cb` were added in lesson 18</span></span>
<span id="cb48-31">        <span class="cf" style="color: #003B4F;">for</span> cb <span class="kw" style="color: #003B4F;">in</span> cbs: <span class="va" style="color: #111111;">self</span>.cbs.append(cb)</span>
<span id="cb48-32">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb48-33">            <span class="va" style="color: #111111;">self</span>.n_epochs <span class="op" style="color: #5E5E5E;">=</span> n_epochs</span>
<span id="cb48-34">            <span class="va" style="color: #111111;">self</span>.epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">range</span>(n_epochs)</span>
<span id="cb48-35">            <span class="va" style="color: #111111;">self</span>.opt <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.opt_func(<span class="va" style="color: #111111;">self</span>.model.parameters(), <span class="va" style="color: #111111;">self</span>.lr <span class="cf" style="color: #003B4F;">if</span> lr <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span> <span class="cf" style="color: #003B4F;">else</span> lr)</span>
<span id="cb48-36">            <span class="cf" style="color: #003B4F;">with</span> <span class="va" style="color: #111111;">self</span>.cb_ctx(<span class="st" style="color: #20794D;">'fit'</span>):</span>
<span id="cb48-37">                <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.epoch <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.epochs:</span>
<span id="cb48-38">                    <span class="cf" style="color: #003B4F;">if</span> train: <span class="va" style="color: #111111;">self</span>.one_epoch(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb48-39">                    <span class="cf" style="color: #003B4F;">if</span> valid: torch.no_grad()(<span class="va" style="color: #111111;">self</span>.one_epoch)(<span class="va" style="color: #111111;">False</span>)</span>
<span id="cb48-40">        <span class="cf" style="color: #003B4F;">finally</span>:</span>
<span id="cb48-41">            <span class="cf" style="color: #003B4F;">for</span> cb <span class="kw" style="color: #003B4F;">in</span> cbs: <span class="va" style="color: #111111;">self</span>.cbs.remove(cb)</span>
<span id="cb48-42"></span>
<span id="cb48-43">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getattr__</span>(<span class="va" style="color: #111111;">self</span>, name):</span>
<span id="cb48-44">        <span class="cf" style="color: #003B4F;">if</span> name <span class="kw" style="color: #003B4F;">in</span> (<span class="st" style="color: #20794D;">'predict'</span>,<span class="st" style="color: #20794D;">'get_loss'</span>,<span class="st" style="color: #20794D;">'backward'</span>,<span class="st" style="color: #20794D;">'step'</span>,<span class="st" style="color: #20794D;">'zero_grad'</span>): <span class="cf" style="color: #003B4F;">return</span> partial(<span class="va" style="color: #111111;">self</span>.callback, name)</span>
<span id="cb48-45">        <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">AttributeError</span>(name)</span>
<span id="cb48-46"></span>
<span id="cb48-47">    <span class="kw" style="color: #003B4F;">def</span> callback(<span class="va" style="color: #111111;">self</span>, method_nm): run_cbs(<span class="va" style="color: #111111;">self</span>.cbs, method_nm, <span class="va" style="color: #111111;">self</span>)</span>
<span id="cb48-48">    </span>
<span id="cb48-49">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb48-50">    <span class="kw" style="color: #003B4F;">def</span> training(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.model.training</span></code></pre></div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=35}</p>
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="kw" style="color: #003B4F;">class</span> TrainCB(Callback):</span>
<span id="cb49-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_inp<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>): <span class="va" style="color: #111111;">self</span>.n_inp <span class="op" style="color: #5E5E5E;">=</span> n_inp</span>
<span id="cb49-3">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, learn): learn.preds <span class="op" style="color: #5E5E5E;">=</span> learn.model(<span class="op" style="color: #5E5E5E;">*</span>learn.batch[:<span class="va" style="color: #111111;">self</span>.n_inp])</span>
<span id="cb49-4">    <span class="kw" style="color: #003B4F;">def</span> get_loss(<span class="va" style="color: #111111;">self</span>, learn): learn.loss <span class="op" style="color: #5E5E5E;">=</span> learn.loss_func(learn.preds, <span class="op" style="color: #5E5E5E;">*</span>learn.batch[<span class="va" style="color: #111111;">self</span>.n_inp:])</span>
<span id="cb49-5">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>, learn): learn.loss.backward()</span>
<span id="cb49-6">    <span class="kw" style="color: #003B4F;">def</span> step(<span class="va" style="color: #111111;">self</span>, learn): learn.opt.step()</span>
<span id="cb49-7">    <span class="kw" style="color: #003B4F;">def</span> zero_grad(<span class="va" style="color: #111111;">self</span>, learn): learn.opt.zero_grad()</span></code></pre></div>
<p>:::</p>
<p>NB: I added <code>self.n_inp</code> after the lesson. This allows us to train models with more than one input or output.</p>
<p>And it’s going to do our thing. And look, we’re going to have charts and stuff. All right? So the basic idea is going to look very similar. So we’re going to call fit. So when we construct it, we’re going to be passing in exactly the same things as before. But we’ve got one extra thing callbacks, which we’ll see in a moment, store the attributes as before, and we’re going to be doing some stuff with the callbacks. So when we call Fit for this number of epochs, we’re going to store away how many epochs we’re going to do. We’re also going to store away the actual range that we’re going to loop through as soft epochs. So here’s that looping through so epochs, we’re going to create the optimizer using the optimizer function and the parameters, and then we’re going to call _fit. Now, I want to know if is underscore fit, why didn’t we just copy and paste so this into here Why why do this? It’s because we’ve created a special decorator with callbacks. What does that do?</p>
<p>So it’s up here with callbacks. With callbacks is a class. It’s going to just store one thing, which is the name. In this case, the name is fit. And what it’s going to do is Now, this is a decorator, right? So when we call it remember, decorators get past a function. So it’s going to get past whole function. And that’s going to be called if that’s a __call. Remember, is what happens when a class is treated, an object is treated as if it’s a function. So it’s going to get past this function. So this function is underscore fit. And so what we want to do is we want to return a different function. It’s going to, cause, call the function that we were asked to call using the arguments and key documents we were asked to use. But before it calls that function, it’s going to call a special method called callback passing in the string before in this case before underscore, fit. After it’s completed, it’s going to call that method called callback and passing the string after underscore fit, and it’s going to wrap the whole thing in a try except. BLOCK And it’s going to be looking for an exception called Cancel Fit Exception. And if it gets one, it’s not going to complain. So let me explain what’s going on with all of those things.</p>
<p>Let’s look at example of a callback. Let’s change this DeviceCB So for example, here is a callback called DeviceCB Device callback and before FIT will be called automatically before that underscore fit method is called and it’s going to put the model onto our device CUDA or MPS if we have one. Otherwise it would just be on GPU. So what’s going to happen here? So it’s going to call, we’re going to call fit. It’s going to go through these lines of code. It’s going to call, underscore, a fit. Underscore fit is not this function underscore fit is this function with F is this function. So it’s going to call our learn act callback passing in before underscore, fit and callback is defined here. What’s callback going to do? It’s going to be past the string before underscore it. It’s going to then go through each of our callbacks sorted based on their order. And you can see here callback can have an order and it’s going to look at that callback and try to get an attribute called before underscore fit and it will find one. And so then it’s going to call that method. Now, if that method doesn’t exist, it doesn’t appear at all, then getattr will return this instead. Identity is a function just here. This is an identity function. All it does is whatever arguments it gets passed, it returns them. If it’s not past any arguments, it just returns. So there’s a lot of python going on here and that is why we did that.</p>
<p>Foundations lesson. And so for people who haven’t done a lot of this python, there’s going to be a lot of stuff to experiment with and learn about. And so do ask on the forums if any of these bits get confusing. But the best way to learn about these things is to open up this Jupyter Notebook and try and create really simple versions of things. Right? So for example, let’s try identity. Identity. How exactly does identity work? I can call it, and it gets nothing. I can call it with one spec one I could call it with. I get back, I got it with a yes, fully tested this, call it with a one and get a one. And how is it doing that exactly. So remember we can add a break point and this is be a great time to really test your debugging skills. Okay so remember in our debugger we can hit H to find out what the commands are, but you really should do a tutorial on the debugger if you’re not familiar with it. And then we can step through each one. So I can now print args and it’s actually a trick which I like is that args is actually a command funnily enough, which I’ll just tell you the arguments to any function, regardless of what they’re called, which is kind of nice. And so then we can step through by pressing and, and after this we can check like, okay, what is X now and what is args now, right? So remember it really experiment with these things. So anyway, we’re going to talk about this a lot more in the next lesson. But before that, if you’re not familiar with try except blocks, you know, spend some time practicing them. If you’re not familiar with decorators, well, we’ve seen them before, So go back and look at them again really carefully. If you’re not familiar with the debugger practice with that, if you haven’t spent much time with get at, try remind yourself about that. So try to get yourself really familiar and comfortable as much as possible with the pieces, because if you’re not comfortable with the pieces and the way put the pieces together is going to be confusing. There’s actually something in education in kind of the theory of education called cognitive load theory. And The theory of cognitive basically cognitive load theory says if you’re trying to learn something, but your cognitive load is really high because of all lots of other things going on at the same time, you’re not going to learn it. So it’s going to be hard for you to learn this framework that we’re building. If you have too much cognitive load of like, what the hell’s a decorator or what they getattr or what to sort of do or what’s possible, you know, all these things. Now, I actually spent quite a bit of time trying to make this as simple as possible, but but also as flexible as it needs to be for the rest of the course. And this is this is this is as simple as I could get it. So these are kind of things that you actually have to learn. But in doing so, you’re going to be able to write some really, you know, powerful and general code yourself. So hopefully you’ll find this a really valuable and mind expanding exercise in in bringing high level software engineering skills to your data science work. Okay. So with that, this looks like a good place to leave it and look forward to seeing you next time.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=36}</p>
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="kw" style="color: #003B4F;">class</span> ProgressCB(Callback):</span>
<span id="cb50-2">    order <span class="op" style="color: #5E5E5E;">=</span> MetricsCB.order<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb50-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, plot<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>): <span class="va" style="color: #111111;">self</span>.plot <span class="op" style="color: #5E5E5E;">=</span> plot</span>
<span id="cb50-4">    <span class="kw" style="color: #003B4F;">def</span> before_fit(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb50-5">        learn.epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.mbar <span class="op" style="color: #5E5E5E;">=</span> master_bar(learn.epochs)</span>
<span id="cb50-6">        <span class="va" style="color: #111111;">self</span>.first <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span></span>
<span id="cb50-7">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">hasattr</span>(learn, <span class="st" style="color: #20794D;">'metrics'</span>): learn.metrics._log <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._log</span>
<span id="cb50-8">        <span class="va" style="color: #111111;">self</span>.losses <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb50-9"></span>
<span id="cb50-10">    <span class="kw" style="color: #003B4F;">def</span> _log(<span class="va" style="color: #111111;">self</span>, d):</span>
<span id="cb50-11">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.first:</span>
<span id="cb50-12">            <span class="va" style="color: #111111;">self</span>.mbar.write(<span class="bu" style="color: null;">list</span>(d), table<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb50-13">            <span class="va" style="color: #111111;">self</span>.first <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span></span>
<span id="cb50-14">        <span class="va" style="color: #111111;">self</span>.mbar.write(<span class="bu" style="color: null;">list</span>(d.values()), table<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb50-15"></span>
<span id="cb50-16">    <span class="kw" style="color: #003B4F;">def</span> before_epoch(<span class="va" style="color: #111111;">self</span>, learn): learn.dl <span class="op" style="color: #5E5E5E;">=</span> progress_bar(learn.dl, leave<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, parent<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.mbar)</span>
<span id="cb50-17">    <span class="kw" style="color: #003B4F;">def</span> after_batch(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb50-18">        learn.dl.comment <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f'</span><span class="sc" style="color: #5E5E5E;">{</span>learn<span class="sc" style="color: #5E5E5E;">.</span>loss<span class="sc" style="color: #5E5E5E;">:.3f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb50-19">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.plot <span class="kw" style="color: #003B4F;">and</span> <span class="bu" style="color: null;">hasattr</span>(learn, <span class="st" style="color: #20794D;">'metrics'</span>) <span class="kw" style="color: #003B4F;">and</span> learn.training:</span>
<span id="cb50-20">            <span class="va" style="color: #111111;">self</span>.losses.append(learn.loss.item())</span>
<span id="cb50-21">            <span class="va" style="color: #111111;">self</span>.mbar.update_graph([[fc.L.<span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.losses), <span class="va" style="color: #111111;">self</span>.losses]])</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">model <span class="op" style="color: #5E5E5E;">=</span> get_model()</span></code></pre></div>
</div>
<div class="cell" data-outputid="74b623d8-3494-47da-84fb-0b5dbad236bc" data-execution_count="38">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">metrics <span class="op" style="color: #5E5E5E;">=</span> MetricsCB(accuracy<span class="op" style="color: #5E5E5E;">=</span>MulticlassAccuracy())</span>
<span id="cb52-2">cbs <span class="op" style="color: #5E5E5E;">=</span> [TrainCB(), DeviceCB(), metrics, ProgressCB(plot<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)]</span>
<span id="cb52-3">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(model, dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs)</span>
<span id="cb52-4">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>accuracy</th>
      <th>loss</th>
      <th>epoch</th>
      <th>train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.596</td>
      <td>1.167</td>
      <td>0</td>
      <td>train</td>
    </tr>
    <tr>
      <td>0.729</td>
      <td>0.794</td>
      <td>0</td>
      <td>eval</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index_files/figure-html/cell-39-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="updated-versions-since-the-lesson" class="level2">
<h2 class="anchored" data-anchor-id="updated-versions-since-the-lesson">Updated versions since the lesson</h2>
<p>After the lesson we noticed that <code>contextlib.context_manager</code> has a surprising “feature” which doesn’t let us raise an exception before the <code>yield</code>. Therefore we’ve replaced the context manager with a decorator in this updated version of <code>Learner</code>. We have also added a few more callbacks in <code>one_epoch()</code>.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=39}</p>
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="kw" style="color: #003B4F;">class</span> with_cbs:</span>
<span id="cb53-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, nm): <span class="va" style="color: #111111;">self</span>.nm <span class="op" style="color: #5E5E5E;">=</span> nm</span>
<span id="cb53-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, f):</span>
<span id="cb53-4">        <span class="kw" style="color: #003B4F;">def</span> _f(o, <span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb53-5">            <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb53-6">                o.callback(<span class="ss" style="color: #20794D;">f'before_</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>nm<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb53-7">                f(o, <span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb53-8">                o.callback(<span class="ss" style="color: #20794D;">f'after_</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>nm<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb53-9">            <span class="cf" style="color: #003B4F;">except</span> <span class="bu" style="color: null;">globals</span>()[<span class="ss" style="color: #20794D;">f'Cancel</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>nm<span class="sc" style="color: #5E5E5E;">.</span>title()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">Exception'</span>]: <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb53-10">            <span class="cf" style="color: #003B4F;">finally</span>: o.callback(<span class="ss" style="color: #20794D;">f'cleanup_</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>nm<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb53-11">        <span class="cf" style="color: #003B4F;">return</span> _f</span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=40}</p>
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="kw" style="color: #003B4F;">class</span> Learner():</span>
<span id="cb54-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model, dls<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">0</span>,), loss_func<span class="op" style="color: #5E5E5E;">=</span>F.mse_loss, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>, cbs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, opt_func<span class="op" style="color: #5E5E5E;">=</span>optim.SGD):</span>
<span id="cb54-3">        cbs <span class="op" style="color: #5E5E5E;">=</span> fc.L(cbs)</span>
<span id="cb54-4">        fc.store_attr()</span>
<span id="cb54-5"></span>
<span id="cb54-6">    <span class="at" style="color: #657422;">@with_cbs</span>(<span class="st" style="color: #20794D;">'batch'</span>)</span>
<span id="cb54-7">    <span class="kw" style="color: #003B4F;">def</span> _one_batch(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb54-8">        <span class="va" style="color: #111111;">self</span>.predict()</span>
<span id="cb54-9">        <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_predict'</span>)</span>
<span id="cb54-10">        <span class="va" style="color: #111111;">self</span>.get_loss()</span>
<span id="cb54-11">        <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_loss'</span>)</span>
<span id="cb54-12">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.training:</span>
<span id="cb54-13">            <span class="va" style="color: #111111;">self</span>.backward()</span>
<span id="cb54-14">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_backward'</span>)</span>
<span id="cb54-15">            <span class="va" style="color: #111111;">self</span>.step()</span>
<span id="cb54-16">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_step'</span>)</span>
<span id="cb54-17">            <span class="va" style="color: #111111;">self</span>.zero_grad()</span>
<span id="cb54-18"></span>
<span id="cb54-19">    <span class="at" style="color: #657422;">@with_cbs</span>(<span class="st" style="color: #20794D;">'epoch'</span>)</span>
<span id="cb54-20">    <span class="kw" style="color: #003B4F;">def</span> _one_epoch(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb54-21">        <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">iter</span>,<span class="va" style="color: #111111;">self</span>.batch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.dl): <span class="va" style="color: #111111;">self</span>._one_batch()</span>
<span id="cb54-22"></span>
<span id="cb54-23">    <span class="kw" style="color: #003B4F;">def</span> one_epoch(<span class="va" style="color: #111111;">self</span>, training):</span>
<span id="cb54-24">        <span class="va" style="color: #111111;">self</span>.model.train(training)</span>
<span id="cb54-25">        <span class="va" style="color: #111111;">self</span>.dl <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.dls.train <span class="cf" style="color: #003B4F;">if</span> training <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.dls.valid</span>
<span id="cb54-26">        <span class="va" style="color: #111111;">self</span>._one_epoch()</span>
<span id="cb54-27"></span>
<span id="cb54-28">    <span class="at" style="color: #657422;">@with_cbs</span>(<span class="st" style="color: #20794D;">'fit'</span>)</span>
<span id="cb54-29">    <span class="kw" style="color: #003B4F;">def</span> _fit(<span class="va" style="color: #111111;">self</span>, train, valid):</span>
<span id="cb54-30">        <span class="cf" style="color: #003B4F;">for</span> <span class="va" style="color: #111111;">self</span>.epoch <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.epochs:</span>
<span id="cb54-31">            <span class="cf" style="color: #003B4F;">if</span> train: <span class="va" style="color: #111111;">self</span>.one_epoch(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb54-32">            <span class="cf" style="color: #003B4F;">if</span> valid: torch.no_grad()(<span class="va" style="color: #111111;">self</span>.one_epoch)(<span class="va" style="color: #111111;">False</span>)</span>
<span id="cb54-33"></span>
<span id="cb54-34">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, n_epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, train<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, valid<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, cbs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb54-35">        cbs <span class="op" style="color: #5E5E5E;">=</span> fc.L(cbs)</span>
<span id="cb54-36">        <span class="co" style="color: #5E5E5E;"># `add_cb` and `rm_cb` were added in lesson 18</span></span>
<span id="cb54-37">        <span class="cf" style="color: #003B4F;">for</span> cb <span class="kw" style="color: #003B4F;">in</span> cbs: <span class="va" style="color: #111111;">self</span>.cbs.append(cb)</span>
<span id="cb54-38">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb54-39">            <span class="va" style="color: #111111;">self</span>.n_epochs <span class="op" style="color: #5E5E5E;">=</span> n_epochs</span>
<span id="cb54-40">            <span class="va" style="color: #111111;">self</span>.epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">range</span>(n_epochs)</span>
<span id="cb54-41">            <span class="cf" style="color: #003B4F;">if</span> lr <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: lr <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.lr</span>
<span id="cb54-42">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.opt_func: <span class="va" style="color: #111111;">self</span>.opt <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.opt_func(<span class="va" style="color: #111111;">self</span>.model.parameters(), lr)</span>
<span id="cb54-43">            <span class="va" style="color: #111111;">self</span>._fit(train, valid)</span>
<span id="cb54-44">        <span class="cf" style="color: #003B4F;">finally</span>:</span>
<span id="cb54-45">            <span class="cf" style="color: #003B4F;">for</span> cb <span class="kw" style="color: #003B4F;">in</span> cbs: <span class="va" style="color: #111111;">self</span>.cbs.remove(cb)</span>
<span id="cb54-46"></span>
<span id="cb54-47">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getattr__</span>(<span class="va" style="color: #111111;">self</span>, name):</span>
<span id="cb54-48">        <span class="cf" style="color: #003B4F;">if</span> name <span class="kw" style="color: #003B4F;">in</span> (<span class="st" style="color: #20794D;">'predict'</span>,<span class="st" style="color: #20794D;">'get_loss'</span>,<span class="st" style="color: #20794D;">'backward'</span>,<span class="st" style="color: #20794D;">'step'</span>,<span class="st" style="color: #20794D;">'zero_grad'</span>): <span class="cf" style="color: #003B4F;">return</span> partial(<span class="va" style="color: #111111;">self</span>.callback, name)</span>
<span id="cb54-49">        <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">AttributeError</span>(name)</span>
<span id="cb54-50"></span>
<span id="cb54-51">    <span class="kw" style="color: #003B4F;">def</span> callback(<span class="va" style="color: #111111;">self</span>, method_nm): run_cbs(<span class="va" style="color: #111111;">self</span>.cbs, method_nm, <span class="va" style="color: #111111;">self</span>)</span>
<span id="cb54-52">    </span>
<span id="cb54-53">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb54-54">    <span class="kw" style="color: #003B4F;">def</span> training(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.model.training</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="38b469e5-ef0b-4717-e3aa-b931be8955ec" data-execution_count="41">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">model <span class="op" style="color: #5E5E5E;">=</span> get_model()</span>
<span id="cb55-2"></span>
<span id="cb55-3">metrics <span class="op" style="color: #5E5E5E;">=</span> MetricsCB(accuracy<span class="op" style="color: #5E5E5E;">=</span>MulticlassAccuracy())</span>
<span id="cb55-4">cbs <span class="op" style="color: #5E5E5E;">=</span> [TrainCB(), DeviceCB(), metrics, ProgressCB(plot<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)]</span>
<span id="cb55-5">learn <span class="op" style="color: #5E5E5E;">=</span> Learner(model, dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs)</span>
<span id="cb55-6">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>accuracy</th>
      <th>loss</th>
      <th>epoch</th>
      <th>train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.616</td>
      <td>1.168</td>
      <td>0</td>
      <td>train</td>
    </tr>
    <tr>
      <td>0.719</td>
      <td>0.789</td>
      <td>0</td>
      <td>eval</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index_files/figure-html/cell-42-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="trainlearner-and-momentumlearner" class="level2">
<h2 class="anchored" data-anchor-id="trainlearner-and-momentumlearner">TrainLearner and MomentumLearner</h2>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=42}</p>
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><span class="kw" style="color: #003B4F;">class</span> TrainLearner(Learner):</span>
<span id="cb56-2">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.preds <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.model(<span class="va" style="color: #111111;">self</span>.batch[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb56-3">    <span class="kw" style="color: #003B4F;">def</span> get_loss(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.loss <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.loss_func(<span class="va" style="color: #111111;">self</span>.preds, <span class="va" style="color: #111111;">self</span>.batch[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb56-4">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.loss.backward()</span>
<span id="cb56-5">    <span class="kw" style="color: #003B4F;">def</span> step(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.opt.step()</span>
<span id="cb56-6">    <span class="kw" style="color: #003B4F;">def</span> zero_grad(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.opt.zero_grad()</span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=43}</p>
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="kw" style="color: #003B4F;">class</span> MomentumLearner(TrainLearner):</span>
<span id="cb57-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, model, dls, loss_func, lr<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, cbs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, opt_func<span class="op" style="color: #5E5E5E;">=</span>optim.SGD, mom<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>):</span>
<span id="cb57-3">        <span class="va" style="color: #111111;">self</span>.mom <span class="op" style="color: #5E5E5E;">=</span> mom</span>
<span id="cb57-4">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>(model, dls, loss_func, lr, cbs, opt_func)</span>
<span id="cb57-5"></span>
<span id="cb57-6">    <span class="kw" style="color: #003B4F;">def</span> zero_grad(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb57-7">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb57-8">            <span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.model.parameters(): p.grad <span class="op" style="color: #5E5E5E;">*=</span> <span class="va" style="color: #111111;">self</span>.mom</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="4f96e455-7050-4b07-c6a2-0a5580b24104" data-execution_count="44">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><span class="co" style="color: #5E5E5E;"># NB: No TrainCB</span></span>
<span id="cb58-2">metrics <span class="op" style="color: #5E5E5E;">=</span> MetricsCB(accuracy<span class="op" style="color: #5E5E5E;">=</span>MulticlassAccuracy())</span>
<span id="cb58-3">cbs <span class="op" style="color: #5E5E5E;">=</span> [DeviceCB(), metrics, ProgressCB(plot<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)]</span>
<span id="cb58-4">learn <span class="op" style="color: #5E5E5E;">=</span> MomentumLearner(get_model(), dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs)</span>
<span id="cb58-5">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>accuracy</th>
      <th>loss</th>
      <th>epoch</th>
      <th>train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.676</td>
      <td>0.936</td>
      <td>0</td>
      <td>train</td>
    </tr>
    <tr>
      <td>0.791</td>
      <td>0.585</td>
      <td>0</td>
      <td>eval</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index_files/figure-html/cell-45-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="lrfindercb" class="level2">
<h2 class="anchored" data-anchor-id="lrfindercb">LRFinderCB</h2>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><span class="kw" style="color: #003B4F;">class</span> LRFinderCB(Callback):</span>
<span id="cb59-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, lr_mult<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.3</span>): fc.store_attr()</span>
<span id="cb59-3">    </span>
<span id="cb59-4">    <span class="kw" style="color: #003B4F;">def</span> before_fit(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb59-5">        <span class="va" style="color: #111111;">self</span>.lrs,<span class="va" style="color: #111111;">self</span>.losses <span class="op" style="color: #5E5E5E;">=</span> [],[]</span>
<span id="cb59-6">        <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span> <span class="op" style="color: #5E5E5E;">=</span> math.inf</span>
<span id="cb59-7"></span>
<span id="cb59-8">    <span class="kw" style="color: #003B4F;">def</span> after_batch(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb59-9">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> learn.training: <span class="cf" style="color: #003B4F;">raise</span> CancelEpochException()</span>
<span id="cb59-10">        <span class="va" style="color: #111111;">self</span>.lrs.append(learn.opt.param_groups[<span class="dv" style="color: #AD0000;">0</span>][<span class="st" style="color: #20794D;">'lr'</span>])</span>
<span id="cb59-11">        loss <span class="op" style="color: #5E5E5E;">=</span> to_cpu(learn.loss)</span>
<span id="cb59-12">        <span class="va" style="color: #111111;">self</span>.losses.append(loss)</span>
<span id="cb59-13">        <span class="cf" style="color: #003B4F;">if</span> loss <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span>: <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span> <span class="op" style="color: #5E5E5E;">=</span> loss</span>
<span id="cb59-14">        <span class="cf" style="color: #003B4F;">if</span> loss <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">3</span>: <span class="cf" style="color: #003B4F;">raise</span> CancelFitException()</span>
<span id="cb59-15">        <span class="cf" style="color: #003B4F;">for</span> g <span class="kw" style="color: #003B4F;">in</span> learn.opt.param_groups: g[<span class="st" style="color: #20794D;">'lr'</span>] <span class="op" style="color: #5E5E5E;">*=</span> <span class="va" style="color: #111111;">self</span>.lr_mult</span></code></pre></div>
</div>
<div class="cell" data-outputid="0887173b-104f-4721-aec5-66a38879a6b4" data-execution_count="46">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">lrfind <span class="op" style="color: #5E5E5E;">=</span> LRFinderCB()</span>
<span id="cb60-2">cbs <span class="op" style="color: #5E5E5E;">=</span> [DeviceCB(), lrfind]</span>
<span id="cb60-3">learn <span class="op" style="color: #5E5E5E;">=</span> MomentumLearner(get_model(), dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-4</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs)</span>
<span id="cb60-4">learn.fit(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb60-5">plt.plot(lrfind.lrs, lrfind.losses)</span>
<span id="cb60-6">plt.xscale(<span class="st" style="color: #20794D;">'log'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index_files/figure-html/cell-47-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=47}</p>
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="im" style="color: #00769E;">from</span> torch.optim.lr_scheduler <span class="im" style="color: #00769E;">import</span> ExponentialLR</span></code></pre></div>
<p>:::</p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR">ExponentialLR</a></p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=48}</p>
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><span class="kw" style="color: #003B4F;">class</span> LRFinderCB(Callback):</span>
<span id="cb62-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, gamma<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.3</span>, max_mult<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>): fc.store_attr()</span>
<span id="cb62-3">    </span>
<span id="cb62-4">    <span class="kw" style="color: #003B4F;">def</span> before_fit(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb62-5">        <span class="va" style="color: #111111;">self</span>.sched <span class="op" style="color: #5E5E5E;">=</span> ExponentialLR(learn.opt, <span class="va" style="color: #111111;">self</span>.gamma)</span>
<span id="cb62-6">        <span class="va" style="color: #111111;">self</span>.lrs,<span class="va" style="color: #111111;">self</span>.losses <span class="op" style="color: #5E5E5E;">=</span> [],[]</span>
<span id="cb62-7">        <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span> <span class="op" style="color: #5E5E5E;">=</span> math.inf</span>
<span id="cb62-8"></span>
<span id="cb62-9">    <span class="kw" style="color: #003B4F;">def</span> after_batch(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb62-10">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> learn.training: <span class="cf" style="color: #003B4F;">raise</span> CancelEpochException()</span>
<span id="cb62-11">        <span class="va" style="color: #111111;">self</span>.lrs.append(learn.opt.param_groups[<span class="dv" style="color: #AD0000;">0</span>][<span class="st" style="color: #20794D;">'lr'</span>])</span>
<span id="cb62-12">        loss <span class="op" style="color: #5E5E5E;">=</span> to_cpu(learn.loss)</span>
<span id="cb62-13">        <span class="va" style="color: #111111;">self</span>.losses.append(loss)</span>
<span id="cb62-14">        <span class="cf" style="color: #003B4F;">if</span> loss <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span>: <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span> <span class="op" style="color: #5E5E5E;">=</span> loss</span>
<span id="cb62-15">        <span class="cf" style="color: #003B4F;">if</span> math.isnan(loss) <span class="kw" style="color: #003B4F;">or</span> (loss <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.<span class="bu" style="color: null;">min</span><span class="op" style="color: #5E5E5E;">*</span><span class="va" style="color: #111111;">self</span>.max_mult):</span>
<span id="cb62-16">            <span class="cf" style="color: #003B4F;">raise</span> CancelFitException()</span>
<span id="cb62-17">        <span class="va" style="color: #111111;">self</span>.sched.step()</span>
<span id="cb62-18"></span>
<span id="cb62-19">    <span class="kw" style="color: #003B4F;">def</span> cleanup_fit(<span class="va" style="color: #111111;">self</span>, learn):</span>
<span id="cb62-20">        plt.plot(<span class="va" style="color: #111111;">self</span>.lrs, <span class="va" style="color: #111111;">self</span>.losses)</span>
<span id="cb62-21">        plt.xscale(<span class="st" style="color: #20794D;">'log'</span>)</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="9624d555-6d3a-4e78-8017-d37016d853d3" data-execution_count="49">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">cbs <span class="op" style="color: #5E5E5E;">=</span> [DeviceCB()]</span>
<span id="cb63-2">learn <span class="op" style="color: #5E5E5E;">=</span> MomentumLearner(get_model(), dls, F.cross_entropy, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-5</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs)</span>
<span id="cb63-3">learn.fit(<span class="dv" style="color: #AD0000;">3</span>, cbs<span class="op" style="color: #5E5E5E;">=</span>LRFinderCB())</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index_files/figure-html/cell-50-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=50}</p>
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><span class="at" style="color: #657422;">@fc.patch</span></span>
<span id="cb64-2"><span class="kw" style="color: #003B4F;">def</span> lr_find(<span class="va" style="color: #111111;">self</span>:Learner, gamma<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.3</span>, max_mult<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, start_lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-5</span>, max_epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>):</span>
<span id="cb64-3">    <span class="va" style="color: #111111;">self</span>.fit(max_epochs, lr<span class="op" style="color: #5E5E5E;">=</span>start_lr, cbs<span class="op" style="color: #5E5E5E;">=</span>LRFinderCB(gamma<span class="op" style="color: #5E5E5E;">=</span>gamma, max_mult<span class="op" style="color: #5E5E5E;">=</span>max_mult))</span></code></pre></div>
<p>:::</p>
<p><code>lr_find</code> was added in lesson 18. It’s just a shorter way of using <code>LRFinderCB</code>.</p>
<div class="cell" data-outputid="bf26f919-9869-4956-cf9e-8d96dc160ee7" data-execution_count="51">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">MomentumLearner(get_model(), dls, F.cross_entropy, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs).lr_find()</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 11/index.html</guid>
  <pubDate>Mon, 03 Apr 2023 19:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 10</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index.html</link>
  <description><![CDATA[ 



<p>All credits goes to fast.ai All mistakes are mine. You should know and practice following after this blog post : 1- Convolution in concept and in coding</p>
<div class="cell" data-outputid="b852a311-62af-48e0-9630-080f71d445fd">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uqq git<span class="op" style="color: #5E5E5E;">+</span>https:<span class="op" style="color: #5E5E5E;">//</span>github.com<span class="op" style="color: #5E5E5E;">/</span>fastai<span class="op" style="color: #5E5E5E;">/</span>course22p2</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Preparing metadata (setup.py) ... done
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 3.2 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.4/158.4 kB 13.4 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.9/934.9 kB 41.1 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 3.1 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 69.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 8.2 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 9.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 9.8 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 8.4 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 13.4 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 48.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.9/87.9 kB 10.2 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 15.6 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 29.7 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 19.0 MB/s eta 0:00:00
  Building wheel for miniai (setup.py) ... done</code></pre>
</div>
</div>
<p>Hi, all, and welcome to lesson 15. And what we’re going to endeavor to do today is to create a convolutional auto encoder. And in the process, we will see why doing that well is a tricky thing to do. And time permitting, we will begin to work on a framework, a deep learning framework to make life a lot easier. Not sure how far we’ll get on that today. Time wise, let’s see how we go and get straight into it. So, okay, so today let’s start by talking before we can create a convolutional auto encoder we need to talk about convolutions and and what are they and what are they for.</p>
<p>Broadly speaking, convolutions are something that allows us to to tell our neural network a little bit about the structure of the problem. That’s going to make it a lot easier for it to solve the problem. And in particular, the structure of our problem is we’re doing things with images. Images are laid out on a grid, a 2D grid with black and white, or a 3 to 4 color or a 44 color video or whatever. And so we would say, you know, there’s a relationship between the pixels going across the pixels going down. They tend to be similar to each other. Differences in those pixels across those dimensions tend to have meaning . Patterns of pixels that appear in different places often represent the same thing. So, for example, a cat in the top left is still a cat, even if it’s in the bottom right. These kinds of this kind of prior information is something that is naturally captured by a convolutional neural network, something that is convolutions. Generally speaking, this is a good thing because it means that we will be able to use less parameters and less computation, and because more of that information about the problem solving is kind of encoded directly into our architecture.Maybe I should plug my other life her. Let me do that as well as how to get enough light. Okay. Let me say yes, there are other architectures that don’t encode that prior information as strongly, such as a multilayer perceptron, which we’ve been looking at so far, or a Transformers network, which we haven’t looked at yet.</p>
<p>Those kinds of architectures could potentially give us what they do, give us more flexibility and given enough time, compute and data, they could potentially find things that maybe CNN’s would struggle to find. So we’re not always going to use convolutional neural networks, but they’re pretty good starting point and certainly something important to understand. They’re not just used for images. We can also take advantage of one dimensional convolutions for language based tasks. For instance, the convolutions come up a lot. So in this notebook, one thing you’ll notice that might be of interest is we are importing stuff from miniai now. Now miniai is this little library that we’re starting to create and we’re creating nbdev. So we’ve got a miniai training and a mini A.I. data sets. And so if we look, for example, at the Datasets notebook, it starts with something that says that the default export module is called datasets, and some of the cells have a export directive on them. And at the very bottom we had something that called nbdev export. Now what that’s going to do is it’s going to create a file called dataset.py Just here, datasets.py why and it contains that’s those cells that we exported. And why does it why is it called miniai.datasets? That’s because everything miniai is stored in settings.mini and there’s something here. So create a library lib_name called miniai. I you can’t use this library until you install it. Now we haven’t uploaded it to 2 the pipi installable package from the public server, but you can actually install a local directory as if it’s a python module that you’ve kind of installed from the internet. And to do that you say pip install in the usual way, but you say minus e is set to editable and that means set up the current directory as a python module. Well, current directory actually any directory you like. I just put dot to be in the current directory and so you’ll see that’s going to go ahead and actually install all my library. And so after I’ve done that I can now import things from that library, as you say. Okay, so this is just the same as before. We’re going to grab Our MNIST data set and we’re going to create a convolutional neural network on it. So before we do that, we’re going to talk about what are convolutions. And one of my favorite descriptions of convolutions comes from the student in I think was our very first course, Matt Kleinsmith, who wrote this really nice medium article, CNN’s from Different Viewpoints. https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c</p>
<p>which I’m going to steal from. And here’s the basic idea. I say that this is our image. It’s a three by three image with nine pixels labeled from A-J as capital letters.Now a convolution uses something called a kernel, and a kernel is just another tensor. In this case, it’s a two by two matrix again. So it’s this one. We’re going to have Alpha, beta, gamma Delta as our four values in this convolution now in this kernel. Now one thing I mentioned I can’t remember I’ve said this before is the Greek letters are things that you want to be able to I think I have mentioned this. You want to be able to pronounce them. So if you don’t know how to read these and say what these names are, make sure you head over to Wikipedia or whatever and learn the names of all the Greek letters so that you can because they come up all the time. Okay, so what happens when we apply a convolution with this two by two kernel, two this three by three image? I mean, it doesn’t have to be an image. It’s in this case it’s just a rank two tensor, but it might represent an image. What happens is we take the kernel and we overlay it over the first. They don’t two by two separate like so. And specifically what we do is we match color the color. So the output of this first two by two overlay would be alpha times A plus, beta times B plus gamma times D plus delta times E, and that would yield some value P And that’s going to end up in the top left of a two by two output. So the top right of the two by two output, we’re going to slide. It’s like a slide in window. We’re going to slide our kernel over to here and apply each of our coefficients to these respectively colored squares. And then ditto for the bottom left and then ditto for the bottom right. So we end up with this equation. P, as we discussed, is Alpha A plus, beta B plus eight plus delta E plus some bias term. Q So the top right as you can say, it’s just alpha in this case times B And so we’re just multiplying them together and adding them up. Multiply together at the map, multiplied together and add them up. So we’re basically you can imagine that we’re basically flattening these out into rank one tensors into vectors. And then doing a dot product would be one way of thinking about what’s happening as we slide this kernel over these windows. And so this is called a convolution. So let’s try and create a convolution. So for example, let’s grab our training images and take a look at one and let’s create a three by three kernel. So remember, a kernel is just we’ve already got all appears has a lot of times in computer science and math. We’ve already seen the term kernel to mean a piece of code that we run on a year across lots of parallel kind of virtual devices or potentially in a grid. There’s a similar idea here. We’ve got a computation which is in this case kind of this dot product or something like a dot product, okay, sliding over occurring lots of times over a grid. But it’s yeah, it’s a bit different. So that’s kind of another use of the word kernel.</p>
<section id="convolutions" class="level1">
<h1>Convolutions</h1>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> default_collate</span>
<span id="cb3-5"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Mapping</span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="im" style="color: #00769E;">from</span> miniai.training <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb3-8"><span class="im" style="color: #00769E;">from</span> miniai.datasets <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
<p>:::</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">import</span> pickle,gzip,math,os,time,shutil,torch,matplotlib <span class="im" style="color: #00769E;">as</span> mpl, numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb4-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd,matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb4-4"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor,nn</span>
<span id="cb4-5"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb4-6"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader</span>
<span id="cb4-7"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Mapping</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span>
<span id="cb5-2"></span>
<span id="cb5-3">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb5-4">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb5-5">np.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb5-8">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb5-9">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb5-10">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb6-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<div class="cell" data-outputid="0903d0e9-fe4f-4385-e858-c5af78285165">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 May 21 12:38 mnist.pkl.gz</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span>
<span id="cb9-2">x_train, y_train, x_valid, y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, [x_train, y_train, x_valid, y_valid])</span>
<span id="cb9-3"></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span></code></pre></div>
</div>
<p>In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that.</p>
<p>It turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use a <em>convolution</em>. A convolution requires nothing more than multiplication, and addition.</p>
<section id="understanding-the-convolution-equations" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-convolution-equations">Understanding the Convolution Equations</h3>
<p>To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing <a href="https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c">CNNs from different viewpoints</a>.</p>
<p>Here’s the input:</p>
<p><img alt="The image" width="75" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00032.png?raw=1"></p>
<p>Here’s our kernel:</p>
<p><img alt="The kernel" width="55" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00033.png?raw=1"></p>
<p>Since the filter fits in the image four times, we have four results:</p>
<p><img alt="The activations" width="52" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00034.png?raw=1"></p>
<p><img alt="Applying the kernel" width="366" caption="Applying the kernel" id="apply_kernel" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00035.png?raw=1"></p>
<p><img alt="The equation" width="436" caption="The equation" id="eq_view" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00036.png?raw=1"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">x_imgs <span class="op" style="color: #5E5E5E;">=</span> x_train.view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>)</span>
<span id="cb11-2">xv_imgs <span class="op" style="color: #5E5E5E;">=</span> x_valid.view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">mpl.rcParams[<span class="st" style="color: #20794D;">'figure.dpi'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="f2ce2149-e88e-416c-927d-b1abb836111d">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">im3 <span class="op" style="color: #5E5E5E;">=</span> x_imgs[<span class="dv" style="color: #AD0000;">7</span>]</span>
<span id="cb13-2">show_image(im3)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">top_edge <span class="op" style="color: #5E5E5E;">=</span> tensor([[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb14-2">                   [ <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>],</span>
<span id="cb14-3">                   [ <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>]]).<span class="bu" style="color: null;">float</span>()</span></code></pre></div>
</div>
<p>We’re going to call this our kernel (because that’s what fancy computer vision researchers call these).</p>
<div class="cell" data-outputid="7de2403d-161c-4b6f-e41b-6fb91e2cb809">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">show_image(top_edge, noframe<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The filter will take any window of size 3×3 in our images, and if we name the pixel values like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bmatrix%7D%20a1%20&amp;%20a2%20&amp;%20a3%20%5C%5C%20a4%20&amp;%20a5%20&amp;%20a6%20%5C%5C%20a7%20&amp;%20a8%20&amp;%20a9%20%5Cend%7Bmatrix%7D"></p>
<p>it will return <img src="https://latex.codecogs.com/png.latex?-a1-a2-a3+a7+a8+a9">.</p>
<p>So in this case, a kernel is a in this case it’s going to be a rank two tensor. And so let’s create a kernel with these values in the three by three matrix rank, two tensor and we could draw what that looks like. Not surprisingly, this looks like a bunch of lines perhaps. Okay, so what would happen if we slide this over , nine pixels over this 28 by 28? Well, what’s going to happen is if we’ve got some the top left, for example, three by three section has these names, then we’re going to end up with negative a1 because the top three or negative, right. Negative a1, minus a2 minus a3 the next to exist zero. So that won’t do anything. And then plus a7 plus a8 plus a9. Why is that interesting? That’s interesting. Well, let’s try here. What I’ve done here is I’ve grabbed just the first 13 rows and first 23 columns of our image, and I’m actually showing the numbers and also using Gray kind of conditional formatting, if you like, or the equivalent in PANDAS to show this top bit. So we’re looking at just this top bit. So what happens if we take rows three, four and five? Remember, this is not inclusive, right? So it’s rows three, four and five columns 14,15 , 16 , So we’re looking at this these three here. What’s that going to give us if we multiply it by this kernel, it gives us a fairly large positive value because the three that we have negatives on is the top, right? Well, they’re all zero. And the three that we have positives on there or close to one. So we end up with quite a large number. What about the same columns but four rows 7,8,9 Here the top is all positive and the bottom is all zero. So that means that we’re going to get a lot of negative terms. And not surprisingly, that’s exactly what we see. This if we do this kind of dot product equivalent, which all you need a numpad to do, that is just an element. My multiplication followed by a sum, right? So that’s going to be a quite a large negative number. And so perhaps you’re saying what this is doing and maybe you got a hint from the name of the tensor. So we created it, something that is going to find the top edge. All right. So this one is a top edge. So it’s a positive and this one is a bottom edge. So it’s a negative. So we would like to apply that this kernel to every single three by three section in here. So we could do that by creating a little apply kernel function that takes some particular row in some particular column and some particular tensor as a kernel. And does that multiplication that some that we just saw. So for example, we could replicate this one by calling apply kernel and this here is the center of that three by three grid area.</p>
<div class="cell" data-outputid="5b49405f-3010-47e1-90b6-5330c0c6599b">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(im3[:<span class="dv" style="color: #AD0000;">13</span>,:<span class="dv" style="color: #AD0000;">23</span>])</span>
<span id="cb16-2">df.style.<span class="bu" style="color: null;">format</span>(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>).set_properties(<span class="op" style="color: #5E5E5E;">**</span>{<span class="st" style="color: #20794D;">'font-size'</span>:<span class="st" style="color: #20794D;">'7pt'</span>}).background_gradient(<span class="st" style="color: #20794D;">'Greys'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">


<table id="T_8149e" class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_8149e_level0_col0" class="col_heading level0 col0">0</th>
      <th id="T_8149e_level0_col1" class="col_heading level0 col1">1</th>
      <th id="T_8149e_level0_col2" class="col_heading level0 col2">2</th>
      <th id="T_8149e_level0_col3" class="col_heading level0 col3">3</th>
      <th id="T_8149e_level0_col4" class="col_heading level0 col4">4</th>
      <th id="T_8149e_level0_col5" class="col_heading level0 col5">5</th>
      <th id="T_8149e_level0_col6" class="col_heading level0 col6">6</th>
      <th id="T_8149e_level0_col7" class="col_heading level0 col7">7</th>
      <th id="T_8149e_level0_col8" class="col_heading level0 col8">8</th>
      <th id="T_8149e_level0_col9" class="col_heading level0 col9">9</th>
      <th id="T_8149e_level0_col10" class="col_heading level0 col10">10</th>
      <th id="T_8149e_level0_col11" class="col_heading level0 col11">11</th>
      <th id="T_8149e_level0_col12" class="col_heading level0 col12">12</th>
      <th id="T_8149e_level0_col13" class="col_heading level0 col13">13</th>
      <th id="T_8149e_level0_col14" class="col_heading level0 col14">14</th>
      <th id="T_8149e_level0_col15" class="col_heading level0 col15">15</th>
      <th id="T_8149e_level0_col16" class="col_heading level0 col16">16</th>
      <th id="T_8149e_level0_col17" class="col_heading level0 col17">17</th>
      <th id="T_8149e_level0_col18" class="col_heading level0 col18">18</th>
      <th id="T_8149e_level0_col19" class="col_heading level0 col19">19</th>
      <th id="T_8149e_level0_col20" class="col_heading level0 col20">20</th>
      <th id="T_8149e_level0_col21" class="col_heading level0 col21">21</th>
      <th id="T_8149e_level0_col22" class="col_heading level0 col22">22</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_8149e_level0_row0" class="row_heading level0 row0">0</th>
      <td id="T_8149e_row0_col0" class="data row0 col0">0.00</td>
      <td id="T_8149e_row0_col1" class="data row0 col1">0.00</td>
      <td id="T_8149e_row0_col2" class="data row0 col2">0.00</td>
      <td id="T_8149e_row0_col3" class="data row0 col3">0.00</td>
      <td id="T_8149e_row0_col4" class="data row0 col4">0.00</td>
      <td id="T_8149e_row0_col5" class="data row0 col5">0.00</td>
      <td id="T_8149e_row0_col6" class="data row0 col6">0.00</td>
      <td id="T_8149e_row0_col7" class="data row0 col7">0.00</td>
      <td id="T_8149e_row0_col8" class="data row0 col8">0.00</td>
      <td id="T_8149e_row0_col9" class="data row0 col9">0.00</td>
      <td id="T_8149e_row0_col10" class="data row0 col10">0.00</td>
      <td id="T_8149e_row0_col11" class="data row0 col11">0.00</td>
      <td id="T_8149e_row0_col12" class="data row0 col12">0.00</td>
      <td id="T_8149e_row0_col13" class="data row0 col13">0.00</td>
      <td id="T_8149e_row0_col14" class="data row0 col14">0.00</td>
      <td id="T_8149e_row0_col15" class="data row0 col15">0.00</td>
      <td id="T_8149e_row0_col16" class="data row0 col16">0.00</td>
      <td id="T_8149e_row0_col17" class="data row0 col17">0.00</td>
      <td id="T_8149e_row0_col18" class="data row0 col18">0.00</td>
      <td id="T_8149e_row0_col19" class="data row0 col19">0.00</td>
      <td id="T_8149e_row0_col20" class="data row0 col20">0.00</td>
      <td id="T_8149e_row0_col21" class="data row0 col21">0.00</td>
      <td id="T_8149e_row0_col22" class="data row0 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row1" class="row_heading level0 row1">1</th>
      <td id="T_8149e_row1_col0" class="data row1 col0">0.00</td>
      <td id="T_8149e_row1_col1" class="data row1 col1">0.00</td>
      <td id="T_8149e_row1_col2" class="data row1 col2">0.00</td>
      <td id="T_8149e_row1_col3" class="data row1 col3">0.00</td>
      <td id="T_8149e_row1_col4" class="data row1 col4">0.00</td>
      <td id="T_8149e_row1_col5" class="data row1 col5">0.00</td>
      <td id="T_8149e_row1_col6" class="data row1 col6">0.00</td>
      <td id="T_8149e_row1_col7" class="data row1 col7">0.00</td>
      <td id="T_8149e_row1_col8" class="data row1 col8">0.00</td>
      <td id="T_8149e_row1_col9" class="data row1 col9">0.00</td>
      <td id="T_8149e_row1_col10" class="data row1 col10">0.00</td>
      <td id="T_8149e_row1_col11" class="data row1 col11">0.00</td>
      <td id="T_8149e_row1_col12" class="data row1 col12">0.00</td>
      <td id="T_8149e_row1_col13" class="data row1 col13">0.00</td>
      <td id="T_8149e_row1_col14" class="data row1 col14">0.00</td>
      <td id="T_8149e_row1_col15" class="data row1 col15">0.00</td>
      <td id="T_8149e_row1_col16" class="data row1 col16">0.00</td>
      <td id="T_8149e_row1_col17" class="data row1 col17">0.00</td>
      <td id="T_8149e_row1_col18" class="data row1 col18">0.00</td>
      <td id="T_8149e_row1_col19" class="data row1 col19">0.00</td>
      <td id="T_8149e_row1_col20" class="data row1 col20">0.00</td>
      <td id="T_8149e_row1_col21" class="data row1 col21">0.00</td>
      <td id="T_8149e_row1_col22" class="data row1 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row2" class="row_heading level0 row2">2</th>
      <td id="T_8149e_row2_col0" class="data row2 col0">0.00</td>
      <td id="T_8149e_row2_col1" class="data row2 col1">0.00</td>
      <td id="T_8149e_row2_col2" class="data row2 col2">0.00</td>
      <td id="T_8149e_row2_col3" class="data row2 col3">0.00</td>
      <td id="T_8149e_row2_col4" class="data row2 col4">0.00</td>
      <td id="T_8149e_row2_col5" class="data row2 col5">0.00</td>
      <td id="T_8149e_row2_col6" class="data row2 col6">0.00</td>
      <td id="T_8149e_row2_col7" class="data row2 col7">0.00</td>
      <td id="T_8149e_row2_col8" class="data row2 col8">0.00</td>
      <td id="T_8149e_row2_col9" class="data row2 col9">0.00</td>
      <td id="T_8149e_row2_col10" class="data row2 col10">0.00</td>
      <td id="T_8149e_row2_col11" class="data row2 col11">0.00</td>
      <td id="T_8149e_row2_col12" class="data row2 col12">0.00</td>
      <td id="T_8149e_row2_col13" class="data row2 col13">0.00</td>
      <td id="T_8149e_row2_col14" class="data row2 col14">0.00</td>
      <td id="T_8149e_row2_col15" class="data row2 col15">0.00</td>
      <td id="T_8149e_row2_col16" class="data row2 col16">0.00</td>
      <td id="T_8149e_row2_col17" class="data row2 col17">0.00</td>
      <td id="T_8149e_row2_col18" class="data row2 col18">0.00</td>
      <td id="T_8149e_row2_col19" class="data row2 col19">0.00</td>
      <td id="T_8149e_row2_col20" class="data row2 col20">0.00</td>
      <td id="T_8149e_row2_col21" class="data row2 col21">0.00</td>
      <td id="T_8149e_row2_col22" class="data row2 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row3" class="row_heading level0 row3">3</th>
      <td id="T_8149e_row3_col0" class="data row3 col0">0.00</td>
      <td id="T_8149e_row3_col1" class="data row3 col1">0.00</td>
      <td id="T_8149e_row3_col2" class="data row3 col2">0.00</td>
      <td id="T_8149e_row3_col3" class="data row3 col3">0.00</td>
      <td id="T_8149e_row3_col4" class="data row3 col4">0.00</td>
      <td id="T_8149e_row3_col5" class="data row3 col5">0.00</td>
      <td id="T_8149e_row3_col6" class="data row3 col6">0.00</td>
      <td id="T_8149e_row3_col7" class="data row3 col7">0.00</td>
      <td id="T_8149e_row3_col8" class="data row3 col8">0.00</td>
      <td id="T_8149e_row3_col9" class="data row3 col9">0.00</td>
      <td id="T_8149e_row3_col10" class="data row3 col10">0.00</td>
      <td id="T_8149e_row3_col11" class="data row3 col11">0.00</td>
      <td id="T_8149e_row3_col12" class="data row3 col12">0.00</td>
      <td id="T_8149e_row3_col13" class="data row3 col13">0.00</td>
      <td id="T_8149e_row3_col14" class="data row3 col14">0.00</td>
      <td id="T_8149e_row3_col15" class="data row3 col15">0.00</td>
      <td id="T_8149e_row3_col16" class="data row3 col16">0.00</td>
      <td id="T_8149e_row3_col17" class="data row3 col17">0.00</td>
      <td id="T_8149e_row3_col18" class="data row3 col18">0.00</td>
      <td id="T_8149e_row3_col19" class="data row3 col19">0.00</td>
      <td id="T_8149e_row3_col20" class="data row3 col20">0.00</td>
      <td id="T_8149e_row3_col21" class="data row3 col21">0.00</td>
      <td id="T_8149e_row3_col22" class="data row3 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row4" class="row_heading level0 row4">4</th>
      <td id="T_8149e_row4_col0" class="data row4 col0">0.00</td>
      <td id="T_8149e_row4_col1" class="data row4 col1">0.00</td>
      <td id="T_8149e_row4_col2" class="data row4 col2">0.00</td>
      <td id="T_8149e_row4_col3" class="data row4 col3">0.00</td>
      <td id="T_8149e_row4_col4" class="data row4 col4">0.00</td>
      <td id="T_8149e_row4_col5" class="data row4 col5">0.00</td>
      <td id="T_8149e_row4_col6" class="data row4 col6">0.00</td>
      <td id="T_8149e_row4_col7" class="data row4 col7">0.00</td>
      <td id="T_8149e_row4_col8" class="data row4 col8">0.00</td>
      <td id="T_8149e_row4_col9" class="data row4 col9">0.00</td>
      <td id="T_8149e_row4_col10" class="data row4 col10">0.00</td>
      <td id="T_8149e_row4_col11" class="data row4 col11">0.00</td>
      <td id="T_8149e_row4_col12" class="data row4 col12">0.00</td>
      <td id="T_8149e_row4_col13" class="data row4 col13">0.00</td>
      <td id="T_8149e_row4_col14" class="data row4 col14">0.00</td>
      <td id="T_8149e_row4_col15" class="data row4 col15">0.00</td>
      <td id="T_8149e_row4_col16" class="data row4 col16">0.00</td>
      <td id="T_8149e_row4_col17" class="data row4 col17">0.00</td>
      <td id="T_8149e_row4_col18" class="data row4 col18">0.00</td>
      <td id="T_8149e_row4_col19" class="data row4 col19">0.00</td>
      <td id="T_8149e_row4_col20" class="data row4 col20">0.00</td>
      <td id="T_8149e_row4_col21" class="data row4 col21">0.00</td>
      <td id="T_8149e_row4_col22" class="data row4 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row5" class="row_heading level0 row5">5</th>
      <td id="T_8149e_row5_col0" class="data row5 col0">0.00</td>
      <td id="T_8149e_row5_col1" class="data row5 col1">0.00</td>
      <td id="T_8149e_row5_col2" class="data row5 col2">0.00</td>
      <td id="T_8149e_row5_col3" class="data row5 col3">0.00</td>
      <td id="T_8149e_row5_col4" class="data row5 col4">0.00</td>
      <td id="T_8149e_row5_col5" class="data row5 col5">0.00</td>
      <td id="T_8149e_row5_col6" class="data row5 col6">0.00</td>
      <td id="T_8149e_row5_col7" class="data row5 col7">0.00</td>
      <td id="T_8149e_row5_col8" class="data row5 col8">0.00</td>
      <td id="T_8149e_row5_col9" class="data row5 col9">0.00</td>
      <td id="T_8149e_row5_col10" class="data row5 col10">0.00</td>
      <td id="T_8149e_row5_col11" class="data row5 col11">0.15</td>
      <td id="T_8149e_row5_col12" class="data row5 col12">0.17</td>
      <td id="T_8149e_row5_col13" class="data row5 col13">0.41</td>
      <td id="T_8149e_row5_col14" class="data row5 col14">1.00</td>
      <td id="T_8149e_row5_col15" class="data row5 col15">0.99</td>
      <td id="T_8149e_row5_col16" class="data row5 col16">0.99</td>
      <td id="T_8149e_row5_col17" class="data row5 col17">0.99</td>
      <td id="T_8149e_row5_col18" class="data row5 col18">0.99</td>
      <td id="T_8149e_row5_col19" class="data row5 col19">0.99</td>
      <td id="T_8149e_row5_col20" class="data row5 col20">0.68</td>
      <td id="T_8149e_row5_col21" class="data row5 col21">0.02</td>
      <td id="T_8149e_row5_col22" class="data row5 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row6" class="row_heading level0 row6">6</th>
      <td id="T_8149e_row6_col0" class="data row6 col0">0.00</td>
      <td id="T_8149e_row6_col1" class="data row6 col1">0.00</td>
      <td id="T_8149e_row6_col2" class="data row6 col2">0.00</td>
      <td id="T_8149e_row6_col3" class="data row6 col3">0.00</td>
      <td id="T_8149e_row6_col4" class="data row6 col4">0.00</td>
      <td id="T_8149e_row6_col5" class="data row6 col5">0.00</td>
      <td id="T_8149e_row6_col6" class="data row6 col6">0.00</td>
      <td id="T_8149e_row6_col7" class="data row6 col7">0.00</td>
      <td id="T_8149e_row6_col8" class="data row6 col8">0.00</td>
      <td id="T_8149e_row6_col9" class="data row6 col9">0.17</td>
      <td id="T_8149e_row6_col10" class="data row6 col10">0.54</td>
      <td id="T_8149e_row6_col11" class="data row6 col11">0.88</td>
      <td id="T_8149e_row6_col12" class="data row6 col12">0.88</td>
      <td id="T_8149e_row6_col13" class="data row6 col13">0.98</td>
      <td id="T_8149e_row6_col14" class="data row6 col14">0.99</td>
      <td id="T_8149e_row6_col15" class="data row6 col15">0.98</td>
      <td id="T_8149e_row6_col16" class="data row6 col16">0.98</td>
      <td id="T_8149e_row6_col17" class="data row6 col17">0.98</td>
      <td id="T_8149e_row6_col18" class="data row6 col18">0.98</td>
      <td id="T_8149e_row6_col19" class="data row6 col19">0.98</td>
      <td id="T_8149e_row6_col20" class="data row6 col20">0.98</td>
      <td id="T_8149e_row6_col21" class="data row6 col21">0.62</td>
      <td id="T_8149e_row6_col22" class="data row6 col22">0.05</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row7" class="row_heading level0 row7">7</th>
      <td id="T_8149e_row7_col0" class="data row7 col0">0.00</td>
      <td id="T_8149e_row7_col1" class="data row7 col1">0.00</td>
      <td id="T_8149e_row7_col2" class="data row7 col2">0.00</td>
      <td id="T_8149e_row7_col3" class="data row7 col3">0.00</td>
      <td id="T_8149e_row7_col4" class="data row7 col4">0.00</td>
      <td id="T_8149e_row7_col5" class="data row7 col5">0.00</td>
      <td id="T_8149e_row7_col6" class="data row7 col6">0.00</td>
      <td id="T_8149e_row7_col7" class="data row7 col7">0.00</td>
      <td id="T_8149e_row7_col8" class="data row7 col8">0.00</td>
      <td id="T_8149e_row7_col9" class="data row7 col9">0.70</td>
      <td id="T_8149e_row7_col10" class="data row7 col10">0.98</td>
      <td id="T_8149e_row7_col11" class="data row7 col11">0.98</td>
      <td id="T_8149e_row7_col12" class="data row7 col12">0.98</td>
      <td id="T_8149e_row7_col13" class="data row7 col13">0.98</td>
      <td id="T_8149e_row7_col14" class="data row7 col14">0.99</td>
      <td id="T_8149e_row7_col15" class="data row7 col15">0.98</td>
      <td id="T_8149e_row7_col16" class="data row7 col16">0.98</td>
      <td id="T_8149e_row7_col17" class="data row7 col17">0.98</td>
      <td id="T_8149e_row7_col18" class="data row7 col18">0.98</td>
      <td id="T_8149e_row7_col19" class="data row7 col19">0.98</td>
      <td id="T_8149e_row7_col20" class="data row7 col20">0.98</td>
      <td id="T_8149e_row7_col21" class="data row7 col21">0.98</td>
      <td id="T_8149e_row7_col22" class="data row7 col22">0.23</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row8" class="row_heading level0 row8">8</th>
      <td id="T_8149e_row8_col0" class="data row8 col0">0.00</td>
      <td id="T_8149e_row8_col1" class="data row8 col1">0.00</td>
      <td id="T_8149e_row8_col2" class="data row8 col2">0.00</td>
      <td id="T_8149e_row8_col3" class="data row8 col3">0.00</td>
      <td id="T_8149e_row8_col4" class="data row8 col4">0.00</td>
      <td id="T_8149e_row8_col5" class="data row8 col5">0.00</td>
      <td id="T_8149e_row8_col6" class="data row8 col6">0.00</td>
      <td id="T_8149e_row8_col7" class="data row8 col7">0.00</td>
      <td id="T_8149e_row8_col8" class="data row8 col8">0.00</td>
      <td id="T_8149e_row8_col9" class="data row8 col9">0.43</td>
      <td id="T_8149e_row8_col10" class="data row8 col10">0.98</td>
      <td id="T_8149e_row8_col11" class="data row8 col11">0.98</td>
      <td id="T_8149e_row8_col12" class="data row8 col12">0.90</td>
      <td id="T_8149e_row8_col13" class="data row8 col13">0.52</td>
      <td id="T_8149e_row8_col14" class="data row8 col14">0.52</td>
      <td id="T_8149e_row8_col15" class="data row8 col15">0.52</td>
      <td id="T_8149e_row8_col16" class="data row8 col16">0.52</td>
      <td id="T_8149e_row8_col17" class="data row8 col17">0.74</td>
      <td id="T_8149e_row8_col18" class="data row8 col18">0.98</td>
      <td id="T_8149e_row8_col19" class="data row8 col19">0.98</td>
      <td id="T_8149e_row8_col20" class="data row8 col20">0.98</td>
      <td id="T_8149e_row8_col21" class="data row8 col21">0.98</td>
      <td id="T_8149e_row8_col22" class="data row8 col22">0.23</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row9" class="row_heading level0 row9">9</th>
      <td id="T_8149e_row9_col0" class="data row9 col0">0.00</td>
      <td id="T_8149e_row9_col1" class="data row9 col1">0.00</td>
      <td id="T_8149e_row9_col2" class="data row9 col2">0.00</td>
      <td id="T_8149e_row9_col3" class="data row9 col3">0.00</td>
      <td id="T_8149e_row9_col4" class="data row9 col4">0.00</td>
      <td id="T_8149e_row9_col5" class="data row9 col5">0.00</td>
      <td id="T_8149e_row9_col6" class="data row9 col6">0.00</td>
      <td id="T_8149e_row9_col7" class="data row9 col7">0.00</td>
      <td id="T_8149e_row9_col8" class="data row9 col8">0.00</td>
      <td id="T_8149e_row9_col9" class="data row9 col9">0.02</td>
      <td id="T_8149e_row9_col10" class="data row9 col10">0.11</td>
      <td id="T_8149e_row9_col11" class="data row9 col11">0.11</td>
      <td id="T_8149e_row9_col12" class="data row9 col12">0.09</td>
      <td id="T_8149e_row9_col13" class="data row9 col13">0.00</td>
      <td id="T_8149e_row9_col14" class="data row9 col14">0.00</td>
      <td id="T_8149e_row9_col15" class="data row9 col15">0.00</td>
      <td id="T_8149e_row9_col16" class="data row9 col16">0.00</td>
      <td id="T_8149e_row9_col17" class="data row9 col17">0.05</td>
      <td id="T_8149e_row9_col18" class="data row9 col18">0.88</td>
      <td id="T_8149e_row9_col19" class="data row9 col19">0.98</td>
      <td id="T_8149e_row9_col20" class="data row9 col20">0.98</td>
      <td id="T_8149e_row9_col21" class="data row9 col21">0.67</td>
      <td id="T_8149e_row9_col22" class="data row9 col22">0.03</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row10" class="row_heading level0 row10">10</th>
      <td id="T_8149e_row10_col0" class="data row10 col0">0.00</td>
      <td id="T_8149e_row10_col1" class="data row10 col1">0.00</td>
      <td id="T_8149e_row10_col2" class="data row10 col2">0.00</td>
      <td id="T_8149e_row10_col3" class="data row10 col3">0.00</td>
      <td id="T_8149e_row10_col4" class="data row10 col4">0.00</td>
      <td id="T_8149e_row10_col5" class="data row10 col5">0.00</td>
      <td id="T_8149e_row10_col6" class="data row10 col6">0.00</td>
      <td id="T_8149e_row10_col7" class="data row10 col7">0.00</td>
      <td id="T_8149e_row10_col8" class="data row10 col8">0.00</td>
      <td id="T_8149e_row10_col9" class="data row10 col9">0.00</td>
      <td id="T_8149e_row10_col10" class="data row10 col10">0.00</td>
      <td id="T_8149e_row10_col11" class="data row10 col11">0.00</td>
      <td id="T_8149e_row10_col12" class="data row10 col12">0.00</td>
      <td id="T_8149e_row10_col13" class="data row10 col13">0.00</td>
      <td id="T_8149e_row10_col14" class="data row10 col14">0.00</td>
      <td id="T_8149e_row10_col15" class="data row10 col15">0.00</td>
      <td id="T_8149e_row10_col16" class="data row10 col16">0.00</td>
      <td id="T_8149e_row10_col17" class="data row10 col17">0.33</td>
      <td id="T_8149e_row10_col18" class="data row10 col18">0.95</td>
      <td id="T_8149e_row10_col19" class="data row10 col19">0.98</td>
      <td id="T_8149e_row10_col20" class="data row10 col20">0.98</td>
      <td id="T_8149e_row10_col21" class="data row10 col21">0.56</td>
      <td id="T_8149e_row10_col22" class="data row10 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row11" class="row_heading level0 row11">11</th>
      <td id="T_8149e_row11_col0" class="data row11 col0">0.00</td>
      <td id="T_8149e_row11_col1" class="data row11 col1">0.00</td>
      <td id="T_8149e_row11_col2" class="data row11 col2">0.00</td>
      <td id="T_8149e_row11_col3" class="data row11 col3">0.00</td>
      <td id="T_8149e_row11_col4" class="data row11 col4">0.00</td>
      <td id="T_8149e_row11_col5" class="data row11 col5">0.00</td>
      <td id="T_8149e_row11_col6" class="data row11 col6">0.00</td>
      <td id="T_8149e_row11_col7" class="data row11 col7">0.00</td>
      <td id="T_8149e_row11_col8" class="data row11 col8">0.00</td>
      <td id="T_8149e_row11_col9" class="data row11 col9">0.00</td>
      <td id="T_8149e_row11_col10" class="data row11 col10">0.00</td>
      <td id="T_8149e_row11_col11" class="data row11 col11">0.00</td>
      <td id="T_8149e_row11_col12" class="data row11 col12">0.00</td>
      <td id="T_8149e_row11_col13" class="data row11 col13">0.00</td>
      <td id="T_8149e_row11_col14" class="data row11 col14">0.00</td>
      <td id="T_8149e_row11_col15" class="data row11 col15">0.00</td>
      <td id="T_8149e_row11_col16" class="data row11 col16">0.34</td>
      <td id="T_8149e_row11_col17" class="data row11 col17">0.74</td>
      <td id="T_8149e_row11_col18" class="data row11 col18">0.98</td>
      <td id="T_8149e_row11_col19" class="data row11 col19">0.98</td>
      <td id="T_8149e_row11_col20" class="data row11 col20">0.98</td>
      <td id="T_8149e_row11_col21" class="data row11 col21">0.05</td>
      <td id="T_8149e_row11_col22" class="data row11 col22">0.00</td>
    </tr>
    <tr>
      <th id="T_8149e_level0_row12" class="row_heading level0 row12">12</th>
      <td id="T_8149e_row12_col0" class="data row12 col0">0.00</td>
      <td id="T_8149e_row12_col1" class="data row12 col1">0.00</td>
      <td id="T_8149e_row12_col2" class="data row12 col2">0.00</td>
      <td id="T_8149e_row12_col3" class="data row12 col3">0.00</td>
      <td id="T_8149e_row12_col4" class="data row12 col4">0.00</td>
      <td id="T_8149e_row12_col5" class="data row12 col5">0.00</td>
      <td id="T_8149e_row12_col6" class="data row12 col6">0.00</td>
      <td id="T_8149e_row12_col7" class="data row12 col7">0.00</td>
      <td id="T_8149e_row12_col8" class="data row12 col8">0.00</td>
      <td id="T_8149e_row12_col9" class="data row12 col9">0.00</td>
      <td id="T_8149e_row12_col10" class="data row12 col10">0.00</td>
      <td id="T_8149e_row12_col11" class="data row12 col11">0.00</td>
      <td id="T_8149e_row12_col12" class="data row12 col12">0.00</td>
      <td id="T_8149e_row12_col13" class="data row12 col13">0.00</td>
      <td id="T_8149e_row12_col14" class="data row12 col14">0.36</td>
      <td id="T_8149e_row12_col15" class="data row12 col15">0.83</td>
      <td id="T_8149e_row12_col16" class="data row12 col16">0.96</td>
      <td id="T_8149e_row12_col17" class="data row12 col17">0.98</td>
      <td id="T_8149e_row12_col18" class="data row12 col18">0.98</td>
      <td id="T_8149e_row12_col19" class="data row12 col19">0.98</td>
      <td id="T_8149e_row12_col20" class="data row12 col20">0.80</td>
      <td id="T_8149e_row12_col21" class="data row12 col21">0.04</td>
      <td id="T_8149e_row12_col22" class="data row12 col22">0.00</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell" data-outputid="e38e70ad-d9af-4fae-c273-767644bd22a6">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">(im3[<span class="dv" style="color: #AD0000;">3</span>:<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">14</span>:<span class="dv" style="color: #AD0000;">17</span>] <span class="op" style="color: #5E5E5E;">*</span> top_edge).<span class="bu" style="color: null;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>tensor(2.97)</code></pre>
</div>
</div>
<div class="cell" data-outputid="f793a80c-0f3b-4a61-98a8-7af3a5f4222d">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">(im3[<span class="dv" style="color: #AD0000;">7</span>:<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">14</span>:<span class="dv" style="color: #AD0000;">17</span>] <span class="op" style="color: #5E5E5E;">*</span> top_edge).<span class="bu" style="color: null;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>tensor(-2.96)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;">def</span> apply_kernel(row, col, kernel): <span class="cf" style="color: #003B4F;">return</span> (im3[row<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>:row<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span>,col<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>:col<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span>] <span class="op" style="color: #5E5E5E;">*</span> kernel).<span class="bu" style="color: null;">sum</span>()</span></code></pre></div>
</div>
<p>So now we could apply that kernel to every one of the three by three windows in this 28 by 28 image. So we’re going to be sliding over like this red bit sliding over here, but we’ve actually got a 28 by 28 input, not just a five by five input. So to get all of the coordinates, let’s just simplify it. To do this five by five, we can go, we can create a list comprehension. We can take i through every value in range five and then for each of those we can take a j for every value in range Five. And so if we just look at that tuple, you can say we get a list of lists containing all of those coordinates. So this is a list comprehension in list comprehension which when you first say it may be surprising or confusing, but it’s a really helpful idiom and I certainly recommend getting used to it. Now, what we’re going to do is we’re not just going to create this tuple, but we’re actually going to call applay kernel for each of those. So if we go through from 1 to 27, we’ll actually 1 to 26 because 27 is exclusive. So we’re going to go through everything from 1 to 26 and then for each of those go through from 1 to 26 again and call apply kernel and that’s going to give us the result of applying that convolutional kernel to every one of those coordinates. And there’s a result and you can see what it’s done as we hoped is it is highlighting the top edges. So yeah, you might find that kind of surprising that it’s that that easy to do this kind of image processing. We’re literally just doing an element wise multiplication and a sum for each window. Okay. So,that is called a convolution. So we can do another convolution. This time we could do one with the left edge tensor.As You can see looks just a rotated version or transpose version I guess, of our top edge tensor. Here’s what it looks like. And so if we apply that kernel, so this time we’re going to apply the left edge kernel and so notice here that we’re actually passing in a function, right? We’re passing in a tensor , It’s just a it’s just a tensor actually. So we’re going to pass in the left edge tensor for the same list Comprehension in a list comprehension. And this time we’re getting back on the left edge as it’s highlighting all of the left edges in the digit. So yeah, this is basically what’s happening here is that a two by two can be looped over an image creating these outputs. Now you’ll see here that in the process of doing so, we are losing the outermost pixels of our image. We’ll learn about how to fix that later, but just for now, notice that that as we are putting in our three by three through for example, in this five by five, there’s only 1 two 3 places that we can put it going across, not five places, because we need some kind of edge.</p>
<div class="cell" data-outputid="1f5d3cfb-b523-4396-e0d6-b0cbea5c472a">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">apply_kernel(<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">15</span>,top_edge)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(2.97)</code></pre>
</div>
</div>
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/chapter9_nopadconv.svg?raw=1" id="nopad_conv" caption="Applying a kernel across a grid" alt="Applying a kernel across a grid" width="400"></p>
<div class="cell" data-outputid="cec6479b-2895-4325-ed96-f28b42f64db0">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">[[(i,j) <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>)] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>)]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>[[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)],
 [(1, 0), (1, 1), (1, 2), (1, 3), (1, 4)],
 [(2, 0), (2, 1), (2, 2), (2, 3), (2, 4)],
 [(3, 0), (3, 1), (3, 2), (3, 3), (3, 4)],
 [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]]</code></pre>
</div>
</div>
<div class="cell" data-outputid="b35d1be3-2cf4-4576-b2b3-1387212dfed0">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">rng <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">27</span>)</span>
<span id="cb26-2">top_edge3 <span class="op" style="color: #5E5E5E;">=</span> tensor([[apply_kernel(i,j,top_edge) <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> rng] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> rng])</span>
<span id="cb26-3">show_image(top_edge3)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">left_edge <span class="op" style="color: #5E5E5E;">=</span> tensor([[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb27-2">                    [<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb27-3">                    [<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>]]).<span class="bu" style="color: null;">float</span>()</span></code></pre></div>
</div>
<div class="cell" data-outputid="8aae7015-2063-46db-efba-bbcef32a152c">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">show_image(left_edge, noframe<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="4e2d99e4-7bec-4036-a982-ce3a47e6a382">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">left_edge3 <span class="op" style="color: #5E5E5E;">=</span> tensor([[apply_kernel(i,j,left_edge) <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> rng] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> rng])</span>
<span id="cb29-2">show_image(left_edge3)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><img alt="Result of applying a 3×3 kernel to a 4×4 image" width="782" caption="Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent Dumoulin and Francesco Visin)" id="three_ex_four_conv" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00028.png?raw=1"></p>
<p>Oops. Somebody pointed out that this is not quite the same. It should be. It should be moved. Now, check this out. If you had an option or alt as you highlight, you can copy, you can copy and paste whole columns at once. Isn’t that cool? Yeah. We go. All right. It doesn’t change the output match, of course, but it’s nice to have a consistent. All right, So that’s cool. That’s a convolution. And hopefully if you remember back to kind of the Zeiler and Fergus pictures from from lesson one, you might recognize that the kind of first layer of a convolutional network is often looking for kind of edges and gradients and things like that. And this is how, this is how it does it. And then the convolutions on top of convolutions with nonlinear activations between them can combine those into into curves or corners or stuff like that and so on and so forth. Okay, so how do we do this quickly? Because currently this is going to be super, super slow during this in Python. So one of the very earliest or probably the earliest publicly available general purpose deep learning GPU accelerator tape running thing I saw it was called Cafe that was created by somebody called Young Teenager and he actually described what happened where where cafe how cafe went about implementing a fast conversation on a GPU. And basically he said, well I had two months to do it and I had to finish my thesis. And so I ended up doing something where I said, Well, there was some other code out there. Could I ask who you might have come across him and, and Hinton set up a little start up which, which Google bought and that kind of became the start of Google’s deep learning, the Google brain. Basically. Codrescu had all this fancy stuff in in his library, but Yuncheng , Oh, I didn’t know how to do all that stuff. So I said, Well, I already know how to multiply matrices, so maybe I can convert a convolution into a matrix multiplication. And so that I became known as im2col,</p>
<p>im2col a way of converting a convolution into a matrix. Multiply. And so actually, I don’t know if a suspect young teenager kind of accidentally reinvented it because it actually had been around for a while, even at the point that he was writing his thesis. I believe. So it was actually this is this is the place I believe it was created in this paper. https://hal.inria.fr/inria-00112631/document</p>
<p>So that was in 2006, which is a while ago. And so this is actually from that paper. And what they describe is, is let’s say you are putting this two by two kernel over this, three by three bit of an image. So here you’ve got this this window needs to match to this part of this window. Right. What you could do is you could unwrap this to 1 2,1 2.</p>
<p>unroll to here. One, two, one, two. To unroll it like so and you could unroll the kernel here. Yeah. So this is one, two, one one. So this is bit is here. One, two, one one and then you can unroll the kernel one, one, two, two, to here. One, one, two, two. And then once they, they’ve been moved, flattened out and moved in that way and then you’re doing exactly the same thing for this next patch here.2, 0 , 1 , 3 flatten out and put it here if you basically take those kernels and flatten them out in this format, then you end up with a matrix multiply. If you multiply this matrix by this matrix, you’ll end up with the output that you want from the convolution. So this is a basically a way of unrolling your kernels and your input features into matrices such as when you do the matrix multiply, you get the right answer. So it’s kind of a nifty trick. And so that is called im2col and I guess we’re kind of chatting a little bit. Implementing that is kind of boring is just a bunch of copying, intensive manipulation. So I actually haven’t done it. Instead, I’ve linked to a numpy implementation, which is here. And it also part of it is this get indices which is here. And as you can see, it’s a little bit tedious with repeats and tiles and re shapes and whatnot. So I’m not going to call it homework, but if you want to practice your tensor indexing manipulation skills, try creating a PyTorch version from scratch. I’ve got to admit I didn’t bother. Instead,</p>
<p>I used the one that’s built into PyTorch and in PyTorch it’s called unfold. So if we take our image and PyTorch expects there to be a batch axis and dimension and a channel dimension. So we’ll add two unit leading dimensions to it, then we can unfold our input for a three by three and that will give us a 9 by 676 input. And so then we can take that We can take that and then we will make our we will take it out kernal and just flatten it out into a vector. So here changes the shape and minus one just says dump everything into this dimension. So that’s going to create a nine long vector length, nine vector. And so now we can do the matrix multiply just like they’ve done here of the kernel matrix. That’s our weights by the unrolled input features. And so that gives us a 676 long. We can then view that as 26 by 26 and we get back as we hoped, our left edge tensor result. And so this is yeah, this is how we can kind of from scratch create a a better implementation of convolutions. The reason I’m cheating I’m allowed to cheat here is because we did actually create convolution from scratch.</p>
<p>So I think that’s fair, but it’s cool that we can kind of hack out GPU optimized version in the same way that the kind of original Deep Learning library did. So if we use apply kernel, we get nearly 9 milliseconds. If we use unfold with matrix multiply we get 20 microseconds. So that’s what about 400 times faster. So that’s pretty cool. Now of course we don’t have to use unfold and matrix multiply because PyTorch has a copy of covv2d so we can run that. And that interestingly is about the same speed at least on GPU. But this would also work on on, on GPU just as well. Yeah. I’m not sure this will always be the case in this case. It’s a pretty small image. I haven’t experimented a whole lot to see whereabouts. There’s a big difference in space between these. Obviously I always just use conv2d that if there’s some more tricky convolution you need to do with some weird thing around channels or dimensions or something. You can always try this unfold trick. It’s nice to know it’s there. I think. So we could do the same thing for diagonal edges. So here’s our diagonal edge kernel or the other diagonal. So if we just grab the first 16 images on our whole batch with all of our kernels at once. So this is a nice optimized thing that we can do. And you end up with your 26 by 26, you’ve got your four kernels and you’ve got your 16 images. And so that’s summarized here. So that’s generally what we’re doing to get good GPU acceleration as we’re doing a bunch of kernels and a bunch of images all at once across across all of their pixels. And so yeah, we go, that’s what happens when we take a look at our various kernels for a particular image left edge, I guess, top edge and then diagonal, top left and top right. Okay, so that is optimized convolutions on and that works just as well in CPU or GPU.</p>
<p>Obviously GPU will be faster if you have one. Now how do we deal with the problem that way? Losing one pixel on each side? What we can do is we can add something called padding and padding. What we basically do is rather than starting our window here, we start at a right over here and we actually would be up one as well. And so these three on the left here, we just take the input for each of those as zero. So we’re basically just assuming that they’re all zero. I mean, that’s there’s other options we could choose. We could assume they’re the same as the one next to them</p>
<p>There’s various things we can do, but the simplest and the one we normally do is just assume that they’re zero. So now so let’s say, for example, this is this is called one pixel padding.</p>
<p>we’re just going to treated at zero.</p>
<p>So generally odd numbered edge sized kernels are easier to deal with to make sure you end up with the same thing. You start with. Okay, so yeah. So as it says here with you’ve got an odd numbered size case by case size kernel, then case truncate divide two, that’s what segments will give you the right size. And so another trick you can do is you don’t always have to just move your window across by one each time. You could move it by a different amount. Each time the amount you move it by is called the stride. So, for example, here’s a case of doing a stride two so stride two padding one. So we start out here and then we jump across two and then we jump across to then then we go to the next tray.</p>
<p>So that’s called a stride to convolution stride to convoluted ends are handy because they actually reduce the dimensionality of your input by a factor of two. And that’s actually what we want to do a lot. For example, with an auto encoder, we want to do that. And in fact, for most classification architectures we do exactly that. We keep on reducing the kind of the grid size by a factor of two again and again and again, using stride to compilations with padding of one. So that strides in padding. So let’s go ahead and create a convnet using these approaches. So we’re going to put get our size of our training set. This is all the same as before a number of categories than per digit size of our hidden layer. So right previously with our sequential linear models with our MLPs, we basically went from size from the number of pixels to the number of hidden, and then relu and then the number of hidden to the number of outputs. So here’s the equivalent with a convolution.</p>
</section>
<section id="convolutions-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="convolutions-in-pytorch">Convolutions in PyTorch</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb30-2"><span class="im" style="color: #00769E;">import</span> torch</span></code></pre></div>
</div>
<p>What to do if you have <a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo">2 months to complete your thesis</a>? Use <a href="https://hal.inria.fr/inria-00112631/">im2col</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/attachment:image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>Here’s a sample <a href="https://github.com/3outeille/CNNumpy/blob/5394f13e7ed67a808a3e39fd381f168825d65ff5/src/fast/utils.py#L360">numpy implementation</a>.</p>
<div class="cell" data-outputid="19d0c347-285c-4829-83a2-b607a7d0afb8">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">inp <span class="op" style="color: #5E5E5E;">=</span> im3[<span class="va" style="color: #111111;">None</span>,<span class="va" style="color: #111111;">None</span>,:,:].<span class="bu" style="color: null;">float</span>()</span>
<span id="cb31-2">inp_unf <span class="op" style="color: #5E5E5E;">=</span> F.unfold(inp, (<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">3</span>))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb31-3">inp_unf.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>torch.Size([9, 676])</code></pre>
</div>
</div>
<div class="cell" data-outputid="a0535ac9-78fd-412f-c6b7-16e9100b1a04">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">w <span class="op" style="color: #5E5E5E;">=</span> left_edge.view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb33-2">w.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>torch.Size([9])</code></pre>
</div>
</div>
<div class="cell" data-outputid="67a14ba5-3dec-4587-833b-611065521262">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">out_unf <span class="op" style="color: #5E5E5E;">=</span> w<span class="op" style="color: #5E5E5E;">@</span>inp_unf</span>
<span id="cb35-2">out_unf.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>torch.Size([676])</code></pre>
</div>
</div>
<div class="cell" data-outputid="211652dd-b550-4950-ae53-154250e5c28f">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">out <span class="op" style="color: #5E5E5E;">=</span> out_unf.view(<span class="dv" style="color: #AD0000;">26</span>,<span class="dv" style="color: #AD0000;">26</span>)</span>
<span id="cb37-2">show_image(out)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="a9998896-dbd2-4d4e-cbec-0ec040098fed">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">1</span> tensor([[apply_kernel(i,j,left_edge) <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> rng] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> rng])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>13.5 ms ± 353 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<div class="cell" data-outputid="978740b9-dde5-47fa-eb15-e1854bc7b8da">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">100</span> (w<span class="op" style="color: #5E5E5E;">@</span>F.unfold(inp, (<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">3</span>))[<span class="dv" style="color: #AD0000;">0</span>]).view(<span class="dv" style="color: #AD0000;">26</span>,<span class="dv" style="color: #AD0000;">26</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>56.2 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</code></pre>
</div>
</div>
<div class="cell" data-outputid="58edb76c-4062-4305-ca7c-2fa316566c04">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">100</span> F.conv2d(inp, left_edge[<span class="va" style="color: #111111;">None</span>,<span class="va" style="color: #111111;">None</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The slowest run took 7.39 times longer than the fastest. This could mean that an intermediate result is being cached.
25.4 µs ± 29.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">diag1_edge <span class="op" style="color: #5E5E5E;">=</span> tensor([[ <span class="dv" style="color: #AD0000;">0</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb44-2">                     [<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>],</span>
<span id="cb44-3">                     [ <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>]]).<span class="bu" style="color: null;">float</span>()</span></code></pre></div>
</div>
<div class="cell" data-outputid="1eb2d5a9-e34d-4e71-d226-8955e0167aa0">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">show_image(diag1_edge, noframe<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">diag2_edge <span class="op" style="color: #5E5E5E;">=</span> tensor([[ <span class="dv" style="color: #AD0000;">1</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>],</span>
<span id="cb46-2">                     [ <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb46-3">                     [ <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>]]).<span class="bu" style="color: null;">float</span>()</span></code></pre></div>
</div>
<div class="cell" data-outputid="dc83ab5c-e690-4abb-e511-c291817e6a1c">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">show_image(diag2_edge, noframe<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-36-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="e08baded-ece6-41ec-e728-cffa1cef4586">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">xb <span class="op" style="color: #5E5E5E;">=</span> x_imgs[:<span class="dv" style="color: #AD0000;">16</span>][:,<span class="va" style="color: #111111;">None</span>]</span>
<span id="cb48-2">xb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>torch.Size([16, 1, 28, 28])</code></pre>
</div>
</div>
<div class="cell" data-outputid="ec4860c2-0569-4021-c979-452d2ff0e4c0">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">edge_kernels <span class="op" style="color: #5E5E5E;">=</span> torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])[:,<span class="va" style="color: #111111;">None</span>]</span>
<span id="cb50-2">edge_kernels.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>torch.Size([4, 1, 3, 3])</code></pre>
</div>
</div>
<div class="cell" data-outputid="ddb2f48e-745d-4521-a7b2-03594f410e00">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">batch_features <span class="op" style="color: #5E5E5E;">=</span> F.conv2d(xb, edge_kernels)</span>
<span id="cb52-2">batch_features.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>torch.Size([16, 4, 26, 26])</code></pre>
</div>
</div>
<p>The output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually:</p>
<div class="cell" data-outputid="27a38693-30d4-49d8-a9b4-1dea3505f967">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">img0 <span class="op" style="color: #5E5E5E;">=</span> xb[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb54-2">show_image(img0)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="a2dfeac7-886c-4233-c2dc-4d4dce74175a">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">show_images([batch_features[<span class="dv" style="color: #AD0000;">1</span>,i] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">4</span>)])</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="strides-and-padding" class="level3">
<h3 class="anchored" data-anchor-id="strides-and-padding">Strides and Padding</h3>
<p>With appropriate padding, we can ensure that the output activation map is the same size as the original image.</p>
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/chapter9_padconv.svg?raw=1" id="pad_conv" caption="A convolution with padding" alt="A convolution with padding" width="600"></p>
<p>With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map.</p>
<p><img alt="A 4×4 kernel with 5×5 input and 2 pixels of padding" width="783" caption="A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin)" id="four_by_five_conv" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00029.png?raw=1"></p>
<p>If we add a kernel of size <code>ks</code> by <code>ks</code> (with <code>ks</code> an odd number), the necessary padding on each side to keep the same shape is <code>ks//2</code>.</p>
<p>We could move over two pixels after each kernel application. This is known as a <em>stride-2</em> convolution.</p>
<p><img alt="A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding" width="774" caption="A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin)" id="three_by_five_conv" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00030.png?raw=1"></p>
<p>Now the problem is that you can’t just do that because the output is not now ten probabilities for each item in our batch, but it’s ten probabilities for each item in our batch for each of 28 by 28 pixels, because we don’t even have a stride or anything. So you can’t just use the same simple approach that we had for MLP. We have to be a bit more careful. So to make life easier, let’s create a little conv function that does a conv2d with a stride of two. Optionally followed by an activation. So if act is true, we will add in every relu activation. So this is going to either return conv2d or a little sequential containing a conv2d followed by a relu. And so now we can create a cnn from, you know, from scratch. That’s the sequential model. And so since activation is true by default, this is going to take out 28 by 28 image starting with one channel and creating an output of four channels. So this is the number of in this is the number of filters. Sometimes we’ll say filters to describe the number of kind of channels that our convolution has. That’s the number of outputs. And it’s very similar to the idea of the number of outputs in a linear layer. This is the number of outputs in your convolution. So what I like to do when I create stuff like this is I add a little comment just to remind myself what is my grid size after this. So I had a 28 by 28 input. So then I’ve then put it through a stride to conv, so the output of this will be 14 by 14. So then we’ll do the same thing again, but this time we’ll go from a four channel input to an eight channel output and then from 8 to 16. So by this point we’re now down to a 4x4 and then down to a two by two. And then finally we’re down to a one by one. So on the very last layer, we want add an activation and the very last layer is going to create ten outputs. And since we’re now down to a one by one, we can just call flatten and that’s going to remove those unnecessary unit axes. So if we take that pop mini batch through it, we end up with exactly what we want a 16 by ten. So for each of our 16 images, we’ve got ten probabilities of each possible digit. So if we take our training set and make it into 28 by 28 images and we do the same thing for our validation set, and then we create two data sets, one for each which are called train data set and valid data set. And we’re now going to train this on the GPU.</p>
</section>
<section id="creating-the-cnn" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-cnn">Creating the CNN</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">n,m <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb56-2">c <span class="op" style="color: #5E5E5E;">=</span> y_train.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb56-3">nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1">model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">broken_cnn <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb58-2">    nn.Conv2d(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">30</span>, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, padding<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb58-3">    nn.ReLU(),</span>
<span id="cb58-4">    nn.Conv2d(<span class="dv" style="color: #AD0000;">30</span>,<span class="dv" style="color: #AD0000;">10</span>, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, padding<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb58-5">)</span></code></pre></div>
</div>
<div class="cell" data-outputid="2cf902a1-c253-4f38-ab44-b0a2771cbca2">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">broken_cnn(xb).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>torch.Size([16, 10, 28, 28])</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="kw" style="color: #003B4F;">def</span> conv(ni, nf, ks<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, act<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>):</span>
<span id="cb61-2">    res <span class="op" style="color: #5E5E5E;">=</span> nn.Conv2d(ni, nf, stride<span class="op" style="color: #5E5E5E;">=</span>stride, kernel_size<span class="op" style="color: #5E5E5E;">=</span>ks, padding<span class="op" style="color: #5E5E5E;">=</span>ks<span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb61-3">    <span class="cf" style="color: #003B4F;">if</span> act: res <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(res, nn.ReLU())</span>
<span id="cb61-4">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
<p>:::</p>
<p>Refactoring parts of your neural networks like this makes it much less likely you’ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">simple_cnn <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb62-2">    conv(<span class="dv" style="color: #AD0000;">1</span> ,<span class="dv" style="color: #AD0000;">4</span>),            <span class="co" style="color: #5E5E5E;">#14x14</span></span>
<span id="cb62-3">    conv(<span class="dv" style="color: #AD0000;">4</span> ,<span class="dv" style="color: #AD0000;">8</span>),            <span class="co" style="color: #5E5E5E;">#7x7</span></span>
<span id="cb62-4">    conv(<span class="dv" style="color: #AD0000;">8</span> ,<span class="dv" style="color: #AD0000;">16</span>),           <span class="co" style="color: #5E5E5E;">#4x4</span></span>
<span id="cb62-5">    conv(<span class="dv" style="color: #AD0000;">16</span>,<span class="dv" style="color: #AD0000;">16</span>),           <span class="co" style="color: #5E5E5E;">#2x2</span></span>
<span id="cb62-6">    conv(<span class="dv" style="color: #AD0000;">16</span>,<span class="dv" style="color: #AD0000;">10</span>, act<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>), <span class="co" style="color: #5E5E5E;">#1x1</span></span>
<span id="cb62-7">    nn.Flatten(),</span>
<span id="cb62-8">)</span></code></pre></div>
</div>
<p>You now if you’ve got a mac, you can use a device called Apple. If you’ve got an Apple Silicon Mac, you’ve got a device called NPS, which is going to use the know Macs GPU here or if you’ve got it in video, you can use Cuda, which will use your and video creators, you know, ten times or more, possibly much more faster than a mac. So you definitely want to use in video if you can, but if you just running it on a laptop or whatever, you can use amps. So basically you’re going to know what device to use. Do we want to use Coda or amps? You can check if you can check torchlight back backhands that is available to see if you’re running on a mac with amps you can check</p>
<p>torch.backends.mps.is_available()</p>
<p>.It is available to see if you’ve got an in video GPU, in which case you’ve got CUDA. And if you’ve got neither, of course you’ll have to use the CPU to do computation. So I’ve created a little function to device which takes a tensor or a dictionary or a list of tensors or whatever, and a device to move it to. And it just goes through and moves everything onto that device. Or if it’s a dictionary, a dictionary of things, value has moved onto that device that has a handy little function. And so we can create a custom collate function which calls the PyTorch default collation function and then puts those tensors onto our device. And so with that, we’ve now got enough to run train this neural net on the GPU. You we created this get_dls function in the last lesson. So we’re going to use that passing in the datasets that we just created and our default collation function, we’re going to create our optimizer using our CNN’s parameters and then we call fit(), now fit for a member that we also created in our last lesson and it’s done. Yeah, it’s really, it’s a lot less code than last time I ran it. I don’t know if I’ve changed something weird. Let’s say now. There we go.</p>
<p>I must have done something weird. Okay, so I then what I did then was I reduced the learning rate by a factor of four and ran it again.</p>
<div class="cell" data-outputid="2dc7460f-678f-41b8-99f5-20ae7c394ed9">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">simple_cnn(xb).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>torch.Size([16, 10])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">x_imgs <span class="op" style="color: #5E5E5E;">=</span> x_train.view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>)</span>
<span id="cb65-2">xv_imgs <span class="op" style="color: #5E5E5E;">=</span> x_valid.view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>)</span>
<span id="cb65-3">train_ds,valid_ds <span class="op" style="color: #5E5E5E;">=</span> Dataset(x_imgs, y_train),Dataset(xv_imgs, y_valid)</span></code></pre></div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">def_device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'mps'</span> <span class="cf" style="color: #003B4F;">if</span> torch.backends.mps.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb66-2"></span>
<span id="cb66-3"><span class="kw" style="color: #003B4F;">def</span> to_device(x, device<span class="op" style="color: #5E5E5E;">=</span>def_device):</span>
<span id="cb66-4">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(x, torch.Tensor): <span class="cf" style="color: #003B4F;">return</span> x.to(device)</span>
<span id="cb66-5">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(x, Mapping): <span class="cf" style="color: #003B4F;">return</span> {k:v.to(device) <span class="cf" style="color: #003B4F;">for</span> k,v <span class="kw" style="color: #003B4F;">in</span> x.items()}</span>
<span id="cb66-6">    <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">type</span>(x)(to_device(o, device) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> x)</span>
<span id="cb66-7"></span>
<span id="cb66-8"><span class="kw" style="color: #003B4F;">def</span> collate_device(b): <span class="cf" style="color: #003B4F;">return</span> to_device(default_collate(b))</span></code></pre></div>
<p>:::</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> optim</span>
<span id="cb67-2"></span>
<span id="cb67-3">bs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">256</span></span>
<span id="cb67-4">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.4</span></span>
<span id="cb67-5">train_dl,valid_dl <span class="op" style="color: #5E5E5E;">=</span> get_dls(train_ds, valid_ds, bs, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate_device)</span>
<span id="cb67-6">opt <span class="op" style="color: #5E5E5E;">=</span> optim.SGD(simple_cnn.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span></code></pre></div>
</div>
<div class="cell" data-outputid="f7ff16e2-bbd3-4d85-e5ff-dee0372675c8">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">loss,acc <span class="op" style="color: #5E5E5E;">=</span> fit(<span class="dv" style="color: #AD0000;">5</span>, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 2.2963203880310057 0.10639999992847443
1 0.35034381108283996 0.8865999997138977
2 0.2884100193500519 0.9101999996185303
3 0.3795499787807465 0.8783999994277955
4 0.14102031185626984 0.957599999332428</code></pre>
</div>
</div>
<p>And eventually, yeah, I got to a fairly similar accuracy to what we did on our multi on our MLP. So yeah, we’ve got a convolutional network working. I think that’s pretty encouraging and it’s nice that to train it, we didn’t have to write much code right? We were able to use code that we had already built. We were able to use the dataset class that we made to get_dls function that we made and the fit function that we made. And you know, because those things are written in a fairly general way, they work just as well for conv net as they did for an MLP. Nothing had to change. So that was nice. Notice we had to take the model and put it on the device as well. So that will go through and basically put all of the tensors that are in that model onto the MPS or CUDA device, if appropriate. So if we’ve got a batch size of 64 and as we do one channel channel, height width. So normally this is referred to as NCW, so n generally when you see N in a in a paper or whatever, in this way it’s referring to the batch size and being the number that’s the mnemonic, the number of items in the batch C is the number of channels height by width, and W. TensorFlow doesn’t use that TensorFlow uses an NHWC So we generally call these that channels last since channels are at the and this one we normally call channels first. Now of course it’s not actually channels first, it’s actually channel second, but we ignore the batch bit</p>
<p>in some models, particularly some more modern models. It turns out the channels last is faster. So PyTorch has recently added support for channels last. And so you’ll see that being used more and more as well.</p>
<p>Yeah, we go. All right. So a couple of comments and questions from our chat. The first is Sam Watkins pointing out that where we’ve actually had a bit of a win here, which is that the number of parameters now CNN is is pretty small by comparison. So the of in the MLP version, the number of parameters is equal to basically the size of this matrix. Right. So M times and NH, plus the number in this, which will be an H times ten. And you know something that at some point we probably should do is actually create something that allows us to automatically calculate the number of parameters. And I’m ignoring the bias there. Of course, let’s say what would be a good way to do that? Maybe np.product. Yeah. Okay. So what we could do, what we could do is just calculate this automatically by doing a little list comprehension here.</p>
<p>So there’s the number of parameters across all of the different layers. So both bias and weights. And then we could, I guess, just well, we could just use well, let’s use pytouch so we could turn that into a tensor and sum it up. So that’s the number in our MLP. And then the number in our simple CNN. So that’s pretty cool. We’ve gone down from 40000 to 5000 and got about the same number there. Oh, thank you, Jonathan. Jonathan’s reminding me that there’s a better way than np.product shape, which is just to say I dot number of elements num of element. Very nice.</p>
<p>Now, one person asked a very good question, which is I thought convolutional neural networks can handle any sized image and actually know this convolutional network cannot handle any sized image. This convolutional neural network only handles images that once they go through these tried to convs end up with a one by one because otherwise you can’t dot Flatten it and end up with 16 by ten. So we will learn how to create comv nets that can handle any sized input. But there’s nothing particularly about a net that necessitates that it has to be any sized input that it can handle. Okay.</p>
<p>So just let’s briefly finish this section off by talking about this. Yeah. That this well, particularly on to talk about the idea of receptive field, consider this. Yeah. One input Channel four output. Channel three by three kernal. Right. So that’s just here just to show you what we’re doing here conv one. Well actually so simple. See it in simple CNN. This is the model we created. Remember, is like a sequential model containing sequential models because that’s how our current function worked. So simple. CNN zero is our first layer. It contains both conv and a relu so I simple CNN zero zero is the actual conv. So if we grab that whole conf one, it’s a four by one, by three by three. So number of outputs, number of input channels and height by weight for that kernal and then it’s got its bias as well. So that’s how we could kind of deconstruct what’s going on with our weight matrices or our parameters inside a convolution. Now I’m going to switch over to Excel.</p>
<div class="cell" data-outputid="ab206540-caf3-4487-e40b-16e48da8ac8d">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">opt <span class="op" style="color: #5E5E5E;">=</span> optim.SGD(simple_cnn.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb70-2">loss,acc <span class="op" style="color: #5E5E5E;">=</span> fit(<span class="dv" style="color: #AD0000;">5</span>, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 0.10183437039852142 0.9675999994277954
1 0.10473719484806061 0.9674999994277954
2 0.10602079322338104 0.9673999995231628
3 0.09614700574874878 0.9709999995231628
4 0.09545752574205399 0.9696999994277954</code></pre>
</div>
</div>
<section id="understanding-convolution-arithmetic" class="level3">
<h3 class="anchored" data-anchor-id="understanding-convolution-arithmetic">Understanding Convolution Arithmetic</h3>
<p>In an input of size <code>64x1x28x28</code> the axes are <code>batch,channel,height,width</code>. This is often represented as <code>NCHW</code> (where <code>N</code> refers to batch size). Tensorflow, on the other hand, uses <code>NHWC</code> axis order (aka “channels-last”). Channels-last is faster for many models, so recently it’s become more common to see this as an option in PyTorch too.</p>
<p>We have 1 input channel, 4 output channels, and a 3×3 kernel.</p>
<div class="cell" data-outputid="3d6b4a40-9da7-4888-dddf-3dcd648f36cf">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1">simple_cnn[<span class="dv" style="color: #AD0000;">0</span>][<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</code></pre>
</div>
</div>
<div class="cell" data-outputid="6c03ec94-b415-4fe5-9ce7-b07ef9625a27">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">conv1 <span class="op" style="color: #5E5E5E;">=</span> simple_cnn[<span class="dv" style="color: #AD0000;">0</span>][<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb74-2">conv1.weight.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>torch.Size([4, 1, 3, 3])</code></pre>
</div>
</div>
<div class="cell" data-outputid="c7cc6d2d-9bf0-497b-9849-074f3a24a26a">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">conv1.bias.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>torch.Size([4])</code></pre>
</div>
</div>
<p>The <em>receptive field</em> is the area of an image that is involved in the calculation of a layer. <em>conv-example.xlsx</em> shows the calculation of two stride-2 convolutional layers using an MNIST digit. Here’s what we see if we click on one of the cells in the <em>conv2</em> section, which shows the output of the second convolutional layer, and click <em>trace precedents</em>.</p>
<p><img alt="Immediate precedents of conv2 layer" width="308" caption="Immediate precedents of Conv2 layer" id="preced1" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00068.png?raw=1"></p>
<p>The blue highlighted cells are its <em>precedents</em>—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Click <em>trace precedents</em> again:</p>
<p><img alt="Secondary precedents of conv2 layer" width="601" caption="Secondary precedents of Conv2 layer" id="preced2" src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/att_00069.png?raw=1"></p>
<p>In this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the <em>receptive field</em></p>
<p>The deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer.</p>
</section>
</section>
<section id="color-images" class="level2">
<h2 class="anchored" data-anchor-id="color-images">Color Images</h2>
<p>A colour picture is a rank-3 tensor:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><span class="im" style="color: #00769E;">from</span> torchvision.io <span class="im" style="color: #00769E;">import</span> read_image</span></code></pre></div>
</div>
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/https:/github.com/fastai/course22p2/blob/master/nbs/images/chapter9_rgbconv.svg?raw=1" id="rgbconv" caption="Convolution over an RGB image" alt="Convolution over an RGB image" width="550"></p>


</section>
</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/index.html</guid>
  <pubDate>Sat, 01 Apr 2023 19:30:00 GMT</pubDate>
  <media:content url="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 10/attachment:image.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 9</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 9/index.html</link>
  <description><![CDATA[ 



<p>All credits goes to fast.ai All mistakes are mine.</p>
<p>You should know and practice following after this blog post : <br> 1- Callbacks , callable class <br> 2- Partial <br> 3- Lambda <br> 4- <strong>dunder</strong> thingies <br></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> random</span></code></pre></div>
</div>
<p>So we’ve kind of got nearly got all of our infrastructure in place before we do this. Some pieces of python, which not everybody knows and I want to kind of talk about and kind of computer science concepts I want to talk about. So that’s what our six foundations is about. So this whole section is just going to tell. It is going to talk about some stuff in Python that you might not have come across before, or maybe it’s a review for some of you as well. And it’s all stuff we’re going to be using basically in the next notebook.</p>
<p>So that’s why I want to talk to cover it. So we’re going to be creating a learner class. So a learner class is going to be a very general purpose training loop, which we can get to to do anything that we want it to do. And we’re going to be creating things called callbacks to make that happen. And so therefore we’re going to spend a few moments talking about what are callbacks, how are they used in in computer science, how are they implemented? Look at some examples. They come up a lot.</p>
<p>Perhaps the most common place that you see callbacks in software is for doing events of events from some graphical user interface. So the main graphical user interface library in Jupyter notebooks is called ipywidgets, and we can create a widget like a pattern . And when we display it, it shows me a button and at the moment it doesn’t do anything. If I click on it. What we can do though, is we can add and onclick callback to it,we’re going to pass it a function which is called when you click it. So to find that function. So I’m going to say w.on_click(f) is going to assign the f function to the on_click callback. Now, if I click this, there you go. It’s doing it. Now, what does that mean?</p>
<p>Well, a callback is simply a callable that you’ve provided. So remember, a callable is a more general version of a function. So in this case, it is a function that you’ve provided that will be called back to when something happens. So in this case, so something that’s happening is that they’re clicking a button. So this is how we are defining and using a callback as a GUI event. So basically everything in ipywidgets, if you want to create your own graphical user interfaces for Jupyter, you can do it with ipy widgets and by using these callbacks. So these particular kinds of callbacks are called events, but it’s just a callback. All right, so that’s somebody else’s callback.</p>
<section id="callbacks" class="level2">
<h2 class="anchored" data-anchor-id="callbacks">Callbacks</h2>
<section id="callbacks-as-gui-events" class="level3">
<h3 class="anchored" data-anchor-id="callbacks-as-gui-events">Callbacks as GUI events</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> ipywidgets <span class="im" style="color: #00769E;">as</span> widgets</span></code></pre></div>
</div>
<p>From the <a href="https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Events.html">ipywidget docs</a>:</p>
<ul>
<li><em>the button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked</em></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">w <span class="op" style="color: #5E5E5E;">=</span> widgets.Button(description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Click me'</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">w</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b1f2b837ac664a7997b42e074f65280f","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> f(o): <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">'hi'</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">w.on_click(f)</span></code></pre></div>
</div>
<p><em>NB: When callbacks are used in this way they are often called “events”.</em></p>
<p>Let’s create our own callback. So let’s say we’ve some very slow calculation, and so it takes a very long time to add up the numbers 0 to 5 squared because we sleep for a second after each one. So let’s run our slow calculation. Still running. Oh, how’s it going? Come on, finish our calculation.</p>
<p>There we go. The answer is 30. Now, for a slow calculation like that, such as training, a model, it’s a slow calculation. It’ll be nice to do things like, I don’t know, you know, print out loss from time to time or show a progress bar or whatever. So generally, for those kinds of things, we would like to define a callback that is called at the end of each epoch or batch or every few seconds or something like that.</p>
</section>
<section id="creating-your-own-callback" class="level3">
<h3 class="anchored" data-anchor-id="creating-your-own-callback">Creating your own callback</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> time <span class="im" style="color: #00769E;">import</span> sleep</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">def</span> slow_calculation():</span>
<span id="cb8-2">    res <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb8-3">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb8-4">        res <span class="op" style="color: #5E5E5E;">+=</span> i<span class="op" style="color: #5E5E5E;">*</span>i</span>
<span id="cb8-5">        sleep(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb8-6">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">slow_calculation()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
<p>So here’s how we can modify our calculation routine such that you can optionally pass it a callback. And so all of these codes are the same, except we’ve added this one line of code that says if there’s a callback, then call it and pass in what what we’re up to. So then we could create our callback function. So this is just like we created a our callback function f let’s create a show_progress callback function that’s going to tell us how far we’ve got. So now if we call slow calculation passing in our callback, you can say it’s going to call this function at the end of each step.</p>
<p>So here we’ve created our own callback so there’s nothing special about a callback. Like it doesn’t require its own like syntax. It’s not a new concept, it’s just an idea really, which is the idea of passing in a function which some other function will call at particular times, such as at the end of a step or such as when you click a button. So that’s what we mean by callbacks.</p>
<p>We don’t have to define the function ahead of time. We could define the function at the same time that we call the slow calculation by using Lambda. So as we’ve discussed before, Lambda just defines a function, but it doesn’t give it a name. So here’s a function that takes one parameter and prints out exactly the same thing as before. So here’s the same way as doing it, but using a lambda, we could make it more sophisticated now and rather than always saying also we finished epoc, whatever we could have let you pass in an exclamation and we print that out. And so in this case, we could now have our lambda call that function. And so one of the things that we can do now is to again, we can create a function that returns a function. And so we could create a make_ _show_progress function where you pass in the exclamation mark. We could then create in this no need to give it a name. it’s just return it directly. We can return a function that calls that exclamation. So here we are passing in Nice, and that’s exactly the same as doing something like what we’ve done before. We could say instead of using a lambda we can create in a function like this. So here is now a function that returns, a function that says exactly the same thing. Okay, so one way with the lambda when we’re allowed to lambda and one of the reasons I wanted to show you that is so we can do exactly the same thing using partial. So with partial, it’s going to do exactly the same thing as this kind of makes show progress. It’s going to call, show progress and pass. Okay. I guess so is again an example of a function returning a function. And so this is a function that calls show progress, passing in this as the first parameter. And Again, it does exactly the same thing. Okay. So where you get we tend to use partial a lot. So that’s certainly something worth spending time practicing. Now, as we’ve discussed, Python doesn’t care about types in particular, and there’s nothing about any of this that requires cb to be a function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> slow_calculation(cb<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb11-2">    res <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb11-3">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb11-4">        res <span class="op" style="color: #5E5E5E;">+=</span> i<span class="op" style="color: #5E5E5E;">*</span>i</span>
<span id="cb11-5">        sleep(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb11-6">        <span class="cf" style="color: #003B4F;">if</span> cb: cb(i)</span>
<span id="cb11-7">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;">def</span> show_progress(epoch): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Awesome! We've finished epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">!"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">slow_calculation(show_progress)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Awesome! We've finished epoch 0!
Awesome! We've finished epoch 1!
Awesome! We've finished epoch 2!
Awesome! We've finished epoch 3!
Awesome! We've finished epoch 4!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
</section>
<section id="lambdas-and-partials" class="level3">
<h3 class="anchored" data-anchor-id="lambdas-and-partials">Lambdas and partials</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">slow_calculation(<span class="kw" style="color: #003B4F;">lambda</span> o: <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Awesome! We've finished epoch </span><span class="sc" style="color: #5E5E5E;">{</span>o<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">!"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Awesome! We've finished epoch 0!
Awesome! We've finished epoch 1!
Awesome! We've finished epoch 2!
Awesome! We've finished epoch 3!
Awesome! We've finished epoch 4!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="kw" style="color: #003B4F;">def</span> show_progress(exclamation, epoch): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>exclamation<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">! We've finished epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">!"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">slow_calculation(<span class="kw" style="color: #003B4F;">lambda</span> o: show_progress(<span class="st" style="color: #20794D;">"OK I guess"</span>, o))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OK I guess! We've finished epoch 0!
OK I guess! We've finished epoch 1!
OK I guess! We've finished epoch 2!
OK I guess! We've finished epoch 3!
OK I guess! We've finished epoch 4!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;">def</span> make_show_progress(exclamation):</span>
<span id="cb23-2">    <span class="kw" style="color: #003B4F;">def</span> _inner(epoch): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>exclamation<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">! We've finished epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">!"</span>)</span>
<span id="cb23-3">    <span class="cf" style="color: #003B4F;">return</span> _inner</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">slow_calculation(make_show_progress(<span class="st" style="color: #20794D;">"Nice!"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nice!! We've finished epoch 0!
Nice!! We've finished epoch 1!
Nice!! We've finished epoch 2!
Nice!! We've finished epoch 3!
Nice!! We've finished epoch 4!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="im" style="color: #00769E;">from</span> functools <span class="im" style="color: #00769E;">import</span> partial</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">slow_calculation(partial(show_progress, <span class="st" style="color: #20794D;">"OK I guess"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OK I guess! We've finished epoch 0!
OK I guess! We've finished epoch 1!
OK I guess! We've finished epoch 2!
OK I guess! We've finished epoch 3!
OK I guess! We've finished epoch 4!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">f2 <span class="op" style="color: #5E5E5E;">=</span> partial(show_progress, <span class="st" style="color: #20794D;">"OK I guess"</span>)</span></code></pre></div>
</div>
<p>It just happens to be it just has to be a callable.</p>
<p>A callable is something that that you can that you can call. And so as we’ve discussed another way of creating a callable is defining to__call__.</p>
<p>So here’s a class and this is going to work exactly the same as our make show progress thing but now as a class so there’s a <strong>init</strong> which store the explanation and <strong>call</strong> the prints and so now we’re creating a object which is callable and does exactly the same thing</p>
<p>so these are all like fundamental ideas that I want you to get really comfortable with the idea of <strong>call</strong> , dunder things in general, partials, classes because they come up all the time in PyTorch code and, and in the code we’ll be writing and, in fact, pretty much all frameworks. So it’s really important to feel comfortable with them. And remember, you don’t have to rely on the resources we’re providing, you know, if there are certain things here that are very new to you, you know, Google around for some tutorials, so ask for help in the forums, for finding things and so forth.</p>
</section>
<section id="callbacks-as-callable-classes" class="level3">
<h3 class="anchored" data-anchor-id="callbacks-as-callable-classes">Callbacks as callable classes</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;">class</span> ProgressShowingCallback():</span>
<span id="cb32-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, exclamation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Awesome"</span>): <span class="va" style="color: #111111;">self</span>.exclamation <span class="op" style="color: #5E5E5E;">=</span> exclamation</span>
<span id="cb32-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, epoch): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>exclamation<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">! We've finished epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">!"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">cb <span class="op" style="color: #5E5E5E;">=</span> ProgressShowingCallback(<span class="st" style="color: #20794D;">"Just super"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">slow_calculation(cb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Just super! We've finished epoch 0!
Just super! We've finished epoch 1!
Just super! We've finished epoch 2!
Just super! We've finished epoch 3!
Just super! We've finished epoch 4!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
</section>
<section id="multiple-callback-funcs-args-and-kwargs" class="level3">
<h3 class="anchored" data-anchor-id="multiple-callback-funcs-args-and-kwargs">Multiple callback funcs; <code>*args</code> and <code>**kwargs</code></h3>
<p>And then I’m just going to briefly recover something I’ve mentioned before, which is *args, **kwargs because again, they come up a lot. I just want to show you how they work. So if we create a function that has *args and **kwargs, nothing else, and I’m just going to this function, just print them now, I’m going to call the function, I’m going to pass three, I’m going to pass a and I’m going to pass thing one equals.(f(3, ‘a’, thing1=“hello”)) Hello. Now, these are past what we would say by position. We haven’t got a block equals. They’re just stuck. They’re things that are passed by position are placed in *args if you have one. It doesn’t have to be called args, you can call it anything you like but in the star bit.</p>
<p>And so you can see here that args is a tuple containing the positionally path documents.</p>
<p>And then kwargs is a dictionary containing the name arguments. So that is all that *args and **kwargs does. And as I say, there’s nothing special about these names. I call this a I’ll call this b, okay. And it’ll do exactly the same thing.</p>
<p>def f(*a, **b): print(f”args: {a}; kwargs: {b}“)</p>
<p>Okay, so this comes up a lot. And so it’s it’s important to remember that this is literally all that they’re doing. And then on the other hand, let’s say we had a function which takes a couple of let’s try that print. I actually just put them directly a, b, c, okay. We can also, rather than just using them as parameters, we can also use some of them when calling something. So let’s say I create something called args again. It doesn’t have to be called args called, which contains one comma two. And I create something called kwargs that contains a dictionary</p>
<p>args = [1,2] kwargs = {‘c’:3}</p>
<p>G and I can pass in star args,star star kwargs. And that’s going to take this one two and pass them as individual arguments for positionally. And it’s going to take the {‘c’:3} and pass that as a named argument. c equals three. And there it is. Okay, so they’re kind of two linked but different ways that use star and star star.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="kw" style="color: #003B4F;">def</span> f(<span class="op" style="color: #5E5E5E;">*</span>a, <span class="op" style="color: #5E5E5E;">**</span>b): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"args: </span><span class="sc" style="color: #5E5E5E;">{</span>a<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">; kwargs: </span><span class="sc" style="color: #5E5E5E;">{</span>b<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">f(<span class="dv" style="color: #AD0000;">3</span>, <span class="st" style="color: #20794D;">'a'</span>, thing1<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"hello"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>args: (3, 'a'); kwargs: {'thing1': 'hello'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="kw" style="color: #003B4F;">def</span> g(a,b,c<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>): <span class="bu" style="color: null;">print</span>(a,b,c)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">args <span class="op" style="color: #5E5E5E;">=</span> [<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb41-2">kwargs <span class="op" style="color: #5E5E5E;">=</span> {<span class="st" style="color: #20794D;">'c'</span>:<span class="dv" style="color: #AD0000;">3</span>}</span>
<span id="cb41-3">g(<span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 2 3</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;">def</span> slow_calculation(cb<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb43-2">    res <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb43-3">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb43-4">        <span class="cf" style="color: #003B4F;">if</span> cb: cb.before_calc(i)</span>
<span id="cb43-5">        res <span class="op" style="color: #5E5E5E;">+=</span> i<span class="op" style="color: #5E5E5E;">*</span>i</span>
<span id="cb43-6">        sleep(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb43-7">        <span class="cf" style="color: #003B4F;">if</span> cb: cb.after_calc(i, val<span class="op" style="color: #5E5E5E;">=</span>res)</span>
<span id="cb43-8">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
</div>
<p>Okay. Now here’s a slightly different way of doing callbacks, which I really like in this case. I’ve now passing in a callback that’s not callable, but instead it’s going to have a method called before_calc and another method called after_calc. And I’m so now my callback is going to be a class containing a before_calc and after_calc method. And so if I run that, you can see that there it goes.</p>
<p>Okay. And so this is printing before and after every step by call, calling before_calc and after_calc. So callback actually doesn’t have to be a callable. It doesn’t have to be a function. A callback could be something that contains methods. So we could have a version of this which actually, as you can see here, it’s going to pass int after_calc, both the epoch number and the value it’s up to. But by using star args and star star kwargs I can just safely ignore them if I don’t want them. Right? So it’s just going to chew them up and not complain.</p>
<p>If I didn’t have those here, it won’t work because it got passed in value equals and there’s nothing here looking for val equals that doesn’t like that. So this is one good use star args and star star kwargs eat up arguments You don’t want.</p>
<p>Or we could use the argument. So let’s actually use epoch and Val and print them out and there it is. So this is a more sophisticated callback that’s giving us status as we go.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="kw" style="color: #003B4F;">class</span> PrintStepCallback():</span>
<span id="cb44-2">    <span class="kw" style="color: #003B4F;">def</span> before_calc(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"About to start"</span>)</span>
<span id="cb44-3">    <span class="kw" style="color: #003B4F;">def</span> after_calc (<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Done step"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">slow_calculation(PrintStepCallback())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>About to start
Done step
About to start
Done step
About to start
Done step
About to start
Done step
About to start
Done step</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;">class</span> PrintStatusCallback():</span>
<span id="cb48-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb48-3">    <span class="kw" style="color: #003B4F;">def</span> before_calc(<span class="va" style="color: #111111;">self</span>, epoch, <span class="op" style="color: #5E5E5E;">**</span>kwargs): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"About to start: </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb48-4">    <span class="kw" style="color: #003B4F;">def</span> after_calc (<span class="va" style="color: #111111;">self</span>, epoch, val, <span class="op" style="color: #5E5E5E;">**</span>kwargs): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"After </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">: </span><span class="sc" style="color: #5E5E5E;">{</span>val<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">slow_calculation(PrintStatusCallback())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>About to start: 0
After 0: 0
About to start: 1
After 1: 1
About to start: 2
After 2: 5
About to start: 3
After 3: 14
About to start: 4
After 4: 30</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>30</code></pre>
</div>
</div>
</section>
<section id="modifying-behavior" class="level3">
<h3 class="anchored" data-anchor-id="modifying-behavior">Modifying behavior</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><span class="kw" style="color: #003B4F;">def</span> slow_calculation(cb<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb52-2">    res <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb52-3">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb52-4">        <span class="cf" style="color: #003B4F;">if</span> cb <span class="kw" style="color: #003B4F;">and</span> <span class="bu" style="color: null;">hasattr</span>(cb,<span class="st" style="color: #20794D;">'before_calc'</span>): cb.before_calc(i)</span>
<span id="cb52-5">        res <span class="op" style="color: #5E5E5E;">+=</span> i<span class="op" style="color: #5E5E5E;">*</span>i</span>
<span id="cb52-6">        sleep(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb52-7">        <span class="cf" style="color: #003B4F;">if</span> cb <span class="kw" style="color: #003B4F;">and</span> <span class="bu" style="color: null;">hasattr</span>(cb,<span class="st" style="color: #20794D;">'after_calc'</span>):</span>
<span id="cb52-8">            <span class="cf" style="color: #003B4F;">if</span> cb.after_calc(i, res):</span>
<span id="cb52-9">                <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"stopping early"</span>)</span>
<span id="cb52-10">                <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb52-11">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="kw" style="color: #003B4F;">class</span> PrintAfterCallback():</span>
<span id="cb53-2">    <span class="kw" style="color: #003B4F;">def</span> after_calc (<span class="va" style="color: #111111;">self</span>, epoch, val):</span>
<span id="cb53-3">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"After </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">: </span><span class="sc" style="color: #5E5E5E;">{</span>val<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb53-4">        <span class="cf" style="color: #003B4F;">if</span> val<span class="op" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">10</span>: <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">True</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">slow_calculation(PrintAfterCallback())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After 0: 0
After 1: 1
After 2: 5
After 3: 14
stopping early</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>14</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="kw" style="color: #003B4F;">class</span> SlowCalculator():</span>
<span id="cb57-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, cb<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>): <span class="va" style="color: #111111;">self</span>.cb,<span class="va" style="color: #111111;">self</span>.res <span class="op" style="color: #5E5E5E;">=</span> cb,<span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb57-3">    </span>
<span id="cb57-4">    <span class="kw" style="color: #003B4F;">def</span> callback(<span class="va" style="color: #111111;">self</span>, cb_name, <span class="op" style="color: #5E5E5E;">*</span>args):</span>
<span id="cb57-5">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">self</span>.cb: <span class="cf" style="color: #003B4F;">return</span></span>
<span id="cb57-6">        cb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">getattr</span>(<span class="va" style="color: #111111;">self</span>.cb,cb_name, <span class="va" style="color: #111111;">None</span>)</span>
<span id="cb57-7">        <span class="cf" style="color: #003B4F;">if</span> cb: <span class="cf" style="color: #003B4F;">return</span> cb(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>args)</span>
<span id="cb57-8"></span>
<span id="cb57-9">    <span class="kw" style="color: #003B4F;">def</span> calc(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb57-10">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb57-11">            <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'before_calc'</span>, i)</span>
<span id="cb57-12">            <span class="va" style="color: #111111;">self</span>.res <span class="op" style="color: #5E5E5E;">+=</span> i<span class="op" style="color: #5E5E5E;">*</span>i</span>
<span id="cb57-13">            sleep(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb57-14">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.callback(<span class="st" style="color: #20794D;">'after_calc'</span>, i):</span>
<span id="cb57-15">                <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"stopping early"</span>)</span>
<span id="cb57-16">                <span class="cf" style="color: #003B4F;">break</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><span class="kw" style="color: #003B4F;">class</span> ModifyingCallback():</span>
<span id="cb58-2">    <span class="kw" style="color: #003B4F;">def</span> after_calc (<span class="va" style="color: #111111;">self</span>, calc, epoch):</span>
<span id="cb58-3">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"After </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">: </span><span class="sc" style="color: #5E5E5E;">{</span>calc<span class="sc" style="color: #5E5E5E;">.</span>res<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb58-4">        <span class="cf" style="color: #003B4F;">if</span> calc.res<span class="op" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">10</span>: <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">True</span></span>
<span id="cb58-5">        <span class="cf" style="color: #003B4F;">if</span> calc.res<span class="op" style="color: #5E5E5E;">&lt;</span><span class="dv" style="color: #AD0000;">3</span>: calc.res <span class="op" style="color: #5E5E5E;">=</span> calc.res<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">calculator <span class="op" style="color: #5E5E5E;">=</span> SlowCalculator(ModifyingCallback())</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">calculator.calc()</span>
<span id="cb60-2">calculator.res</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After 0: 0
After 1: 1
After 2: 6
After 3: 15
stopping early</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>15</code></pre>
</div>
</div>
</section>
</section>
<section id="dunder__-thingies" class="level2">
<h2 class="anchored" data-anchor-id="dunder__-thingies"><code>__dunder__</code> thingies</h2>
<p>Anything that looks like <code>__this__</code> is, in some way, <em>special</em>. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call <code>__init__</code>. These are defined as part of the python <a href="https://docs.python.org/3/reference/datamodel.html#object.__init__">data model</a>.</p>
<p>For instance, if python sees <code>+</code>, then it will call the special method <code>__add__</code>. If you try to display an object in Jupyter (or lots of other places in Python) it will call <code>__repr__</code>.</p>
<p>Okay, So finally, let’s just review this idea dunder, which we’ve mentioned before, but just to, to really nail this home, anything that looks like this underscore, underscore something, underscore, underscore something is special. And basically it could be that Python has to find that special thing or PyTorch has to find that special thing or numpy as to find that special thing. But this special these are called under methods. And some of them are defined as part of the Python data model.</p>
<p>And so if you go to the Python documentation, it’ll tell you about these various different his <strong>repr</strong> which we used earlier is <strong>init</strong> that we used earlier. So they’re all here. PyTorch has some of its own, numpy has some of its own.</p>
<p>So for example, if python says plus what it actually does is it calls <strong>add</strong>. So if we want to create something that’s not very good at adding things, it actually also always adds point. I want to it that I can say sloppy at a one plus floppy at a two equals 3.01. So plus here is actually calling <strong>add</strong>. So if you’re not familiar with this, click on this data model link and read about these specific one two, three, four, five, six, seven, eight, nine, ten, 11 methods because we’ll be using all of these in the course.</p>
<p>So I’ll try to revise them when we can. But I’m generally going to assume that, you know, a particularly interesting one is <strong>getattr</strong> and <strong>getitem</strong>. We’ve seen <strong>setattr</strong> already get across just the opposite. Take a look at this. Here’s a class. It just contains two attributes a, b, that are set one and two. So create that an object of that class a.b equals two because I set b to two. Okay. Now when you say dot B, that’s just in texture. Good. Basically in Python, what it’s actually calling behind the scenes is <strong>getattr</strong>, it calls, <strong>getattr</strong> on the object. And so this one here is the same <strong>getattr</strong> a comma b which hopefully</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="kw" style="color: #003B4F;">class</span> SloppyAdder():</span>
<span id="cb63-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>,o): <span class="va" style="color: #111111;">self</span>.o<span class="op" style="color: #5E5E5E;">=</span>o</span>
<span id="cb63-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__add__</span>(<span class="va" style="color: #111111;">self</span>,b): <span class="cf" style="color: #003B4F;">return</span> SloppyAdder(<span class="va" style="color: #111111;">self</span>.o <span class="op" style="color: #5E5E5E;">+</span> b.o <span class="op" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb63-4">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">str</span>(<span class="va" style="color: #111111;">self</span>.o)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">a <span class="op" style="color: #5E5E5E;">=</span> SloppyAdder(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb64-2">b <span class="op" style="color: #5E5E5E;">=</span> SloppyAdder(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb64-3">a<span class="op" style="color: #5E5E5E;">+</span>b</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>3.01</code></pre>
</div>
</div>
<p>Special methods you should probably know about (see data model link above) are:</p>
<ul>
<li><code>__getitem__</code></li>
<li><code>__getattr__</code></li>
<li><code>__setattr__</code></li>
<li><code>__del__</code></li>
<li><code>__init__</code></li>
<li><code>__new__</code></li>
<li><code>__enter__</code></li>
<li><code>__exit__</code></li>
<li><code>__len__</code></li>
<li><code>__repr__</code></li>
<li><code>__str__</code></li>
</ul>
<section id="getattr__-and-getattr" class="level3">
<h3 class="anchored" data-anchor-id="getattr__-and-getattr"><code>__getattr__</code> and <code>getattr</code></h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><span class="kw" style="color: #003B4F;">class</span> A: a,b<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">a <span class="op" style="color: #5E5E5E;">=</span> A()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">a.b</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>2</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><span class="bu" style="color: null;">getattr</span>(a, <span class="st" style="color: #20794D;">'b'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>2</code></pre>
</div>
</div>
<p>I’ll actually that’ll be yes our calls get a b and this can kind of be fun because you could call, <strong>getattr</strong> a comma and either b or a randomly . So if I run this 21112 as you can say, it’s random. So yeah, Python such a dynamic language, you can even set it up so it literally don’t know what attributes are going to be called. Now getattr behind the scenes. It’s actually calling something called <strong>getattr</strong> and by default it’ll use the version in the object based class. So here’s something just like a it’s got a and b defined, but I’ve also got time getattr defined and so <strong>getattr</strong> It’s only called for stuff that hasn’t been defined yet and it’ll pass in the key of the the name of the attribute. So generally speaking, if the first character is an underscore, it’s going to be private or special. So That’s going to raise an attribute error. Otherwise I’m going to steal it and return hello from k. So if I go b.athat’s defined so it gives me one. If I go b.foo, that’s not defined. So calls <strong>getattr</strong> and I get back hello from foo. And so this gets used a lot in both fastai code and also huggingface code to you know often make it more convenient to access things. So that’s yeah that’s how we getattr function and <strong>getattr</strong> method work.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><span class="bu" style="color: null;">getattr</span>(a, <span class="st" style="color: #20794D;">'b'</span> <span class="cf" style="color: #003B4F;">if</span> random.random()<span class="op" style="color: #5E5E5E;">&gt;</span><span class="fl" style="color: #AD0000;">0.5</span> <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'a'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>1</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><span class="kw" style="color: #003B4F;">class</span> B:</span>
<span id="cb74-2">    a,b<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb74-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getattr__</span>(<span class="va" style="color: #111111;">self</span>, k):</span>
<span id="cb74-4">        <span class="cf" style="color: #003B4F;">if</span> k[<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">'_'</span>: <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">AttributeError</span>(k)</span>
<span id="cb74-5">        <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f'Hello from </span><span class="sc" style="color: #5E5E5E;">{</span>k<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">b <span class="op" style="color: #5E5E5E;">=</span> B()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">b.a</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>1</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">b.foo</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'Hello from foo'</code></pre>
</div>
</div>
<p>Okay so I went over that pretty quickly since I know for quite a few folks this will be all review, but I know for folks who haven’t seen any of this, this is a lot to cover. So I’m hoping that you all kind of go back over this, revise it slowly, experiment with it, and look up some additional resources and ask on the forum and stuff. That’s not clear. Remember, everybody has parts of the course that’s really easy for them and parts of the course that are completely unfamiliar for them. And so if this particular part of the course is completely unfamiliar to you, it’s not because this is harder or going to be more difficult or whatever. It’s just so happens that this is a bit that you’re less familiar with. Or maybe this stuff about calculus in the last lesson was a bit that you’re less familiar with. There isn’t really anything, particularly in the course, that’s more difficult than other parts. It’s just that, you know, based on whether you happen to have that background.</p>
<p>And so yeah, if you spend a few hours studying and practicing, you know, you’ll be able to pick up these things and yeah, so don’t stress if there are things that you don’t get right away, just take the time. And if you Yeah, if you do get lost, please ask because people are very keen to help. If you’ve tried asking on the forum, hopefully you noticed that people are really keen to help. All right. So I think this has been a pretty successful lesson. We’ve we’ve got to a point where we’ve got a pretty nicely optimized training loop. We actually understand exactly what data load is and data sets do. We’ve got an optimizer. We’ve been playing with hugging face data sets and we’ve got those working really smoothly. So we really feel like we’re in a pretty good position to to write our generic learner training loop and then we can start building and experimenting with lots of models. So look forward to seeing you next. Time to doing that together. Okay.</p>


</section>
</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 9/index.html</guid>
  <pubDate>Tue, 28 Mar 2023 19:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 8</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index.html</link>
  <description><![CDATA[ 



<p>Things you will should know and practice after reading this : <br> 1- Getting data from dataloader of huggingface dataset <br> 2- <span class="citation" data-cites="inplace">@inplace</span> in python <br> 3- plotting <br></p>
<p>You need to install stuff in colab so the notebook works. I have some extra installation in compare to Jeremy original notebook. For more infor check this link in fastai forum: https://forums.fast.ai/t/is-there-any-workaround-to-use-mini-ai-in-colab/104732</p>
<div class="cell" data-outputid="9ec82d92-f503-4d9e-8fcb-a004f5e6c65e" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">!</span>pip install torch<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">1.13.1</span> torchvision<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">0.14.1</span> torchaudio<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">0.13.1</span> torchtext<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">0.14.1</span> fastai<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">2.7.11</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting torch==1.13.1
  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.5/887.5 MB 1.6 MB/s eta 0:00:00
Collecting torchvision==0.14.1
  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/24.2 MB 66.6 MB/s eta 0:00:00
Collecting torchaudio==0.13.1
  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 100.3 MB/s eta 0:00:00
Collecting torchtext==0.14.1
  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 101.8 MB/s eta 0:00:00
Collecting fastai==2.7.11
  Downloading fastai-2.7.11-py3-none-any.whl (232 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.8/232.8 kB 29.3 MB/s eta 0:00:00
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.5.0)
Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)
  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 72.5 MB/s eta 0:00:00
Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)
  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 2.7 MB/s eta 0:00:00
Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)
  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 4.9 MB/s eta 0:00:00
Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)
  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 55.9 MB/s eta 0:00:00
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (1.22.4)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (2.27.1)
Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (8.4.0)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.65.0)
Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (23.1.2)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (23.1)
Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (0.0.7)
Requirement already satisfied: fastcore&lt;1.6,&gt;=1.4.5 in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (1.5.29)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (3.7.1)
Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (1.5.3)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (6.0)
Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (1.0.3)
Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (1.2.2)
Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (1.10.1)
Requirement already satisfied: spacy&lt;4 in /usr/local/lib/python3.10/dist-packages (from fastai==2.7.11) (3.5.2)
Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==1.13.1) (67.7.2)
Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==1.13.1) (0.40.0)
Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (3.0.12)
Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (1.0.4)
Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (1.0.9)
Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (2.0.7)
Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (3.0.8)
Requirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (8.1.9)
Requirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (1.1.1)
Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (2.4.6)
Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (2.0.8)
Requirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (0.7.0)
Requirement already satisfied: pathy&gt;=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (0.10.1)
Requirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (6.3.0)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (1.10.7)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (3.1.2)
Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;4-&gt;fastai==2.7.11) (3.3.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision==0.14.1) (1.26.15)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision==0.14.1) (2022.12.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision==0.14.1) (2.0.12)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision==0.14.1) (3.4)
Requirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;fastai==2.7.11) (1.0.7)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;fastai==2.7.11) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;fastai==2.7.11) (4.39.3)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;fastai==2.7.11) (1.4.4)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;fastai==2.7.11) (3.0.9)
Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;fastai==2.7.11) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;fastai==2.7.11) (2022.7.1)
Requirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;fastai==2.7.11) (1.2.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;fastai==2.7.11) (3.1.0)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;fastai==2.7.11) (1.16.0)
Requirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai==2.7.11) (0.7.9)
Requirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai==2.7.11) (0.0.4)
Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai==2.7.11) (8.1.3)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai==2.7.11) (2.1.2)
Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision, torchtext, torchaudio, fastai
  Attempting uninstall: torch
    Found existing installation: torch 2.0.0+cu118
    Uninstalling torch-2.0.0+cu118:
      Successfully uninstalled torch-2.0.0+cu118
  Attempting uninstall: torchvision
    Found existing installation: torchvision 0.15.1+cu118
    Uninstalling torchvision-0.15.1+cu118:
      Successfully uninstalled torchvision-0.15.1+cu118
  Attempting uninstall: torchtext
    Found existing installation: torchtext 0.15.1
    Uninstalling torchtext-0.15.1:
      Successfully uninstalled torchtext-0.15.1
  Attempting uninstall: torchaudio
    Found existing installation: torchaudio 2.0.1+cu118
    Uninstalling torchaudio-2.0.1+cu118:
      Successfully uninstalled torchaudio-2.0.1+cu118
  Attempting uninstall: fastai
    Found existing installation: fastai 2.7.12
    Uninstalling fastai-2.7.12:
      Successfully uninstalled fastai-2.7.12
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.
Successfully installed fastai-2.7.11 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchaudio-0.13.1 torchtext-0.14.1 torchvision-0.14.1</code></pre>
</div>
</div>
<div class="cell" data-outputid="b64d79a4-3bda-4f49-aa53-2f36ae85df2d" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="op" style="color: #5E5E5E;">!</span>pip install tokenizers</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tokenizers
  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 84.9 MB/s eta 0:00:00
Installing collected packages: tokenizers
Successfully installed tokenizers-0.13.3</code></pre>
</div>
</div>
<div class="cell" data-outputid="19b1a2c8-180a-4076-dad4-ebf6887e4d7e" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="op" style="color: #5E5E5E;">!</span>pip install torchdata<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">0.5.1</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting torchdata==0.5.1
  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 71.1 MB/s eta 0:00:00
Requirement already satisfied: urllib3&gt;=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.15)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)
Collecting portalocker&gt;=2.0.0 (from torchdata==0.5.1)
  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1-&gt;torchdata==0.5.1) (4.5.0)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1-&gt;torchdata==0.5.1) (11.7.99)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1-&gt;torchdata==0.5.1) (8.5.0.96)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1-&gt;torchdata==0.5.1) (11.10.3.66)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1-&gt;torchdata==0.5.1) (11.7.99)
Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==1.13.1-&gt;torchdata==0.5.1) (67.7.2)
Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch==1.13.1-&gt;torchdata==0.5.1) (0.40.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchdata==0.5.1) (2022.12.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchdata==0.5.1) (2.0.12)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchdata==0.5.1) (3.4)
Installing collected packages: portalocker, torchdata
  Attempting uninstall: torchdata
    Found existing installation: torchdata 0.6.0
    Uninstalling torchdata-0.6.0:
      Successfully uninstalled torchdata-0.6.0
Successfully installed portalocker-2.7.0 torchdata-0.5.1</code></pre>
</div>
</div>
<p>Let`s learn how to use huggingface datasets.</p>
<p>So one thing that we’re going to want to be able to do now that we’ve got a training loop is to grab data and there’s a really fantastic library of datasets available on huggingface nowadays. And so let’s look at how we use those datasets now that we know how to bring things into data loader and stuff, so that now we can use the entire world of huggingface datasets with our code.So you need to pip install datasets. And once you’ve pip install datasets, you have to say from datasets import and you can import a few things. Just these two things now like dataset like Dataset Builder, and we’re going to look at a dataset called fashion MNIST. And so the way things tend to work with hacking faces is something called the Hugging Face Hub, which has models and it has datasets, amongst other things. And generally you’ll give them a name and you can then say, in this case, load a dataset builder for fashion MNIST. Now a dataset builder is just basically something which has some metadata about about this dataset. So the dataset builder has a dot info and the dot info has a dot description. And here’s a description of this. And as you can see again, we’ve got 28 by 28 to grayscale, so it’s going to be very familiar to us because this is just like MNIST. And again, we’ve got ten categories and again we’ve got 60,000 training examples and again we’ve got 10,000 test examples. So this is this is code, as it says, direct drop in replacement for MNIST.</p>
<div class="cell" data-outputid="70a52df4-42e8-4abd-947c-94270a982d94" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uqq git<span class="op" style="color: #5E5E5E;">+</span>https:<span class="op" style="color: #5E5E5E;">//</span>github.com<span class="op" style="color: #5E5E5E;">/</span>fastai<span class="op" style="color: #5E5E5E;">/</span>course22p2</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Preparing metadata (setup.py) ... done
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 17.3 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.4/158.4 kB 19.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.9/934.9 kB 47.4 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 6.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 549.1/549.1 kB 36.5 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 14.4 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 26.7 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 18.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 48.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 29.6 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.9/87.9 kB 11.5 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 17.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 35.5 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 14.0 MB/s eta 0:00:00
  Building wheel for miniai (setup.py) ... done</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=8}</p>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;">from</span> __future__ <span class="im" style="color: #00769E;">import</span> annotations</span>
<span id="cb9-2"><span class="im" style="color: #00769E;">import</span> math,numpy <span class="im" style="color: #00769E;">as</span> np,matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb9-3"><span class="im" style="color: #00769E;">from</span> operator <span class="im" style="color: #00769E;">import</span> itemgetter</span>
<span id="cb9-4"><span class="im" style="color: #00769E;">from</span> itertools <span class="im" style="color: #00769E;">import</span> zip_longest</span>
<span id="cb9-5"><span class="im" style="color: #00769E;">import</span> fastcore.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">as</span> fc</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> default_collate</span>
<span id="cb9-8"></span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;">import</span> logging,pickle,gzip,os,time,shutil,torch,matplotlib <span class="im" style="color: #00769E;">as</span> mpl</span>
<span id="cb10-2"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb10-3"></span>
<span id="cb10-4"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor,nn,optim</span>
<span id="cb10-5"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader</span>
<span id="cb10-6"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb10-7"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset,load_dataset_builder</span>
<span id="cb10-8"></span>
<span id="cb10-9"><span class="im" style="color: #00769E;">import</span> torchvision.transforms.functional <span class="im" style="color: #00769E;">as</span> TF</span>
<span id="cb10-10"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb11-2">torch.manual_seed(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb11-3">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">logging.disable(logging.WARNING)</span></code></pre></div>
</div>
<p>And so the dataset builder also will tell us what are what’s in this dataset. And so huggingface stuff generally uses dictionaries rather than tuples. So there’s going to be an image of type image and there’s going to be a label of type class label. There’s ten classes and these are the names of the classes. So it’s quite nice that in huggingface datasets, you know, we can kind of get this information directly. It also tells us if there are some recommended training test splits, we can find out those as well. So this is the size of the training split and the number of examples. So now that we’re ready to start playing it with that, we can load the dataset. Okay, so this is a different train load dataset builder versus load dataset. So this will actually download it, cache it, and here it is, and it creates a dataset dictionary. So a dataset dictionary, if you’ve used fastai is basically just like what we call the datasets class they call the dataset dict class. So it’s a dictionary that contains, in this case, a train and a test item. And those are datasets. And these datasets are very much like the datasets that we created in the previous notebook. So we can now grab the training and test items from that dictionary and just pop them into variables. And so we can now have a look at the zero index thing in training. And just like we were promised, it contains an image and a label. So as you can see, we’re not getting tuples anymore. We’re getting dictionaries containing the x and the y in this case image and label. So I’m going to get pretty writing image and label and strings all the time, so I’m just going to store them as x and y. So x is going to be the string image and y will be the string label. I guess the other way I could have done that would have been to say x comma, y equals that probably a bit neater because it’s coming straight from the features. And if you, if you iterate into a dictionary you get back its, its keys, that’s why that works. So anyway, I’ve done it manually here, which is a bit sad, but there you go. Okay, so we can now grab the from train[0][x], which we’ve already seen. We can grab the x by the image and there it is. It’s the image where you could grab the first five images in the first five labels, for example. And there they are. Now, we already know what the names of the classes are, so we could now see what these map two by grabbing those features. So there they are. So This is a special huggingface class, which most libraries have something including fast ai, that works like this. There’s something called int to string, which is going to take these and convert them to these. So if I call it on our y batch, you’ll see we’ve got first is ankle boots.</p>
<section id="hugging-face-datasets" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-datasets">Hugging Face Datasets</h2>
<div class="cell" data-outputid="adfd9cd3-264c-4e4d-a80b-c0700120ad7c" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">name <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"fashion_mnist"</span></span>
<span id="cb13-2">ds_builder <span class="op" style="color: #5E5E5E;">=</span> load_dataset_builder(name)</span>
<span id="cb13-3"><span class="bu" style="color: null;">print</span>(ds_builder.info.description)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"5d335526879f4caca3d64e441080745f"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"1c72c1c98e9b434c821c6ad304078055"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"800f3d2c50144788857198816d788dc5"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of
60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,
associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in
replacement for the original MNIST dataset for benchmarking machine learning algorithms.
It shares the same image size and structure of training and testing splits.
</code></pre>
</div>
</div>
<div class="cell" data-outputid="972d8746-5f63-4d72-90cf-b7bbcd08bfac" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">ds_builder.info.features</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>{'image': Image(decode=True, id=None),
 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}</code></pre>
</div>
</div>
<div class="cell" data-outputid="33c0e6ca-16de-4f04-d171-5a31c4ef672c" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">ds_builder.info.splits</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>{'train': SplitInfo(name='train', num_bytes=31296655, num_examples=60000, shard_lengths=None, dataset_name=None),
 'test': SplitInfo(name='test', num_bytes=5233818, num_examples=10000, shard_lengths=None, dataset_name=None)}</code></pre>
</div>
</div>
<div class="cell" data-outputid="6e37aa69-4103-4e8e-e918-bde72a7fa279" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">dsd <span class="op" style="color: #5E5E5E;">=</span> load_dataset(name)</span>
<span id="cb19-2">dsd</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading and preparing dataset fashion_mnist/fashion_mnist to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"052e29c6a8f34c408b184df2d14a862a"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"6db57ebb01f44e2c950deff0e0a4e2db"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"42d5d51e80144639b1943470967c4c22"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"68a07ea9296e45c5b179dcce57306542"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"79743d3e227e420e88b718860aa2abd9"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"b54e4816a8bc4774a24f03c20f7e0118"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"4146e2adf55d4e939053cd8dea2d37a1"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"41de2ee01ae04e9fadb88b63fb061079"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48. Subsequent calls will reuse this data.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"69250481dde142bfb86b51b1e672eb3a"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'label'],
        num_rows: 60000
    })
    test: Dataset({
        features: ['image', 'label'],
        num_rows: 10000
    })
})</code></pre>
</div>
</div>
<div class="cell" data-outputid="215a0113-bc43-4935-e324-35df4e97e863" data-execution_count="14">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">train,test <span class="op" style="color: #5E5E5E;">=</span> dsd[<span class="st" style="color: #20794D;">'train'</span>],dsd[<span class="st" style="color: #20794D;">'test'</span>]</span>
<span id="cb23-2">train[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F7D5C52E6E0&gt;,
 'label': 9}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">x,y <span class="op" style="color: #5E5E5E;">=</span> ds_builder.info.features</span></code></pre></div>
</div>
<div class="cell" data-outputid="82913917-663e-4fef-e3b8-1e4103c403e9" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">x,y</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>('image', 'label')</code></pre>
</div>
</div>
<div class="cell" data-outputid="24ba4851-f26f-4a5c-acc5-4945b28eacbe" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">x,y <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'image'</span>,<span class="st" style="color: #20794D;">'label'</span></span>
<span id="cb28-2">img <span class="op" style="color: #5E5E5E;">=</span> train[<span class="dv" style="color: #AD0000;">0</span>][x]</span>
<span id="cb28-3">img</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>That is indeed an ankle boot. They might have a couple of t shirts and a dress. Okay, so how do we use this to train a model? Well, we’re going to need a data loader, and we want a data loader that for now we’re going to just, like, return it before it’s going to return. Well, actually, we’re going to do something different. We’re going to have our collate function is actually going to return a dictionary. Actually, this is pretty common for huggingface stuff.</p>
<p>And PyTorch doesn’t mind, it’s happy for you to return a dictionary from a collation function. So rather than returning a tuple of the stacked up actually this looks very familiar. This looks a lot like the thing that goes through the dataset for each one and stacks them up just like we did in the previous notebook. So We’re doing all all in one step here in our collate function. And then again, exactly the same thing. Go through our batch grab the y and this is just stacking them up with the integers so we don’t have to call stack. And so we’re now going to have the image and label bits in our dictionary. So if we create a data loader using that collation function, grab one batch so we can go a batch x.shape. It’s a 16 by one by 28 by 28. And our y if a batch here, here it is. So the thing to notice here is that we haven’t done any transforms or anything or written own dataset class or anything. We’re actually putting all the work directly in the collation function. So this is like a really nice way to skip all the kind of abstractions of your framework if you want to is you can just do all of your work and collate functions so it’s going to pass you each item. So it’s going to you’re going to get the batch directly and it’s going to go through each item. And so here we’re saying, okay, grab the x key from that dictionary, convert it to a tensor, and then do that for everything in the batch and then stack them all together. So this is yeah, this is like can be quite a nice way to do things if you want to do things just very manually without having to think too much about, you know, a framework, particularly if you’re doing really custom stuff, this can be quite helpful.</p>
<div class="cell" data-outputid="b21adb42-e37a-4d52-faba-2cb3bea63817" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">xb <span class="op" style="color: #5E5E5E;">=</span> train[:<span class="dv" style="color: #AD0000;">5</span>][x]</span>
<span id="cb29-2">yb <span class="op" style="color: #5E5E5E;">=</span> train[:<span class="dv" style="color: #AD0000;">5</span>][y]</span>
<span id="cb29-3">yb</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>[9, 0, 0, 3, 0]</code></pre>
</div>
</div>
<div class="cell" data-outputid="1ead4f63-92e7-4586-a7fe-c3290d6a1855" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">featy <span class="op" style="color: #5E5E5E;">=</span> train.features[y]</span>
<span id="cb31-2">featy</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)</code></pre>
</div>
</div>
<div class="cell" data-outputid="b35eee9b-7dc8-45ed-9138-d5857c9ac8f6" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">featy.int2str(yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>['Ankle boot',
 'T - shirt / top',
 'T - shirt / top',
 'Dress',
 'T - shirt / top']</code></pre>
</div>
</div>
<div class="cell" data-outputid="762672b4-b688-4c8c-bd60-fdc4cab458f2" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">train[<span class="st" style="color: #20794D;">'label'</span>][:<span class="dv" style="color: #AD0000;">5</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>[9, 0, 0, 3, 0]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="kw" style="color: #003B4F;">def</span> collate_fn(b):</span>
<span id="cb37-2">    <span class="cf" style="color: #003B4F;">return</span> {x:torch.stack([TF.to_tensor(o[x]) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> b]),</span>
<span id="cb37-3">            y:tensor([o[y] <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> b])}</span></code></pre></div>
</div>
<div class="cell" data-outputid="48e5af16-2600-4e15-b5c4-1b490ef1e971" data-execution_count="23">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate_fn, batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb38-2">b <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dl))</span>
<span id="cb38-3">b[x].shape,b[y]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>(torch.Size([16, 1, 28, 28]),
 tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))</code></pre>
</div>
</div>
<p>Having said that, huggingface data sets absolutely lets you avoid doing everything in collate function, which if we want to create really simple applications, that’s where we’re going to eventually went ahead. So we can do this using a transform instead. And so the way we do that is we create a function. It’s going to take our batch, it’s going to replace the x in our batch with the tensor version of each of those ???? images, and I’m not even stacking them or anything. And then we’re going to return that batch. And so huggingface datasets has something called with_transform, and that’s going to take your huggingface dataset. it’s going to apply this function to every element and it doesn’t run that all now. It’s going to basically run when it behind the scenes, when it calls <strong>getitem</strong>, it will call this function on the fly. So in other words, this could have data augmentation, which can be random or whatever, because it’s going to be rerun time. You grab an item, it’s not cached or anything like that. So other than that, this data set has exactly the API, same API as any other data set.</p>
<p>It has a length it has a <strong>getitem</strong> so you can pass it to a data loader. And so PyTorch already knows how to collate dictionaries of tensors. So we’ve got a dictionary of tensors now. So that means we don’t need a collate function anymore. I can create a data loader from this without a collate function. As you can see. And so this is to give you the exactly the same thing as before about without having to create a custom collate function. Now, even this is a bit more code than I want having to return. This seems a bit silly, but the reason I had to do this is because huggingfaced datasets expects the with transform function to return to the the new version of the of the data. So I wanted to be able to write it like this transform in place and just say the change I want to make and have it automatically return that. So if I call, if I create this function, that’s exactly the same as a previous one that doesn’t have return.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="kw" style="color: #003B4F;">def</span> transforms(b):</span>
<span id="cb40-2">    b[x] <span class="op" style="color: #5E5E5E;">=</span> [TF.to_tensor(o) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> b[x]]</span>
<span id="cb40-3">    <span class="cf" style="color: #003B4F;">return</span> b</span></code></pre></div>
</div>
<div class="cell" data-outputid="f0e83052-91b5-4655-b433-716d0e4abffb" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">tds <span class="op" style="color: #5E5E5E;">=</span> train.with_transform(transforms)</span>
<span id="cb41-2">dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(tds, batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb41-3">b <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dl))</span>
<span id="cb41-4">b[x].shape,b[y]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>(torch.Size([16, 1, 28, 28]),
 tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;">def</span> _transformi(b): b[x] <span class="op" style="color: #5E5E5E;">=</span> [torch.flatten(TF.to_tensor(o)) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> b[x]]</span></code></pre></div>
</div>
<p>How would I turn this into something which does return the result? So here’s an interesting trick.</p>
<p>We could take that function, pass it to another function to create a new function, which is the a version of this in-place function that returns the result. And the way I do that is by creating a function called inplace. It takes a function, it returns a function. The function it returns is one that calls my original function and then returns the result. So this is the function. This is a function generating function, and it’s modifying an in-place function to become function that returns that a new version of that data. And so this is a function this function is passed to this function which returns a function. And here it is. So here’s the version that huggingface you’ll be able to use so I can now pass that to with_transform. And it does exactly the same thing. So this is very, very common in Python.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=27}</p>
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="kw" style="color: #003B4F;">def</span> inplace(f):</span>
<span id="cb44-2">    <span class="kw" style="color: #003B4F;">def</span> _f(b):</span>
<span id="cb44-3">        f(b)</span>
<span id="cb44-4">        <span class="cf" style="color: #003B4F;">return</span> b</span>
<span id="cb44-5">    <span class="cf" style="color: #003B4F;">return</span> _f</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">transformi <span class="op" style="color: #5E5E5E;">=</span> inplace(_transformi)</span></code></pre></div>
</div>
<div class="cell" data-outputid="d0d66fb0-04ee-46f8-b52d-63168b1a1488" data-execution_count="29">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">r <span class="op" style="color: #5E5E5E;">=</span> train.with_transform(transformi)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb46-2">r[x].shape,r[y]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([784]), 9)</code></pre>
</div>
</div>
<p>It’s so common that line of code can be entirely removed and replaced with this little token. If you have a function and put that at the start, you can then put that before a function. And what it says is:</p>
<p>take this whole function, pass it to this function and replace it with a result. So this is exactly the same as the combination of this and this.</p>
<p>And when we do it this way, this kind of little syntax, sugar is called a decorator. So there’s nothing, nothing magic about decorators. It’s literally, literally identical to this. Well, I guess the only difference is we don’t end up with this unnecessary intermediate underscore version, but the result is exactly the same. And therefore, I can create a transformed dataset by using this. And there we go. It’s all working fine. Yeah. So, I mean, none of this is particularly necessary, but what we’re doing is we’re just kind of like saying, you know, the pieces that we can make and put in place to make this stuff as easy as possible, and we don’t have to think about things too much. All right, Now, with all this, we can basically make things pretty automatic. And the way we can make things pretty automatic is we’re going to use a cool thing in Python code itemgetter.An itemgetter is a function that returns a function. So hopefully you’re getting used to this idea. Now, this creates a function that gets the a , c items from a dictionary or something that looks like a dictionary. So here’s a dictionary, it contains Keys, a, b, and c, So this function will take a dictionary and return the a and c values. And as you can see, it has done exactly that. Explain why this is useful in a moment. I just wanted to briefly mention what did I mean when I said something that looks like a dictionary? I mean, this is a dictionary. Okay, That looks like a dictionary, but it python doesn’t care about what type things actually are. It only cares about what they look like. And remember that when we call something with square brackets, when we index into something behind the scenes, it’s just call <strong>getitem</strong> so we could create our own class and it’s <strong>getitem</strong> gets the key and it’s just going to manually return one if k equals a, two if k is b or 3 otherwise. And look, that class also works just fine with an itemgetter.</p>
<p>The reason this is interesting is because like a lot of people write Python as if it’s like C++ or Java or something. They write as stiff as if it’s this kind of statically thing. But I really wanted to point out that it’s an extremely dynamic language and there’s a lot more flexibility than you might have realized anyway. That’s a little aside. So what we can do is think about a batch, for example, where we’ve got these two dictionaries. Okay, so PyTorch comes with a default collation function called not surprisingly, default_collate. So that’s part of PyTorch. And what default_collate does with dictionaries is it simply takes the matching keys and then grabs their values and stacks them together. And so that’s why if I called default_collate a is now one, three, b is now two four. That’s actually what happened before when we created this data loader is that used default_collate function which does that. It also works on things that are tuples, not dictionaries, which is what most of you would have used before.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="at" style="color: #657422;">@inplace</span></span>
<span id="cb48-2"><span class="kw" style="color: #003B4F;">def</span> transformi(b): b[x] <span class="op" style="color: #5E5E5E;">=</span> [torch.flatten(TF.to_tensor(o)) <span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> b[x]]</span></code></pre></div>
</div>
<div class="cell" data-outputid="11da9519-eb0d-4490-8701-28592086f92c" data-execution_count="31">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">tdsf <span class="op" style="color: #5E5E5E;">=</span> train.with_transform(transformi)</span>
<span id="cb49-2">r <span class="op" style="color: #5E5E5E;">=</span> tdsf[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb49-3">r[x].shape,r[y]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>(torch.Size([784]), 9)</code></pre>
</div>
</div>
<div class="cell" data-outputid="9c764c59-9bff-4936-c634-790b3a626bc6" data-execution_count="32">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">d <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(a<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,b<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>,c<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb51-2">ig <span class="op" style="color: #5E5E5E;">=</span> itemgetter(<span class="st" style="color: #20794D;">'a'</span>,<span class="st" style="color: #20794D;">'c'</span>)</span>
<span id="cb51-3">ig(d)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(1, 3)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="kw" style="color: #003B4F;">class</span> D:</span>
<span id="cb53-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, k): <span class="cf" style="color: #003B4F;">return</span> <span class="dv" style="color: #AD0000;">1</span> <span class="cf" style="color: #003B4F;">if</span> k<span class="op" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">'a'</span> <span class="cf" style="color: #003B4F;">else</span> <span class="dv" style="color: #AD0000;">2</span> <span class="cf" style="color: #003B4F;">if</span> k<span class="op" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">'b'</span> <span class="cf" style="color: #003B4F;">else</span> <span class="dv" style="color: #AD0000;">3</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="c6477a98-864b-4be2-da3f-9786e21a5443" data-execution_count="34">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">d <span class="op" style="color: #5E5E5E;">=</span> D()</span>
<span id="cb54-2">ig(d)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>(1, 3)</code></pre>
</div>
</div>
<div class="cell" data-outputid="ec28856d-13a6-4e64-fc45-00a5cc5fd656" data-execution_count="35">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><span class="bu" style="color: null;">list</span>(tdsf.features)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>['image', 'label']</code></pre>
</div>
</div>
<div class="cell" data-outputid="2a5fb3c8-d420-45af-ff77-f793f8d59000" data-execution_count="36">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">batch <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(a<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">1</span>],b<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">2</span>]), <span class="bu" style="color: null;">dict</span>(a<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">3</span>],b<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb58-2">default_collate(batch)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>{'a': [tensor([1, 3])], 'b': [tensor([2, 4])]}</code></pre>
</div>
</div>
<p>And what we can do there for is we could create something called collate_dict, which is something which is going to take a dataset and it’s going to create a itemgetter function for the features in that data set, which in this case is image and label. So this is a function which will get the image and label items. And so we’re not going to return a function and that function is simply going to call our itemgetter on default_collate. And what this is going to do is it’s going to take a dictionary and collate it into a tuple just like we did up here. So if we run that so we’re not going to call data loader on our transform dataset passing in. And remember, this is a function that attends a function. So it’s a collation function for this dataset and there it is. So now this looks a lot like what we had in our previous notebook. This is not returning a dictionary, that it’s returning a tuple. So this is a really important idea for particularly for working with hugging face data sets is that they tend to do things with dictionaries and most things in the pytouch world tend to work with tuples. So you can just use this now to convert anything that takes that returns dictionaries into something that provides tuples by passing it as a collation function to your data letter.</p>
<p>So remember, you know the thing you want to be doing this this week is, is doing things like import pdb, pdb.set.trace(), dataset,right, put breakpoints, step through, see what’s happening, you know, not just here, but also even more importantly, doing it inside, the innermost inner function. So then you can see as I do wrong, that oh, today set underscore, trace. So then we can see exactly what’s going on. Print out b, less the code and I could step into it and look, I’m now inside the default function, which is inside pytouch. And so I can now see exactly how that works. There it all is. So it’s going to go through and this code is going to look very familiar because we’ve implemented all this ourselves, except it’s being careful that it works for lots of different types of things. Dictionaries, numpy, arrays, so on and so forth.</p>
<p>so the first thing I want to do, oh, actually something I do want to mention here. This is so useful. We want to be able to use it in all of our notebooks. So rather than copying and pasting this every time, it would be really nice to create a python module that contains this definition. So we’ve created a library called nbdev. It’s really a whole system called nbdev, which does exactly that. It creates modules. You can use from your notebooks and the way you do it is you use this special thing we call comment directives, which is hash pipe export. So you put this at the top of a cell and it says do something special for this. So what this does is it says, put this into a python module for me, please export it to a python module. What python module is it going to put it in. Well, if you go all the way to the top, you tell it what default export module to create. So it’s going to create a module called datasets. So what I do at the very end of this module is I’ve got this line that says import nbdev, nbdev.nbdev_export(). And what that’s going to do for me is create a library, a Python library. I’m going to have a dataset.py in it and we’ll see everything that we export it. Here it is collate_dict. It will appear this for me. And so what that means is now in the future, in my notebooks, I will be able to import collate_dict from that from my datasets. Now you might wonder how does it know to call it Mini AI? What’s mini AI Well an nbdev, you create a setting start any file, right? You say what the name of your library is. So we’re going to be using this quite a lot now because we’re getting to the point where we’re starting to implement stuff that didn’t exist before. So previously most of this stuff was pretty much all the stuff we’ve created. I’ve said like, Oh, that already exists in PyTorch, so we don’t need it, we just use pytorch. But we’re now getting to a point where we’re starting to create stuff that doesn’t anywhere and we’ve created it ourselves and so therefore we want to be able to use it again. So during the rest of this course we’re going to be building together a library called miniai. That’s going to be our framework, our version of something like fastai, maybe it’s something like what fastai 3 will end up being. Well, see, So that’s what’s going on here too. So we’re going to be using once I start using miniai, I’ll show you exactly how to install this. But that’s what this export is. And so you might have noticed I also had an export on this inplace thing and I also had it on my necessary import statements. Okay. And we want to be able to see what this dataset looks like.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=37}</p>
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="kw" style="color: #003B4F;">def</span> collate_dict(ds):</span>
<span id="cb60-2">    get <span class="op" style="color: #5E5E5E;">=</span> itemgetter(<span class="op" style="color: #5E5E5E;">*</span>ds.features)</span>
<span id="cb60-3">    <span class="kw" style="color: #003B4F;">def</span> _f(b): <span class="cf" style="color: #003B4F;">return</span> get(default_collate(b))</span>
<span id="cb60-4">    <span class="cf" style="color: #003B4F;">return</span> _f</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="8b5750ab-2cdc-485f-aaf0-d03bb5d31a3b" data-execution_count="38">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">dlf <span class="op" style="color: #5E5E5E;">=</span> DataLoader(tdsf, batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate_dict(tdsf))</span>
<span id="cb61-2">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dlf))</span>
<span id="cb61-3">xb.shape,yb</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(torch.Size([4, 784]), tensor([9, 0, 0, 3]))</code></pre>
</div>
</div>
</section>
<section id="plotting-images" class="level2">
<h2 class="anchored" data-anchor-id="plotting-images">Plotting images</h2>
<p>So I thought it now is a good time to talk a bit about plotting because how to visualize things well is really important. And again, the idea is we know we’re not allowed to use fastai plotting library, so we got to learn how to do everything ourselves.</p>
<p>So here’s the basic way to plot such an image using matplotlib so we can create a batch, grab the x part of it, grab the very first thing in that and imshow and it show an image. And here it is. There is our ankle boot. So let’s start to think about what stuff we might create, which we can export to make this a bit easier. So let’s create something called show_image, which basically does imshow, but we’re going to do a few extra things. We will make sure that it’s in the correct access order. We will make sure it’s not uncleared character that’s on the CPU here. If it’s not a numpy array will convert it to a numpy array will be get a pass in an existing access, which we’ll talk about soon. If we want to, we’ll be able to set a title if we want to. And also this thing here removes all this ugly zero five blah blah, blah access because we’re showing an image. We don’t want any of that. So if we try that, you can see there you go. We’ve also been able to say what size we want. The image there at all is. Now here’s something interesting. When I say help, the help shows the things that I implemented, but it also shows a whole lot more things. How did that magic thing happen? And you can see they work because his fixed size, which I didn’t add all Oh, sorry, I did that. Well, okay, that’s a bad example. Anyway, these other ones all work as well. So how did that happen?</p>
<p>Well, the trick is that I added **kwargs here:</p>
<p>And <strong>kwargs says, You can pass as many any other arguments as you like that aren’t listed and they’ll all be put into a dictionary with this name and then when I call iamshow. I pass that entire dictionary. </strong> here means as separate arguments. And that’s how come it works. And then how come doesn’t know how come it knows what help to provide. The reason why is that fastcore has a special thing called delegate’s, which is a decorator. So now you know what a decorator is and you tell it. What is it that you’re going to be passing kwargs to? I’m going to be passing it to you, iamshow. And then it automatically creates the documentation correctly to show you what kwargs can do.</p>
<div class="cell" data-outputid="66594c2e-600f-4ee1-fb29-361486cedbaa" data-execution_count="39">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">b <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dl))</span>
<span id="cb63-2">xb <span class="op" style="color: #5E5E5E;">=</span> b[<span class="st" style="color: #20794D;">'image'</span>]</span>
<span id="cb63-3">img <span class="op" style="color: #5E5E5E;">=</span> xb[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb63-4">plt.imshow(img[<span class="dv" style="color: #AD0000;">0</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=40}</p>
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><span class="at" style="color: #657422;">@fc.delegates</span>(plt.Axes.imshow)</span>
<span id="cb64-2"><span class="kw" style="color: #003B4F;">def</span> show_image(im, ax<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, figsize<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, title<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, noframe<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb64-3">    <span class="co" style="color: #5E5E5E;">"Show a PIL or PyTorch image on `ax`."</span></span>
<span id="cb64-4">    <span class="cf" style="color: #003B4F;">if</span> fc.hasattrs(im, (<span class="st" style="color: #20794D;">'cpu'</span>,<span class="st" style="color: #20794D;">'permute'</span>,<span class="st" style="color: #20794D;">'detach'</span>)):</span>
<span id="cb64-5">        im <span class="op" style="color: #5E5E5E;">=</span> im.detach().cpu()</span>
<span id="cb64-6">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(im.shape)<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">3</span> <span class="kw" style="color: #003B4F;">and</span> im.shape[<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">&lt;</span><span class="dv" style="color: #AD0000;">5</span>: im<span class="op" style="color: #5E5E5E;">=</span>im.permute(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb64-7">    <span class="cf" style="color: #003B4F;">elif</span> <span class="kw" style="color: #003B4F;">not</span> <span class="bu" style="color: null;">isinstance</span>(im,np.ndarray): im<span class="op" style="color: #5E5E5E;">=</span>np.array(im)</span>
<span id="cb64-8">    <span class="cf" style="color: #003B4F;">if</span> im.shape[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>: im<span class="op" style="color: #5E5E5E;">=</span>im[...,<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb64-9">    <span class="cf" style="color: #003B4F;">if</span> ax <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: _,ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>figsize)</span>
<span id="cb64-10">    ax.imshow(im, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb64-11">    <span class="cf" style="color: #003B4F;">if</span> title <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>: ax.set_title(title)</span>
<span id="cb64-12">    ax.set_xticks([]) </span>
<span id="cb64-13">    ax.set_yticks([]) </span>
<span id="cb64-14">    <span class="cf" style="color: #003B4F;">if</span> noframe: ax.axis(<span class="st" style="color: #20794D;">'off'</span>)</span>
<span id="cb64-15">    <span class="cf" style="color: #003B4F;">return</span> ax</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="3e157927-b82a-441e-dba6-b44737ee6f0f" data-execution_count="41">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><span class="bu" style="color: null;">help</span>(show_image)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on function show_image in module __main__:

show_image(im, ax=None, figsize=None, title=None, noframe=True, *, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, interpolation_stage=None, filternorm=True, filterrad=4.0, resample=None, url=None, data=None)
    Show a PIL or PyTorch image on `ax`.
</code></pre>
</div>
</div>
<div class="cell" data-outputid="502f6f58-b209-4f8a-9e41-b1d66692944d" data-execution_count="42">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">show_image(img, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>))<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-43-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="a2a08162-63a8-4a9f-d420-c743999e96bd" data-execution_count="43">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">fig,axs <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb68-2">show_image(img, axs[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb68-3">show_image(xb[<span class="dv" style="color: #AD0000;">1</span>], axs[<span class="dv" style="color: #AD0000;">1</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-44-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So this is a really helpful way of being able to kind of extend existing functions like iamshow and still get all of their functionality and all of their documentation and at your right. So delegates is one of the most useful things we have in fast core, in my opinion. So we’re going to export that. So now we can use show image anytime you want, which is nice. Something that’s really helpful to know about matplotlib is how to create subplots. So for example, what happens if you want to plot two images next to each other. So in matplotlib subplots creates multiple plots and you pass it number of rows and the number of columns. So this here has, as you see, one row and two columns and it returns axes. Now, what it calls axes is what it refers to as the individual plots. So if we now call show image on the first image passing in Axes zero, it’s going to get that here. Right Then we call iamshow that means put the image on this subplot that I call it a subplot. Unfortunately, they call it an axis. Put it on this axis. So that’s how come we’re able to show an image, one image on the first axis and then show a second image on the second axis, by which we mean subplot. And there’s are two images. So that’s pretty handy. So I’ve decided to add some additional functionality to subplots. So therefore, I use delegates on subplots because I’m adding functionality to it and I’m going to be taking kwargs and passing it through to subplots. And the main thing I wanted to do is to automatically create an appropriate figure size by just finding out. You tell us what image size you want. And I also want to be able to add a title for the whole set of subplots. And so there it is. And then I also want to show you that in it automatically, if we want to create documentation for us as well for our library and here is the documentation. So as you can see here for the stuff I’ve added, it’s telling me exactly what each of these parameters are, that type, the defaults and information about each one and that is automatically coming from these little comets, these we call these documents. This is all automatic stuff done by fast core and native. And so you might have noticed when you look at Fastai library documentation, it always has all this info. So that’s that’s that’s why you don’t actually have to show doc it automatically added to your documentation for you.</p>
<p>I’m just showing you here what it’s going to end up looking like. And you can see that it’s worked with delegates. It’s put all the extra stuff from delegates in here as well, and they all stood out here as well. So anyway, subplots. So let’s create a three by three set of plots and we’ll grab the first images. And so now we can go through each of the subplots. Now it returns it as a three by three, basically a list of three lists of three items. So I flatten them all out into a single list. So I go through each of those subplots and go through each image and show each image on each axis. And so here’s a quick way to quickly show them all. As you can see, it’s a little bit ugly here, so we’ll keep on adding more useful, plotting functionality. So here’s something that again. It calls our subplots delegates to it, but we’re going to be able to say, for example, how many subplots do we want? And it’ll automatically calculate the rows in the columns and it’s going to remove the axes for any ones that we’re not actually using. And so here we got that. So that’s what get grids going to let us do. So we’re getting quite close. And so finally, why don’t we just create a single thing called show images that’s going to get our grid and it’s going to go through our images optionally with lists of titles and show each one.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=44}</p>
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><span class="at" style="color: #657422;">@fc.delegates</span>(plt.subplots, keep<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb69-2"><span class="kw" style="color: #003B4F;">def</span> subplots(</span>
<span id="cb69-3">    nrows:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="co" style="color: #5E5E5E;"># Number of rows in returned axes grid</span></span>
<span id="cb69-4">    ncols:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="co" style="color: #5E5E5E;"># Number of columns in returned axes grid</span></span>
<span id="cb69-5">    figsize:<span class="bu" style="color: null;">tuple</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Width, height in inches of the returned figure</span></span>
<span id="cb69-6">    imsize:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, <span class="co" style="color: #5E5E5E;"># Size (in inches) of images that will be displayed in the returned figure</span></span>
<span id="cb69-7">    suptitle:<span class="bu" style="color: null;">str</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Title to be set to returned figure</span></span>
<span id="cb69-8">    <span class="op" style="color: #5E5E5E;">**</span>kwargs</span>
<span id="cb69-9">): <span class="co" style="color: #5E5E5E;"># fig and axs</span></span>
<span id="cb69-10">    <span class="co" style="color: #5E5E5E;">"A figure and set of subplots to display images of `imsize` inches"</span></span>
<span id="cb69-11">    <span class="cf" style="color: #003B4F;">if</span> figsize <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: figsize<span class="op" style="color: #5E5E5E;">=</span>(ncols<span class="op" style="color: #5E5E5E;">*</span>imsize, nrows<span class="op" style="color: #5E5E5E;">*</span>imsize)</span>
<span id="cb69-12">    fig,ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(nrows, ncols, figsize<span class="op" style="color: #5E5E5E;">=</span>figsize, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb69-13">    <span class="cf" style="color: #003B4F;">if</span> suptitle <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>: fig.suptitle(suptitle)</span>
<span id="cb69-14">    <span class="cf" style="color: #003B4F;">if</span> nrows<span class="op" style="color: #5E5E5E;">*</span>ncols<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>: ax <span class="op" style="color: #5E5E5E;">=</span> np.array([ax])</span>
<span id="cb69-15">    <span class="cf" style="color: #003B4F;">return</span> fig,ax</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="32654ef4-680b-40a4-8d2f-8817a8ae778c" data-execution_count="47">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">fig,axs <span class="op" style="color: #5E5E5E;">=</span> subplots(<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">3</span>, imsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb70-2">imgs <span class="op" style="color: #5E5E5E;">=</span> xb[:<span class="dv" style="color: #AD0000;">8</span>]</span>
<span id="cb70-3"><span class="cf" style="color: #003B4F;">for</span> ax,img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(axs.flat,imgs): show_image(img, ax)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-46-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>And we can use that here. You can see where you have successfully got all of our labeled images. And so we yeah, I think all this stuff for the plotting is pretty useful. So as you might have noticed, they were all exported. So in our datasets.py, we’ve got our get_grid, we’ve got our subplots, we’ve got our show_image. So that’s going to make life easier for us now since we have to create everything from scratch. We have created all of those things. So as I mentioned at the very end, we have this one line of code to run. And so just to show you, if I remove miniai data sets,it’s all empty. And then I run this line of code and now it’s back, as you can see, and it tells you it’s auto generated. All right, So we are nearly at the point where we can build our learner. And once we’ve built learner, we’re going to be able to really dive deep into training and studying models.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=48}</p>
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><span class="at" style="color: #657422;">@fc.delegates</span>(subplots)</span>
<span id="cb71-2"><span class="kw" style="color: #003B4F;">def</span> get_grid(</span>
<span id="cb71-3">    n:<span class="bu" style="color: null;">int</span>, <span class="co" style="color: #5E5E5E;"># Number of axes</span></span>
<span id="cb71-4">    nrows:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Number of rows, defaulting to `int(math.sqrt(n))`</span></span>
<span id="cb71-5">    ncols:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Number of columns, defaulting to `ceil(n/rows)`</span></span>
<span id="cb71-6">    title:<span class="bu" style="color: null;">str</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># If passed, title set to the figure</span></span>
<span id="cb71-7">    weight:<span class="bu" style="color: null;">str</span><span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bold'</span>, <span class="co" style="color: #5E5E5E;"># Title font weight</span></span>
<span id="cb71-8">    size:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">14</span>, <span class="co" style="color: #5E5E5E;"># Title font size</span></span>
<span id="cb71-9">    <span class="op" style="color: #5E5E5E;">**</span>kwargs,</span>
<span id="cb71-10">): <span class="co" style="color: #5E5E5E;"># fig and axs</span></span>
<span id="cb71-11">    <span class="co" style="color: #5E5E5E;">"Return a grid of `n` axes, `rows` by `cols`"</span></span>
<span id="cb71-12">    <span class="cf" style="color: #003B4F;">if</span> nrows: ncols <span class="op" style="color: #5E5E5E;">=</span> ncols <span class="kw" style="color: #003B4F;">or</span> <span class="bu" style="color: null;">int</span>(np.floor(n<span class="op" style="color: #5E5E5E;">/</span>nrows))</span>
<span id="cb71-13">    <span class="cf" style="color: #003B4F;">elif</span> ncols: nrows <span class="op" style="color: #5E5E5E;">=</span> nrows <span class="kw" style="color: #003B4F;">or</span> <span class="bu" style="color: null;">int</span>(np.ceil(n<span class="op" style="color: #5E5E5E;">/</span>ncols))</span>
<span id="cb71-14">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb71-15">        nrows <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(math.sqrt(n))</span>
<span id="cb71-16">        ncols <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(np.floor(n<span class="op" style="color: #5E5E5E;">/</span>nrows))</span>
<span id="cb71-17">    fig,axs <span class="op" style="color: #5E5E5E;">=</span> subplots(nrows, ncols, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb71-18">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n, nrows<span class="op" style="color: #5E5E5E;">*</span>ncols): axs.flat[i].set_axis_off()</span>
<span id="cb71-19">    <span class="cf" style="color: #003B4F;">if</span> title <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>: fig.suptitle(title, weight<span class="op" style="color: #5E5E5E;">=</span>weight, size<span class="op" style="color: #5E5E5E;">=</span>size)</span>
<span id="cb71-20">    <span class="cf" style="color: #003B4F;">return</span> fig,axs</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="06e50182-c7b8-4e51-c788-3eaf96883026" data-execution_count="49">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1">fig,axs <span class="op" style="color: #5E5E5E;">=</span> get_grid(<span class="dv" style="color: #AD0000;">8</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, imsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb72-2"><span class="cf" style="color: #003B4F;">for</span> ax,img <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(axs.flat,imgs): show_image(img, ax)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-48-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=50}</p>
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="at" style="color: #657422;">@fc.delegates</span>(subplots)</span>
<span id="cb73-2"><span class="kw" style="color: #003B4F;">def</span> show_images(ims:<span class="bu" style="color: null;">list</span>, <span class="co" style="color: #5E5E5E;"># Images to show</span></span>
<span id="cb73-3">                nrows:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">|</span><span class="va" style="color: #111111;">None</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Number of rows in grid</span></span>
<span id="cb73-4">                ncols:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">|</span><span class="va" style="color: #111111;">None</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Number of columns in grid (auto-calculated if None)</span></span>
<span id="cb73-5">                titles:<span class="bu" style="color: null;">list</span><span class="op" style="color: #5E5E5E;">|</span><span class="va" style="color: #111111;">None</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="co" style="color: #5E5E5E;"># Optional list of titles for each image</span></span>
<span id="cb73-6">                <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb73-7">    <span class="co" style="color: #5E5E5E;">"Show all images `ims` as subplots with `rows` using `titles`"</span></span>
<span id="cb73-8">    axs <span class="op" style="color: #5E5E5E;">=</span> get_grid(<span class="bu" style="color: null;">len</span>(ims), nrows, ncols, <span class="op" style="color: #5E5E5E;">**</span>kwargs)[<span class="dv" style="color: #AD0000;">1</span>].flat</span>
<span id="cb73-9">    <span class="cf" style="color: #003B4F;">for</span> im,t,ax <span class="kw" style="color: #003B4F;">in</span> zip_longest(ims, titles <span class="kw" style="color: #003B4F;">or</span> [], axs): show_image(im, ax<span class="op" style="color: #5E5E5E;">=</span>ax, title<span class="op" style="color: #5E5E5E;">=</span>t)</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">yb <span class="op" style="color: #5E5E5E;">=</span> b[<span class="st" style="color: #20794D;">'label'</span>]</span>
<span id="cb74-2">lbls <span class="op" style="color: #5E5E5E;">=</span> yb[:<span class="dv" style="color: #AD0000;">8</span>]</span></code></pre></div>
</div>
<div class="cell" data-outputid="850aab42-04f8-4c06-a513-eb9fdefc9918" data-execution_count="52">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">names <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"Top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot"</span>.split()</span>
<span id="cb75-2">titles <span class="op" style="color: #5E5E5E;">=</span> itemgetter(<span class="op" style="color: #5E5E5E;">*</span>lbls)(names)</span>
<span id="cb75-3"><span class="co" style="color: #5E5E5E;">' '</span>.join(titles)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>'Boot Top Top Dress Top Pullover Sneaker Pullover'</code></pre>
</div>
</div>
<div class="cell" data-outputid="654c90f9-a230-4cc4-f3b4-8911b4735d6c" data-execution_count="53">
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1">show_images(imgs, imsize<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.7</span>, titles<span class="op" style="color: #5E5E5E;">=</span>titles)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=54}</p>
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><span class="kw" style="color: #003B4F;">class</span> DataLoaders:</span>
<span id="cb78-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>dls): <span class="va" style="color: #111111;">self</span>.train,<span class="va" style="color: #111111;">self</span>.valid <span class="op" style="color: #5E5E5E;">=</span> dls[:<span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb78-3"></span>
<span id="cb78-4">    <span class="at" style="color: #657422;">@classmethod</span></span>
<span id="cb78-5">    <span class="kw" style="color: #003B4F;">def</span> from_dd(cls, dd, batch_size, as_tuple<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb78-6">        f <span class="op" style="color: #5E5E5E;">=</span> collate_dict(dd[<span class="st" style="color: #20794D;">'train'</span>])</span>
<span id="cb78-7">        <span class="cf" style="color: #003B4F;">return</span> cls(<span class="op" style="color: #5E5E5E;">*</span>get_dls(<span class="op" style="color: #5E5E5E;">*</span>dd.values(), bs<span class="op" style="color: #5E5E5E;">=</span>batch_size, collate_fn<span class="op" style="color: #5E5E5E;">=</span>f))</span></code></pre></div>
<p>:::</p>


</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 8/index.html</guid>
  <pubDate>Mon, 27 Mar 2023 19:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 7</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 7/index.html</link>
  <description><![CDATA[ 



<p>All credits goes to fast.ai <br> All mistakes are mine. <br> I have to put code from previous lessons scence they are connected.</p>
<p>You should know and practice following after this blog post : 1- Refactor pervious code to make it cleaner <br> 2- Know how nn module works in pytorch <br> 3- how setattr works ? <br> 4- <strong>repr</strong> ? <br> 5- yeild from ? <br> 6- supper() and object ? <br> 7- reduce, map ? <br> 8- optimizer <br> 9- learning rate ? <br> 10- Sampler <br> 11- Collate function <br> 12- Multi processing data loader <br></p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=2}</p>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pickle,gzip,math,os,time,shutil,torch,matplotlib <span class="im" style="color: #00769E;">as</span> mpl,numpy <span class="im" style="color: #00769E;">as</span> np,matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor,nn</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span>
<span id="cb2-2"></span>
<span id="cb2-3">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb2-4">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb2-5">np.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb2-8">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb2-9">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-10">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<div class="cell" data-outputid="2272b66b-e737-422d-f60a-bb30d822ec39" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 Mar 29 06:53 mnist.pkl.gz</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span>
<span id="cb6-2">x_train, y_train, x_valid, y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, [x_train, y_train, x_valid, y_valid])</span></code></pre></div>
</div>
<section id="initial-setup" class="level2">
<h2 class="anchored" data-anchor-id="initial-setup">Initial setup</h2>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">n,m <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb7-2">c <span class="op" style="color: #5E5E5E;">=</span> y_train.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-3">nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb8-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb8-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb8-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]</span>
<span id="cb8-5">        </span>
<span id="cb8-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb8-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb8-8">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<div class="cell" data-outputid="6cffc6fb-58f5-4171-8df5-370f334dc612" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(m, nh, <span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb9-2">pred <span class="op" style="color: #5E5E5E;">=</span> model(x_train)</span>
<span id="cb9-3">pred.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>torch.Size([50000, 10])</code></pre>
</div>
</div>
</section>
<section id="cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss">Cross entropy loss</h3>
<p>First, we will need to compute the softmax of our activations. This is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chbox%7Bsoftmax(x)%7D_%7Bi%7D%20=%20%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7Be%5E%7Bx_%7B0%7D%7D%20+%20e%5E%7Bx_%7B1%7D%7D%20+%20%5Ccdots%20+%20e%5E%7Bx_%7Bn-1%7D%7D%7D"></p>
<p>or more concisely:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chbox%7Bsoftmax(x)%7D_%7Bi%7D%20=%20%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7B%5Csum%5Climits_%7B0%20%5Cleq%20j%20%5Clt%20n%7D%20e%5E%7Bx_%7Bj%7D%7D%7D"></p>
<p>In practice, we will need the log of the softmax when we calculate the loss.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> (x.exp()<span class="op" style="color: #5E5E5E;">/</span>(x.exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>))).log()</span></code></pre></div>
</div>
<div class="cell" data-outputid="3b62999d-ea60-49d7-d28f-9255a042e070" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">log_softmax(pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Note that the formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cleft%20(%20%5Cfrac%7Ba%7D%7Bb%7D%20%5Cright%20)%20=%20%5Clog(a)%20-%20%5Clog(b)"></p>
<p>gives a simplification when we compute the log softmax:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> x <span class="op" style="color: #5E5E5E;">-</span> x.exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>).log()</span></code></pre></div>
</div>
<p>Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the <a href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp trick</a>. The idea is to use the following formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cleft%20(%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D%7D%20%5Cright%20)%20=%20%5Clog%20%5Cleft%20(%20e%5E%7Ba%7D%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D-a%7D%20%5Cright%20)%20=%20a%20+%20%5Clog%20%5Cleft%20(%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D-a%7D%20%5Cright%20)"></p>
<p>where a is the maximum of the <img src="https://latex.codecogs.com/png.latex?x_%7Bj%7D">.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;">def</span> logsumexp(x):</span>
<span id="cb15-2">    m <span class="op" style="color: #5E5E5E;">=</span> x.<span class="bu" style="color: null;">max</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb15-3">    <span class="cf" style="color: #003B4F;">return</span> m <span class="op" style="color: #5E5E5E;">+</span> (x<span class="op" style="color: #5E5E5E;">-</span>m[:,<span class="va" style="color: #111111;">None</span>]).exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>).log()</span></code></pre></div>
</div>
<p>This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> x <span class="op" style="color: #5E5E5E;">-</span> x.logsumexp(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="2b19a3b4-23b2-4f36-b017-d07625ec92de" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">test_close(logsumexp(pred), pred.logsumexp(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb17-2">sm_pred <span class="op" style="color: #5E5E5E;">=</span> log_softmax(pred)</span>
<span id="cb17-3">sm_pred</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;SubBackward0&gt;)</code></pre>
</div>
</div>
<p>The cross entropy loss for some target <img src="https://latex.codecogs.com/png.latex?x"> and some prediction <img src="https://latex.codecogs.com/png.latex?p(x)"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20-%5Csum%20x%5C,%20%5Clog%20p(x)%20"></p>
<p>But since our <img src="https://latex.codecogs.com/png.latex?x">s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as <img src="https://latex.codecogs.com/png.latex?-%5Clog(p_%7Bi%7D)"> where i is the index of the desired target.</p>
<p>This can be done using numpy-style <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing">integer array indexing</a>. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.</p>
<div class="cell" data-outputid="ee9347df-3ac0-42be-98b3-3fb5984d1d69" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">y_train[:<span class="dv" style="color: #AD0000;">3</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([5, 0, 4])</code></pre>
</div>
</div>
<div class="cell" data-outputid="7605ed23-d3b5-4dcc-a852-a50a0292f21a" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">sm_pred[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">5</span>],sm_pred[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>],sm_pred[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">4</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>(tensor(-2.20, grad_fn=&lt;SelectBackward0&gt;),
 tensor(-2.37, grad_fn=&lt;SelectBackward0&gt;),
 tensor(-2.36, grad_fn=&lt;SelectBackward0&gt;))</code></pre>
</div>
</div>
<div class="cell" data-outputid="21bd364b-09c0-4f72-e186-47805b27b192" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">sm_pred[[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>], y_train[:<span class="dv" style="color: #AD0000;">3</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>tensor([-2.20, -2.37, -2.36], grad_fn=&lt;IndexBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="kw" style="color: #003B4F;">def</span> nll(<span class="bu" style="color: null;">input</span>, target): <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span><span class="bu" style="color: null;">input</span>[<span class="bu" style="color: null;">range</span>(target.shape[<span class="dv" style="color: #AD0000;">0</span>]), target].mean()</span></code></pre></div>
</div>
<div class="cell" data-outputid="e59f60e3-b676-4fba-b0b6-525f1845fe91" data-execution_count="21">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">loss <span class="op" style="color: #5E5E5E;">=</span> nll(sm_pred, y_train)</span>
<span id="cb26-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(2.30, grad_fn=&lt;NegBackward0&gt;)</code></pre>
</div>
</div>
<p>Then use PyTorch’s implementation.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">test_close(F.nll_loss(F.log_softmax(pred, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>), y_train), loss, <span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
<p>In PyTorch, <code>F.log_softmax</code> and <code>F.nll_loss</code> are combined in one optimized function, <code>F.cross_entropy</code>.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">test_close(F.cross_entropy(pred, y_train), loss, <span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="basic-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="basic-training-loop">Basic training loop</h2>
<p>Basically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">loss_func <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy</span></code></pre></div>
</div>
<div class="cell" data-outputid="cc721e88-421a-4b84-dddb-6f4e188b4dea" data-execution_count="25">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>                  <span class="co" style="color: #5E5E5E;"># batch size</span></span>
<span id="cb31-2"></span>
<span id="cb31-3">xb <span class="op" style="color: #5E5E5E;">=</span> x_train[<span class="dv" style="color: #AD0000;">0</span>:bs]     <span class="co" style="color: #5E5E5E;"># a mini-batch from x</span></span>
<span id="cb31-4">preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)      <span class="co" style="color: #5E5E5E;"># predictions</span></span>
<span id="cb31-5">preds[<span class="dv" style="color: #AD0000;">0</span>], preds.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=&lt;SelectBackward0&gt;),
 torch.Size([50, 10]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="d5695218-1855-43eb-b010-ac16d38c904f" data-execution_count="26">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">yb <span class="op" style="color: #5E5E5E;">=</span> y_train[<span class="dv" style="color: #AD0000;">0</span>:bs]</span>
<span id="cb33-2">yb</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,
        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])</code></pre>
</div>
</div>
<div class="cell" data-outputid="be947a14-7b6d-4afd-ffbf-230c17bd0874" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">loss_func(preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor(2.30, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-outputid="192c81ce-fffa-478a-bd67-0617d7dc7533" data-execution_count="28">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">preds.argmax(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3,
        9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=29}</p>
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">def</span> accuracy(out, yb): <span class="cf" style="color: #003B4F;">return</span> (out.argmax(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)<span class="op" style="color: #5E5E5E;">==</span>yb).<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="48f75b1f-c246-4ff1-ba87-0691661e5170" data-execution_count="30">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">accuracy(preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor(0.08)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>   <span class="co" style="color: #5E5E5E;"># learning rate</span></span>
<span id="cb42-2">epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> <span class="co" style="color: #5E5E5E;"># how many epochs to train for</span></span></code></pre></div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=32}</p>
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;">def</span> report(loss, preds, yb): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'</span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">, </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, yb)<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<p>:::</p>
<div class="cell" data-outputid="04924c1c-0921-4758-d9bb-61c2e8c11ac7" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[:bs],y_train[:bs]</span>
<span id="cb44-2">preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb44-3">report(loss_func(preds, yb), preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.30, 0.08</code></pre>
</div>
</div>
<div class="cell" data-outputid="163cfbfa-2b56-458f-c858-2c2595310bb7" data-execution_count="34">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb46-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb46-3">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(i, <span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb46-4">        xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[s],y_train[s]</span>
<span id="cb46-5">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb46-6">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb46-7">        loss.backward()</span>
<span id="cb46-8">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb46-9">            <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> model.layers:</span>
<span id="cb46-10">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">hasattr</span>(l, <span class="st" style="color: #20794D;">'weight'</span>):</span>
<span id="cb46-11">                    l.weight <span class="op" style="color: #5E5E5E;">-=</span> l.weight.grad <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb46-12">                    l.bias   <span class="op" style="color: #5E5E5E;">-=</span> l.bias.grad   <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb46-13">                    l.weight.grad.zero_()</span>
<span id="cb46-14">                    l.bias  .grad.zero_()</span>
<span id="cb46-15">    report(loss, preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.11, 0.96
0.13, 0.96
0.10, 0.96</code></pre>
</div>
</div>
</section>
<section id="using-parameters-and-optim" class="level2">
<h2 class="anchored" data-anchor-id="using-parameters-and-optim">Using parameters and optim</h2>
<p>And so we’re now going to I’m going to show you something that’s part of pytorch and then going to show you how to build it, and then you’ll see why this is really useful. So PyTorch has a sub Module starting nn.&nbsp;And in there there’s something called the Module class. Now we can we don’t normally use it this way, but I just want to show you how it works. We can create an instance of it in the usual way we create instances of classes and then we can assign things to attributes of that module. So for example, it’s assign a linear layer to it. And if we now print out that, you’ll see it says, Oh, this is a module containing something called f00, which is a linear layer. But here’s something quite tricky. This module we can say, show me all of the named children of that module and it says, Oh, this one code foo and it’s a linear layer. And we can say, Oh, show me all of the parameters of this module. And it says, Oh, okay, sure. There’s two of them. There’s this four by three tensor, that’s the weights and there’s this four long vector. That’s the biases. And so somehow just by creating this module and assigning this to it, it’s automatically tracked what’s in this module and what are its parameters. That’s pretty neat. So we’re going to see both how and why it does that. I’m just going to point out, by the way, why did I add list here? If I just said m1.named_children(), it just prints out generate an object which is not very helpful and that’s because this is a kind of iterator called a generator, and it’s something which is going to only produce the contents of this when I actually do something with it, such as list them out.</p>
<p>So just popping a list around a generator is one way to like run the generator and get its output. That little trick when you want to look inside a generator.</p>
<p>Okay, so now, as I said, we don’t normally use it this way. What we normally do is we create our own class. So, for example, we create our own multilayer perception and we inherit it. We inherit from a nn.Module. And so then in <strong>init</strong>, this is the thing that constructs an object of the class. This is the special magic method that does that well, say, okay, well, how many inputs are there to this multilayer perceptron? How many hidden activations and how many output activations are there? So just be one hidden layer. And then here we can do just like we did up here, where we assign things as attributes. We can do that in this constructor, so we create an l1 attribute, which is a linear layer from number into number. Hidden l2 is a linear layer from number hidden number out, and we’ll also create a relu. And so when we call that module(<strong>call</strong>), we can take the input that we get and run the linear layer and then run the relu here and then run the l2. And so I can create one of these, as you say, and I can have a look and see like, Oh, here’s the attribute l1 and there it is, like I said, and I can say print out the model and the model knows all the stuff that’s in it. And I can go through each of the named children and print out the name and the layer. Now, of course, if you’re a member, although you can use <strong>call</strong>, we actually showed how we can refactor things using forward such that it would automatically kind of do the things necessary to make all the, you know, or automatic gradient stuff work correctly.</p>
<p>And so in practice we’re actually not going to do it <strong>call</strong> we would do forward. So this is an example of creating a custom PyTorch module. And the key thing to recognize is that it knows what are the attributes you added to it, and it also knows what all the parameters.</p>
<p>So if I go through the parameters and print out the shapes, you can see I’ve got my linear layers, weights first, my first linear layers weights, my first linear layers biases second linear layers weights, second linear layers biases. And this is because we set nh, the number of hidden to 50.</p>
<p>So why is that interesting? Well, because now I don’t have to write all this anymore. Going through layers and having to make sure that they’ve all been put into a list where you’ve just been able to add them as attributes and they’re automatically going to appear as parameters. So we can just say, go through each parameter and update it based on the gradient and the learning rate. And furthermore, you can actually just go model.zero_grad and it’ll zero out all of the gradients. So that’s really made our code quite a lot nicer and quite a lot more flexible, which is cool. we do</p>
<p>So let’s check that this still works. There we go. So just to clarify with if I called report on this before I ran it, as you would expect, the accuracy is about 8% with about 10% less and the loss is pretty high. And so after I run this fit this model, the accuracy goes up and the loss goes down. So basically it’s all of this is exactly the same as before. The only thing I’ve changed are these two lines of code, so that’s a really useful refactoring. So what how on earth did this happen? How did it know what the parameters and layers are? Automatically it used a trick called dunder <strong>setattr</strong>. and we’re going to create our own and nn.module.</p>
<section id="parameters" class="level3">
<h3 class="anchored" data-anchor-id="parameters">Parameters</h3>
<div class="cell" data-outputid="1b4fe53d-f021-4e9f-ba97-813bb609d500" data-execution_count="35">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">m1 <span class="op" style="color: #5E5E5E;">=</span> nn.Module()</span>
<span id="cb48-2">m1.foo <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb48-3">m1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>Module(
  (foo): Linear(in_features=3, out_features=4, bias=True)
)</code></pre>
</div>
</div>
<div class="cell" data-outputid="fdb8698c-bdd9-4717-bc9a-d70d578a3c7c" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="bu" style="color: null;">list</span>(m1.named_children())</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>[('foo', Linear(in_features=3, out_features=4, bias=True))]</code></pre>
</div>
</div>
<div class="cell" data-outputid="3bf1e94a-fa34-4218-a70e-a717d631174c" data-execution_count="37">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">m1.named_children()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>&lt;generator object Module.named_children at 0x7f64aaeec190&gt;</code></pre>
</div>
</div>
<div class="cell" data-outputid="3e3dfbcf-6a2d-446b-9afa-d8fae96bc92f" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="bu" style="color: null;">list</span>(m1.parameters())</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>[Parameter containing:
 tensor([[ 0.57,  0.43, -0.30],
         [ 0.13, -0.32, -0.24],
         [ 0.51,  0.04,  0.22],
         [ 0.13, -0.17, -0.24]], requires_grad=True), Parameter containing:
 tensor([-0.01, -0.51, -0.39,  0.56], requires_grad=True)]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><span class="kw" style="color: #003B4F;">class</span> MLP(nn.Module):</span>
<span id="cb56-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb56-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb56-4">        <span class="va" style="color: #111111;">self</span>.l1 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_in,nh)</span>
<span id="cb56-5">        <span class="va" style="color: #111111;">self</span>.l2 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(nh,n_out)</span>
<span id="cb56-6">        <span class="va" style="color: #111111;">self</span>.relu <span class="op" style="color: #5E5E5E;">=</span> nn.ReLU()</span>
<span id="cb56-7">        </span>
<span id="cb56-8">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.l2(<span class="va" style="color: #111111;">self</span>.relu(<span class="va" style="color: #111111;">self</span>.l1(x)))</span></code></pre></div>
</div>
<div class="cell" data-outputid="c7353b70-12cc-4cd0-aa92-5d770265dd43" data-execution_count="40">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1">model <span class="op" style="color: #5E5E5E;">=</span> MLP(m, nh, <span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb57-2">model.l1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>Linear(in_features=784, out_features=50, bias=True)</code></pre>
</div>
</div>
<div class="cell" data-outputid="25aeb0a6-b23e-4981-b27b-5b650ece60fe" data-execution_count="41">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>MLP(
  (l1): Linear(in_features=784, out_features=50, bias=True)
  (l2): Linear(in_features=50, out_features=10, bias=True)
  (relu): ReLU()
)</code></pre>
</div>
</div>
<div class="cell" data-outputid="dab066f2-cf78-4e9d-f157-65ddac0cf574" data-execution_count="42">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="cf" style="color: #003B4F;">for</span> name,l <span class="kw" style="color: #003B4F;">in</span> model.named_children(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">: </span><span class="sc" style="color: #5E5E5E;">{</span>l<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>l1: Linear(in_features=784, out_features=50, bias=True)
l2: Linear(in_features=50, out_features=10, bias=True)
relu: ReLU()</code></pre>
</div>
</div>
<div class="cell" data-outputid="baa2b052-df5b-419c-8643-88c32fca8dde" data-execution_count="43">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> model.parameters(): <span class="bu" style="color: null;">print</span>(p.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([50, 784])
torch.Size([50])
torch.Size([10, 50])
torch.Size([10])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><span class="kw" style="color: #003B4F;">def</span> fit():</span>
<span id="cb65-2">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb65-3">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb65-4">            s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(i, <span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb65-5">            xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[s],y_train[s]</span>
<span id="cb65-6">            preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb65-7">            loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb65-8">            loss.backward()</span>
<span id="cb65-9">            <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb65-10">                <span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> model.parameters(): p <span class="op" style="color: #5E5E5E;">-=</span> p.grad <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb65-11">                model.zero_grad()</span>
<span id="cb65-12">        report(loss, preds, yb)</span></code></pre></div>
</div>
<div class="cell" data-outputid="4e134716-8bb9-49ae-caa6-612a095a23fd" data-execution_count="45">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.19, 0.96
0.11, 0.96
0.04, 1.00</code></pre>
</div>
</div>
<p>Behind the scenes, PyTorch overrides the <code>__setattr__</code> function in <code>nn.Module</code> so that the submodules you define are properly registered as parameters of the model.</p>
<p>Now. So if there was no such thing as an nn.module, here’s how would build it. And so let’s actually build it and also add some things to it. So in <strong>init</strong>, we would have to create a dictionary for our named children. This is going to contain a list dictionary of all of the layers. Okay. And then just like before, we’ll create a couple of linear layers, right? And then what we’re going to do is going to define this special magic thing that Python has <strong>setattr</strong>. And this is called automatically by Python if you have it, every time you set an attribute such as here or here and it’s going to be past the name of the attribute, the key and the value is the actual thing on the right hand side of the equals sign. Now, generally speaking, things that start with an underscore where we use for either private stuff. So we check that it doesn’t start with an underscore. And if it doesn’t start with an underscore, <strong>setattr</strong> will put this value into the modules dictionary with this key and then call Python’s the normal python <strong>setattr</strong>, try to make sure it just actually does the attribute setting. So super is how you call whatever is in the the superclass, the base class. So another useful thing to know about is how do we how does how does it do this nifty thing where you can just type the name and it kind of lists out all this information about it.</p>
<p>That’s a special thing called <strong>repr</strong>. So here, <strong>repr</strong> will just have it return a stringified version of the module’s dictionary. And then here we’ve got parameters. How did parameters work? So how did this thing work? Well, we can go through each of those modules, go through each value. So the values of the modules is all the actual layers and then go through each of the parameters in each module and yield p.&nbsp;So that’s going to, that’s going to create an iterator. If you remember when we looked at iterates for all the parameters, So let’s try it so we can create one of these modules.</p>
<p>there they are now just mentioned something that’s optional, kind of like advanced Python that a lot of people don’t know about, which is there’s no need to loop through a list or a generator or I guess I say look for an iterator and yield. There’s actually a shortcut, which is you can just say yield from and then give it the iterator.</p>
<p>And so with that we can get this all down to one line of code and it’ll do exactly the same thing. So that’s basically saying yield one at a time. Everything in here, that’s what yield from does. So there’s a little advanced python thing, totally optional. But if you’re interested I think it can be kind of neat.</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><span class="kw" style="color: #003B4F;">class</span> MyModule:</span>
<span id="cb68-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb68-3">        <span class="va" style="color: #111111;">self</span>._modules <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb68-4">        <span class="va" style="color: #111111;">self</span>.l1 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_in,nh)</span>
<span id="cb68-5">        <span class="va" style="color: #111111;">self</span>.l2 <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(nh,n_out)</span>
<span id="cb68-6"></span>
<span id="cb68-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__setattr__</span>(<span class="va" style="color: #111111;">self</span>,k,v):</span>
<span id="cb68-8">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> k.startswith(<span class="st" style="color: #20794D;">"_"</span>): <span class="va" style="color: #111111;">self</span>._modules[k] <span class="op" style="color: #5E5E5E;">=</span> v</span>
<span id="cb68-9">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__setattr__</span>(k,v)</span>
<span id="cb68-10"></span>
<span id="cb68-11">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f'</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>_modules<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb68-12">    </span>
<span id="cb68-13">    <span class="kw" style="color: #003B4F;">def</span> parameters(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb68-14">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>._modules.values(): <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> l.parameters()</span></code></pre></div>
</div>
<div class="cell" data-outputid="bc056413-53f3-4a45-da0e-b04497ea826d" data-execution_count="47">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">mdl <span class="op" style="color: #5E5E5E;">=</span> MyModule(m,nh,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb69-2">mdl</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}</code></pre>
</div>
</div>
<div class="cell" data-outputid="a1d75396-b272-427e-d32e-644aa8a57cda" data-execution_count="48">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> mdl.parameters(): <span class="bu" style="color: null;">print</span>(p.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([50, 784])
torch.Size([50])
torch.Size([10, 50])
torch.Size([10])</code></pre>
</div>
</div>
<p>So we’ve now learned how to create our own implementation of an nn.module and therefore we are now allowed to use pytouches nn.module. So that’s good news. So how would we do using the PyTorch and nn.module? How would we create the model that we started with, which is where we had this self,layers because we want to somehow register all of these all at once? That’s not going to happen based on the code we just wrote. So to do that, let’s have a look. We can so let’s make a list of the layers we want. And so we’ll create again, a subclass of nn.module. Make sure you call the super classes in it first(super()<strong>init</strong>) and we’re just store list of layers. And then to tell PyTorch about all those layers, we basically have to loop through them and call add_module and say what the name of the module is and what the module is. And again, because should probably should have used forward to here in the first place and you can say this is now done exactly the same thing. Okay. So if you’ve used a sequential model before, you’ll see or you can say that we’re on the path to creating a sequential model.</p>
<p>Okay. So Gonash asked an interesting question, which is what on earth is super calling? Because we actually in fact, we don’t even need the parentheses here. We actually don’t have a base class. That’s because if you don’t put any parentheses or if you put empty parentheses, it’s actually a shortcut for writing that. And so Python has stuff in object which does, you know, all the normal object, things like storing your attributes so that you can get them back later. So that’s what’s happening there. Okay.</p>
<p>So this is a little bit awkward is to have to store the list and then enumerate and call add_module. So now that we’ve implemented that from scratch, we can use PyTorch is version, which is they’ve just got something called ModuleList that just does that for you. Okay. So if you use ModuleList and pass that list of layers, it will just go ahead and register them all those modules for you. So here’s something called sequential model. So this is just like nn.sequential now. So if I create it passing in the layers, there you go. You can see that’s my module containing my module list with my layers. And so to know why I never used forward for these things, it’s silly because it doesn’t add a terribly in this stage. But anyhow, okay, so call fit. And there we go. Okay, so, so in forward here, I just go through each layer and I set the result of that equal to calling that layer on the previous result and then pass and return it at the end.</p>
</section>
<section id="registering-modules" class="level3">
<h3 class="anchored" data-anchor-id="registering-modules">Registering modules</h3>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="im" style="color: #00769E;">from</span> functools <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">reduce</span></span></code></pre></div>
</div>
<p>We can use the original <code>layers</code> approach, but we have to register the modules.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">layers <span class="op" style="color: #5E5E5E;">=</span> [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>)]</span></code></pre></div>
</div>
<p>Now there’s little another way of doing this, which I think is kind of fun. It’s not like shorter or anything at this stage. I just wanted to show an example of something that you see quite a lot in machine learning code, which is the use of reduce.</p>
<p>This implementation here is exactly the same as this thing here. So let me explain how it works. A lot reduced. So reduced is a very common kind of like fundamental or computer science concept reductions. This is something that does a reduction.</p>
<p>And what a reduction is??</p>
<p>is it’s something that says start with the third parameter, some initial value. So we’re going to start with x, the thing with being passed and then loop through a sequence. So look through each of our layers and then for each layer, call some function. Here is our function and the function is going to get passed. First time around, it’ll be past the initial value and the first thing in your list. So your first layer and x. So it’s just going to call the layer function on x the second time around to take the output of that and passes that in as a second as the first parameter and passes in the second layer. So then the second time this goes through, it’s going to be calling the second layer on the result of the first layer and so forth, and that’s what a reduction is. And so you might see reduce, you’ll certainly see it talked about quite a lot in in papers and books and you might sometimes also see it in code. It’s a very general concept. And so here’s how you can implement a sequential model using reduce. So there’s no explicit loop there, although it’s still happening internally.</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb75-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, layers):</span>
<span id="cb75-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb75-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> layers</span>
<span id="cb75-5">        <span class="cf" style="color: #003B4F;">for</span> i,l <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.layers): <span class="va" style="color: #111111;">self</span>.add_module(<span class="ss" style="color: #20794D;">f'layer_</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>, l)</span>
<span id="cb75-6"></span>
<span id="cb75-7">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x): <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">reduce</span>(<span class="kw" style="color: #003B4F;">lambda</span> val,layer: layer(val), <span class="va" style="color: #111111;">self</span>.layers, x)</span></code></pre></div>
</div>
<div class="cell" data-outputid="3fca77e4-0de3-4bfa-cc68-38ca74c56693" data-execution_count="52">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(layers)</span>
<span id="cb76-2">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>Model(
  (layer_0): Linear(in_features=784, out_features=50, bias=True)
  (layer_1): ReLU()
  (layer_2): Linear(in_features=50, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<div class="cell" data-outputid="35622aa5-051d-4bb5-c79d-94653832b175" data-execution_count="53">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">model(xb).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>torch.Size([50, 10])</code></pre>
</div>
</div>
</section>
<section id="nn.modulelist" class="level3">
<h3 class="anchored" data-anchor-id="nn.modulelist">nn.ModuleList</h3>
<p><code>nn.ModuleList</code> does this for us.</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><span class="kw" style="color: #003B4F;">class</span> SequentialModel(nn.Module):</span>
<span id="cb80-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, layers):</span>
<span id="cb80-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb80-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> nn.ModuleList(layers)</span>
<span id="cb80-5">        </span>
<span id="cb80-6">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb80-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb80-8">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<div class="cell" data-outputid="d5f2157d-6f27-4179-a5f5-383c21769c18" data-execution_count="55">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1">model <span class="op" style="color: #5E5E5E;">=</span> SequentialModel(layers)</span>
<span id="cb81-2">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>SequentialModel(
  (layers): ModuleList(
    (0): Linear(in_features=784, out_features=50, bias=True)
    (1): ReLU()
    (2): Linear(in_features=50, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<div class="cell" data-outputid="65105f38-d3be-4aa9-86fd-be0e9e4fe994" data-execution_count="56">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.12, 0.96
0.11, 0.96
0.07, 0.98</code></pre>
</div>
</div>
<p>All right. So now that we’ve re implemented sequential, we can just go ahead and use PyTorch as version.there’s an nn.Sequential we can pass in our layers and we can fit, not surprisingly, we can see the model. So yeah, it looks very similar to the one we built ourselves. All right.</p>
</section>
<section id="nn.sequential" class="level3">
<h3 class="anchored" data-anchor-id="nn.sequential">nn.Sequential</h3>
<p><code>nn.Sequential</code> is a convenient class which does the same as the above:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</div>
<div class="cell" data-outputid="b51a35bf-f618-47f9-d9f8-15daf46d6519" data-execution_count="58">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1">fit()</span>
<span id="cb86-2">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.16, 0.94
0.13, 0.96
0.08, 0.96</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>(tensor(0.03, grad_fn=&lt;NllLossBackward0&gt;), tensor(1.))</code></pre>
</div>
</div>
<div class="cell" data-outputid="7ee2efc3-5fcc-4b21-f951-e1132a337011" data-execution_count="59">
<div class="sourceCode cell-code" id="cb89" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1">model</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>Sequential(
  (0): Linear(in_features=784, out_features=50, bias=True)
  (1): ReLU()
  (2): Linear(in_features=50, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<p>So this thing of looping through parameters and updating our parameters based on gradients and aligning right and then zeroing them is very common.</p>
<p>So common that there is something that does at all for us and that’s called an optimizer. It’s the stuff in Optim. So let’s create our own optimizer. And as you can see, it’s just going to do the two things we just saw.</p>
<p>1-It’s going to go through each of the parameters and update them using the gradient in the lending rate.</p>
<p>2-And there’s also zero grad which will go through each parameter and set their gradients to zero.</p>
<p>If you used .data like it’s just a way of avoiding having to say touch.no_grid basically.</p>
<p>Okay, so in Optimizer we’re going to pass at the parameters that we want to optimize. I’m going to pass at the learning, right? And we’re just going to store them away. And since the parameters might be a generator, we’ll call list to turn them into a list. So we are going to create our optimizer, pass it in the model. parameters which have been automatically constructed for us by an nn.module. And so here’s our new loop. Now, we don’t have to do any of the stuff manually. We can just say opt.step. So that’s going to call this and opt.zero_grad and that’s going to call this. There it is. So we’ve now built our own SGD optimizer from scratch.</p>
<p>So I think this is really interesting right? Like these things which seem like they must be big and complicated once we have this nice structure in place, you know, an SGD to optimize, it doesn’t take much code at all. And so it’s all very transparent, simple clear. If you’re having trouble using complex library code that you’ve found elsewhere, you know, this can be a really good approach is to actually just go all the way back and move as you know, as many of these abstractions as you can and like run everything by hand to see exactly what’s going on. It can be really freeing to see that you can do all this anyway, since PyTorch has this for us In torch.optim. It’s got a optim.SGD. And just like our version, you pass in the parameters and you pass in the learning, right? So you really see it is just the same. So let’s define something called get model that’s going to return the model, the sequential model and the optimizer for it. So if we go model, comma opt equals get model, and then we can call the lost function to see where it’s starting. And so then we can write our training loop again, go through each epoch, go through each starting point for our for our batches, grab the slice, slice into our x and y in the training set to get a predictions, calculate our loss to the backward pass, to the optimizer, step to the zero gradient and print out how you’re going at the end of each one. And then we go,</p>
</section>
<section id="optim" class="level3">
<h3 class="anchored" data-anchor-id="optim">optim</h3>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb91" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><span class="kw" style="color: #003B4F;">class</span> Optimizer():</span>
<span id="cb91-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, params, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>): <span class="va" style="color: #111111;">self</span>.params,<span class="va" style="color: #111111;">self</span>.lr<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">list</span>(params),lr</span>
<span id="cb91-3"></span>
<span id="cb91-4">    <span class="kw" style="color: #003B4F;">def</span> step(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb91-5">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb91-6">            <span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.params: p <span class="op" style="color: #5E5E5E;">-=</span> p.grad <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.lr</span>
<span id="cb91-7"></span>
<span id="cb91-8">    <span class="kw" style="color: #003B4F;">def</span> zero_grad(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb91-9">        <span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.params: p.grad.data.zero_()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb93" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1">opt <span class="op" style="color: #5E5E5E;">=</span> Optimizer(model.parameters())</span></code></pre></div>
</div>
<div class="cell" data-outputid="913c4481-e710-4497-dc6f-1962dbf7eb0c" data-execution_count="63">
<div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb94-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb94-3">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(i, <span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb94-4">        xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[s],y_train[s]</span>
<span id="cb94-5">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb94-6">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb94-7">        loss.backward()</span>
<span id="cb94-8">        opt.step()</span>
<span id="cb94-9">        opt.zero_grad()</span>
<span id="cb94-10">    report(loss, preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.18, 0.94
0.13, 0.96
0.11, 0.94</code></pre>
</div>
</div>
<p>PyTorch already provides this exact functionality in <code>optim.SGD</code> (it also handles stuff like momentum, which we’ll look at later)</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> optim</span></code></pre></div>
</div>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb97" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><span class="kw" style="color: #003B4F;">def</span> get_model():</span>
<span id="cb97-2">    model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,<span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb97-3">    <span class="cf" style="color: #003B4F;">return</span> model, optim.SGD(model.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span></code></pre></div>
</div>
<div class="cell" data-outputid="3aea759a-a19b-4a3d-8ed7-f91e37151d99" data-execution_count="66">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span>
<span id="cb98-2">loss_func(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>tensor(2.33, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-outputid="83cac206-7a46-483d-cd8e-c48671e9901d" data-execution_count="67">
<div class="sourceCode cell-code" id="cb100" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb100-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb100-3">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(i, <span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb100-4">        xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[s],y_train[s]</span>
<span id="cb100-5">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb100-6">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb100-7">        loss.backward()</span>
<span id="cb100-8">        opt.step()</span>
<span id="cb100-9">        opt.zero_grad()</span>
<span id="cb100-10">    report(loss, preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.12, 0.98
0.09, 0.98
0.07, 0.98</code></pre>
</div>
</div>
</section>
</section>
<section id="dataset-and-dataloader" class="level2">
<h2 class="anchored" data-anchor-id="dataset-and-dataloader">Dataset and DataLoader</h2>
<p>All right, so let’s keep making this simpler. This don’t say much code. So one thing we could do is we could replace these lines of code with one line of code by using something, call the dataset class.</p>
<p>So the dataset class is just something that we’re going to pass in our independent and dependent variable. Well, store them away as self.x and self.y. Why we’ll have something. So if you if you define <strong>len</strong>, then that’s the thing that allows the len function to work. So the length of the dataset would just be the length of the independent variables.</p>
<p>And then <strong>getitem</strong> is a thing that will be called automatically any time you use square brackets in Python. So that task is going to call this function passing in the indices you want. So when we grab some items from our dataset, we’re going to return a couple of the x values and the y values.</p>
<p>So then we’ll be able to do this. So let’s create a data set using this tiny little tree line class. it’s going to be a dataset containing the x and y training, and they’ll create another dataset containing the x and y valid. And those two datasets will call train_ds and valid_ds. So let’s check the length of those data sets should be the same as the length of the xs and they are. And so now we can do exactly what we hope we could do. We can say xb,yb equals train_ds and passing some slice. So that’s going to give us back our check The shapes are correct. It should be five by 28, by 28. And the y is should just be five. And so here they are, the xs and the y’s. So that’s nice. We’ve created a dataset from scratch and again, it’s not complicated at all. And if you look at the actual PyTorch source code, this is basically your data sets do so let’s try it. We call get_model() And so now we’ve replaced our dataset line with this one and as usual it still runs. And so this is what I do when I’m writing code is I try to like always make sure that my starting code works as I refactor. And so you can see all the steps. And so somebody reading my code can then see exactly like, why am I building everything I’m building? How does it all fit in? Say that it still works and I can also keep it clear in my own head. So I think this is a really nice way of implementing libraries as well. All right. So now we’re going to replace these two lines of code with this one line of code. So we’re going to create something called a data loader.</p>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<p>It’s clunky to iterate through minibatches of x and y values separately:</p>
<div class="sourceCode" id="cb102" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1">    xb <span class="op" style="color: #5E5E5E;">=</span> x_train[s]</span>
<span id="cb102-2">    yb <span class="op" style="color: #5E5E5E;">=</span> y_train[s]</span></code></pre></div>
<p>Instead, let’s do these two steps together, by introducing a <code>Dataset</code> class:</p>
<div class="sourceCode" id="cb103" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1">    xb,yb <span class="op" style="color: #5E5E5E;">=</span> train_ds[s]</span></code></pre></div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=68}</p>
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><span class="kw" style="color: #003B4F;">class</span> Dataset():</span>
<span id="cb104-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, x, y): <span class="va" style="color: #111111;">self</span>.x,<span class="va" style="color: #111111;">self</span>.y <span class="op" style="color: #5E5E5E;">=</span> x,y</span>
<span id="cb104-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__len__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.x)</span>
<span id="cb104-4">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, i): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.x[i],<span class="va" style="color: #111111;">self</span>.y[i]</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1">train_ds,valid_ds <span class="op" style="color: #5E5E5E;">=</span> Dataset(x_train, y_train),Dataset(x_valid, y_valid)</span>
<span id="cb105-2"><span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(train_ds)<span class="op" style="color: #5E5E5E;">==</span><span class="bu" style="color: null;">len</span>(x_train)</span>
<span id="cb105-3"><span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(valid_ds)<span class="op" style="color: #5E5E5E;">==</span><span class="bu" style="color: null;">len</span>(x_valid)</span></code></pre></div>
</div>
<div class="cell" data-outputid="491e4b5f-077e-4313-bb0f-db7393564124" data-execution_count="70">
<div class="sourceCode cell-code" id="cb106" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> train_ds[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">5</span>]</span>
<span id="cb106-2"><span class="cf" style="color: #003B4F;">assert</span> xb.shape<span class="op" style="color: #5E5E5E;">==</span>(<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">28</span><span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">28</span>)</span>
<span id="cb106-3"><span class="cf" style="color: #003B4F;">assert</span> yb.shape<span class="op" style="color: #5E5E5E;">==</span>(<span class="dv" style="color: #AD0000;">5</span>,)</span>
<span id="cb106-4">xb,yb</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb108" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span></code></pre></div>
</div>
<div class="cell" data-outputid="b7411200-1752-4c52-8517-9c62d31c6413" data-execution_count="72">
<div class="sourceCode cell-code" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb109-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb109-3">        xb,yb <span class="op" style="color: #5E5E5E;">=</span> train_ds[i:<span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs)]</span>
<span id="cb109-4">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb109-5">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb109-6">        loss.backward()</span>
<span id="cb109-7">        opt.step()</span>
<span id="cb109-8">        opt.zero_grad()</span>
<span id="cb109-9">    report(loss, preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.17, 0.96
0.11, 0.94
0.09, 0.96</code></pre>
</div>
</div>
</section>
<section id="dataloader" class="level3">
<h3 class="anchored" data-anchor-id="dataloader">DataLoader</h3>
<p>So now we’re going to replace these two lines of code with this one line of code. So we’re going to create something called a data loader. And a data loader is something that’s just going to do this. Okay. So we need to create an iterator. So an iterator is a class that has a <strong>iter</strong> method. When you say for in in Python behind the scenes, it’s actually calling <strong>iter</strong> to get a special object, which it can then loop through using yield. So it’s basically getting this thing that you can iterate through using the yield. So a data loader is something that’s going to have a data set and a batch size because we’re going to go through the batches and grab one batch at a time. So we have to store away the data set in the batch size. And so when we when we call the for loop, it’s going to code <strong>iter</strong>. We’re going to want to do exactly what we saw before, go through the range just like we did before, and then yield that bit of the data set. And that’s all. So that’s a data letter. So we can now create a train data loader and a validator loader from our train data set and validator set. And so now we can, if you remember the way you can create one thing out of an iterator so you don’t need to use a for loop, you can just say <strong>iter</strong>. And that will also code and data. Next, we’ll just grab one value from it. So here we will run this and you can see we’ve now just confirmed wave xb is a 50 by 784. And why yb, there it is. And then we can check what it looks like. So let’s grab the first element of our x batch, make it 28 by 28. And there it is. So now that we’ve got a date loader again, we can grab our model and we can simplify our fit function to just go for xb,yb and train_dl.</p>
<p>Previously, our loop iterated over batches (xb, yb) like this:</p>
<div class="sourceCode" id="cb111" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb111-2">    xb,yb <span class="op" style="color: #5E5E5E;">=</span> train_ds[i:<span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs)]</span>
<span id="cb111-3">    ...</span></code></pre></div>
<p>Let’s make our loop much cleaner, using a data loader:</p>
<div class="sourceCode" id="cb112" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> train_dl:</span>
<span id="cb112-2">    ...</span></code></pre></div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb113" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><span class="kw" style="color: #003B4F;">class</span> DataLoader():</span>
<span id="cb113-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, ds, bs): <span class="va" style="color: #111111;">self</span>.ds,<span class="va" style="color: #111111;">self</span>.bs <span class="op" style="color: #5E5E5E;">=</span> ds,bs</span>
<span id="cb113-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb113-4">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.ds), <span class="va" style="color: #111111;">self</span>.bs): <span class="cf" style="color: #003B4F;">yield</span> <span class="va" style="color: #111111;">self</span>.ds[i:i<span class="op" style="color: #5E5E5E;">+</span><span class="va" style="color: #111111;">self</span>.bs]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb114" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, bs)</span>
<span id="cb114-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, bs)</span></code></pre></div>
</div>
<div class="cell" data-outputid="f4e72342-a017-43be-9476-bcd92cfa3d18" data-execution_count="75">
<div class="sourceCode cell-code" id="cb115" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(valid_dl))</span>
<span id="cb115-2">xb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="75">
<pre><code>torch.Size([50, 784])</code></pre>
</div>
</div>
<div class="cell" data-outputid="71be89a3-f09f-4d10-bec5-a131c8dcb5d4" data-execution_count="76">
<div class="sourceCode cell-code" id="cb117" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1">yb</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7,
        6, 8, 9, 0, 3, 8, 3, 7, 7, 8, 4])</code></pre>
</div>
</div>
<div class="cell" data-outputid="f16e6854-91d0-43b1-a25b-35c1e21eb4be" data-execution_count="77">
<div class="sourceCode cell-code" id="cb119" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1">plt.imshow(xb[<span class="dv" style="color: #AD0000;">0</span>].view(<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb119-2">yb[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>tensor(3)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 7/index_files/figure-html/cell-77-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb121" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb122" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><span class="kw" style="color: #003B4F;">def</span> fit():</span>
<span id="cb122-2">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb122-3">        <span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> train_dl:</span>
<span id="cb122-4">            pred <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb122-5">            loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(pred, yb)</span>
<span id="cb122-6">            loss.backward()</span>
<span id="cb122-7">            opt.step()</span>
<span id="cb122-8">            opt.zero_grad()</span>
<span id="cb122-9">        report(loss, preds, yb)</span></code></pre></div>
</div>
<div class="cell" data-outputid="bc0706a7-8ad5-4e80-d6ac-45cd8d7d7ad6" data-execution_count="80">
<div class="sourceCode cell-code" id="cb123" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1">fit()</span>
<span id="cb123-2">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.11, 0.96
0.09, 0.96
0.06, 0.96</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="80">
<pre><code>(tensor(0.03, grad_fn=&lt;NllLossBackward0&gt;), tensor(1.))</code></pre>
</div>
</div>
</section>
<section id="random-sampling" class="level3">
<h3 class="anchored" data-anchor-id="random-sampling">Random sampling</h3>
<p>So this is getting nice and small, don’t you think? And it still works the same way. Okay, so this is really cool. And now that it’s nice and concise, we can start adding features to it. So one feature I think we should add is that our training set each time we go through it, it should be in a different order. It should be randomized the order. So instead of always just going through these indexes in order, we want some way to say use random indexes. So the way we can do that is create a class called sampler. And what sampler is going to do here is if we create a sampler without shuffle, without randomizing it, it’s going to simply return all the numbers from zero up to n in order and it’ll be an iterator.</p>
<p>So the way we can do that is create a class called sampler. And what sampler is going to do here is if we create a sampler without shuffle, without randomizing it, it’s going to simply return all the numbers from zero up to end in order and it’ll be an iterator. So this is done data, but if I do want to, then it will randomly shuffle them. So here you can see I’ve created a sampler without shuffle. So if I then make an iterator from that and print a few things in the iterator, you can see it’s just printing out the indexes. It’s going to want, or I can do exactly the same thing as we learned earlier in the course using Isolés, we can grab the first five. So here’s the first five things from a sampler when it’s not shuffled. So as you can see, these are just indexes. So we could add shuffle equals true. And now that’s going to call random dot shuffle, which just randomly permits them. And now if I do the same thing, I’ve got random indexes. If my source data. So why is that useful? Well, what we can now do is create something called a batch sampler, and the batch sampler is going to do is it’s going to basically do this isolates thing for us. So we’re going to say, okay, pass in a sampler, set something that generates indices and pass in a batch size. And remember, we’ve looked at chunking before. It’s going to chunk that iterator by that batch size. And so if I now say, all right, please take our sampler and create batches of four, as you can see here, it’s creating batches of four indices at a time. So rather than just looping through them in order, I can now loop through this batch sampler. So we’re going to change our data loader so that now it’s going to take some batch sampler and it’s going to look through the batch sampler that’s going to give us indices and then we’re going to get that data set item from that batch for everything in that batch.</p>
<p>So that’s going to give us a list and then we have to stack all of the (x es) and all of the (y eys) together into tensers. So I’ve created something here called Collate function and we’re going to default that to this little function here, which is going to grab our batch, pull out the (x es) and (y eys) separately, and then stack them up into tensors. So this is called our collate function. Okay, So if we put all that together, we can create a training sampler, which is a batch sampler over the training set with shuffle True, a validation sampler will be a batch sampler over the validation set with shuffle false. And so then we can pass that into this data loader class, The training data set and the training sampler and the COLLATE function, which we don’t really need because we’re just using the default one. So I guess we can just get rid of that. And so now here we go. We can do exactly the same thing as before. xb,yb =next(iter(valid_dl)) this time we use the validator loader, check the shapes. And so now check. That still works. And it does. So this is how PyTorch is actual data loaders work. This is the this is all the pieces they have. They have samplers, they have batch samplers, they have a collation function and they have data letters. So remember that what I want you to be doing for your homework is experimenting with these carefully to see exactly what each thing is taking in. Okay, so here is asking on the chat what is this collate thing doing?</p>
<p>Okay, so collate function and it defaults to collate. What does it do? Well, let’s see. Let’s go through each of these steps. Okay, so we need so we’ve got a batch sampler. So let’s do just the valid sampler. No fit didn’t work. We have to look at that to. Okay, so the batch sampler, here it is. So we’re going to go through each thing in the batch sampler. So let’s just grab one thing from the batch sampler. Okay? So the output with the batch sampler will be next. It’s a okay, so here’s what the batch sampler contains. All right. Just the first 50 digits, not surprisingly, because this is a validation sampler. If we did a training sampler, that would be randomized. Yeah, they are. Okay, so then what we then do is we go self.ds[i] for i and b, so let’s copy that copy paste. And so rather than self.ds[i] will just say train_ds[i] for i in o ,Sorry. Training. Okay. So what it’s created here is a list of tuples of tensors, I think. Let’s have a look. So let’s have a look. So we’ll call this a whatever. So p zero. Okay, is a tuple. It’s got the x and the y independent, independent variable. So that’s not what we want. What we want is something that we can live through. We want to get batches. So what the collation function is going to do is it’s going to take all of our x’s and all of our y’s and collect them into two tensors, one tensor x’s and one tensor y’s. So the way it does that is it. First of all, calls zip. So zip is a very, very commonly used python function. It’s got nothing to do with the compression program zip. But instead what it does is it effectively allows us to like transpose things so that now, as you can see, we’ve got all of the second elements, index one elements all together and all of the index zero elements together. And so then we can stack those all up together and that gives us our y`s for our batch. So that’s what collate it does. So the collate function is used an awful lot in in PyTorch increasingly nowadays where hackingface stuff uses it a lot and so we’ll be using it a lot as well. And basically it’s a thing that allows us to customize how the data that we get back from our dataset set. Once it’s been kind of generating a list of things from the dataset, how do we put it together into some into a bunch of things that our model can take as inputs, because that’s really what we want here. So that’s what the collection function does. All right. So let’s try this again. And the reason these are wrong, it’ll be something to do with a report. It’s a accuracy. So something about the accuracy is not printing out correctly. Let’s see if we can figure it out. So probably does it down here. Yeah, it’d be something to do with that shuffling. If we tend to shuffle off, probably find out our work. Yes, it does. Okay, sir, my shuffling bracket. Hmm. Oh, I see why this should be preds. Better.So when we run our model and we call fit(), we get the same results. One trick I was just going to mention normally in <strong>init</strong> the way we we very, very often want to grab all the stuff that we’ve been passed as parameters and store it away like so this is the wrong way around. Like so this is something that I do so often that fast core has a quick little shortcut for it. Just called Store at Trust all attributes. And so if you just put that in your <strong>init</strong>, then you just need one line of code and it does exactly the same thing. So there’s a little shortcut as you see. And so you’ll see that quite a bit. All right, let’s have a seven minute break and see you back here very soon.</p>
<p>We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn’t be randomized.</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb126" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><span class="im" style="color: #00769E;">import</span> random</span></code></pre></div>
</div>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb127" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><span class="kw" style="color: #003B4F;">class</span> Sampler():</span>
<span id="cb127-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, ds, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>): <span class="va" style="color: #111111;">self</span>.n,<span class="va" style="color: #111111;">self</span>.shuffle <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(ds),shuffle</span>
<span id="cb127-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb127-4">        res <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.n))</span>
<span id="cb127-5">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.shuffle: random.shuffle(res)</span>
<span id="cb127-6">        <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">iter</span>(res)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb128" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><span class="im" style="color: #00769E;">from</span> itertools <span class="im" style="color: #00769E;">import</span> islice</span></code></pre></div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb129" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1">ss <span class="op" style="color: #5E5E5E;">=</span> Sampler(train_ds)</span></code></pre></div>
</div>
<div class="cell" data-outputid="489aa89a-bb0a-4034-f54b-29f42437cf98" data-execution_count="85">
<div class="sourceCode cell-code" id="cb130" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(ss)</span>
<span id="cb130-2"><span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>): <span class="bu" style="color: null;">print</span>(<span class="bu" style="color: null;">next</span>(it))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0
1
2
3
4</code></pre>
</div>
</div>
<div class="cell" data-outputid="066ec75f-60c0-470d-9a92-d7aaeb1f8401" data-execution_count="86">
<div class="sourceCode cell-code" id="cb132" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><span class="bu" style="color: null;">list</span>(islice(ss, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>[0, 1, 2, 3, 4]</code></pre>
</div>
</div>
<div class="cell" data-outputid="14277b28-5703-442f-b67a-d3da68f37e9c" data-execution_count="87">
<div class="sourceCode cell-code" id="cb134" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1">ss <span class="op" style="color: #5E5E5E;">=</span> Sampler(train_ds, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb134-2"><span class="bu" style="color: null;">list</span>(islice(ss, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>[28659, 39049, 23211, 13983, 38058]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb136" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><span class="im" style="color: #00769E;">import</span> fastcore.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">as</span> fc</span></code></pre></div>
</div>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb137" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><span class="kw" style="color: #003B4F;">class</span> BatchSampler():</span>
<span id="cb137-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, sampler, bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>): fc.store_attr()</span>
<span id="cb137-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> fc.chunked(<span class="bu" style="color: null;">iter</span>(<span class="va" style="color: #111111;">self</span>.sampler), <span class="va" style="color: #111111;">self</span>.bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.drop_last)</span></code></pre></div>
</div>
<div class="cell" data-outputid="1ec22ab0-4142-4f55-b25b-cf7456f47a5f" data-execution_count="90">
<div class="sourceCode cell-code" id="cb138" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1">batchs <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(ss, <span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb138-2"><span class="bu" style="color: null;">list</span>(islice(batchs, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>[[7445, 36933, 36891, 13229],
 [47783, 46860, 1239, 20962],
 [8646, 29897, 9202, 31355],
 [48398, 35167, 44700, 27769],
 [7834, 22128, 40411, 5830]]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb140" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><span class="kw" style="color: #003B4F;">def</span> collate(b):</span>
<span id="cb140-2">    xs,ys <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">zip</span>(<span class="op" style="color: #5E5E5E;">*</span>b)</span>
<span id="cb140-3">    <span class="cf" style="color: #003B4F;">return</span> torch.stack(xs),torch.stack(ys)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb141" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><span class="kw" style="color: #003B4F;">class</span> DataLoader():</span>
<span id="cb141-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, ds, batchs, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate): fc.store_attr()</span>
<span id="cb141-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> (<span class="va" style="color: #111111;">self</span>.collate_fn(<span class="va" style="color: #111111;">self</span>.ds[i] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> b) <span class="cf" style="color: #003B4F;">for</span> b <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.batchs)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb142" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1">train_samp <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(Sampler(train_ds, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span> ), bs)</span>
<span id="cb142-2">valid_samp <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(Sampler(valid_ds, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>), bs)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb143" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, batchs<span class="op" style="color: #5E5E5E;">=</span>train_samp)</span>
<span id="cb143-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, batchs<span class="op" style="color: #5E5E5E;">=</span>valid_samp)</span></code></pre></div>
</div>
<div class="cell" data-outputid="1f1c9e02-46e7-4d98-8604-3920bdb7cdd7" data-execution_count="95">
<div class="sourceCode cell-code" id="cb144" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(valid_dl))</span>
<span id="cb144-2">plt.imshow(xb[<span class="dv" style="color: #AD0000;">0</span>].view(<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb144-3">yb[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>tensor(3)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 7/index_files/figure-html/cell-95-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="0b49d8a1-d436-4c3e-d399-323129d94f62" data-execution_count="96">
<div class="sourceCode cell-code" id="cb146" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1">xb.shape,yb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>(torch.Size([50, 784]), torch.Size([50]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb148" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span></code></pre></div>
</div>
<div class="cell" data-outputid="21896cb5-8f7d-4280-9257-081b2d0d45a6" data-execution_count="98">
<div class="sourceCode cell-code" id="cb149" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1">fit()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.33, 0.12
0.22, 0.06
0.02, 0.04</code></pre>
</div>
</div>
</section>
<section id="multiprocessing-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="multiprocessing-dataloader">Multiprocessing DataLoader</h3>
<p>And we’re going to look at a multi processing data loader and then we’ll have nearly finished this notebook.</p>
<p>All right. So you send it because wants to know how it implements store_attr. You can always look it up. Right. So if we could go look at here is the docs and there’s always a link to the source. Yeah, go. You can create a version that’s a lot less code than this. This is dealing with a few extra things like slots and store_args and stuff. So basically the trick is to see who’s calling you using some rather internal ish stuff on code. All right, let’s keep going. So we’ve seen how to create a data loader and sampling from it. The PyTorch data loader works exactly like this, but it uses a lot more code because it implements multi processing. And so multi processing means that the actual this thing here, that code can be run in multiple processes, that can be run in parallel for multiple items. So this code, for example, might be opening up a jpeg, rotating it, flipping it, etc.. Right. So because remember, this is just calling the <strong>getitem</strong> for a data set. So that could be doing a lot of work for each item and we’re doing it for every item in the batch. So we’d love to do those all in parallel. So I’ll show you a very quick and dirty way that basically does the job. So Python has a multiprocessor library. It doesn’t work particularly well with PyTorch tensors, so PyTorch has created an exact implementation of it. So it’s identical API wise, but it does work well with tensors. So this is basically what a script the multi processing. So this is not quite shading because multi processing is in the standard library and this is API equivalent. So I’m going to say we’re allowed to do that. So as we’ve discussed, you know, when we call square brackets on a class, it’s actually identical to calling the <strong>getitem</strong> function on on the object. So you can see here, if we say give me items three, six, eight and one, it’s the same as calling <strong>getitem</strong> passing in three, six, eight and one.</p>
<p>Now why does this matter??</p>
<p>Well I’ll show you why it matters because we’re going to be able to use map and explain why we want to use map the moment map is a really important concept. You might have heard of MapReduce. So we’ve already talked about reductions and what those are maps are kind of the other key piece map is something which takes a sequence and calls a function on every element of that sequence.</p>
<p>So imagine we had a couple of batches of indices three and six and eight and one. Then we’re going to call <strong>getitem</strong> on each of those batches. So that’s what map does. map calls this function on every element of the sequence. And so that’s going to give us the same stuff. But now this, same as this, but now batched into two batches. Now why do we want to do that? Because multi processing has something called Pool where you can tell it. How many workers do you want to read and how many processes you want to run? And it then has a map which works just like the python normal python map, but it runs this function in parallel over the items from this iterator. So this is how we can create a multi processing data loader. So here we are creating our data loader and again, we don’t actually need to pass in the collate function because we using the default one. So if we say n_workers equals two and then create that if we say next, see how it’s taking a moment took a moment because it was firing off those two workers in the background. So the first batch actually comes out more slowly. But the reason that we would use a multi processing data loader is if this is doing a lot of work, we want it to run in parallel. And even though the first item might come out a bit slower, once those processes are fired up, it’s going to be faster to run. So this is yeah, this is a really simplified multi processing data loader because this needs to be super, super efficient. PyTorch has lots more code than this to make it much more efficient. But the idea is this and this is actually a perfectly good way of experimenting or building your own data loader to make things work exactly how you want. So now that we’ve really implemented all this from PyTorch, let’s just grab the PyTorch. And as you can see, they’re exactly the same data laoder. They don’t have one thing called sampler that you pass shuffle to. They have two separate classes called sequential sampling random sampler. I don’t know why they do it that way. It’s a bit more work to me, but same idea. And they’ve got that sampler. And so it’s exactly the same idea. That training sampler is a batch sampler with a random sampler. The validation sampler is a batch sampler with a sequential sampler passing in batch sizes. And so we can now pass those samplers to the data loader. This is now the PyTorch data letter. And just like ours, it also takes a collate function. Okay. And it works cool. So that’s as you can see, it’s it’s doing exactly the same stuff that ours is doing with exactly the same API. And it’s got some shortcuts, as I’m sure you’ve noticed when you’ve used data loaders.</p>
<p>So, for example, calling batch sampler is very going to be very, very common. So you can actually just pass the batch size directly to a data loader and it will then auto create the batch samples for you so you don’t have to pass in batch sampler at all. Instead, you can just say sampler and it will automatically wrap that in the batch sampler for you. That does exactly the same thing. And in fact, because it’s so common to create a random sampler or a sequential sampler for a data set, you don’t have to do that manually. You can just pass in shuffle equals true or shuffle equals false to the data loader. And that does again, exactly the same thing. There it is.</p>
<p>Now, something that is very interesting is that when you think about it, the batch sampler and the collation function are things which are taking the result of the sampler looping through them and then collating them together. But what we could do is actually because our datasets know how to grab multiple indices at once, we can actually just use the batch sampler as a sampler. We don’t actually have to look through them and collate them because they’re basically instantly collated. They come pre collated. So this is a trick which actually huggingface stuff can use as I won’t be saying it again. So this is an important thing to understand is how come we can pass a batch sample to the sampler. What’s it doing?</p>
<p>And so rather than trying to look through the PyTorch code, I suggest going back to our non multi processing pure Python code to see exactly how that would work, because it’s a really nifty trick for things that you can grab multiple things from at once and it can save a whole lot of time. It can make your code a lot faster. Okay, so now that we’ve got all that nicely implemented,</p>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb151" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><span class="im" style="color: #00769E;">import</span> torch.multiprocessing <span class="im" style="color: #00769E;">as</span> mp</span>
<span id="cb151-2"><span class="im" style="color: #00769E;">from</span> fastcore.basics <span class="im" style="color: #00769E;">import</span> store_attr</span></code></pre></div>
</div>
<div class="cell" data-outputid="534a4508-f012-47d6-c049-c9cda44b43e3" data-execution_count="100">
<div class="sourceCode cell-code" id="cb152" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1">train_ds[[<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">1</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1, 1, 0]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="2c55749e-2747-4bb7-c847-f141abe36aba" data-execution_count="101">
<div class="sourceCode cell-code" id="cb154" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1">train_ds.<span class="fu" style="color: #4758AB;">__getitem__</span>([<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">1</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1, 1, 0]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="c31518f5-ed52-4b64-f388-7457e5ab0f82" data-execution_count="102">
<div class="sourceCode cell-code" id="cb156" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><span class="cf" style="color: #003B4F;">for</span> o <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">map</span>(train_ds.<span class="fu" style="color: #4758AB;">__getitem__</span>, ([<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">6</span>],[<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">1</span>])): <span class="bu" style="color: null;">print</span>(o)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))
(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb158" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><span class="kw" style="color: #003B4F;">class</span> DataLoader():</span>
<span id="cb158-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, ds, batchs, n_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate): fc.store_attr()</span>
<span id="cb158-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__iter__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb158-4">        <span class="cf" style="color: #003B4F;">with</span> mp.Pool(<span class="va" style="color: #111111;">self</span>.n_workers) <span class="im" style="color: #00769E;">as</span> ex: <span class="cf" style="color: #003B4F;">yield</span> <span class="cf" style="color: #003B4F;">from</span> ex.<span class="bu" style="color: null;">map</span>(<span class="va" style="color: #111111;">self</span>.ds.<span class="fu" style="color: #4758AB;">__getitem__</span>, <span class="bu" style="color: null;">iter</span>(<span class="va" style="color: #111111;">self</span>.batchs))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb159" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, batchs<span class="op" style="color: #5E5E5E;">=</span>train_samp, n_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb159-2">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(train_dl)</span></code></pre></div>
</div>
<div class="cell" data-outputid="ad0f005d-a495-41cd-eaae-5b53cbf49321" data-execution_count="105">
<div class="sourceCode cell-code" id="cb160" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(it)</span>
<span id="cb160-2">xb.shape,yb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="105">
<pre><code>(torch.Size([50, 784]), torch.Size([50]))</code></pre>
</div>
</div>
</section>
<section id="pytorch-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-dataloader">PyTorch DataLoader</h3>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=106}</p>
<div class="sourceCode cell-code" id="cb162" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader, SequentialSampler, RandomSampler, BatchSampler</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb163" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1">train_samp <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(RandomSampler(train_ds),     bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb163-2">valid_samp <span class="op" style="color: #5E5E5E;">=</span> BatchSampler(SequentialSampler(valid_ds), bs, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb164" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, batch_sampler<span class="op" style="color: #5E5E5E;">=</span>train_samp, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate)</span>
<span id="cb164-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, batch_sampler<span class="op" style="color: #5E5E5E;">=</span>valid_samp, collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate)</span></code></pre></div>
</div>
<div class="cell" data-outputid="8cbf63e3-c417-4ac9-e0c5-1412f59b0e75" data-execution_count="109">
<div class="sourceCode cell-code" id="cb165" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span>
<span id="cb165-2">fit()</span>
<span id="cb165-3">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.10, 0.06
0.10, 0.04
0.27, 0.06</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre><code>(tensor(0.04, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.98))</code></pre>
</div>
</div>
<p>PyTorch can auto-generate the BatchSampler for us:</p>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb168" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, bs, sampler<span class="op" style="color: #5E5E5E;">=</span>RandomSampler(train_ds), collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate)</span>
<span id="cb168-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, bs, sampler<span class="op" style="color: #5E5E5E;">=</span>SequentialSampler(valid_ds), collate_fn<span class="op" style="color: #5E5E5E;">=</span>collate)</span></code></pre></div>
</div>
<p>PyTorch can also generate the Sequential/RandomSamplers too:</p>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb169" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb169-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, num_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="44e38b96-b346-47d2-feaf-1da219692926" data-execution_count="112">
<div class="sourceCode cell-code" id="cb170" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span>
<span id="cb170-2">fit()</span>
<span id="cb170-3"></span>
<span id="cb170-4">loss_func(model(xb), yb), accuracy(model(xb), yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.21, 0.14
0.15, 0.16
0.05, 0.10</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>(tensor(0.06, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.98))</code></pre>
</div>
</div>
<p>Our dataset actually already knows how to sample a batch of indices all at once:</p>
<div class="cell" data-outputid="5fa470c7-5fe7-4925-fa4c-589cfef5027d" data-execution_count="113">
<div class="sourceCode cell-code" id="cb173" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1">train_ds[[<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">7</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>(tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([9, 1, 3]))</code></pre>
</div>
</div>
<p>…that means that we can actually skip the batch_sampler and collate_fn entirely:</p>
<div class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb175" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1">train_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(train_ds, sampler<span class="op" style="color: #5E5E5E;">=</span>train_samp)</span>
<span id="cb175-2">valid_dl <span class="op" style="color: #5E5E5E;">=</span> DataLoader(valid_ds, sampler<span class="op" style="color: #5E5E5E;">=</span>valid_samp)</span></code></pre></div>
</div>
<div class="cell" data-outputid="6091c2ce-4c67-4a7f-d25a-c3beecbcf459" data-execution_count="115">
<div class="sourceCode cell-code" id="cb176" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(train_dl))</span>
<span id="cb176-2">xb.shape,yb.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="115">
<pre><code>(torch.Size([1, 50, 784]), torch.Size([1, 50]))</code></pre>
</div>
</div>
</section>
</section>
<section id="validation" class="level2">
<h2 class="anchored" data-anchor-id="validation">Validation</h2>
<p>we should now add a validation set and there’s not really too much to talk here. We’ll just take our fit function. And this is exactly the same code that we had before. And then we’re just going to add something which goes through the validation set and gets the predictions and sums up the losses and accuracies and from time to time prints out the loss and accuracy. And so get deals we will implement by using the PyTorch data loader now. And so now our whole process will be get deals passing in the training and validation data set that is set for our validation data loader.</p>
<p>???I’m doubling the batch size because</p>
<p>it doesn’t have to do back propagation,???</p>
<p>so it should use about half as much memory. So I can use a bigger batch size, get our model and then call this fit. And now it’s printing out the loss and accuracy on the validation set. So finally we actually know how we’re doing, which is that we’re getting 97% accuracy on the validation set, and that’s on the whole thing, not just on the last batch. So that’s cool. We’ve now implemented a proper working, sensible training loop. It’s still, you know, a bit more code than I would like, but it’s not bad. And every line of code in there and every line of code it’s calling, it’s all stuff that we have built ourselves, reimplemented ourselves. So we know what’s going on. And that means it’s going to be much easier for us to create anything we can think of. We don’t have to rely on other people’s code, so hopefully you’re as excited about that as I am because it really opens up a whole world for us.</p>
<p>You <strong>always</strong> should also have a <a href="http://www.fast.ai/2017/11/13/validation-sets/">validation set</a>, in order to identify if you are overfitting.</p>
<p>We will calculate and print the validation loss at the end of each epoch.</p>
<p>(Note that we always call <code>model.train()</code> before training, and <code>model.eval()</code> before inference, because these are used by layers such as <code>nn.BatchNorm2d</code> and <code>nn.Dropout</code> to ensure appropriate behaviour for these different phases.)</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=116}</p>
<div class="sourceCode cell-code" id="cb178" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><span class="kw" style="color: #003B4F;">def</span> fit(epochs, model, loss_func, opt, train_dl, valid_dl):</span>
<span id="cb178-2">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb178-3">        model.train()</span>
<span id="cb178-4">        <span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> train_dl:</span>
<span id="cb178-5">            loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(model(xb), yb)</span>
<span id="cb178-6">            loss.backward()</span>
<span id="cb178-7">            opt.step()</span>
<span id="cb178-8">            opt.zero_grad()</span>
<span id="cb178-9"></span>
<span id="cb178-10">        model.<span class="bu" style="color: null;">eval</span>()</span>
<span id="cb178-11">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb178-12">            tot_loss,tot_acc,count <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>,<span class="fl" style="color: #AD0000;">0.</span>,<span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb178-13">            <span class="cf" style="color: #003B4F;">for</span> xb,yb <span class="kw" style="color: #003B4F;">in</span> valid_dl:</span>
<span id="cb178-14">                pred <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb178-15">                n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(xb)</span>
<span id="cb178-16">                count <span class="op" style="color: #5E5E5E;">+=</span> n</span>
<span id="cb178-17">                tot_loss <span class="op" style="color: #5E5E5E;">+=</span> loss_func(pred,yb).item()<span class="op" style="color: #5E5E5E;">*</span>n</span>
<span id="cb178-18">                tot_acc  <span class="op" style="color: #5E5E5E;">+=</span> accuracy (pred,yb).item()<span class="op" style="color: #5E5E5E;">*</span>n</span>
<span id="cb178-19">        <span class="bu" style="color: null;">print</span>(epoch, tot_loss<span class="op" style="color: #5E5E5E;">/</span>count, tot_acc<span class="op" style="color: #5E5E5E;">/</span>count)</span>
<span id="cb178-20">    <span class="cf" style="color: #003B4F;">return</span> tot_loss<span class="op" style="color: #5E5E5E;">/</span>count, tot_acc<span class="op" style="color: #5E5E5E;">/</span>count</span></code></pre></div>
<p>:::</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=117}</p>
<div class="sourceCode cell-code" id="cb179" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1"><span class="kw" style="color: #003B4F;">def</span> get_dls(train_ds, valid_ds, bs, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb179-2">    <span class="cf" style="color: #003B4F;">return</span> (DataLoader(train_ds, batch_size<span class="op" style="color: #5E5E5E;">=</span>bs, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs),</span>
<span id="cb179-3">            DataLoader(valid_ds, batch_size<span class="op" style="color: #5E5E5E;">=</span>bs<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs))</span></code></pre></div>
<p>:::</p>
<p>Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:</p>
<div class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb180" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1">train_dl,valid_dl <span class="op" style="color: #5E5E5E;">=</span> get_dls(train_ds, valid_ds, bs)</span>
<span id="cb180-2">model,opt <span class="op" style="color: #5E5E5E;">=</span> get_model()</span></code></pre></div>
</div>
<div class="cell" data-outputid="be5c9c8e-ba61-4f91-a9b4-10dc56858f7c" data-execution_count="119">
<div class="sourceCode cell-code" id="cb181" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><span class="op" style="color: #5E5E5E;">%</span>time loss,acc <span class="op" style="color: #5E5E5E;">=</span> fit(<span class="dv" style="color: #AD0000;">5</span>, model, loss_func, opt, train_dl, valid_dl)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 0.14236383258365096 0.958100004196167
1 0.12564025239087642 0.9632000041007995
2 0.13069150418043138 0.9645000052452087
3 0.10988456704188138 0.9670000064373017
4 0.11636368061415851 0.9678000068664551
CPU times: user 6.57 s, sys: 25.2 ms, total: 6.59 s
Wall time: 6.66 s</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 7/index.html</guid>
  <pubDate>Sun, 26 Mar 2023 19:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 6</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 6/index.html</link>
  <description><![CDATA[ 



<p>What you should learn and practice after this : <br> 1- log , e rules 2- what is softmax 3- 4- Train a simple linear model from scratch</p>
<p>So we’re now up to a point where we can train a model. So let’s do that mini batch training Notebook number four. So same first cell as before. We won’t go through it. The cells also the same as before, so we won’t go through it is the same model that we had before, so we won’t go through it. So I just rerunning all that to see. Okay,</p>
<p>so the first thing we should do, I think, is to improve our loss function. So it’s not total rubbish anymore. So if you watch part one, you might recall that there are some Excel notebooks. One of those Excel notebooks is entropy example. Okay, so this is what we looked at. So just to remind you, what we’re doing now is which we’re saying, okay, rather than rather than outputting a single number for each image, we’re going to instead output ten numbers for each image. And so that’s going to be a one hot encoded set of it’ll be like 1000, etc.. And so then that’s going to be well, actually the outputs wont be 1000, they’ll be basically probabilities won’t they. So it’ll be like .99 comma, you know, 3.01 etc. and the targets will be one hot encoded. So if it’s the digit zero, for example, it might be 10000 that dot dot for all the ten possibilities. And so to see, you know, how good is it. So in this case, it’s very good. It had 0.99 probability prediction that it’s zero. And indeed it is, because this is this is the 100 coded version. And so the way we implement that is we don’t even need to actually do the one hot encoding, thanks to some tricks, we can actually just directly store the integer, but we can treat it as if it’s one hot encoded so we can just store the actual target zero as an integer. So the way we do that is we say, for example, for a single output, oh, it could be cat, let’s say cat, dog, plain fish building the neural net spits out a bunch of outputs. What we do for softmax is we go e to of the power of each of those outputs. We sum up all of those e of the power offs. So here’s the e of the power of each of those outputs is the sum of them. And then we divide one each one by the sum. So we divide each one by the sum that gives us a softMax’s. And then for the lost function, we then compare the softMax’s to the one encoded version. So let’s say it was a dog that’s going to have a one for dog and zero everywhere else. And then softMax, this is from this nice blog post here. This is the calculation, some of the ones and zeros. So each of the ones and zeros multiplied by the log of the probabilities. So here is the log probability times the actuals. And since the actual is either zero or one and only one of them is going to be a one, we’re only going to end up with one value here. And so if we add them up, it’s all zero except for one of them. So that’s a cross entropy. So in the special case where the outputs one had encoded, then during the one hot encoded multiplied by the log, softmax is actually identical to simply saying, Oh, dog is in this row, let’s just look it up directly and take its log soft max.</p>
<p>We can just index directly into it. So it’s exactly the same thing. So that’s just review. So if you haven’t seen that before, then yeah, go and watch the part one video where we went into that in a lot more detail. Okay, So here’s our softmax calculation. It’s e to the power of each output divided by the sum of them all, we can use sigma notation to say exactly the same thing. And as you can see, Jupyter notebook lets us use latex. If you haven’t used latex before, it’s actually surprisingly easy to learn. You just put dollar signs around your equations like this and your equations backslash is going to be kind of like your functions if you like, and curly parentheses actually, Curly are used to kind of fit for arguments. So you can see here, here is e to the power of and then underscore as used just subscript. So this is x subscript i and power of is used for super scripts. So his dots you can see here it is dots so it’s actually yeah learning like tech is easier than you might expect. It can be quite convenient for writing these functions when you want to. So anyway, that’s what soft Max is as we’ll see in a moment. Well, actually, as you’ve already seen in cross entropy, we don’t really want soft Maxs. We want log of soft Macs. So a log of softMaxs is here it is. So we’ve got x.exp divided by x.exp.sum and we’re going to sum up over the last dimension and then we actually want to keep that dimension so that when we do the divided by, we want a trailing unit axis for exactly the same reason we saw when we did our MSE last function. So if you sum with keepdim = true, that leaves the unit axis in in that last position. So we don’t have to put it back to avoid that horrible out of product issue. So this is the equivalent of this and then .log. So that’s log of soft Maxs. So there is the log of the soft Maxs that are predictions. Now in terms of high school math, you may have forgotten, but you definitely are going to want to know a Key piece that in that list of things is is log and exponent rules.</p>
<p>So check out Khan Academy or similar if you’ve forgotten them. But a quick reminder is, for example, the one we mentioned here log of A over B equals log of A minus log of B and equivalently log of A times B equals log of A plus log of B. And these are very handy because, for example, division can take a long time, multiply, can create really big numbers and have lots of floating point error. Being able to replace these things with pluses and minuses is very handy indeed. In fact, I used to give people an interview question which I did a lot of stuff with school and math school actually only has a some function for group by clauses. And I used to ask people how you would deal with calculate a compound interest column where the answer is basically that you have to say because compound interest is taking products. So it has be the sum of the log of the column and then e of the power of all that. So it’s like all kinds of little places that these things coming in handy, but they come in to neural nets all the time. So we’re going to take advantage of that because we’ve got a divided by it’s being logged and also rather handily. We’re going to have therefore the log of x.exp.log minus the log of this, but expand log opposites. So that is going to end up being x minus. So log soft max is just x minus, all this logged. And here it is, all this logged. So that’s nice. So here’s our simplified version. Okay, Now there’s another very cool trick, which is one of these things I figured out myself and then discovered other people have known it for years. So not my trick, but it’s always to rediscover things. The trick is what’s written here. Let me explain what’s going on. This piece here, the log of this sum right this sum here we’ve got x.exp.sum. Now x could be some pretty big numbers. And e of the power of that’s going to be really big numbers. And e of the power of things creating really big numbers. Well, really big numbers. There’s much less precision in your computers floating point handling the further, you get away from zero, basically. So we don’t want really big numbers, particularly because we’re going to be taking derivatives. And so if you’re in an area that’s not very precise as far as 13 point math is concerned, then the derivatives are going to be a disaster. They might even be zero because you’ve got two numbers that the computer can’t even recognize is different.</p>
<p>So this is bad, but there’s a nice trick we can do to make it a lot better. What we can do is we can calculate the max of the max of X, right. And we’ll call that A. And so then rather than doing the log of the sum of e of x i, we’re instead going to define A as being the maximum of all of our X values. It’s our biggest number. Now, if we then subtract that from every number, that means none of the numbers are going to be big by definition because we have subtracted it from all of them. Now the problem is that’s given us a different result. Right? But if you think about it, let’s expand this sum. It’s e to the power of x one , say plus e of the power of X to plus e of the power of x three and so forth. Okay, Now we just subtracted a from our exponents, which is we’re now wrong. But I’ve got good news, I’ve got good news and bad is. The bad news is that you’ve got more high school math to remember, which is exponent rules. So X to the a plus b equals x to the a, times x to the b, and x to the b minus b equals x to the a divided by x to the b, And to convince yourself that’s true, consider for example, two to the power of two three. What is that what you’ve got to do?</p>
<p>The power of two is just two times two and to the power of two plus three. Well, it’s two times. Two times it is to the power of five. So you’ve got two to the power of two, you’ve got two of them here and you’ve got another three of them here. So we’re just adding up the number to get the total index so we can take advantage of this here and say like, Oh well this is equal to a to the e to the x1 over e to the a plus e to the x2 over e to the a … And this is a common denominator. So we can put all that together why do we do all that. Because if we now multiply that all by e of the a these would cancel out and we get the thing we originally wanted. So that means we simply have to multiply this by that. And this gives us exactly the same thing as we had before.</p>
<p>But with critically, this no longer ever going to be a giant number, so this might seem a bit weird. We’re doing extra calculations. It’s not a simplification, it’s a complexification, but it’s one that’s going to make it easier for our floating point unit. So that’s our trick. It’s rather than doing log of this sum, what we actually do is log of e of times the sum of e to the X minus a, And since we’ve got a log of a product that’s just a log, that’s just the sum of the logs and log of e to the a so it’s a plus that so this here is called</p>
<p>the log sum x trick people are pointing out. Thank you for that of course should have been inside the log you got this go sticking it on the outside like a crazy person. Yeah. Yeah, That’s what I meant to say. Let’s check if you’ve got any questions. Any questions yet? Okay, so here is the log.call it a (m in notebook) But anyway, so we find the maximum on the last dimension. And then here is the m plus that exact thing. Okay, so that’s just another way of doing that. Okay, So that’s the log, sum exp. So now we can rewrite log soft max as x minus log sum exp, and we’re not going to use our evasion because PyTorch already has one, so we’re just use pytorches.</p>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pickle,gzip,math,os,time,shutil,torch,matplotlib <span class="im" style="color: #00769E;">as</span> mpl,numpy <span class="im" style="color: #00769E;">as</span> np,matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor,nn</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span></code></pre></div>
<p>:::</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span>
<span id="cb2-2"></span>
<span id="cb2-3">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb2-4">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb2-5">np.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb2-8">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb2-9">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-10">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 Mar 27 05:00 mnist.pkl.gz</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span>
<span id="cb6-2">x_train, y_train, x_valid, y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, [x_train, y_train, x_valid, y_valid])</span></code></pre></div>
</div>
<section id="initial-setup" class="level2">
<h2 class="anchored" data-anchor-id="initial-setup">Initial setup</h2>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">n,m <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb7-2">c <span class="op" style="color: #5E5E5E;">=</span> y_train.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-3">nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb8-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb8-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb8-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]</span>
<span id="cb8-5">        </span>
<span id="cb8-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb8-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb8-8">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(m, nh, <span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb9-2">pred <span class="op" style="color: #5E5E5E;">=</span> model(x_train)</span>
<span id="cb9-3">pred.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([50000, 10])</code></pre>
</div>
</div>
</section>
<section id="cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss">Cross entropy loss</h3>
<p>First, we will need to compute the softmax of our activations. This is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chbox%7Bsoftmax(x)%7D_%7Bi%7D%20=%20%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7Be%5E%7Bx_%7B0%7D%7D%20+%20e%5E%7Bx_%7B1%7D%7D%20+%20%5Ccdots%20+%20e%5E%7Bx_%7Bn-1%7D%7D%7D"></p>
<p>or more concisely:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chbox%7Bsoftmax(x)%7D_%7Bi%7D%20=%20%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7B%5Csum%5Climits_%7B0%20%5Cleq%20j%20%5Clt%20n%7D%20e%5E%7Bx_%7Bj%7D%7D%7D"></p>
<p>In practice, we will need the log of the softmax when we calculate the loss.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> (x.exp()<span class="op" style="color: #5E5E5E;">/</span>(x.exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>))).log()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">log_softmax(pred)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Note that the formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cleft%20(%20%5Cfrac%7Ba%7D%7Bb%7D%20%5Cright%20)%20=%20%5Clog(a)%20-%20%5Clog(b)"></p>
<p>gives a simplification when we compute the log softmax:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> x <span class="op" style="color: #5E5E5E;">-</span> x.exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>).log()</span></code></pre></div>
</div>
<p>Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the <a href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp trick</a>. The idea is to use the following formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cleft%20(%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D%7D%20%5Cright%20)%20=%20%5Clog%20%5Cleft%20(%20e%5E%7Ba%7D%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D-a%7D%20%5Cright%20)%20=%20a%20+%20%5Clog%20%5Cleft%20(%20%5Csum_%7Bj=1%7D%5E%7Bn%7D%20e%5E%7Bx_%7Bj%7D-a%7D%20%5Cright%20)"></p>
<p>where a is the maximum of the <img src="https://latex.codecogs.com/png.latex?x_%7Bj%7D">.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;">def</span> logsumexp(x):</span>
<span id="cb15-2">    m <span class="op" style="color: #5E5E5E;">=</span> x.<span class="bu" style="color: null;">max</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb15-3">    <span class="cf" style="color: #003B4F;">return</span> m <span class="op" style="color: #5E5E5E;">+</span> (x<span class="op" style="color: #5E5E5E;">-</span>m[:,<span class="va" style="color: #111111;">None</span>]).exp().<span class="bu" style="color: null;">sum</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>).log()</span></code></pre></div>
</div>
<p>This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> log_softmax(x): <span class="cf" style="color: #003B4F;">return</span> x <span class="op" style="color: #5E5E5E;">-</span> x.logsumexp(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">test_close(logsumexp(pred), pred.logsumexp(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb17-2">sm_pred <span class="op" style="color: #5E5E5E;">=</span> log_softmax(pred)</span>
<span id="cb17-3">sm_pred</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],
        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],
        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],
        ...,
        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],
        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],
        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=&lt;SubBackward0&gt;)</code></pre>
</div>
</div>
<p>The cross entropy loss for some target <img src="https://latex.codecogs.com/png.latex?x"> and some prediction <img src="https://latex.codecogs.com/png.latex?p(x)"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20-%5Csum%20x%5C,%20%5Clog%20p(x)%20"></p>
<p>But since our <img src="https://latex.codecogs.com/png.latex?x">s are 1-hot encoded (actually, they’re just the integer indices), this can be rewritten as <img src="https://latex.codecogs.com/png.latex?-%5Clog(p_%7Bi%7D)"> where i is the index of the desired target.</p>
<p>This can be done using numpy-style <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing">integer array indexing</a>. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">y_train[:<span class="dv" style="color: #AD0000;">3</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([5, 0, 4])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">sm_pred[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">5</span>],sm_pred[<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>],sm_pred[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">4</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(-2.20, grad_fn=&lt;SelectBackward0&gt;),
 tensor(-2.37, grad_fn=&lt;SelectBackward0&gt;),
 tensor(-2.36, grad_fn=&lt;SelectBackward0&gt;))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">sm_pred[[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>], y_train[:<span class="dv" style="color: #AD0000;">3</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-2.20, -2.37, -2.36], grad_fn=&lt;IndexBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="kw" style="color: #003B4F;">def</span> nll(<span class="bu" style="color: null;">input</span>, target): <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span><span class="bu" style="color: null;">input</span>[<span class="bu" style="color: null;">range</span>(target.shape[<span class="dv" style="color: #AD0000;">0</span>]), target].mean()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">loss <span class="op" style="color: #5E5E5E;">=</span> nll(sm_pred, y_train)</span>
<span id="cb26-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(2.30, grad_fn=&lt;NegBackward0&gt;)</code></pre>
</div>
</div>
<p>And if we check, we yeah, we go, it’s our results. And so then as we’ve discussed, the cross entropy loss is the sum of the outputs times the log probabalities. And as we discussed, our outputs are one hot and coded or actually they’re just the integers better still. So what we can do is we can I guess I should make that more clear. Actually the just the integer indices so we can simply rewrite that as negative log of the target. So that’s what we had in our Excel. And so how do we do that in PyTorch? So this is quite interesting. There’s A lot of cool things you can do with array indexing in PyTorch in Numpy. So basically they use the same approaches. Let’s take a look. Here is the first three actual values in y_train. They’re five zero and four. Now what we want to do is we want to find in our soft max predictions, we want to get five the fifth prediction in the zeroth row, the zeroth prediction in the first row and the fourth in the index two row. So these are the numbers that we want. This is going to be what we add up for the first few rows of last function. So how do we do that all in one go? Well, here’s a cool trick. See here, I’ve got 0,1,2. If we index using a two lists, we can put here 0, 1,2, and for the second list we can put y_train[:3]. So it’s going to be it’s going to be zero comma 5, 1 comma zero, 2 comma four, which is, as you say, exactly the same thing. So therefore, this is actually giving us what we need for the cross entropy loss. So if we take range of our targets first dimension or zero index dimension, which is all this is and the target and then take the negative of that dot mean, that gives us our cross entropy loss, which is pretty neat in my opinion. All right. So pytorch calls this negative log likelihood loss and that’s all it is. And so if we take the negative log likelihood and so get negative likelihood and we pass that to , the log soft max, then we get the loss. And this particular combination in PyTorch called F.across_entropy. So let’s check. Yep. F.across_entropy gives us exactly the same thing. So that’s cool. So we have now re-implemented cross entropy loss and there’s a lot of confusing things going on there. A lot. And so this is one of those where you should pause the video and go back and look at each step and think not just like, what is it doing, but why is it doing it? And also try in lots of different values yourself to see if you can see what’s going on and then put this aside and test yourself by reimplementing log soft max and nll_loss and cross entropy yourself and compare them to pytouches values. And so that’s the piece of homework for you for this week</p>
<p>Then use PyTorch’s implementation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">test_close(F.nll_loss(F.log_softmax(pred, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>), y_train), loss, <span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
<p>In PyTorch, <code>F.log_softmax</code> and <code>F.nll_loss</code> are combined in one optimized function, <code>F.cross_entropy</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">test_close(F.cross_entropy(pred, y_train), loss, <span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="basic-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="basic-training-loop">Basic training loop</h2>
<p>So now that we’ve got that we can actually create a training loop. So let’s set our last function to be cross entropy. Let’s create a batch size of 64. And so here’s our first mini batch. Okay, so xb is the x mini batch. It’s going to be from zero up to 64 from our training set, so we can now calculate our predictions. So that’s 64 by ten. So for each of the 64 images in the mini batch, we have ten probability one for each digit. And our y(s) is just a let’s print those out. So there’s our first 64 target values. So these are the actual digits. And so our loss function. So we’re going to start with a bad loss because it’s entirely random at this point. Okay. So for each of the predictions</p>
<p>we made, so those are our predictions. And so remember, those predictions are a 64 by ten. What did we predict? So for each one of these 64 rows, we have to go in and see where is the highest number. So if we go through here, we can go through each one. Here’s a 0.123 So the highest number is this one. So you got to find the index of the highest number. The function to find the index of the highest number is called argmax. And here it is three. And I guess we could have also written this probably as preds.argmax. Normally you can do them either way. I actually prefer normally to do it this way. Yeah. There’s the same thing. Okay. And the reason we want this is because we want to be at a complete accuracy. We don’t need it for the actual neural net, but we just like to be able to see how we’re going because it’s like it’s a metric. It’s something that we use for understanding. So we take the argmax, we compare it to the actual. So that’s going to give us a bunch of bools. If you turn those into floats, they’ll be ones and zeros. The mean of those flights is the accuracy. So our current accuracy, not surprisingly, is around 10%. It’s 9% because it’s random. That’s what you would expect. Let’s train our first neural net. So we’ll set a learning rate, which are a few epochs. So we’re going to go through each epoch and we’re going to go through from zero up to n.&nbsp;That’s the 50,000 training rows and skipping by 64 the batch size each time. And so we’re going to create a slice that starts at i So starting at zero and goes up 64 unless we’re just got to n And so then we will slice into our training set for the x and for the y to get an x and y batches. We will then calculate our predictions, our loss function, and do it backward.</p>
<p>So the way I did this originally was I had all of these in in separate cells and I just typed in, you know, i equals zero and then went through one cell at a time calculating each one until they all worked. And so then I can put them in a loop. Okay. So once we’ve got done backward, we can then with torch.no_grad go through each layer and if that’s a layer that has weights, will update them to the existing weights minus the gradients times of learning and then zero out. So the weights and biases for the gradients, the gradients of the weights and biases this underscore means do it in place. So that sets this to zero. So if I run that up, it’s going to run all of them. I guess I skipped. So there we go. It’s finished. So you can see that our accuracy on the training sets have been unfair. But 23 epochs is 97%. So we now have a digit recognizer trains pretty quickly and it’s not terrible at all. So that’s a pretty good starting point. All right. So what we’re going to do next time is we’re going to refactor this training loop to make it dramatically dramatic training, dramatically simple step by step, until eventually we will get it down to where is it. So we’ll get it down to something much, much shorter. And then we’re going to add a validation set to it and a multi processing data loader. And then yeah, we’ll be in a pretty good position, I think, to to start training some more interesting models. All right. Hopefully you found that useful and led some interesting things. And so what I’d really like you to do is at this point, now that you’ve kind of like got all these key basic pieces in place, is to really try to recreate them without picking as much as possible. So, you know, recreate your matrix, multiply, recreate those forward and backward passes, recreate something that steps through layers and even see if you can like recreate the idea of the dot forward and the dot backward. Make sure it’s all in your head really clearly so that you fully understand what’s going on. You know, at the very least, if you don’t have time for that, because that’s a big job, you could pick out a smaller part of that, the piece that you’re more interested in, or you could just go through and look really at these notebooks. So if you go to kernel restart and clear output, it’ll delete all the outputs and try to think like what are the shapes of things? Can you guess what they are, can you check them? And so forth. Okay, Thanks everybody. I hope you have a great week and I will see you next time by.</p>
<p>Basically the training loop repeats over the following steps: - get the output of the model on a batch of inputs - compare the output to the labels we have and compute a loss - calculate the gradients of the loss with respect to every parameter of the model - update said parameters with those gradients to make them a little bit better</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">loss_func <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>                  <span class="co" style="color: #5E5E5E;"># batch size</span></span>
<span id="cb31-2"></span>
<span id="cb31-3">xb <span class="op" style="color: #5E5E5E;">=</span> x_train[<span class="dv" style="color: #AD0000;">0</span>:bs]     <span class="co" style="color: #5E5E5E;"># a mini-batch from x</span></span>
<span id="cb31-4">preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)      <span class="co" style="color: #5E5E5E;"># predictions</span></span>
<span id="cb31-5">preds[<span class="dv" style="color: #AD0000;">0</span>], preds.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=&lt;SelectBackward0&gt;),
 torch.Size([50, 10]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">yb <span class="op" style="color: #5E5E5E;">=</span> y_train[<span class="dv" style="color: #AD0000;">0</span>:bs]</span>
<span id="cb33-2">yb</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,
        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">loss_func(preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(2.30, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">preds.argmax(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3,
        9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])</code></pre>
</div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">def</span> accuracy(out, yb): <span class="cf" style="color: #003B4F;">return</span> (out.argmax(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)<span class="op" style="color: #5E5E5E;">==</span>yb).<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
<p>:::</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">accuracy(preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.08)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>   <span class="co" style="color: #5E5E5E;"># learning rate</span></span>
<span id="cb42-2">epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span> <span class="co" style="color: #5E5E5E;"># how many epochs to train for</span></span></code></pre></div>
</div>
<p>::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}</p>
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;">def</span> report(loss, preds, yb): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'</span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">, </span><span class="sc" style="color: #5E5E5E;">{</span>accuracy(preds, yb)<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<p>:::</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[:bs],y_train[:bs]</span>
<span id="cb44-2">preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb44-3">report(loss_func(preds, yb), preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.30, 0.08</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb46-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb46-3">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(i, <span class="bu" style="color: null;">min</span>(n,i<span class="op" style="color: #5E5E5E;">+</span>bs))</span>
<span id="cb46-4">        xb,yb <span class="op" style="color: #5E5E5E;">=</span> x_train[s],y_train[s]</span>
<span id="cb46-5">        preds <span class="op" style="color: #5E5E5E;">=</span> model(xb)</span>
<span id="cb46-6">        loss <span class="op" style="color: #5E5E5E;">=</span> loss_func(preds, yb)</span>
<span id="cb46-7">        loss.backward()</span>
<span id="cb46-8">        <span class="cf" style="color: #003B4F;">with</span> torch.no_grad():</span>
<span id="cb46-9">            <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> model.layers:</span>
<span id="cb46-10">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">hasattr</span>(l, <span class="st" style="color: #20794D;">'weight'</span>):</span>
<span id="cb46-11">                    l.weight <span class="op" style="color: #5E5E5E;">-=</span> l.weight.grad <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb46-12">                    l.bias   <span class="op" style="color: #5E5E5E;">-=</span> l.bias.grad   <span class="op" style="color: #5E5E5E;">*</span> lr</span>
<span id="cb46-13">                    l.weight.grad.zero_()</span>
<span id="cb46-14">                    l.bias  .grad.zero_()</span>
<span id="cb46-15">    report(loss, preds, yb)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.12, 0.98
0.12, 0.94
0.08, 0.96</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 6/index.html</guid>
  <pubDate>Fri, 24 Mar 2023 19:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 5</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 5/index.html</link>
  <description><![CDATA[ 



<p>After This you should be comfortable with following : <br> 1- Chain Rule <br> 2- Basic linear NN architecture <br> 3- SGD concept <br> 4- Coding the model in python and pytorch <br></p>
<p>So now we’re going to take a look at notebook number three in the normal repo course22p2, and we’re going to be looking at the forward and backward passes of a simple multi-layer perceptron, a neural network. The initial stuff up here is just importing things and just settings and stuff that they’re just copying and pasting some stuff from previous notebooks around paths and parameters and stuff like that. So we’ll skip over this now. So will often be kind of copying and pasting stuff from one notebook to another. That’s kind first cell get things set up and have also loading in our data for MNIST as tensors. Okay, so we to start with, need to create the basic architecture of our neural network. And I did mention at the start of the course that we will briefly review everything that we need to cover. So we should briefly review what basic neural networks are and why they are what they are. So to start with, let’s consider a linear model. Oops, that’s not how I do it,let’s take the most simple example possible, which is we’re going to pick a single pixel from from our MNIST pictures. And so that will be our x and for our, for our y values. Then we’ll have some lost function of how good is this model? Sorry, not some lost function that’s even simpler for our y value. We’re going to be looking at how likely is it that this is, say, the number three based on the value of this one pixel? So the pixel, its value will be x and the probability of being the number three we’ll call y and if we just have a linear model, then it’s going to look like this. And so in this case, it’s it’s saying that the brighter this pixel is, the more likely it is that it’s the number three. And so there’s a few problems with this. The first one, obviously, is that as a linear model, it’s very limiting because maybe, you know, we actually are trying to draw something that looks more like this. So how would you do that? Well, there’s actually a neat trick we can use to do that. What we could do is, well, let’s first of all, talk about something we can’t do, something we can’t do is to add a bunch of additional lines. So consider what happens if we say, okay, well, let’s add a few different lines.</p>
<p>So let’s also add this line. So what would be the sum of our two lines? Well, the answer is, of course, that the sum of the two lines will itself be a line. So it’s not going to help us at all match the actual curve that we want. So here’s the trick. Instead, we could create a line like this that actually we could get this line. And now consider what happens if we add this original line with this new what’s not a line, right? It’s it’s two line segments. So what we would get is this everything to the left of this point is going to not be changed. If I add these two lines together, because this is zero all the way and everything to the right of it is going to be reduced. It looks like they’ve got similar slopes, so we might end up with instead. So this would all disappear here and instead we would end up with something like this. And then we could do that again, right? We could add an additional line that looks a bit like that. So it would go, but this time it could go even further out here and it could be something like this. Say, so what if we added that? Well, again, at the point underneath here, it’s always zero, so it won’t do anything at all. But after that it’s going to make it even more negatively sloped. And if you can see using this approach, we could add up lots of these rectified lines. These lines are truncated zero, and we could create any shape we want with enough of them. And these lines are very easy to create because actually all we need to do is to create just a regular line, just to get a regular line. Right. Which we can move up, down, left, right, change its angle, whatever, and then just say if it’s greater than zero, truncate it to zero, or we could do the opposite because through a line going the opposite direction, it has less than zero, we could say truncated to zero, and that would get rid of as we want. This whole section here and make it flat. Okay. So these are rectified lines and so we can sum up a bunch of these together to basically match any arbitrary curve.</p>
<p>(Bahman note )This is amazing , the is intuition of a theory that we can create or mimic any funtion with neural net. Like creating anything fron linear model. (I do not remember the paper or the creator / he is very famous , he said something like this in his paper but did not try for more layers and years after that when people did that new era of AI started) . (Bahman note )</p>
<p>So let’s start by doing that. Oh, the other thing we should mention, of course, is that we’re going to have not just one pixel, but we’re going to have lots of pixels. So to start with, the kind of most, you know, slightly, you know, the only slightly less simple approach, we could have something where we’ve got, you know, pixel number one and pixel number two, we’re looking at two different pixels to see how likely they are to be the number three. And so that would allow us to draw more complex shapes that have some kind of surface between them. Okay. And then we can do exactly the same thing is to create these surfaces.</p>
<p>We can add up lots of these rectified lines together, but now they’re going to be kind of rectified planes, but it’s going to be exactly the same thing. We’re going to be adding together a bunch of lines, each one of which is truncated at zero. Okay. So that’s the quick review. And so to do that, we’ll start out by just defining a few variables. So n is the number of training examples. m Is the number of pixels. c is the number of possible values of our digits. And so here they are, (50000, 784, tensor(10)). Okay. So what we do is to is we basically decide ahead of time how many of these lines segment thing is to add up. And so the number that we create in a layer is called the number of hidden nodes or activations. So we call that nh, So let’s just arbitrarily decide on creating 50 of those. So in order to create lots of lines which where they’re going to truncate at zero, we can do a matrix multiplication. So with the matrix multiplication, we’re going to have something where we’ve got by 500000 rows by 784 column. Yeah, by 784 columns. And we’re going to multiply that by something with 784 rows and ten columns and why is that? Well, that’s because if we take this very first line of this first vector here, write one of 784 values there. The pixel values of the first image. Okay, So this is our first image. And so they’re each going to each of those 700 different values, but we multiply it by each of these 784 values and in the first column, the zero index column. And that’s going to give us a number in our output. So our output is going to be And so that result will multiply those together and add them up and that result is going to end up over here in this first. So and so each of these columns is going to eventually represent if this if this was a this is a linear model in this case, this is just the example of doing a linear model, each of these cells is going to represent the probability. So this first column will be the probability of being zero, and the second column will be the probability of one. The third column will be the probability of being a two and so forth. So that’s why we’re going to have these ten columns, each one allowing us to write the 784 inputs. Now of course, we’re going to do something bit more tricky than that, which is actually we’re going to have a 74 by 50 input going into a 784 by 50 output to create the 50 hidden layers. Then we’re going to truncate those at zero and then multiply that by a 50 by 10 to create an output. So we do it in two steps. So and the way it sgd works is we start with just this is our weight matrix here and this is our data, this is our outputs. The way it works is that this weight matrix is initially filled with random values. So called this contains our pixel values. This contains the results. So w is going to start with random values. So here’s our weight matrix. It’s going to have, as we discussed, 50,000 by 50 random values and it’s not enough just to multiply. We also have to add. So that’s what makes it a linear function. So we call those the biases, the things we add. We can just start those zeros, so we’ll need one for each output. So 50 of those and so that’ll be layer one. And then as we just mentioned, layer two will be a matrix that goes from 50 hidden. And now I’m going to do something totally cheating. To simplify some of the calculations for the calculus, I’m only going to create one output.</p>
<section id="the-forward-and-backward-passes" class="level2">
<h2 class="anchored" data-anchor-id="the-forward-and-backward-passes">The forward and backward passes</h2>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pickle,gzip,math,os,time,shutil,torch,matplotlib <span class="im" style="color: #00769E;">as</span> mpl, numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> test_close</span>
<span id="cb1-5">torch.manual_seed(<span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb1-6"></span>
<span id="cb1-7">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb1-8">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb1-9">np.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">125</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb1-12">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb1-13">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb1-14">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<div class="cell" data-outputid="27219d5b-d56c-4eca-f032-a0ce00b2cad9" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 Mar 27 04:43 mnist.pkl.gz</code></pre>
</div>
</div>
<p>!ls -l data</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span>
<span id="cb5-2">x_train, y_train, x_valid, y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, [x_train, y_train, x_valid, y_valid])</span></code></pre></div>
</div>
</section>
<section id="foundations-version" class="level2">
<h2 class="anchored" data-anchor-id="foundations-version">Foundations version</h2>
<section id="basic-architecture" class="level3">
<h3 class="anchored" data-anchor-id="basic-architecture">Basic architecture</h3>
<div class="cell" data-outputid="ffc4b848-27db-43e9-b070-2bbd17464156" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">n,m <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb6-2">c <span class="op" style="color: #5E5E5E;">=</span> y_train.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb6-3">n,m,c</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(50000, 784, tensor(10))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># num hidden</span></span>
<span id="cb8-2">nh <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span></code></pre></div>
</div>
<p>Well, I’m going to create one output. That’s because I’m not going to use cross entropy just yet. Instead, I’m going to use MSE. So actually I’m going to create one output, which will literally just be what do i think it is? from 0 to 10. And so then we’re going to compare those to the actual. So these will be our y predicted to only use a little hat for that and we’re going to compare that to our actuals. And yeah, in this very hacky approach, let’s say we predict over here the number nine and the actual is the number two and we’ll compare those together using MSE, which will be a stupid way to do it because it’s saying that nine is further away from being true than two, nine is farther away from true than it is from four in terms of how correct it is, which is which is not what we want at all. But this is what we’re going to do just to simplify our starting point. So that’s why we’re going to have a single output for this weight matrix in a single output for this bias. So linear, let’s create a function for putting extra linear layer with these weights in these biases. So it’s a matrix multiply and an add. All right. So we can now try it. So if we multiply we doing x_valid this time? So just to clarify x_valid is 10,000 by 784. So if we put x_valid through our weights and biases with a linear layer, we end up with a 10,000 by 50. So 10,050 long hidden activations, they’re not quite ready yet because we have to put them through relu. And so we’re going to clamp at zero, so anything under zero will become zero. And so here’s what it looks like when we go through the linear layer and then the relu here. And you can see he has a tensor with a bunch of things, some of which are zero, or they’re positive. And so that’s the result of this match, this matrix multiplication. Okay. So to create a basic MLP multi-layer perceptron from scratch, we will take our mini batch of X’s x, b is an x match. We will create our first layers output with a linear and then we will put that through over here and then that will go through the second linear. So the first one uses the w1, b1. Okay, these ones and the second one uses the w2 to b2. And so we’ve now got a simple model and as we hoped, when we pass in the validation set, we get back 10000 digits. So that’s a good start. Okay, so let’s use our ridiculous lost function of MSE. So our results is 10,000 by one and our y_valid is just a vector. Now what’s going to happen if I do res minus y-valid?</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">w1 <span class="op" style="color: #5E5E5E;">=</span> torch.randn(m,nh)</span>
<span id="cb9-2">b1 <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(nh)</span>
<span id="cb9-3">w2 <span class="op" style="color: #5E5E5E;">=</span> torch.randn(nh,<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb9-4">b2 <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;">def</span> lin(x, w, b): <span class="cf" style="color: #003B4F;">return</span> x<span class="op" style="color: #5E5E5E;">@</span>w <span class="op" style="color: #5E5E5E;">+</span> b</span></code></pre></div>
</div>
<div class="cell" data-outputid="33a09a02-0042-4347-ee6f-9e3e6145eb76" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">t <span class="op" style="color: #5E5E5E;">=</span> lin(x_valid, w1, b1)</span>
<span id="cb11-2">t.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>torch.Size([10000, 50])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;">def</span> relu(x): <span class="cf" style="color: #003B4F;">return</span> x.clamp_min(<span class="fl" style="color: #AD0000;">0.</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="fd44eff3-f4cf-4e95-b07c-fe2d954372b0" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">t <span class="op" style="color: #5E5E5E;">=</span> relu(t)</span>
<span id="cb14-2">t</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],
        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],
        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],
        ...,
        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],
        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],
        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> model(xb):</span>
<span id="cb16-2">    l1 <span class="op" style="color: #5E5E5E;">=</span> lin(xb, w1, b1)</span>
<span id="cb16-3">    l2 <span class="op" style="color: #5E5E5E;">=</span> relu(l1)</span>
<span id="cb16-4">    <span class="cf" style="color: #003B4F;">return</span> lin(l2, w2, b2)</span></code></pre></div>
</div>
<div class="cell" data-outputid="bfecebe5-25e1-4f2f-a459-ac60dfb7c461" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">res <span class="op" style="color: #5E5E5E;">=</span> model(x_valid)</span>
<span id="cb17-2">res.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>torch.Size([10000, 1])</code></pre>
</div>
</div>
</section>
<section id="loss-function-mse" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-mse">Loss function: MSE</h3>
<p>So before you continue in the video, have a think about that. What’s going to happen if I do res minus y_valid by thinking about the numpy broadcasting rules we’ve lent? Okay, let’s try it. Oh terrible. We’ve ended up with a 10,000 by 10,000 matrix. So 100 million points now we would expect an MSA to contain a thousand points. Why does that happen? The reason it happened is because we have to start out at the last dimension and go right to left and we compare to the 10,000 to the one and say, are they compatible? And the answer is, that’s right. Alexi And the chats got it right, broadcasting roles. So the answer is that this one will be broadcast over these 10,000. So this pair here will give us the next one and we’ll also move here to the next one. Oh, there is no next one. What happens now? If you remember the rules in search, say, unit access for us. So I now have 10,000 by one. So that means each of the 10,000 outputs from here is will end up being broadcast across the 10,000 rows here. So that means that will end up for each of those 10,000. We’ll have another 10,000. So we’ll end up with a 10,000 by 10,000 output. So that’s not what we want. So how could we fix that? Well, what we really would want, would we want this to be If that was 10,000 comma one, then we’d compare these two right to left and they’re both one. So those those match and there’s nothing to broadcast because they’re the same. And then we’ll go to the next one, 10000 to 10000 those match. So that just go element wise for those and we’d end up with exactly what we want it to hand out with 10,000 results. Or alternatively we could remove this dimension and then again, same thing we’re going to add right to left compatible 10,000. So they will get element wise operation. So in this case I got rid of the trailing comma one. It’s a couple of ways you could do that. One is just to say, okay, grep every row and the zeroth column of res and that’s going to turn it from a 10,000 by one into a 10,000.</p>
<p>Or alternatively we can say dot squeeze. Now dot squeeze removes all trailing unit vectors and possibly also prefix unit vectors. I can’t quite recall. I guess we should try. So let’s say res None comma calling, comma None, q.shape. Okay. So if I go q.squeeze. shape. Okay. So all the unit vectors get removed. Sorry, all the unit dimensions get removed I should say. Okay. So now that we’ve got a way to remove that axis that we didn’t want, we can use it. And if we do that subtraction now we get 10,000 just like we wanted it. So now let’s get our training and validation wise, we’ll turn them into floats because we’re using MSI. So let’s calculate our predictions for the training set, which is 50,000 by one. And so if we create an MSE function that just does what we just said we wanted, so it does a subtraction and then squares it and then takes the mean that the MSE. So there we go. We now have a loss function being applied to our training set. Okay, now we need gradients. So as we briefly discussed last time, gradients slopes and in fact maybe it would even be easier to look at last time. So this is last times notebook. And so we saw how the gradient at this point is the slope here and so it’s the as we discussed rise over run now so that means as we increase in this case time by one the distance increases by how much that’s what the slope is. So why is this interesting? The reason it’s interesting is because let’s consider our neural network. Our neural network is some function that takes two things two groups of things. It contains a matrix of our inputs, and it contains our weight matrix. And we want to and let’s assume we’re also putting it through a loss function. So let’s say, well, I guess we can be explicit about that. So we could say we then take the result of that and we put it through some lost function.</p>
<p>So these are the predictions and we compare it to our actual dependent variable. So that’s our neural net and that’s our last function. Okay. So if we can get the derivative of the loss with respect to, let’s say, one particular weight, so let’s say weight, and I’m like number zero, what is that doing? Well, it’s saying as I increase, increase the weight by a little bit, what happens to the loss? And if it says, oh, well, that would make the loss go down, then obviously I want to increase the weight by a little bit. And if it says, Oh, it makes the loss go up, then obviously I want to do the opposite. So that derivative of the loss with respect to the weights each one of those tells us how to change the weights. And so to remind you, we then change each weight by that derivative times a little bit and subtract it from the original weights. And we do that a bunch of times, and that’s called SGD. Now, there’s something interesting going on here, which is that in this case there’s a single input and a single output, and so the derivative is a single number at any point. It’s the speed. In this case, the vehicle is going. But consider a more complex function like, say, this one. Now, in this case, there’s one output, but there’s two inputs. And so if we want to take the derivative of this function, then we actually need to say, well, what happens if we increase x by a little bit? And also what happens if we increase y by a little bit? And in each case what happens to x? And so in that case, the derivative is actually going to contain two numbers, right? It’s going to contain the derivative of x with respect to y and it’s going to contain the derivative of z with respect to x. What happens if we change each of these two numbers? So, for example, these could be, as we discussed, two different weights in our neural network and Z could be our loss. For example, now we’ve got actually 784 inputs, right? So we would actually have 784 of these. So we don’t normally write them all like that. We would just say, yes, this little squiggly symbol to say the derivative of the loss across all of them with respect to all of the weights. Okay. And that’s just saying that there’s a whole bunch of them. It’s a shorthand way of writing this. Okay. So it gets more complicated still, though, because think about what happens if, for example, you were in the first layer where we’ve got a weight matrix that’s going to end up giving us 50 outputs, right? So for every image, we’re going to have 784 inputs to our function and we’re going to have 50 outputs to our function.</p>
<p>And so in that case, I can’t even draw it right because like for every even if I had two inputs and outputs, then as I increase my first input, I’d actually need to say, how does that change both of the two outputs? And as I change my second input, how does that change both of my two outputs? So for the full thing, you’re actually going to end up with a matrix of derivatives. It basically says for every input that you change by a little bit, how much does it change every output of that function? So you’re going to end up with a matrix. So that’s what we’re going to be doing, is we’re going to be calculating these derivatives, but rather than being single numbers, they’re going to actually contain matrices with a row for every input and a column for every output and a single cell in that matrix. Well, tell us, as I change this input by a little bit, how does it change this output Now? Eventually we will end up with a single number for every input, and that’s because our loss in the end is going to be a single number. And this is like a a requirement that you’ll find when you try to use SGD is that your loss has to be a single number. And so we generally get it by either doing the the sum or a mean or something like that. But as you’ll see on the way there, we’re going to have to be dealing with these matrix of derivatives. So I just want to mention, as I might have said before, I can’t even remember there is this paper that Terence Power and I wrote a while ago, which goes through all this, and it basically assumes that you only know high school calculus. And if you don’t checkout Khan Academy, but then it describes matrix calculus in those terms. So it’s going to explain to you. Exactly. And it works through lots and lots of examples. So, for example, as it mentions here, when you have this matrix of derivatives, we call that a Jacobean matrix. So like there’s all these like words, it doesn’t matter too much if you know them or not, but it’s convenient to be able to talk about, you know, the matrix of all of the derivatives. If somebody just says this Jacobean, it’s a little convenient. It’s a bit, a little bit easier than saying the matrix of all of the derivatives where all of the rows are things that are, are all the inputs and all the columns of the outputs. So yeah, if you want to really understand, get to a point where papers are easier to read particular, it’s quite useful to know this notation and, and, and definitions of words. https://explained.ai/matrix-calculus/</p>
<p>You can certainly get away without it. It’s just something to consider. Okay, so we need to be able to calculate derivatives at least of a single variable. And I am not going to worry too much about that because that is something you do in high school math and b, because your computer can do it for you and so you can do it symbolically using something called sympy, which is really great. So if you create two symbols called x and y, you can say please differentiate x squared with respect to x. And if you do that sympy will tell you the answer is 2x. If you say differentiate three x squared plus nine with respect to x, sympy will tell you that six x and a lot of you probably will have used Wolfram Alpha that does something very similar. I kind of quite like this because I can quickly do it inside my notebook and included in my prose. So I think sympy is pretty cool. So you know, basically yeah. That if you, you know, you can quickly calculate derivatives on a computer. Having said that, I do want to talk about why the derivative of three x squared plus nine equals six x, because that’s going to be very important. So three x squared plus nine. So we’re going to start with the the information that that derivative of a to the b with respect to a equals b<em>a , So for example, the derivative of x squared with respect to x equals 2x. So that’s just something I’m hoping you remember from high school or refresh your memory is in Khan Academy also. So there that is there. So what we could now do is we could rewrite this derivative as 3u plus nine and then we’ll write u equals x squared. Okay, Now this is getting easier. The derivative of two things being added together is simply the sum of the derivatives. Oh, I forgot. B minus one of the exponent. Thank you. So B eight the power of b minus one. That’s what it should be, which would be two x to the power of one and the one is not needed. Thank you for fixing that. All right. So we just some of them up so we get the derivative of 3u is actually just well, it’s going to be the derivative of that plus the derivative of that. Now the derivative of any constant with respect to a variable is zero. Because if I change something an input, it doesn’t change the constant, it’s always nine. So that’s going to end up as zero. And so we’re going to end up with the dy/du equals something plus zero. And the derivative of 3u with respect to u is just three, because it’s just a line that’s its slope. Okay, But that’s not dy/dx Well, the cool thing is that dy/dx is actually just equal to dy/du </em> du/dx. So I’ll explain why in a moment. But for now then let’s recognize we’ve got du/dx = 2x, we know that one 2x so we can now multiply these two bits together and we will end up with two X times three, which is six x, which is what sympy they told us. So fantastic. Okay, this is something we need to know really well, and it’s called the chain rule and it’s best to understand it intuitively.</p>
<p>So to understand it intuitively, we’re going to take a look at an interactive animation. So I found this nice interactive animation on this page here https://webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html</p>
<p>And the idea here that we’ve got a wheel spinning around and each time it spins round, this is x going up. Okay. So at the moment there’s some change in x,dx over a period of time. All right. Now this wheel is eight times bigger than this wheel. So each time this goes round, once, if we connect the two together, this wheel would be going round four times faster because the difference between the multiple between eight and two is four. Maybe I’ll bring this up to here. So now that the this wheel is has got twice as big a circumference as the other wheel, each time this goes around, once this is going around two times. So the change in u each time X goes round, once the change in u will be two. So that’s what du/dx is saying. The change in u for each change in x is two. Now we could make this interesting by connecting this wheel to this well. Now this wheel is twice as small as this wheel. So now we can see that again. Each time this spins round, once this spins round twice because this has twice the circumference of this. So therefore dy/du equals two. But now that means every time this goes round, once this goes round twice every time this one goes round, once this one goes round twice. So therefore every time this one goes round once this one goes round four times So to dy/dx equals four. So you can see here how the two well how the, du/dx has to be multiplied with the dy/du to you to get the total. So this is what’s going on in the chain role and this is what you want to be thinking about is this idea that you’ve got one function that is kind of this intermediary. And so you have to multiply the two impacts to get the impact of the X. Wheel, on the y Wheel, we’ll help you find that useful. I find this personally, I find this intuition quite useful. So why do we care about this? Well, the reason we care about this is because we want to calculate the gradient of our MSE applied to our model. And so our inputs are going through a linear, they’re going through a relu here, they’re going through another linear, and then they’re going through an MSE. So there’s four different steps going on. And so we’re going to have to combine those all together. And so we can do that with a chain rule. So if our steps are that lost function is the so we’ve got the lsot function, which is some function of the predictions and the actuals. And then we’ve got we’ve got the second layer is a function of actually let’s say let’s call this the output of the second layer slightly. We have notation, but hopefully it’s not too bad. It’s going to be a function of the relu of the activations and relu activation are a function of the first layer and the first layer is a function of the inputs. Oh, and of course this also has weights and biases. So we’re basically going to have to calculate the derivative of that. Okay. But then remember that this is itself a function. So then we need to multiply that derivative by the derivative of that, but that’s also a function. So we have to multiply that derivative by this, but that’s also a function. So we have to multiply that derivative by this. So that’s going to be our approach. We’re going to start at the end, we’re going to take its derivative and then we’re going to gradually keep multiplying as we go each step through. And this is called back propagation. So back propagation sounds pretty fancy, but it’s actually just using the chain rule. Gosh, I didn’t spell that very well.it’s just using the chain rule. And as you’ll see, it’s also just taking advantage of a computational trick of memorizing some things on the way. And in our chat, Stephen made a very good point about understanding nonlinear functions in this case, which is just a consider that the wheels could be growing and shrinking all the time as they’re moving, but you’re still going to have the same compound effect, which I really like that. Thank you, Sylvar There’s also a question in the chat about why is this column comma zero being placed in the function given that we can do it outside the function? Well, the point is we want an MSE function that will apply to any output, not that we’re not using it once we want it to work any time. So we haven’t actually modified preds or anything like that or y_train. So we want this to be able apply to anything without us having to like preprocessor. It is basically the idea here. Okay, so let’s take a look at the basic idea. So he is going to do a forward pass in a backward pass. So the forward passes where we calculate the loss.</p>
<p>So the loss is, oh, I’ve got an error here that should be diff. Yeah, we go. So the loss is going to be the output of our neural net minus our target squared. Then take the mean, Okay. And then our output is going to be the output of the second linear layer. The second linear layer’s input will be the relu, the relu input will be the first layer.</p>
<p>It’s going to take our input, put it through a linear layer, put that through a relu, put that through a linear layer and calculate the MSE. Okay, That bit hopefully is pretty straightforward. So</p>
<p>what about the backward pass? So the backward pass, what I’m going to do and you’ll see why in a moment is I’m going to store the gradients of each layer. So for example, the gradients of the loss with respect to its inputs in the layer itself. So I’m going to create a new attribute, I could call it anything I like, and it’s going to call it (.g) .So I’ve got to create, a new attribute called out.g, which is going to contain the gradients. You don’t have to do it this way, but as you’ll see, it turns out pretty convenient. So that’s just going to be two times the difference because we’ve got different squared, right? So that’s just the derivative. And then we have taken the mean here. So we have to do the same thing here divided by the input shape. And so that’s, that’s those gradients, that’s good. And now what we need to do is multiply by the gradients of the previous layer. So what are the gradients of a linear layer? I’ve created a function for that here. So the gradient is a linear layer. We’re going to need to know the weights of the layer. We’re going to need to know the biases of the layer. And then we’re also going to know the input to the linear layer, because that’s the thing that’s actually being manipulated in here. And then we also going to need the output because we have to multiply by the gradients because we’ve got the chain role. So again, we’re going to store the gradients of our input like inp.g . So this would be the gradients of our output with respect to the input (inp.g), and that’s simply the weights because the weights. (w.t). So a matrix multiplier is just a whole bunch of linear functions. So each one slope is just its weight, but you have to multiply it by the gradient of the outputs because of the chain role and then the gradient of the outputs. With respect to the weights (w.g) is going to be the input times the output summed up. I’ll talk more about that in a moment. The derivatives of the bias is very straightforward. It’s the gradients of the output added together because the bias is just a constant value. So for the chain role, we simply just use output times one, which is output. So for this one here again, we have to do the same thing we’ve been doing before, which is multiply by the output gradients because of the chain rule and then we’ve got the input weights. So every single one of those has to be multiplied by the outputs. And so that’s why we have to do an unsqueese minus one. So what I’m going to do now is I’m going to show you how I would experiment with this code in order to understand it, and I would encourage you to do the same thing. It’s a little harder to do this one cell by cell because we kind of want to put it all into this function like this. So we need a way to explore the calculations interactively and the way we do that is by using the Python debugger.</p>
<p>Here is how you let me see a few ways to do this. Here’s one way to use the Python debugger. The Python debugger is called PDB. So if you say PDB dot set trace in your code, then that tells the debugger to stop execution. When it reaches this line, it sets a breakpoint. So if I call forward and backward, you can see here it stopped and the interactive Python debugger I PDB has popped up with an arrow pointing at the line of code. It’s about to run. And at this point there’s a whole range of things we can do to find out what they are. We page for help understanding how to use the Python debugger. It’s one of the most powerful things I think you can do to improve your coding. So one of the most useful things you can do is to print something. You see all these single letter things. They’re just shortcuts. But in a debugger you want to be able to do things quickly instead of typing print, just type p.&nbsp;So for example, let’s take a look at the shape of the input. So I type p for print input shape. So I’ve got a 50,000 by 50 input to the last layer that makes sense. These are the hidden activations coming in to the last layer for every one of our images. What about the output gradients? And there’s that as well. And actually a little trick you can ignore that you don’t have to use the p at all if your variable name is not the same as any of these commands. So I could have just typed out.g.shape, get the same thing. Okay. So you can also put in expressions so let’s have a look at the shape of this. So the output of this is let’s see if it makes sense. We’ve got the input 50,000 by 50. We put a new axis on the end unsqueeze minus one is the same as doing dot is indexing it with dot dot dot comma None sets but a new axis at the end. So that would have become And then the out.g.unsqueeze. We’re putting in the first dimension, so we’re going to have 50,000 by 50 by one times 50,000 by one by one. And so we’re going to end we’re going to end up getting this broadcasting happening over these last two dimensions, which is why we end up with 50,000 by 50 by one. And then with summing up, this makes sense, right? We want to sum up over all of the inputs, each image is individually contributing to the derivative. And so we want to add them all up to find their total impact. Because remember the sum of a bunch of the derivative of the sum of functions is the sum of the derivatives of the functions. So we can just some of them up. Now, this is one of these situations where if you see a times and a sum and unsqueeze, it’s not a bad idea to think about Einstein summation notation. Maybe there’s a way to simplify this. So first of all, let’s just see how we can do some more stuff in the debugger. I’m going to continue so just continue running.</p>
<p>So press c for continue and it keeps running until it comes back again to the same spot. And the reason we’ve come to the same spot twice is because Lin_grad is called two times. So we would expect that the second time we’re going to get a different bunch of inputs and outputs. And so I can print out a couple of the inputs and output gradient. So now, yes, So this is the first layer going into the second layer. So that’s exactly what we expect to find out What called this function, you just type w is where am I? And so you can see here, where am I? Oh, forward and backward was called. See the arrow that called lin_grad the second time and now we’re here in w.g equals. If we want to find out what actually ends up being equal to. I can press n for to say go to the next line and so now we’ve moved from line 5 to line 6, the instruction point is now looking at line six. So I could now print out, for example, w.g.shape, and that’s the shape our weights. One person on the chat has pointed out that you can use breakpoint instead of this import PDB business. Unfortunately, the breakpoint keyword doesn’t currently work in Jupyter or in IPython, so we actually can’t. Sadly. That’s why I’m doing it the old fashioned way. So this way maybe they’ll fix the bug at some point. But for now we have to type all this. Okay, so those are a few things to know about, but I would definitely suggest looking up a Python PDB tutorial to become very familiar with this incredibly powerful tool because it really is so very handy. So if I just press continue again, it keeps running all the way to the end and it’s now finished running forward and backward. https://realpython.com/python-debugging-pdb/</p>
<p>(Of course, <code>mse</code> is not a suitable loss function for multi-class classification; we’ll use a better loss function soon. We’ll use <code>mse</code> for now to keep things simple.)</p>
<div class="cell" data-outputid="16e6e3ef-16ab-4138-ef65-36d2e0f331c1" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">res.shape,y_valid.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>(torch.Size([10000, 1]), torch.Size([10000]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="c0f601a1-848c-460b-a575-1fde598ec791" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">(res<span class="op" style="color: #5E5E5E;">-</span>y_valid).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>torch.Size([10000, 10000])</code></pre>
</div>
</div>
<p>We need to get rid of that trailing (,1), in order to use <code>mse</code>.</p>
<div class="cell" data-outputid="e75a2bde-e949-4da2-b6e9-a1bbb2231d90" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">res[:,<span class="dv" style="color: #AD0000;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>torch.Size([10000])</code></pre>
</div>
</div>
<div class="cell" data-outputid="58019077-191b-4121-f22f-32f0baa6c5dc" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">res.squeeze().shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>torch.Size([10000])</code></pre>
</div>
</div>
<div class="cell" data-outputid="8035d5d8-d648-4b33-de86-715269e397dc" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">(res[:,<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">-</span>y_valid).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>torch.Size([10000])</code></pre>
</div>
</div>
<div class="cell" data-outputid="5989d8b4-ac0b-49f4-d26c-0c20c7b8c6c0" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">y_train,y_valid <span class="op" style="color: #5E5E5E;">=</span> y_train.<span class="bu" style="color: null;">float</span>(),y_valid.<span class="bu" style="color: null;">float</span>()</span>
<span id="cb29-2"></span>
<span id="cb29-3">preds <span class="op" style="color: #5E5E5E;">=</span> model(x_train)</span>
<span id="cb29-4">preds.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>torch.Size([50000, 1])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="kw" style="color: #003B4F;">def</span> mse(output, targ): <span class="cf" style="color: #003B4F;">return</span> (output[:,<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">-</span>targ).<span class="bu" style="color: null;">pow</span>(<span class="dv" style="color: #AD0000;">2</span>).mean()</span></code></pre></div>
</div>
<div class="cell" data-outputid="5a738bfd-7c5b-459f-9499-e5cc28df92d5" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">mse(preds, y_train)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(4308.76)</code></pre>
</div>
</div>
</section>
<section id="gradients-and-backward-pass" class="level3">
<h3 class="anchored" data-anchor-id="gradients-and-backward-pass">Gradients and backward pass</h3>
<div class="cell" data-outputid="7fa3bcee-7633-470b-aa8f-694402c70bec" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="im" style="color: #00769E;">from</span> sympy <span class="im" style="color: #00769E;">import</span> symbols,diff</span>
<span id="cb34-2">x,y <span class="op" style="color: #5E5E5E;">=</span> symbols(<span class="st" style="color: #20794D;">'x y'</span>)</span>
<span id="cb34-3">diff(x<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>, x)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>2*x</code></pre>
</div>
</div>
<div class="cell" data-outputid="aebb072f-4916-46b9-ebd8-f100ea65b2ea" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">diff(<span class="dv" style="color: #AD0000;">3</span><span class="op" style="color: #5E5E5E;">*</span>x<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">9</span>, x)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>6*x</code></pre>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="kw" style="color: #003B4F;">def</span> lin_grad(inp, out, w, b):</span>
<span id="cb38-2">    <span class="co" style="color: #5E5E5E;"># grad of matmul with respect to input</span></span>
<span id="cb38-3">    inp.g <span class="op" style="color: #5E5E5E;">=</span> out.g <span class="op" style="color: #5E5E5E;">@</span> w.t()</span>
<span id="cb38-4">    w.g <span class="op" style="color: #5E5E5E;">=</span> (inp.unsqueeze(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="op" style="color: #5E5E5E;">*</span> out.g.unsqueeze(<span class="dv" style="color: #AD0000;">1</span>)).<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb38-5">    b.g <span class="op" style="color: #5E5E5E;">=</span> out.g.<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="kw" style="color: #003B4F;">def</span> forward_and_backward(inp, targ):</span>
<span id="cb39-2">    <span class="co" style="color: #5E5E5E;"># forward pass:</span></span>
<span id="cb39-3">    l1 <span class="op" style="color: #5E5E5E;">=</span> lin(inp, w1, b1)</span>
<span id="cb39-4">    l2 <span class="op" style="color: #5E5E5E;">=</span> relu(l1)</span>
<span id="cb39-5">    out <span class="op" style="color: #5E5E5E;">=</span> lin(l2, w2, b2)</span>
<span id="cb39-6">    diff <span class="op" style="color: #5E5E5E;">=</span> out[:,<span class="dv" style="color: #AD0000;">0</span>]<span class="op" style="color: #5E5E5E;">-</span>targ</span>
<span id="cb39-7">    loss <span class="op" style="color: #5E5E5E;">=</span> diff.<span class="bu" style="color: null;">pow</span>(<span class="dv" style="color: #AD0000;">2</span>).mean()</span>
<span id="cb39-8">    </span>
<span id="cb39-9">    <span class="co" style="color: #5E5E5E;"># backward pass:</span></span>
<span id="cb39-10">    out.g <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">2.</span><span class="op" style="color: #5E5E5E;">*</span>diff[:,<span class="va" style="color: #111111;">None</span>] <span class="op" style="color: #5E5E5E;">/</span> inp.shape[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb39-11">    lin_grad(l2, out, w2, b2)</span>
<span id="cb39-12">    l1.g <span class="op" style="color: #5E5E5E;">=</span> (l1<span class="op" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">0</span>).<span class="bu" style="color: null;">float</span>() <span class="op" style="color: #5E5E5E;">*</span> l2.g</span>
<span id="cb39-13">    lin_grad(inp, l1, w1, b1)</span></code></pre></div>
</div>
<p>So when it’s finished, we would find that there will now be, for example, w1.g, because this is this is the gradients that it just calculated and there would also be a x_train.g and so forth. Okay, so let’s see if we can simplify this a little bit. So I would be inclined to take these out and give them their own variable names just to make life a bit easier. It would have been better if I’d actually done this before the debugging, so it be a bit easier to type. So let’s set i and o equal to input and output .g.unsqueeze. And actually let’s get rid of the that we just call it. So i will be input. All right. So we’ve got here actually now let’s put the end sequences back. I changed my mind. So. So just by. Oh, okay. So we’ll get rid of our breakpoint and double check that we’ve got our gradients. Okay. And I guess before we re run out, we should probably set those to zero. So what I would do here to try things out is I put my breakpoint there and then I would try things. So let’s go next. And so I realize here that what we’re actually doing is we’re basically doing exactly the same thing as einsum. Some would say. So I could test that out by trying an einsum. Right. Because I’ve just got this is being replicated. And then I’m summing over that dimension because that’s the multiplication that I’m doing. So I’m basically multiplying the first dimension of and then summing over that dimension. So I could try running that and oh, it works. So that’s interesting. I’d be easier if I just read this shaped bit long, isn’t it, dot shape. Okay, so it’s a 50 by one. Oh, and I’ve got zeros because I did exchange that zero. That was silly. Let’s try that again. That should be done. Gradient.zero. Okay, so let’s try doing an einsum. And there we go. That seems to be working. That’s pretty cool. So we’re we’ve multiply at this repeating index. So we were just multiplying the first dimensions together and then summing over them. So there’s no i here. Now, that’s not quite the same thing as a matrix multiplication, but we could turn it into the same thing as more metaphor application just by swapping in i,and j so that the other way around that way would have j, i comma , i k we can swap into dimensions very easily. That’s what’s called the transpose. So that would become a matrix multiplication if we just use the transpose. And in numpy the transpose is the capital T attribute. So here is exactly the same thing. Using a matrix multiply and a transpose. And let’s check. Yeah, that’s the same thing as well. Okay, cool. So that tells us that now we’ve checked in our debugger that we can actually replace all this with a matrix. Multiply. We don’t need that anymore. Let’s see if it works. That does. All right. x_trained.g. Okay. So hopefully that’s convinced you that the debugger is a really handy thing for playing around with numeric programing ideas or coding in general. And so I think now is a good time to take a break. So let’s take a eight minute break and I’ll see you back here after a seven minute break. I’ll see you back here in. 7 minutes. Thank you.</p>
<p>All right. I’m back. What have I missed? Hmm? Okay. Welcome back. So we’ve calculated our derivatives, and we want to test them. Luckily, PyTorch already has derivatives implemented, so I’ve got a totally cheat. And. And is PyTorch to calculate the same derivatives. So don’t worry about how this works yet, because I’m actually going to be doing all this from scratch anyway. For now, I’m just going to run it all through PyTorch and check that that derivatives are the same as ours. And they are. So we’re on the right track. Okay. So this is all pretty clunky. I think we can all agree and obviously it’s clunkier than what we do in PyTorch.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">forward_and_backward(x_train, y_train)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="co" style="color: #5E5E5E;"># Save for testing against later</span></span>
<span id="cb41-2"><span class="kw" style="color: #003B4F;">def</span> get_grad(x): <span class="cf" style="color: #003B4F;">return</span> x.g.clone()</span>
<span id="cb41-3">chks <span class="op" style="color: #5E5E5E;">=</span> w1,w2,b1,b2,x_train</span>
<span id="cb41-4">grads <span class="op" style="color: #5E5E5E;">=</span> w1g,w2g,b1g,b2g,ig <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">tuple</span>(<span class="bu" style="color: null;">map</span>(get_grad, chks))</span></code></pre></div>
</div>
<p>We cheat a little bit and use PyTorch autograd to check our results.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><span class="kw" style="color: #003B4F;">def</span> mkgrad(x): <span class="cf" style="color: #003B4F;">return</span> x.clone().requires_grad_(<span class="va" style="color: #111111;">True</span>)</span>
<span id="cb42-2">ptgrads <span class="op" style="color: #5E5E5E;">=</span> w12,w22,b12,b22,xt2 <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">tuple</span>(<span class="bu" style="color: null;">map</span>(mkgrad, chks))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;">def</span> forward(inp, targ):</span>
<span id="cb43-2">    l1 <span class="op" style="color: #5E5E5E;">=</span> lin(inp, w12, b12)</span>
<span id="cb43-3">    l2 <span class="op" style="color: #5E5E5E;">=</span> relu(l1)</span>
<span id="cb43-4">    out <span class="op" style="color: #5E5E5E;">=</span> lin(l2, w22, b22)</span>
<span id="cb43-5">    <span class="cf" style="color: #003B4F;">return</span> mse(out, targ)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">loss <span class="op" style="color: #5E5E5E;">=</span> forward(xt2, y_train)</span>
<span id="cb44-2">loss.backward()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="cf" style="color: #003B4F;">for</span> a,b <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(grads, ptgrads): test_close(a, b.grad, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span></code></pre></div>
</div>
<p>So how do we simplify things? That’s a really cool refactoring that we can do. So what we’re going to do is we’re going to create a whole class for each of our functions, for the regular function and for the linear function. So the the way that we’re going to do this is we’re going to create a dunder call. What is dunder call do let me show you. So if I create a class and we’re just going to set that to print, hello. So if I create an instance of that class and then I call it as if it was a function oops, missing the dunder bit here, call it as if it’s a function. It says hi. So in other words, you know, everything can be changed in Python. You can change how a class behaves, you can make it look like a function. And to do that you simply define dunder call your passing it an argument like so. Okay, so that’s what dunder call does. It just says it’s just a little bit of syntax, sugary kind of stuff to say. I want to be able to treat it as if it’s a function without any method at all. You can still do it the method way you could have done this. I don’t know why you’d want to, but you can. But because it’s got this special magic named dunder call, you don’t have to write that time to dunder call at all. So here, if we create an instance of the earlier class, we can treat it as a function. And what it’s going to do is it’s going to take its input and do the relu on it. But if you look back at the forward and backward, there’s something very interesting about the backward pass, which is that it has to know about, for example, this intermediate gets passed over here, this intermediate calculation gets passed over here because of the chain role. We’re going to need some of the intermediate calculations and not just because the chain rule, but because of actually how the derivatives are calculated. So we need to actually store each of the layer intermediate calculations.</p>
<p>And so that’s why value doesn’t just calculate and return the output, but it’s also stores its output and it also stores its input. So that way then when we call backward, we know how to calculate that we set the inputs gradient because remember we stored the input. So we can do that right and it’s going to just be oh input greater than zero dot float. Right? So that’s the definition. Okay. Of the derivative of a relu and then chain rule. So that’s how we can calculate the forward pass and the backward pass for relu. And we’re not going to have to then store all this intermediate stuff separately. It’s going to happen automatically so we can do the same thing for a linear layer. Now linear layer needs some additional state weights and biases. relu doesn’t, right? So there’s no in it. So when you create a linear layer, we have to say what are its weights, what are its biases? We store them away. And then when we call it when the forward pass, just like before we store the input. So that’s exactly the same line here. And just like before we calculate the output and store it and then return it. And this time of course we just call Lin and then for the backward pass, it’s the same thing. Okay, so the input gradients we calculate just like before, oh .t brackets is exactly the same with a little t as big T is as a property. So that’s the same thing. That’s just the transpose. Calculate the gradients of the weights again with a chain rule and the bias, just like we do that before and they’re all being stored in the appropriate places. And then for MSE, we can do the same thing. We don’t just calculate the MSE, but we also store it and also now the MSE. And it just needs two things an input and a target. So we’ll store those as well so that in the backward pass we can calculate its gradient of the input as being two times the difference. And there it all is. Okay, so our model now it’s much easier to define. We can just create a bunch of layers, linear one,, w1,b1 relu, linear ,w2,b2, and then we can store an instance of the MSE. So this is not calling MSE, it’s creating an instance of the MSE class, and this is an instance of the Lin class.</p>
<p>This is an instance of relu class. So that is being stored. So then when we call the model, we pass it, our inputs in our target. We go through each layer, set x equal to the result of calling that layer and then pass that to the loss. So there’s something kind of interesting here that you might have noticed, which is that Something interesting here is that we don’t have two separate functions inside and inside our model, the lost function being applied to a separate neural net. But we’ve actually integrated the lost function directly into the neural net, into the model, see how the loss is being calculated inside the model. Now, that’s neither better nor worse than having it separately. It’s just different. And generally a lot of hugging faced stuff does it this way. They actually put the loss inside the forward Most stuff in FastAI and a lot of other libraries does it separately, which is the loss is a whole separate function and the model only returns the result of putting it through the layers. So for this model we’re going to actually do the loss function inside model. So for backward, we just do each thing so self.loss.backward . So that self taught losses the MSE object. So that’s going to call backward, right? And it’s stored when it was called here it was storing remember the inputs, the targets, the outputs so it can calculate the backward and then we go through each layer is in reverse. Right. This is back propagation backwards, reversed, calling backward on each one. So that’s pretty interesting. I think. So now we can calculate the model, we can calculate the loss we can call backward, and then we can check that each of the gradients that we stored earlier equal to each of our new gradients</p>
<p>Okay. So Williams asked a very good question that if you do puts put the loss inside here, how on earth do you actually get predictions? So generally what happens is in practice, huggingface models do something like that. So say self.preds equals x and then they’ll say self.final_loss = self.loss(x,targ) that and then return self.final_loss. And that way I guess you don’t even need that last bit. Well anyway, this is what they do so I’ll leave it there. And so that way you can kind of check like model.preds for example. So it’ll be something like that. Or alternatively, you can return not just the loss, but both as a dictionary, stuff like that. So a few different ways you could do it actually. Now think about it. I think that’s what they do is they actually return both as a dictionary. So it would be, it’d be like return dictionary loss equals that , preds = x .Something like that, I guess is what they would do. Anyway. There’s a few different ways to do it. Okay. So hopefully you can see that this is really making it nice and easy for us to do our forward pass and our backward paths. Without all of this manual fiddling around. Every class now can be totally separately considered and can be combined. However want. We could create layers so you could try creating a bigger neural net if you want to, but we can refactor it more. So basically as a rule of thumb, when you see repeated like self.inp = inp…. That’s a sign can refactor things. And so what we can do is a simple refactoring this to create a new class called module and modules gonna do those things. You said it’s going to store the inputs and it’s going to call something called self.forward in order to create our self.out because remember, that was one of the things we had again and again and again, self.out, return it. And so now that’s going to be a thing called forward which actually in this it doesn’t do anything because the whole purpose of this module is to be inherited. When we call backward, it’s going to call self.backward passing in self.out because notice all of our backwards always wanted to get hold of self.out because we need it for the chain role. So let’s pass that in and pass in those arguments that we start earlier.</p>
</section>
</section>
<section id="refactor-model" class="level2">
<h2 class="anchored" data-anchor-id="refactor-model">Refactor model</h2>
<section id="layers-as-classes" class="level3">
<h3 class="anchored" data-anchor-id="layers-as-classes">Layers as classes</h3>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="kw" style="color: #003B4F;">class</span> Relu():</span>
<span id="cb46-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, inp):</span>
<span id="cb46-3">        <span class="va" style="color: #111111;">self</span>.inp <span class="op" style="color: #5E5E5E;">=</span> inp</span>
<span id="cb46-4">        <span class="va" style="color: #111111;">self</span>.out <span class="op" style="color: #5E5E5E;">=</span> inp.clamp_min(<span class="fl" style="color: #AD0000;">0.</span>)</span>
<span id="cb46-5">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.out</span>
<span id="cb46-6">    </span>
<span id="cb46-7">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.inp.g <span class="op" style="color: #5E5E5E;">=</span> (<span class="va" style="color: #111111;">self</span>.inp<span class="op" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">0</span>).<span class="bu" style="color: null;">float</span>() <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.out.g</span></code></pre></div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="kw" style="color: #003B4F;">class</span> Lin():</span>
<span id="cb47-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, w, b): <span class="va" style="color: #111111;">self</span>.w,<span class="va" style="color: #111111;">self</span>.b <span class="op" style="color: #5E5E5E;">=</span> w,b</span>
<span id="cb47-3"></span>
<span id="cb47-4">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, inp):</span>
<span id="cb47-5">        <span class="va" style="color: #111111;">self</span>.inp <span class="op" style="color: #5E5E5E;">=</span> inp</span>
<span id="cb47-6">        <span class="va" style="color: #111111;">self</span>.out <span class="op" style="color: #5E5E5E;">=</span> lin(inp, <span class="va" style="color: #111111;">self</span>.w, <span class="va" style="color: #111111;">self</span>.b)</span>
<span id="cb47-7">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.out</span>
<span id="cb47-8"></span>
<span id="cb47-9">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb47-10">        <span class="va" style="color: #111111;">self</span>.inp.g <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.out.g <span class="op" style="color: #5E5E5E;">@</span> <span class="va" style="color: #111111;">self</span>.w.t()</span>
<span id="cb47-11">        <span class="va" style="color: #111111;">self</span>.w.g <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.inp.t() <span class="op" style="color: #5E5E5E;">@</span> <span class="va" style="color: #111111;">self</span>.out.g</span>
<span id="cb47-12">        <span class="va" style="color: #111111;">self</span>.b.g <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.out.g.<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;">class</span> Mse():</span>
<span id="cb48-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, inp, targ):</span>
<span id="cb48-3">        <span class="va" style="color: #111111;">self</span>.inp,<span class="va" style="color: #111111;">self</span>.targ <span class="op" style="color: #5E5E5E;">=</span> inp,targ</span>
<span id="cb48-4">        <span class="va" style="color: #111111;">self</span>.out <span class="op" style="color: #5E5E5E;">=</span> mse(inp, targ)</span>
<span id="cb48-5">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.out</span>
<span id="cb48-6">    </span>
<span id="cb48-7">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb48-8">        <span class="va" style="color: #111111;">self</span>.inp.g <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">2.</span> <span class="op" style="color: #5E5E5E;">*</span> (<span class="va" style="color: #111111;">self</span>.inp.squeeze() <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.targ).unsqueeze(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="op" style="color: #5E5E5E;">/</span> <span class="va" style="color: #111111;">self</span>.targ.shape[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="kw" style="color: #003B4F;">class</span> Model():</span>
<span id="cb49-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, w1, b1, w2, b2):</span>
<span id="cb49-3">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> [Lin(w1,b1), Relu(), Lin(w2,b2)]</span>
<span id="cb49-4">        <span class="va" style="color: #111111;">self</span>.loss <span class="op" style="color: #5E5E5E;">=</span> Mse()</span>
<span id="cb49-5">        </span>
<span id="cb49-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x, targ):</span>
<span id="cb49-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb49-8">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.loss(x, targ)</span>
<span id="cb49-9">    </span>
<span id="cb49-10">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb49-11">        <span class="va" style="color: #111111;">self</span>.loss.backward()</span>
<span id="cb49-12">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">reversed</span>(<span class="va" style="color: #111111;">self</span>.layers): l.backward()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(w1, b1, w2, b2)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">loss <span class="op" style="color: #5E5E5E;">=</span> model(x_train, y_train)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">model.backward()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">test_close(w2g, w2.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb53-2">test_close(b2g, b2.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb53-3">test_close(w1g, w1.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb53-4">test_close(b1g, b1.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb53-5">test_close(ig, x_train.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span></code></pre></div>
</div>
</section>
<section id="module.forward" class="level3">
<h3 class="anchored" data-anchor-id="module.forward">Module.forward()</h3>
<p>And so *args means take all of the arguments regardless whether it’s zero one, two or more, and put them into a list. And then that’s what happens when it’s inside the actual signature. And then when you call a function using star, it says take this list and expand them into separate calling backward with each one separately. So now for relu, you look how much simpler it is. Let’s copy the old relu and the new relu. So the old relu you had to do all this storing stuff manually and it had a little self.stuff as well. But now we can get rid of all of that and just implement forward because that’s the thing that’s been called and that’s the thing that we need to implement. And so now the forward is relu just as the one thing we want, which also makes the code much cleaner and more understandable. Ditto for backward. It just does. The one thing we want. So that’s nice. Now we still have to multiply it by two after the chain rule manually, but same thing for linear, same thing for MSE. So these all look a lot nicer. And one thing to point out here is that there’s often opportunities to manually speed things up when you create custom order grad functions in PyTorch. And here’s an example. Look, this calculation is being done twice. It seems like a waste, doesn’t it so at the cost of some memory, we could instead store that calculation as diff. Right. And I guess we’d have to store it fur use it later. So I don’t need to be self.diff if and the cost of that memory, we could now remove this redundant calculation because we’ve done it once before already and stored it and just use it directly. And this is something that you can often do in neural nets. So there’s this compromise between storing things, the memory use of that and then the computational speed up of not having to recalculate it. This is something we come across a lot. And so now we can call it in the same way, create a model passing in all of those layers. So you can see with our model, we’re just so the model hasn’t changed at this point. The definition was up here. We just passed in The weights for the layers calculate the loss called backward and look. It’s the same right? Okay. So thankfully, PyTorch has written all this for us. And remember, according to rules of our game, once we’ve reimplemented it, we’re allowed to use PyTorch as version. So PyTorch calls their version and nn.Module. And so it’s exactly the same you inherit from an nn.Module. So if we want to create a linear layer just like this one, rather than inheriting our module, we will inherit from that module, but everything’s exactly the same. So we create our, we can create our random numbers. So in this case, rather than passing in the already randomized weights, we’re actually going to generate the random weights ourselves and the zeroed biases and. Then here’s our linear layer, which you could also use Lin for that.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="kw" style="color: #003B4F;">class</span> Module():</span>
<span id="cb54-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>args):</span>
<span id="cb54-3">        <span class="va" style="color: #111111;">self</span>.args <span class="op" style="color: #5E5E5E;">=</span> args</span>
<span id="cb54-4">        <span class="va" style="color: #111111;">self</span>.out <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(<span class="op" style="color: #5E5E5E;">*</span>args)</span>
<span id="cb54-5">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.out</span>
<span id="cb54-6"></span>
<span id="cb54-7">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">Exception</span>(<span class="st" style="color: #20794D;">'not implemented'</span>)</span>
<span id="cb54-8">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>): <span class="va" style="color: #111111;">self</span>.bwd(<span class="va" style="color: #111111;">self</span>.out, <span class="op" style="color: #5E5E5E;">*</span><span class="va" style="color: #111111;">self</span>.args)</span>
<span id="cb54-9">    <span class="kw" style="color: #003B4F;">def</span> bwd(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">Exception</span>(<span class="st" style="color: #20794D;">'not implemented'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="kw" style="color: #003B4F;">class</span> Relu(Module):</span>
<span id="cb55-2">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, inp): <span class="cf" style="color: #003B4F;">return</span> inp.clamp_min(<span class="fl" style="color: #AD0000;">0.</span>)</span>
<span id="cb55-3">    <span class="kw" style="color: #003B4F;">def</span> bwd(<span class="va" style="color: #111111;">self</span>, out, inp): inp.g <span class="op" style="color: #5E5E5E;">=</span> (inp<span class="op" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">0</span>).<span class="bu" style="color: null;">float</span>() <span class="op" style="color: #5E5E5E;">*</span> out.g</span></code></pre></div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><span class="kw" style="color: #003B4F;">class</span> Lin(Module):</span>
<span id="cb56-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, w, b): <span class="va" style="color: #111111;">self</span>.w,<span class="va" style="color: #111111;">self</span>.b <span class="op" style="color: #5E5E5E;">=</span> w,b</span>
<span id="cb56-3">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, inp): <span class="cf" style="color: #003B4F;">return</span> inp<span class="op" style="color: #5E5E5E;">@</span>self.w <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.b</span>
<span id="cb56-4">    <span class="kw" style="color: #003B4F;">def</span> bwd(<span class="va" style="color: #111111;">self</span>, out, inp):</span>
<span id="cb56-5">        inp.g <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.out.g <span class="op" style="color: #5E5E5E;">@</span> <span class="va" style="color: #111111;">self</span>.w.t()</span>
<span id="cb56-6">        <span class="va" style="color: #111111;">self</span>.w.g <span class="op" style="color: #5E5E5E;">=</span> inp.t() <span class="op" style="color: #5E5E5E;">@</span> <span class="va" style="color: #111111;">self</span>.out.g</span>
<span id="cb56-7">        <span class="va" style="color: #111111;">self</span>.b.g <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.out.g.<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="kw" style="color: #003B4F;">class</span> Mse(Module):</span>
<span id="cb57-2">    <span class="kw" style="color: #003B4F;">def</span> forward (<span class="va" style="color: #111111;">self</span>, inp, targ): <span class="cf" style="color: #003B4F;">return</span> (inp.squeeze() <span class="op" style="color: #5E5E5E;">-</span> targ).<span class="bu" style="color: null;">pow</span>(<span class="dv" style="color: #AD0000;">2</span>).mean()</span>
<span id="cb57-3">    <span class="kw" style="color: #003B4F;">def</span> bwd(<span class="va" style="color: #111111;">self</span>, out, inp, targ): inp.g <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>(inp.squeeze()<span class="op" style="color: #5E5E5E;">-</span>targ).unsqueeze(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="op" style="color: #5E5E5E;">/</span> targ.shape[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(w1, b1, w2, b2)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">loss <span class="op" style="color: #5E5E5E;">=</span> model(x_train, y_train)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">model.backward()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">test_close(w2g, w2.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb61-2">test_close(b2g, b2.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb61-3">test_close(w1g, w1.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb61-4">test_close(b1g, b1.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb61-5">test_close(ig, x_train.g, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span></code></pre></div>
</div>
<p>And of course,</p>
<p>so define our forward. And why don’t we need to define backward? Because PyTorch already knows the derivatives of all of the functions in PyTorch and it knows how to use the chain rule so we don’t have to do backward at all. It will actually do that entirely for us, which is very cool. So we only need forward. We don’t need backward.</p>
<p>So let’s create a model that uses that nn.Moddule. Otherwise it’s exactly the same as before. And now we’re going to use PyTorch as MSE loss because we’ve already implemented ourselves. It’s very common to use torch.nn.functional as capital P. This is where lots of these handy functions live, including MSE loss. And so now you know why we need the colon comma None because you saw the problem if we don’t have it. And so the model call backward. And remember, we stored our gradients in something called (.g) PyTorch stores them in something called (.grad), but it’s doing exactly the same thing. So there is the exact same values. All right. So let’s see if there’s any questions. Not yet. Okay. All right. If anybody in the class has any questions or comments about any of this, let me know. Remember to upvote questions that you’re interested in.</p>
<p>So we’ve we’ve created a matrix multiplication from scratch.</p>
<p>We’ve created linear layers,</p>
<p>we’ve created a complete backprop system of modules.</p>
<p>We can now calculate both the forward pass and the backward pass for linear layers and relus so we can create a multi-layer perceptron.</p>
<p>So we’re now up to a point where we can train a model.</p>
<p>So let’s do that mini batch training Notebook number 4.</p>
</section>
<section id="autograd" class="level3">
<h3 class="anchored" data-anchor-id="autograd">Autograd</h3>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn</span>
<span id="cb62-2"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span></code></pre></div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="kw" style="color: #003B4F;">class</span> Linear(nn.Module):</span>
<span id="cb63-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, n_out):</span>
<span id="cb63-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb63-4">        <span class="va" style="color: #111111;">self</span>.w <span class="op" style="color: #5E5E5E;">=</span> torch.randn(n_in,n_out).requires_grad_()</span>
<span id="cb63-5">        <span class="va" style="color: #111111;">self</span>.b <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(n_out).requires_grad_()</span>
<span id="cb63-6">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, inp): <span class="cf" style="color: #003B4F;">return</span> inp<span class="op" style="color: #5E5E5E;">@</span>self.w <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.b</span></code></pre></div>
</div>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><span class="kw" style="color: #003B4F;">class</span> Model(nn.Module):</span>
<span id="cb64-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_in, nh, n_out):</span>
<span id="cb64-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb64-4">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]</span>
<span id="cb64-5">        </span>
<span id="cb64-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__call__</span>(<span class="va" style="color: #111111;">self</span>, x, targ):</span>
<span id="cb64-7">        <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.layers: x <span class="op" style="color: #5E5E5E;">=</span> l(x)</span>
<span id="cb64-8">        <span class="cf" style="color: #003B4F;">return</span> F.mse_loss(x, targ[:,<span class="va" style="color: #111111;">None</span>])</span></code></pre></div>
</div>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">model <span class="op" style="color: #5E5E5E;">=</span> Model(m, nh, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb65-2">loss <span class="op" style="color: #5E5E5E;">=</span> model(x_train, y_train)</span>
<span id="cb65-3">loss.backward()</span></code></pre></div>
</div>
<div class="cell" data-outputid="72c85aac-46f1-424f-eacc-1020ae1fb25a" data-execution_count="52">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">l0 <span class="op" style="color: #5E5E5E;">=</span> model.layers[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb66-2">l0.b.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,
          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,
        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,
        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])</code></pre>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 5/index.html</guid>
  <pubDate>Wed, 22 Mar 2023 19:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 4</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index.html</link>
  <description><![CDATA[ 



<p>What you should know and practice after this lecture:<br> 1- Easily plot matrix that is not easily plottable <br> 2- Broadcasting roles <br> 3- Creating sample data <br> 4- meanshift algorithm <br> 5- You can use peresentify to draw on screen <br> 6- Animation</p>
<section id="clustering" class="level1">
<h1>Clustering</h1>
<p>So let’s practice that. That’s practice what we’ve learned. So we’re going to go to zero two meanshift to practice this. And so we’re going to try to exercise our kind of tensor manipulation operation muscles in this section. And the key actually endpoint for this is the homework. And so what you need to be doing is getting yourself to a point that you could implement something like this, but for a different algorithm, why do we care about this? Because this is like learning your times table, your times tables. If you’re doing, you know, mathematics, it’s this kind of like thing that’s going to come up all the time. And if you’re not good at your times, tables, everything else, a lot more, a lot of other things, particularly at primary school and high school, you know, they they get difficult, you get slower and it’s frustrating and you spend time thinking about these mechanical operations rather than getting your work done. It is it’s important that when you have an idea about something you want to try or debug or profile or whatever that you can quickly translate that into working code. And the way that code is written for GPUs or even for fast running on on CPU’s is using broadcasting, Einstein notation ,metrics multiplications and supper important.</p>
<p>So you’ve got to, you’ve got to go to practice super important. So we’re going to practice it by running, by developing a clustering algorithm. And the clustering algorithm we’re going to work on is something called meanshift clustering, which hopefully you’ve never heard of before. And I say that because I just think it’s a really funny algorithm that not many people have come across. Excuse me, and I think you’ll find it really useful. So what is cluster analysis? Cluster analysis is very different to anything that we’ve worked on in this course so far and that there isn’t a dependent variable that we’re trying to match, but instead we’re just trying to find are there groups of similar things in this data? And those groups we call clusters? And as you can see from the wiki page, there’s all kinds of applications of cluster analysis across many different areas. I will say that sometimes cluster analysis can be overused or misused. It’s really best for when your your various columns are the same kind of thing and have the same kind of scale. For example, pixels are all the same kind of thing. They’re all pixels. So one of the examples they use is market research. So I wouldn’t cluster analysis for sociodemographic inputs because they’re all different kinds of things. But the example they give here makes a lot of sense, which is looking at data from surveys. So if you’ve got a whole bunch of like from 1 to 5 answers on surveys. All right. So let’s take a look at this. And the way I like to build my algorithms is to create some often to create some synthetic data that I know how I want it to behave.</p>
<p>And so we’re going to create six clusters that each cluster is going to have 750 samples in it. So first of all, I’m going to randomly six centroid. And so the centroid is going to be like the middle of where my clusters are. So I’m going to randomly create them. I need to (n_clusters,2). So I need an X and Y coordinate for each one. And so now I’m going to randomly generate data around those six centroid. Okay. So to do that, I’m going to call a little function I made here called Sample, and I’m going to run it on each of those six centroid and show you what that looks like. So here’s what that data looks like. So the X’s, the six centroid and the colored dots is the data. So if you were given this data without the X’s, the idea would be to come out, come back with figuring out where the X’s would have been, like where are the where are these clustering around? And so if you can get clusters like that, that’s the goal here, is to find out that there’s a fewer discretely, distinctly different types of data in your data set. So for example, for images, I’ve used this before to discover that there are some images that look completely different to all the other ones. For example, they were taken at night time or they’re of a different object or something like that.</p>
<p>Clustering techniques are unsupervised learning algorithms that try to group unlabelled data into “clusters”, using the (typically spatial) structure of the data itself. It has many <a href="https://en.wikipedia.org/wiki/Cluster_analysis#Applications">applications</a>.</p>
<p>The easiest way to demonstrate how clustering works is to simply generate some data and show them in action. We’ll start off by importing the libraries we’ll be using today.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> math, matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt, operator, torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> functools <span class="im" style="color: #00769E;">import</span> partial</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb2-2">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<section id="create-data" class="level2">
<h2 class="anchored" data-anchor-id="create-data">Create data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">n_clusters<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span></span>
<span id="cb3-2">n_samples <span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">250</span></span></code></pre></div>
</div>
<p>To generate our data, we’re going to pick 6 random points, which we’ll call centroids, and for each point we’re going to generate 250 random points about it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">centroids <span class="op" style="color: #5E5E5E;">=</span> torch.rand(n_clusters, <span class="dv" style="color: #AD0000;">2</span>)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">70</span><span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">35</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;">from</span> torch.distributions.multivariate_normal <span class="im" style="color: #00769E;">import</span> MultivariateNormal</span>
<span id="cb5-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor</span></code></pre></div>
</div>
<p>So how does sample work? Well, we’re passing in the centroid, and so what we want is we’re going to get back. So each of those centroid contains an X in a Y. So multivariate normal is just like normal. It’s going to give you back normally distributed data, but more than one item. That’s why it’s multivariate. And so we passed in two means a main for X and a mean for our Y. And so that’s the mean that we’re going to get. And our standard deviation is going to be five. Why do we use torch.diag(tensor([5.,5.])))? That’s because we’re saying that because that for multivariate normal distributions, there’s not just one standard deviation. Each column that you get back, there could also be a connection between columns. The columns might not be independent. So you actually need so it’s called a covariance matrix, not just to make, not just a variance. We discussed that a little bit more in lesson 9B if you’re interested in learning more about that. Okay, So this is something that’s going to give us back random columns of data with this mean and this standard deviation.</p>
<p>And this is the number of samples that we want and this is coming from PyTorch. So PyTorch has a whole bunch of different distributions that you can use, which can be very handy. So there is our data. Okay. So remember, for sample clustering, we we don’t know the different colors and we don’t know where the X is. That’s kind of our job is to figure that out. We might just briefly also look at how to plot. So in this case, we want to plot the X s and we want to plot the data so it looks like this. So what I do is I look through each centroid and I grab that centroid samples and they’re just all done in order. So I grab it from i<em>n_samples: to (i+1)</em>n_samples, and then I create a scatterplot with the samples on them. And what I’ve done is I’ve created an axis here and you’ll see y later that we can also pass one in. But I’m not passing one it. And so we create a plot and an axis. And so in that matplotlib, you can keep plotting things on the same axis. So then I plot on the centroid a big x, which is black, and then a smaller x, which is what is that magenta? And so that’s how I get these X’s. So that’s how plot data works. Okay, so how do we create something now that starts with all the dots and returns where the X is are ? We’re going to use a particular algorithm, particular clustering algorithm called meanshift.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> sample(m): <span class="cf" style="color: #003B4F;">return</span> MultivariateNormal(m, torch.diag(tensor([<span class="fl" style="color: #AD0000;">5.</span>,<span class="fl" style="color: #AD0000;">5.</span>]))).sample((n_samples,))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">slices <span class="op" style="color: #5E5E5E;">=</span> [sample(c) <span class="cf" style="color: #003B4F;">for</span> c <span class="kw" style="color: #003B4F;">in</span> centroids]</span>
<span id="cb7-2">data <span class="op" style="color: #5E5E5E;">=</span> torch.cat(slices)</span>
<span id="cb7-3">data.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1500, 2])</code></pre>
</div>
</div>
<p>Below we can see each centroid marked w/ X, and the coloring associated to each respective cluster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">def</span> plot_data(centroids, data, n_samples, ax<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb9-2">    <span class="cf" style="color: #003B4F;">if</span> ax <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: _,ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots()</span>
<span id="cb9-3">    <span class="cf" style="color: #003B4F;">for</span> i, centroid <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(centroids):</span>
<span id="cb9-4">        samples <span class="op" style="color: #5E5E5E;">=</span> data[i<span class="op" style="color: #5E5E5E;">*</span>n_samples:(i<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>)<span class="op" style="color: #5E5E5E;">*</span>n_samples]</span>
<span id="cb9-5">        ax.scatter(samples[:,<span class="dv" style="color: #AD0000;">0</span>], samples[:,<span class="dv" style="color: #AD0000;">1</span>], s<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb9-6">        ax.plot(<span class="op" style="color: #5E5E5E;">*</span>centroid, markersize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>, marker<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"x"</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'k'</span>, mew<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb9-7">        ax.plot(<span class="op" style="color: #5E5E5E;">*</span>centroid, markersize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, marker<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"x"</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'m'</span>, mew<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">plot_data(centroids, data, n_samples)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="mean-shift" class="level2">
<h2 class="anchored" data-anchor-id="mean-shift">Mean shift</h2>
<p>Most people that have come across clustering algorithms have learnt about <strong>k-means</strong>. Mean shift clustering is a newer and less well-known approach, but it has some important advantages: * It doesn’t require selecting the number of clusters in advance, but instead just requires a <strong>bandwidth</strong> to be specified, which can be easily chosen automatically * It can handle clusters of any shape, whereas k-means (without using special extensions) requires that clusters be roughly ball shaped.</p>
<p>The algorithm is as follows: * For each data point x in the sample X, find the distance between that point x and every other point in X * Create weights for each point in X by using the <strong>Gaussian kernel</strong> of that point’s distance to x * This weighting approach penalizes points further away from x * The rate at which the weights fall to zero is determined by the <strong>bandwidth</strong>, which is the standard deviation of the Gaussian * Update x as the weighted average of all other points in X, weighted based on the previous step</p>
<p>This will iteratively push points that are close together even closer until they are next to each other.</p>
<p>And and meanshift is a nice clustering approach because you don’t have to say how many clusters there are. So it’s not that often that you’re actually going to know how many clusters there are. So we don’t have to say quite a few things, like the very popular K means required to say how many in step. You just have to pass them in quite a bandwidth, which we’ll learn about, which can actually be chosen automatically. And it can also handle clusters of any shape so they don’t have to be bold shaped like that. But they are here. They can be kind of like L-shaped or ellipse shaped or whatever. And so here’s what’s going to happen. We’re going to pick some point. So let’s say we pick that point just there. Okay? And so what we now do is we go through each data point, so we pick the first one, and so we then find the distance between that point and every other point. Okay. So we’re going to have to say what is the distance between that point and that point? And point and that point and that point and that point and also the ones further away, that point and that point. And you do it for every single point compared to the one that we’re currently looking at. Okay. So we get all of those as a big list. And now what we’re going to do is we’re going to take a weighted average of all of those points. Now That’s not interesting without the weighting. If we just take our average of all of the points and how far away they are, we’re going to end up somewhere here, right? This is the average of all the points. But the key is that we’re going to take an average and find the right spot. The key is we need to find an average that is weighted by how far away things are.</p>
<p>So, for example, this one over here is a very long way away from our point of interest. And so it should have a very low weight and the weighted average where else this point here, which is very close, should have a very high weight in our weighted average. So What we do is we create weights for every point compared to the one that we’re currently interested in using a what’s called a Gaussian kernel that we’ll look at. But the key thing to know is that points that are further away from our point of interest, which is this one, are going to have lower weights. That’s what we mean, that they’re penalized. The rate at which weights for a zero is determined by this thing that we set at the start called the bandwidth. And that’s going to be the standard deviation of our Gaussian. So we take an average of all the points in the dataset, a weighted average weighted by how far away they are. So for our point of interest, right, the this point is going to get a big weight. This point is going to get a big weight. This point is going to get a big weight. That point is going to get a tiny weight.</p>
<p>That point is going to get an even tiny weight. So it’s mainly going to be a weighted average of these points at a nearby. And the weighted average of those points, I would guess, is going to be somewhere around about here. Right. And would have a similar thing for the weighted average of the points near this one. That’s going to probably be somewhere around about here or maybe over here. And so it’s going to move all of these points in closer. It’s almost like a gravity right. They’re kind of going to be moved like closer and closer in towards this kind of gravitational center. And then these ones will go towards their own gravitational center and so forth. Okay.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">midp <span class="op" style="color: #5E5E5E;">=</span> data.mean(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb11-2">midp</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ 9.222, 11.604])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">plot_data([midp]<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">6</span>, data, n_samples)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So let’s take a look at it. All right. So what’s the gaussian kernel? This is the gaussian kernel, which was a sign in the original March for science back in the days when the idea of not following scientists was considered socially unacceptable. We used to have a March for these things, if you remember. So this is this is not normal. So this is the definition of the gaussian kernel, which is also known as the normal distribution. This is the shape of it. So you’ve seen it before. And here is that formula copied directly off the science match sign. Okay, here we see the square root, two pi, etc..</p>
<p>So here’s the definition of the gaussian kernel, which you may remember from high school… This person at the science march certainly remembered!</p>
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/http:/i.imgur.com/nijQLHw.jpg" width="400"></p>
<p>Okay. And bw is the standard deviation. Now what does that look like? It’s very helpful to have something that we can very quickly plot any function that doesn’t come with matplotlib , but it’s very easy to write one. Just say, oh, let’s as X, let’s use all the numbers from 0 to 10, a hundred of them spaced evenly. That’s what linspace Does. it linearly spaced 100 numbers in this range. That’s going to be our Xs. So plot those Xs and plot F of X is the Ys. So here’s a very nice little plot_func, we want. And here it is. And as you can see here, we’ve now got something where if you are this like very close to the point of interest, you’re going to get a very high weight. And if you’re a long way away from the point of interest, you’ll get a very low weight. So that’s the key thing that we wanted to remember is something that penalizes further away points more. Now, you’ll notice here I’ve managed to plot this function for a bandwidth of 2.5, and the way I did that was using this special thing from functools(functools.partial), . Now, the first thing to point out here is that very often drives me crazy. I see people trying to find out what something is in Jupiter, and the way they do it is they’ll scroll up to the top of the notebook and search through the imports and try to find it. That is the dumb way to do it. The smart way to do it is just to type it and press shift enter and it’ll tell you where it comes from and you can get its help with Question Mark and you can get it also source code with two question marks. Okay, So just type it to find out where it comes from. Okay. So this is as Sylver mentioned in the chat, also known as carrying or partial function application. This creates a new function. So let’s just grab it. We create a new function. And this function F is is the function Gaussian, but it’s going to automatically pass. BW equals 2.5. So this is a partially applied function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> gaussian(d, bw): <span class="cf" style="color: #003B4F;">return</span> torch.exp(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.5</span><span class="op" style="color: #5E5E5E;">*</span>((d<span class="op" style="color: #5E5E5E;">/</span>bw))<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>) <span class="op" style="color: #5E5E5E;">/</span> (bw<span class="op" style="color: #5E5E5E;">*</span>math.sqrt(<span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>math.pi))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;">def</span> plot_func(f):</span>
<span id="cb15-2">    x <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb15-3">    plt.plot(x, f(x))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">plot_func(partial(gaussian, bw<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So I could type f of four f(tensor(4.0)), for example, that’s going to be a tensor. There we go. And you can see that’s exactly what this is got to for across. Yep, about .44. So we use partial function application all the time. It’s a very, very, very important tool. Without it, for example, plotting this function would have been more complicated with it. It was trivially easy. I guess the alternative, like one alternative which would be fine but slightly more clunky, would be we could create a little function in line so we could have said, Oh, plot a function. Then I’m going to define right now, which is called lamb, which is lambda X, which is Gaussian and of X with a bandwidth of 2.5. You could do that too. You know, it’s it’s fine, but, but yeah, partials I think are a bit neater, a bit less to think about.</p>
<p>They often produce some nature and clearer code. Okay. Why did we decide to make the bandwidth 2.5 as a as a rule of thumb, choose a bandwidth which covers about a third of the data. So if we kind of found ourselves somewhere over here, write a bandwith which covers about a third of the data would be enough to cover two clusters ish. So it would be kind of like this big. So somewhere in the middle there. So that’s the basic idea. Yeah. So but you can play around with bandwidth and get different amounts of clusters. I should mention, like often when you see something that’s kind of on the complicated side, like a Gaussian, you can often simplify things. I think most implementations and write ups I’ve seen talk about using Gaussians, but if you look at the shape of it, it looks a lot like this shape. So this is a triangular weighting which is just using clamp_min So it’s just using a linear with clamp_min And yeah, it occurred to me that we could probably use this just as well. So I did find it.</p>
<p>I decided to define this triangular weighting and then we can try both anyway. So I will start with we’re going to use the Gaussian version. All right. So we’re going to be move literally moving all the points towards the kind of center of gravity.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">f <span class="op" style="color: #5E5E5E;">=</span> partial(gaussian,bw<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.5</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">f(tensor(<span class="fl" style="color: #AD0000;">4.0</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.044)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">partial</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>functools.partial</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">plot_func(<span class="kw" style="color: #003B4F;">lambda</span> x : gaussian(x,bw<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In our implementation, we choose the bandwidth to be 2.5.</p>
<p>One easy way to choose bandwidth is to find which bandwidth covers one third of the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;">def</span> tri(d, i): <span class="cf" style="color: #003B4F;">return</span> (<span class="op" style="color: #5E5E5E;">-</span>d<span class="op" style="color: #5E5E5E;">+</span>i).clamp_min(<span class="dv" style="color: #AD0000;">0</span>)<span class="op" style="color: #5E5E5E;">/</span>i</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">plot_func(partial(tri, i<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So we don’t want to mess up our original data so we clone it. It’s a PyTorch thing is .clone (data.clone()), it’s very handy. And so Big X is our matrix of data. I mean, it’s actually a That’s right. Matrix of data. Yeah. And then little x will be our first point. And it’s pretty common to use big X a capital letters for matrices. So this is our data. This is the first point.</p>
<p>Okay. So there it is. We’re going to start at 26.2, 26.3. So 26.2, 26.3. So somewhere up here, so little x, its shape is just it’s a rank one tensor of shape two. Big X is a rank two tensor of 1500 data points by two, the X and Y. And if we call x[None], that would add a unit axis to that. And the reason I’m going to show you that is because we want to find the distance from little x to everything in Big X and the way we do a distance is with minus, but you wouldn’t be able to go, you wouldn’t be able to go X minus big X and get the right actually to you get the right answer. Let’s think about that X shape. Oh, we’ve got that already. I know actually that is going to work isn’t it? So, yes. All right. So you can see why we’ve got these two versions here. If we do x[None], we’ve got something of shape. One comma, two. Now we can subtract that from something, a shape 1500 comma two, because the twos match up because they’re the same and the 1500 and the one matches up because we remember our Numpy roles, everything matches up to a unit axis. So it’s going to copy this matrix across every of this matrix and it works.</p>
<p>But you remember there’s a special trick which is if you’ve got two shapes of different lengths, we can use the shorter length and it’s going to add unit axes to the front to make it as long as necessary. So we actually don’t need the x[None]. We can just use little x and it works because it’s going to say, is this compatible with this? Well, the last axis, remember we go right to left the last axis matches the second last axis, Oh, it doesn’t exist. So we pretend that there’s a unit axis, and so it’s going to do exactly the same thing as this. So if you have not studied the broadcasting from last week carefully, that might not have made a lot of sense to you. And so definitely at this point, you might want to pause the video and go back and reread the NumPy broadcasting rules from last time and practice them because that’s what we just did. We use numpy broadcasting rules and we’re going to be doing this dozens more times throughout the rest of the course and many more times, in fact, in this lesson.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">X <span class="op" style="color: #5E5E5E;">=</span> data.clone()</span>
<span id="cb25-2">x <span class="op" style="color: #5E5E5E;">=</span> data[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">x</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([26.204, 26.349])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">x.shape,X.shape,x[<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([2]), torch.Size([1500, 2]), torch.Size([1, 2]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">x<span class="op" style="color: #5E5E5E;">-</span>X</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 0.000,  0.000],
        [ 0.513, -3.865],
        [-4.227, -2.345],
        ...,
        [-4.568, 17.025],
        [-3.151, 22.389],
        [-4.964, 21.040]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">(x[<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">-</span>X)[:<span class="dv" style="color: #AD0000;">8</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 0.000,  0.000],
        [ 0.513, -3.865],
        [-4.227, -2.345],
        [ 0.557, -3.685],
        [-5.033, -3.745],
        [-4.073, -0.638],
        [-3.415, -5.601],
        [-1.920, -5.686]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">(x<span class="op" style="color: #5E5E5E;">-</span>X)[:<span class="dv" style="color: #AD0000;">8</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 0.000,  0.000],
        [ 0.513, -3.865],
        [-4.227, -2.345],
        [ 0.557, -3.685],
        [-5.033, -3.745],
        [-4.073, -0.638],
        [-3.415, -5.601],
        [-1.920, -5.686]])</code></pre>
</div>
</div>
<p>Hi, Dan, The thing I’m using to write on the screen is called presentify.It’s this thing here. It’s very cool. And a graphics tablet graphics tablets are quite cheap nowadays. Oh, excuse me. I use a cheap Chinese equivalent of a webcam on tablet. All right. Hi, everybody. Welcome back. So we had got to the point where we had managed to get the distance between our first point x and all of the other points in the data. And so just looking at the first eight of them here. So the very first instance is of course zero on the X axis and zero on the Y axis because it is the first point. The other thing is that because we the way we created the clusters is they’re all kind of next to each other in the list. So these are all in the first cluster. So none of them are too far away from each other. So now that we’ve got all the distances, it’s easy enough to, well, not the distances on X and Y, it’s easy enough to get the distance, the kind of Euclidean distance, so we can just square their difference and sum and square root. And actually maybe this is a good time to talk about norms and to talk about what we just did there. Um, so we’ve got all these data points. So here’s one of our data points and here’s the other one of our data points, and there’s some, you know, distance across the X axis and there’s some distance along the Y axis. So we could call that change in X and change in Y.</p>
<p>And one way to think about this distance then is it’s this distance here. So to calculate that we couldn’t use Pythagoras, so a squared plus b squared equals C squared or in our case so this would be c, a, and b, so, so in our case it would be the square root of the change in X squared plus the change in Y squared. And rather than saying square root, we could say to the power of a half another way of saying the same thing. But there’s a different way we could find the distance. We could first go along here and then go up here. And so that one would be change in X, if you like, to the one plus change in Y to the one to the power of one one. Yeah, I got a slightly odd way for reasons you’ll see in a moment. It’s just this otherwise, in general, if we’ve got a whole list of numbers, we can add them up. Let’s say they’re some list V, we can add them up, we can do each one to the power of some number alpha and take that sum to the one over alpha. And this thing here is called a norm. So you might have remember we came across that last week and we come across it again this week. They basically come up, I don’t know, they might end up coming up every week.</p>
<p>They come up all the time, particularly because the two norm, which we could write like this or we could write like this or we could write like this, they’re all the two norm this is just saying it’s this equation for alpha equals two And Stefano is pointing out we should actually have an absolute value. I’m not going to worry about that. We’re just doing real numbers. So I keep things simple. Oh, I guess first higher than one. Now you’re probably right for something like three. Yeah, I guess we do need an absolute value there. That’s a good point because okay, we could have this one. And so the distance actually has to be the absolute value. So the change in x is the absolute value of that distance. Yes. Thank you, Stefano. Okay. So we’ll have the absolute value. Okay. So the two, norm, is what happens when every calls to and we would call this in this case, we would call this the Euclidean distance. But actually where it comes up more often is when you’re doing like a lost function.</p>
<p>So the mean squared error is just while the root means squared error, I should say, is just the two norm. Where else the mean absolute error is the one norm. And these are also known as L2 and L1. And remember what we saw in that paper last week. We saw it in this form. There’s a two up here which is where they got rid of the square root again. So would have just been a change in x squared plus change in Y squared. And now we don’t even need the parentheses. Okay, so all of this is to say that for, you know, this comes up all the time because we’re very, very often interested in distances and errors and things like that. I’m trying to think I don’t feel like I’ve ever seen anything other than one or two. So although it is a general concept, I don’t think we’re going to see probably things other than one or two in this course. I’d be excited if we do, that would be kind of cool. So here we’re taking the Euclidean distance, which is the two on. So this has got eight things in it because we’ve summed it over dimension one. So here’s your first homework is to rewrite using torch.einsum, you won’t be able to get rid of the x minus x. You’ll still need to have that in there.</p>
<p>But when you’ve got a multiply followed by a sum, now you want to get rid of the square root. Either you should be able to get rid of the multiply in the sum by doing it in a single torch.einsum. So we’re summing up over the first dimension, which is this dimension. So in other words, with summing up the X in the Y axis, okay, so now we can get the, the weights by passing those distance is into our gaussian. And so as we would expect, the biggest weights, it gets up 0.16. So the closest one is itself, it’s going to be at a big weight. These other ones get reasonable weights and the ones that are in totally different clusters have weights small enough that at three significant figures they appear to be zero. Okay, so we’ve got our weights. So there the weights are 1500 long vector and of course our original data is 1500 by two, the X and the Y for each one. So we now want a weighted average. We want this data, we want it’s average weighted by this. So normally an average is the sum of your data divided by the count. That’s a normal average weighted average item in your data. It’s let’s put some i’s around here. Just to be more clear, each item in your data is going to have a different weight. And so you multiply each one by the weights. And so rather than dividing by n, which is just the sum of ones, we would divide by the sum of weights. So is an important concept to be familiar with. Weighted averages. So we need to multiply every one of these x says by this.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="co" style="color: #5E5E5E;"># rewrite using torch.einsum</span></span>
<span id="cb36-2">dist <span class="op" style="color: #5E5E5E;">=</span> ((x<span class="op" style="color: #5E5E5E;">-</span>X)<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>).<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">1</span>).sqrt()</span>
<span id="cb36-3">dist[:<span class="dv" style="color: #AD0000;">8</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([0.000, 3.899, 4.834, 3.726, 6.273, 4.122, 6.560, 6.002])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">weight <span class="op" style="color: #5E5E5E;">=</span> gaussian(dist, <span class="fl" style="color: #AD0000;">2.5</span>)</span>
<span id="cb38-2">weight</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">weight.shape,X.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([1500]), torch.Size([1500, 2]))</code></pre>
</div>
</div>
<p>Okay, so can we say weights times X? No.&nbsp;All right. Why didn’t that work? So remember, we go right to left. So first of all, it’s going to say, let’s look at the two and multiply that by the 15. Are they compatible? Things are compatible if they’re equal or if at least one of them is one. These are not equal and they’re not one, so they’re not compatible. That’s why it says the size of a tensor a, must match. Now, when it says match, it doesn’t mean they have to be the same. One of them can be one. Okay. That’s what it means to match. They’re either or. One of them is one. So that doesn’t work. On the other hand, what if this was 1500 comma one? If it was 1500 comma one, then they would match because the one and the two match because one of them’s a unit axis and the 1500 and the 1500 match because they had the same. So that’s what we’re going to do because that would then copy this to every one of these, which is what we want. We want weights for each of these (x,y) tuples. So to add the trailing unit axis, we say every row and a trailing unit axis.(weight[:,None]*X) So that’s what that shape looks like. So we can then multiply that by x and as you can see, it’s now weighting each of them. And so each of these x’s and y is down the bottom, they’re all zero. So we can sum that up and then divide by the sum of weights. So let’s now write a function that puts all this together so you can see this really important way of like to me, the only way that makes sense to do a particularly scientific numerical programing.</p>
<p>I actually do all my programing this way, but particularly scientific numerical programing is write it all out step by step, check every piece, have it all that documented for you and for others, and then copy the cells, merge them together and indent them to indent its control+right+spare bracket and put a function header on top. So here’s all those things we just did. And now, rather than just grabbing the first x, we enumerate through all of them. So that’s the distance we had before. That’s the weight we had before. There’s the product we had before. And then finally some across the rows divide by the sum of the weights. So that’s going to calculate for i<code>s It's going to move. So it's actually changing Capital X, so it's changing the i</code>s thing and capital X so that it’s now the weighted sum. Oh, actually sorry, the weighted average of all of the other data weighted by how far it is away. So that’s going to do a single step. So the main shift update is extremely straightforward, which is clone the data, iterate a few times and do the update. So if we run it, take 600 milliseconds. And what I’ve done is I’ve plotted the centroid moved by two pixels or two one up two pixels, two units so that you can see them and so you can see the dots is where our data is. And they’re dots now because every single data point is on top of each other on a cluster. And so you can see they are now in the correct spots. So it is successfully clustered our data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">weight</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([    0.160,     0.047,     0.025,  ...,     0.000,     0.000,     0.000])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">weight[:,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1500, 1])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">weight[:,<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">*</span>X</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[    4.182,     4.205],
        [    1.215,     1.429],
        [    0.749,     0.706],
        ...,
        [    0.000,     0.000],
        [    0.000,     0.000],
        [    0.000,     0.000]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;">def</span> one_update(X):</span>
<span id="cb48-2">    <span class="cf" style="color: #003B4F;">for</span> i, x <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(X):</span>
<span id="cb48-3">        dist <span class="op" style="color: #5E5E5E;">=</span> torch.sqrt(((x<span class="op" style="color: #5E5E5E;">-</span>X)<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>).<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb48-4"><span class="co" style="color: #5E5E5E;">#         weight = gaussian(dist, 2.5)</span></span>
<span id="cb48-5">        weight <span class="op" style="color: #5E5E5E;">=</span> tri(dist, <span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb48-6">        X[i] <span class="op" style="color: #5E5E5E;">=</span> (weight[:,<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">*</span>X).<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">0</span>)<span class="op" style="color: #5E5E5E;">/</span>weight.<span class="bu" style="color: null;">sum</span>()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="kw" style="color: #003B4F;">def</span> meanshift(data):</span>
<span id="cb49-2">    X <span class="op" style="color: #5E5E5E;">=</span> data.clone()</span>
<span id="cb49-3">    <span class="cf" style="color: #003B4F;">for</span> it <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>): one_update(X)</span>
<span id="cb49-4">    <span class="cf" style="color: #003B4F;">return</span> X</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="op" style="color: #5E5E5E;">%</span>time X<span class="op" style="color: #5E5E5E;">=</span>meanshift(data)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1.3 s, sys: 13.4 ms, total: 1.32 s
Wall time: 1.44 s</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">plot_data(centroids<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span>, X, n_samples)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-37-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So that’s great news. And so we could test out our hypothesis. Could we use triangular just as well as we could have used Gaussian. So control slash comments and on comments, yeah, we got exactly the same results. So that’s good. It’s really important to know these keyboard shortcuts hit H to get a list of them. Some things that are really important don’t have keyboard shortcuts. So if you click help edit keyboard shortcuts. This list of all the things Jupyter can do and you can add keyboard shortcuts to things that don’t have them. So for example, I always add keyboard shortcuts to run all cells above and run all cells below. As you can see, I type Q and then A for above and Q and then B for below. All right. Now that was kind of boring in a way, because it did five steps, but we just saw the result. What did it look like? One step at a time. This isn’t just fun. It’s really important to be able to see things happening one step at a time because there are so many algorithms we do which are like updating weights or updating data, you know? So for stable diffusion, for example, very likely to want to show, you know, your incrementally denoising and so forth. So in my opinion, it’s important to know how to do animations. And I found the documentation for this unnecessarily complicated because it’s a lot of it’s about how to make them performant. But most of the time we probably don’t care too much about that. So I want to show you a little trick, a simple way to create animations without any trouble. So that matplotlib animation has something called FuncAnimation. That’s what we’re going to use to create an animation. You have to create a function and the function you’re going to be calling FuncAnimation passing in the name of that function and saying how many times to run it. And that’s what this frames the argument that says run this function this many times and then create an animation that that basically contains the result of that with a 500 millisecond interval between each one. So what’s this do one going to do to create one frame of animation? We will call our one_update.</p>
<p>Here it is one_update, right? We’re going to call this that’s going to update our access and then we’re going to have an access which we’ve created here. So we’re going to clear whatever was on the plot before and plot our new data on that access. And then the only other thing you need to do is that the very first time it calls it, we want to plot it before running and d is going to be passed automatically the frame number. So for the zeroth frame, we’re going to not do the update, but it’s going to plot the data as it is already. I guess another way we could have done that would have been just to say if d then do the update the update, I suppose that should work too. Maybe it’s even simpler. Let’s see if I just break it. Okay So we’re going to clone our data. We’re going to create our figure in our subplots vertical FuncAnimation calling do_one 5 times, and then we’re going to display the animation. And so let’s see, so HTML takes some HTML and displays it and to_jshtml(), creates some HTML.</p>
<p>So that’s why it’s created. This HTML includes JavaScript. And so I click run one, two, three, four, five. That’s the five steps. So if I click loop, you’ll see them running again and again. Fantastic. So that’s how easy it is to create a matplotlib animation. So hopefully now you can use that to play around with some fun stable fusion animations as well. You don’t just have to use to to_jshtml. You can also create Oopsie Daisy. You can also create movies. For example. So you can call to_html5_video would be another option. And you can save an animation as a movie file. So this okay, all these different options for that, but hopefully that’s enough to get you started. So for your homework, I would like you when you create your k means or whatever, to try to create your own animation or create an animation of some stable diffusion thing that you’re playing with. So don’t forget this important ax.chear().without the ax.chear(), it prints it on top of the last one, which sometimes is what you want To be fair. But in this case, it’s not what I wanted. All right, So kind of slow half a second for not that much data, I’m sure would be nice. It was faster. Well, the good news is we can GPU accelerate it. The bad news is it’s not going to GPU You accelerate that Well, because of this loop, this is looping 1500 times. If we so looping is not going to run on the GPU.</p>
</section>
<section id="animation" class="level2">
<h2 class="anchored" data-anchor-id="animation">Animation</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="im" style="color: #00769E;">from</span> matplotlib.animation <span class="im" style="color: #00769E;">import</span> FuncAnimation</span>
<span id="cb53-2"><span class="im" style="color: #00769E;">from</span> IPython.display <span class="im" style="color: #00769E;">import</span> HTML</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="kw" style="color: #003B4F;">def</span> do_one(d):</span>
<span id="cb54-2">    <span class="cf" style="color: #003B4F;">if</span> d: one_update(X)</span>
<span id="cb54-3">    ax.clear()</span>
<span id="cb54-4">    plot_data(centroids<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span>, X, n_samples, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="co" style="color: #5E5E5E;"># create your own animation</span></span>
<span id="cb55-2">X <span class="op" style="color: #5E5E5E;">=</span> data.clone()</span>
<span id="cb55-3">fig,ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots()</span>
<span id="cb55-4">ani <span class="op" style="color: #5E5E5E;">=</span> FuncAnimation(fig, do_one, frames<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, interval<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">500</span>, repeat<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb55-5">plt.close()</span>
<span id="cb55-6">HTML(ani.to_jshtml())</span></code></pre></div>
<div class="cell-output cell-output-display">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<script language="javascript">
  function isInternetExplorer() {
    ua = navigator.userAgent;
    /* MSIE used to detect old browsers and Trident used to newer ones*/
    return ua.indexOf("MSIE ") > -1 || ua.indexOf("Trident/") > -1;
  }

  /* Define the Animation class */
  function Animation(frames, img_id, slider_id, interval, loop_select_id){
    this.img_id = img_id;
    this.slider_id = slider_id;
    this.loop_select_id = loop_select_id;
    this.interval = interval;
    this.current_frame = 0;
    this.direction = 0;
    this.timer = null;
    this.frames = new Array(frames.length);

    for (var i=0; i<frames.length; i++)
    {
     this.frames[i] = new Image();
     this.frames[i].src = frames[i];
    }
    var slider = document.getElementById(this.slider_id);
    slider.max = this.frames.length - 1;
    if (isInternetExplorer()) {
        // switch from oninput to onchange because IE <= 11 does not conform
        // with W3C specification. It ignores oninput and onchange behaves
        // like oninput. In contrast, Microsoft Edge behaves correctly.
        slider.setAttribute('onchange', slider.getAttribute('oninput'));
        slider.setAttribute('oninput', null);
    }
    this.set_frame(this.current_frame);
  }

  Animation.prototype.get_loop_state = function(){
    var button_group = document[this.loop_select_id].state;
    for (var i = 0; i < button_group.length; i++) {
        var button = button_group[i];
        if (button.checked) {
            return button.value;
        }
    }
    return undefined;
  }

  Animation.prototype.set_frame = function(frame){
    this.current_frame = frame;
    document.getElementById(this.img_id).src =
            this.frames[this.current_frame].src;
    document.getElementById(this.slider_id).value = this.current_frame;
  }

  Animation.prototype.next_frame = function()
  {
    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));
  }

  Animation.prototype.previous_frame = function()
  {
    this.set_frame(Math.max(0, this.current_frame - 1));
  }

  Animation.prototype.first_frame = function()
  {
    this.set_frame(0);
  }

  Animation.prototype.last_frame = function()
  {
    this.set_frame(this.frames.length - 1);
  }

  Animation.prototype.slower = function()
  {
    this.interval /= 0.7;
    if(this.direction > 0){this.play_animation();}
    else if(this.direction < 0){this.reverse_animation();}
  }

  Animation.prototype.faster = function()
  {
    this.interval *= 0.7;
    if(this.direction > 0){this.play_animation();}
    else if(this.direction < 0){this.reverse_animation();}
  }

  Animation.prototype.anim_step_forward = function()
  {
    this.current_frame += 1;
    if(this.current_frame < this.frames.length){
      this.set_frame(this.current_frame);
    }else{
      var loop_state = this.get_loop_state();
      if(loop_state == "loop"){
        this.first_frame();
      }else if(loop_state == "reflect"){
        this.last_frame();
        this.reverse_animation();
      }else{
        this.pause_animation();
        this.last_frame();
      }
    }
  }

  Animation.prototype.anim_step_reverse = function()
  {
    this.current_frame -= 1;
    if(this.current_frame >= 0){
      this.set_frame(this.current_frame);
    }else{
      var loop_state = this.get_loop_state();
      if(loop_state == "loop"){
        this.last_frame();
      }else if(loop_state == "reflect"){
        this.first_frame();
        this.play_animation();
      }else{
        this.pause_animation();
        this.first_frame();
      }
    }
  }

  Animation.prototype.pause_animation = function()
  {
    this.direction = 0;
    if (this.timer){
      clearInterval(this.timer);
      this.timer = null;
    }
  }

  Animation.prototype.play_animation = function()
  {
    this.pause_animation();
    this.direction = 1;
    var t = this;
    if (!this.timer) this.timer = setInterval(function() {
        t.anim_step_forward();
    }, this.interval);
  }

  Animation.prototype.reverse_animation = function()
  {
    this.pause_animation();
    this.direction = -1;
    var t = this;
    if (!this.timer) this.timer = setInterval(function() {
        t.anim_step_reverse();
    }, this.interval);
  }
</script>

<style>
.animation {
    display: inline-block;
    text-align: center;
}
input[type=range].anim-slider {
    width: 374px;
    margin-left: auto;
    margin-right: auto;
}
.anim-buttons {
    margin: 8px 0px;
}
.anim-buttons button {
    padding: 0;
    width: 36px;
}
.anim-state label {
    margin-right: 8px;
}
.anim-state input {
    margin: 0;
    vertical-align: middle;
}
</style>

<div class="animation">
  <img id="_anim_imge412bc04d7f74f1e9ef729c5e2e62c22">
  <div class="anim-controls">
    <input id="_anim_slidere412bc04d7f74f1e9ef729c5e2e62c22" type="range" class="anim-slider" name="points" min="0" max="1" step="1" value="0" oninput="anime412bc04d7f74f1e9ef729c5e2e62c22.set_frame(parseInt(this.value));">
    <div class="anim-buttons">
      <button title="Decrease speed" aria-label="Decrease speed" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.slower()">
          <i class="fa fa-minus"></i></button>
      <button title="First frame" aria-label="First frame" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.first_frame()">
        <i class="fa fa-fast-backward"></i></button>
      <button title="Previous frame" aria-label="Previous frame" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.previous_frame()">
          <i class="fa fa-step-backward"></i></button>
      <button title="Play backwards" aria-label="Play backwards" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.reverse_animation()">
          <i class="fa fa-play fa-flip-horizontal"></i></button>
      <button title="Pause" aria-label="Pause" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.pause_animation()">
          <i class="fa fa-pause"></i></button>
      <button title="Play" aria-label="Play" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.play_animation()">
          <i class="fa fa-play"></i></button>
      <button title="Next frame" aria-label="Next frame" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.next_frame()">
          <i class="fa fa-step-forward"></i></button>
      <button title="Last frame" aria-label="Last frame" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.last_frame()">
          <i class="fa fa-fast-forward"></i></button>
      <button title="Increase speed" aria-label="Increase speed" onclick="anime412bc04d7f74f1e9ef729c5e2e62c22.faster()">
          <i class="fa fa-plus"></i></button>
    </div>
    <form title="Repetition mode" aria-label="Repetition mode" action="#n" name="_anim_loop_selecte412bc04d7f74f1e9ef729c5e2e62c22" class="anim-state">
      <input type="radio" name="state" value="once" id="_anim_radio1_e412bc04d7f74f1e9ef729c5e2e62c22" checked="">
      <label for="_anim_radio1_e412bc04d7f74f1e9ef729c5e2e62c22">Once</label>
      <input type="radio" name="state" value="loop" id="_anim_radio2_e412bc04d7f74f1e9ef729c5e2e62c22">
      <label for="_anim_radio2_e412bc04d7f74f1e9ef729c5e2e62c22">Loop</label>
      <input type="radio" name="state" value="reflect" id="_anim_radio3_e412bc04d7f74f1e9ef729c5e2e62c22">
      <label for="_anim_radio3_e412bc04d7f74f1e9ef729c5e2e62c22">Reflect</label>
    </form>
  </div>
</div>


<script language="javascript">
  /* Instantiate the Animation class. */
  /* The IDs given should match those used in the template above. */
  (function() {
    var img_id = "_anim_imge412bc04d7f74f1e9ef729c5e2e62c22";
    var slider_id = "_anim_slidere412bc04d7f74f1e9ef729c5e2e62c22";
    var loop_select_id = "_anim_loop_selecte412bc04d7f74f1e9ef729c5e2e62c22";
    var frames = new Array(5);
    
  frames[0] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsT\
AAALEwEAmpwYAABTb0lEQVR4nO3dd3yV5f3/8Vf2JIMsMoAkhLACBAlDFEhkuCggLvhixdUgWkuh\
RWltC2lVqLNWEY0i0GrhJ0oDBQUZCRshCMgQCJCQScg6Wedkcn5/3Nw354RDSFjJST7Px+P7CNxn\
XYev5c3nuj7XddsYjUYjQgghhJWxbekBCCGEENdDAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJ\
AkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwI\
IYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRV\
kgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVkgAT\
QghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVsm/pAdwqvr6+hIaGtvQwhBDCqmRkZFBYWNjSw2iSNhtg\
oaGhpKamtvQwhBDCqsTExLT0EJqsVU4h1tfXM2DAAMaNGwdAeno6Q4YMISIigscff5yampoWHqEQ\
QoiW1ioD7P3336dXr17a71955RVmzZrF6dOn8fb2ZsmSJS04OiGEEK1Bqwuw7Oxs1q9fz3PPPQeA\
0Whk69atPPLIIwBMmzaNpKSkFhyhEEKI1qDVBdhvf/tb3nzzTWxtlaEVFRXh5eWFvb2yXBcSEkJO\
Tk5LDlEIIUQr0KoCbN26dfj7+zNw4MDren1iYiIxMTHExMRQUFBwk0cnhBCiNWlVXYi7du1i7dq1\
fPvtt1RVVVFWVsbMmTPR6XTU1dVhb29PdnY2wcHBFl8fHx9PfHw8YF2dNEIIIZqvVVVgCxYsIDs7\
m4yMDFauXMk999zDl19+SVxcHF9//TUAy5cvZ8KECS080jaisgh2va/8FEIIK9OqAuxq/v73v/Pu\
u+8SERFBUVERzz77bEsPqW049AVs+ovyUwghrEyrmkI0FRsbS2xsLADh4eHs27evZQfUFkU/Yf7z\
aiqLlJCLfgLcfJr+mBBC3EJWUYGJW8TNB+6aee3gaaxSu5lVnExpCiGaodVWYKIVaaxSa2oV1xRq\
GIISrEII0QgJsLbuZkzxqZVacx9rrpsZhkKINk+mENs6tarZ90nTp+dMp/JuZFqvua9t6pSmEEIg\
FVjbp1YzNXrz6TlLlZl6rUYP2xZefo/rndaTKUEhxC0kAdbWqVVNZRE4ul4OtH2JSkjV6CHuD8o1\
NXBGvgJj/mo+ldeMTsWiKvDx8TGfEqwsomjbJ/iMnC4VlhDippApxPbiiuk5Y4OfQOQD0H0sRD16\
+bmma1zJCyD5DctTi5fCb/4vh9OvbxSnTp0y+8xTa9+l3+Q/Mf+lqbfrGwsh2jipwNqrwdPB0e1y\
lVRZBN//EdK+h9C7wW/m5esNpxVzD4JvT9jzT+V61CNwJpn5B/1IWPszAHFxcSSvX01k5V5OuQ0l\
7nefk1tuJGHpRugyn/nz59/+7yyEaFMkwNqrht2Dh75Qwqv7WPPpQtNpxZFzIWO78jxDifJ4bSV8\
/0fmf/4dCdsu32g0NzeXuNjhJD5gT/x3RnJ1VdpjCQkJUKNn/quvyCZoIcR1kylEoYh+Qln3mvix\
eZio1wdPh8HxgI1y3Xjpp4MbRTFz+PQn8/+UJjIRfakb41YY0OucmchEs8c//TSRom2fyFFWotUr\
rqzhk21nKK6UO8G3NhJg7VVBGnz5qPITLLewN+xU3JcI53YpjwX1v7Re9gg+PQaTvPcwQYGdACW8\
ZjKT93iPUEJ5j/eYyUwtxIJ8PUnetFFp6DBtFpGTOEQrtCo1iwXfnWBValZLD0U0IAHWXqnrXd//\
8erP2Zd4aQ9ZohIqWXuV6+Gx4NpRef13v4fkBUQG+5D8zjMEdbAhhRQyyCCUUJaylFBCySCDFFII\
CuxE8qaNRJZsUd7XdPpQDhcWrdCjMZ35w/09Gd07QCqxVkYCrL0aNB1cfaHvlEY2LZt0Ku5LhLMp\
0LEb3P+20qnYMVy5tm0hHPqCyNFPkRg/DB06Ekgw+7gEEtChI/HTz4is3Avb/q69TqNOV8pJHOIW\
a8q0oPocgOkju7H5eL5UYq2MNHG0R5VF8P1c0BfC9jegMO3yY6YbjwdPB30xHFkFXe5SrhefgaNf\
K3vKis8q17y6QkEap74bT3ziSbzwYh7zzD5yHvOYxSzi4+OV7sSRrwA22h4xDn2htPELcRuo04Kg\
hJOp4soaVqVmoa+p4/0tp7XnPBrTGVAqMvU5j8Z0pqOb4+0dvNBIBdbeVBZB0vNKaHXsBmGxMOwl\
pR0+8oHLFVBBmvK8tO+VoDq5HroOu/QmRuU50b8EexfQnePUlmXE/fMkueVGYonVpg2f5mltOjGW\
WKU78YGJnAp+RNlA7eZzeerwu99fnrIU4hYa3TuAuB5+jO4dcMVjl8PNhj/c35NRYa4AdHRzZPrI\
bnR0c2RVahavrd53RTUmDR+3l1RgbcW1Du013c+ltssHDVCm8rqPVa6BUlnB5TUyz87g0hEMxRA6\
AkKHo3UiZu+FOgNFeohbrie3XJlyTCIJgBRS0KFjFrOIJVa7npt3nrjYkfx05Kj5iR2VhcqUpOnm\
aiFuEtOqafPxfJJPFtAvJAdXR3utkiqurKGoopq7I3wZGenHX+bNY/7337AtJZnIyEjtvQZ4Gij/\
zyzOuv0KRv5Nu66Gn76mTntf9bpUazefBFhbca1zBxseExX5ABxdpeztinpE2bxcU6k8p6YSvLqA\
szeUZsGw34CDi/I+NQZlA3PGDujUHyoK8QkbwK9ivichWa99XBJJBHWw4YvHuxL//zJJKk8yG86v\
7uuvhBeYH3fl5itrYOKqGk7dNWcqb/nudN7fchp9TR3ThoUBoK+pN5tKXJWaReKOdACO/u8zDq/9\
DLi0MT9ZCbFTp07x0IP3UlqYz5sLXsPF0Y758+dTXFmDvqaOmaO6A2jva/rrhtOV4sZIgLUVjd2K\
pLJICaWRc5W9XG4+SrPGtr8rHYUAwYNgwxwY9CvABvZ/duX7bFsIIYOVX6vt9ADpW5k/wh68epHw\
X+UkjqAONiRPcyWyfwTJzoXErbAlt6gcgHkP9WL+B19eHptp5djYob9y9+d273KFU4+ro90V61SN\
s9F+qtOBxZU1uDrama1vFVVUs/KTdzn8vyXaK3Nzc4mLiyMxMZHnfhXP+bxc7bGEBKVhKTDul7y/\
5TR/uL8nj8Z0Nntf9b1vJlmHkzWwtqOxW5Hs+0QJK3Vqbtf7l889VLsIk6Yra13pKUrIDfoVOHsp\
zzcC6TuVX9deOlGjQ5Dy085J+RkymPkvPcG83/+aIE8HJbyCfaBTNJGTXiV58yaCAjsx7+l7mf/v\
HdfXOi9t9u2e2tIORrN1qqaEw/joIO6O8KWkspr3Np2yuE7V0c0Rp3o9x5P/a3Z9IhPR5+oZN24c\
VXl6CxvzP2VUmKs2FtP1MtNf30yyP00qsLbFtJvv1Lcmlcqlf3nWGi43ZoBy6sa+T5RpQUMJZO6C\
8Yuh6Cwc+hJq9eDbXXl55qWKq/Ck8tPdH+prlE5GgMJTkL2P+X1jeSneCR9ff+Wx1E+h+1giB8fz\
097t+GR9q3Q2quOs0SvTmk2ZNpQbXopLxkcHo/yHabyiArFUmRRX1vDauuPsPF3ITqVgw9XRDjCf\
3iuurMHWpQNz/vkflvzhaQry87SN+ROYQAIJzGMeoYQCylR5p8AgkpOTiegSSEQXGh3HzWRaNbZX\
rSrAqqqqGDFiBNXV1dTV1fHII4+QkJBAeno6kydPpqioiIEDB/Lvf/8bR8f2WTI3Sq1QMnZeDqm7\
ZioVlaOreQOHGgKObsrPPf9U1sa6DIJ/DlDCy94Zut+PVrmpzRwADm5KQDl5Ql0VVOmU64Wn8ek1\
HEYlwJlNkLVP+cxDX+BTU6lUgqc2wLndcGarUgGO+euVlaOl6cKbefdnYZVM299dHe1Y8N0JXB3t\
zaYPTde6Zo3pob0u+WQBQ8K8qam7iKO9HaN7B+Dtqvw9om5SLqqoIXHHWWaOimDn9hTi4uJIyU1h\
AhO0jfnA5Y35QUH8d/1GkvPssPOuYPPxfC2wGmvVvxnUyq49a1VTiE5OTmzdupXDhw9z6NAhNmzY\
wN69e3nllVeYNWsWp0+fxtvbmyVLllz7zdqyqx25pG4EHvuG+YZgNx+l2snaqzRkqOcdao0fNpcb\
O3a9f3nPl28vJdiOfKPs9br/HQiPU9ruf3HpedWlUF+tTDd6dYWybGV9LGs3xP0RHl5iMpbLaxCA\
UsGNnGu5opLpQmGB6akY+pp6Zo6KMDsho7iyhgPnlIOmD5zTadOE6uuGhvtyMKuUH9KLif9XKhmF\
lQCsPZTLgu9OcDyvFIA1h3Kx8w7inX8uanRj/jv/XMTig3oWfHeC19YdN5vSUz+zPVdIt1qrqsBs\
bGxwd3cHoLa2ltraWmxsbNi6dSv/+c9/AJg2bRrz589nxowZLTnUlnW1jkPTCsWvQaXy/R+Vaqcs\
B+76rXLNdEpObexQOxWH/QaOr1EeL7+0YH1kBTyZdLk6sjFpd7/jSeg5HlY/C+4B0HkYfP9nyDkA\
/r2VvV1RjwBGZcrSzkEZT+R9ltftZLqwXbrWtJtadXyy7Qzvb0lj5qju/CXpKLvOFLH9VAExod7s\
PF1ENz83dp4u5HdfHeJP43rz//ZnsfdMIYUVNUzoH0jKqQLOFFQy+6tDZBTp6drRlSfv7IqzvS3Z\
JQYyivTM+WwDW955sdGN+dOnT8d90t+IHdSP7v4d6BfiZTGwpOHi1mhVAQZQX1/PwIEDOX36NC++\
+CLdunXDy8sLe3tlqCEhIeTk5LTwKFvY9fzlPvYNKElXNjAnPa9UYZbet6YSsIHzP4HuHDh7QlWp\
sh9s7BvK89SmkJhfgZ0j+HRX2uzPbFJeozsHO968PI2pdiw6uipTltv+rlRe3e658iQO07U7mS5s\
d5pyQsajMZ21kNDX1LHrjDITsetMEX2CPLk7wocgLxeqautJPlmAvuYIP6QXa++zPa2QUkMdoARi\
/UUj54r1/O9wLiX6WoaEeeNquMDmt39DefEFJjJR25hvugYWSyxJRUmw+s+Ud/mQxDNF/OH+nlpA\
mX4XkFb6W6HVBZidnR2HDh1Cp9Px0EMPceLEiWu/6JLExEQSE5VTHAoKCm7VEFve9fzl7tcdnt54\
uYlj3yfKjSlN18rUho9tCy+Hk1dX2P8pRP+f8h6ANgVog/IcexcllO78jdKWH9APeo1X1r+qdEr4\
9Xno8s0vwXxtS6381LW7jJ1X3tZFtAtXa0worqzhNysOsvN0IfqaemaNidSaLgw19RzOLqV/iCcu\
jnbsPG0+tZ5VrMfD2Z6yKiW0Onu7MKZXABuPn+fHTB0dnJVmjhJ9Ld6uDuw+fo7zS1+irlx5n2tt\
zC8rusC3f3+ewbOXMCi0I59sO6OFrL6mDn1NPeOjgyx+L3FjWl2Aqby8vIiLi2PPnj3odDrq6uqw\
t7cnOzub4OBgi6+Jj48nPj4egJiYmNs5XOvg5qMEQ8MTOcwquUvTgm4dYdzbypFSunPK4b1qpRT1\
iFJNqU0Y9TXKOldNpfL7bvcoa2BVOqWL8fEVSmW16S/K6yIfUIJ07BtKKKqfr56FeKnpQyqw9qWx\
abZVqVnsPK10vB44V8yZAqVhYnTvANIuVPBDejGujna8dE939p4tJKvYQG5pFd6uDuSWKls/Onsr\
m/F/yimjsqZeq8LKq+pxdrAl0r8DP+WUYufigVu/sZTuWqF9fhJJuHv74TdqHhWbFpFUmmQ2Pvf+\
95Jb7cAHW9NIPqn843n6yG64OtpfajSxk8rrFmhVAVZQUICDgwNeXl4YDAY2bdrEK6+8QlxcHF9/\
/TWTJ09m+fLlTJgwoaWHav3UEGq4KTjqUaUyi3pU+f2pb5VACb1b+f2mvyjh5+gKsX9SKrDhLyuB\
FfWoMkVoGojq+7uaTHuatvJPXWVeUaoBqzaUyKbldqOx6UOlmqnnwLlidp4uYt6aY+w8XcjGY0oV\
1dnbRQuOH9KVJo67uvnw14lRrD2UQ0llLcknL5BVYuCubj78bmwP3t10iiBPJzYez6fUUEdhRbX2\
eV1GTyMTtBCzc++I16OvYecdTEzfD9n23kvodcrn9X7wGaJ+8Sxdfdw4V1RJ/PDwKypJqbxujVYV\
YHl5eUybNo36+nouXrzIY489xrhx4+jduzeTJ0/mT3/6EwMGDODZZ59t6aFar2sdOWUaWH4zLa+3\
VRYo04wdw2HKV5erKzXYQAmd6Ccsn7KhrqWpP02pz1OnFa82TtHmNHbae0c3R2aNidSuF1VUs/N0\
IQXlSugEebnwxNCujO4dQG39UXaeLqJPsCebj+czbVgYv/vqEFklBgD6BHtyR1dvvnhuiPLBqw7z\
1YFsvFwuV2v39QkgN+Q3HPRy4eS2JAKmLMDOOxhPF3ti+vWm55vL+WzuUwy69xF8Rz7BD+nF5JdV\
k1Gkx9XRXqsgpdX91mpVAdavXz8OHjx4xfXw8HD27dvXAiNqg67VANLw8YbrbXfNhOQFyq+Lzyrd\
jWpDiHqWovq8q4WlX3el8rqRcYo2x/Qv+0+2nbFYjZkeAeXj7sSg0I58sDWNP43rTTc/d4oraxjY\
1ZuBXTuintax92wRkwYEs+dMIVV1RlwczHcP5eiUYHNysCXUx5WMIj2nCyr5MVPHk8/9lotPPMeW\
s3ryyqrwcHbgX3szAXhw/hf07RZC4vaz2thCvF3407jet/qPSlzSqgLMWtSVlFC6ejWekyZh7+3d\
0sNpHksNIM05jxCUjdE1esj/SamiTA/jNZ1CbE4INRyDdCG2G5bWvixNvTV8nhps7zwWzfLd6agn\
c7y/5TTxw8M5llvKHV28SD5ZQHphJVV1RpztbSiprNX2h61KzeJ3Y3vguDWN7gEdSNx+lm5+btpn\
ni3UM7y7P3mHTtDNz40zBZXcHaFMae88XYSts7JvrGtHV37M1PGH+3vSzc+9Wd9VXD8JsOtQuno1\
F956GwAfa5/OVO8PZtqNaPqYaaiY/v7ev135Xg1DpzkhdK2pTdFmWVr7sjT1drU1slWpWdqBvjNH\
decP9/dkR1oBu84UaY0bg0M7UqKvodRQx7/2nsPbzZGfsnUknyzgD/f3ZOnTgymurCEtv5zkkwV0\
cLYj2MuZ2WMiCfVVAm107wCtcWTtoVx6B3pQVXcRGxuYPaYH+zOKr7nWdatP52hvJMCug+ekSWY/\
rY7pviv1vl9XdCNifjSV2lxxtZC50ZPiZcqw3Wpqo4P6uHryhlrFjO4dwPZTBZcqH+VsxNG9A3ht\
3XH83J3IOpCNs4Mdbo72lBrqCPZyBowknyzg7ghf9DV1FFfW0NHNkXcei2b0uykUV9ZSXlXP/oxi\
7ujqzaMxnbXKSQnMNOJ6+GmNI8O7F2uPqUFnqcqSpo6bSwLsOth7e1t35dXwzMTuYy3vu4p+4vJz\
1HBSr4N5aDUMu+aGmEwZtlvXanQwnXZTT+FQq5hHYzprJ3EA7NpbpJ2N+M5j0fxmhbKmfrawUmvQ\
eLBvkHY/sAPnSnh/y2kOnNMxsKsX04aF8dmTg5j91SFGdPc1uyGlehsXMDJzVHfGRwfRLyQHsNHC\
S11zM22lb853Fc0jAdaemFZeoPwMvfvqVZPpvjFL62NqaNXoAaNyTqLs4RI3WcNpN9MqZlVqlhZe\
3fzcGRHpp3Ux/u6rQ+w8XUhcDz9eukfZhN87sAPPxyoBkppRwq4zRQR0cLp0Sn0hP2WX8qdxvZky\
uIvFNTn1/mPqWpd6WLDpc0b3DmBoeL5UWbeBBFgrd1MbRixNATY8M7GhxiojtRJTT5kf+Qp0i5Np\
QHFTNZx2M61iHo3pzPZTynqXt5uDWRdj8skC4nr48c5j0dpG6N5BHvxmxY/U1l/U9otV1dUD4Gxv\
e+noqZ/4Ib1EO0HjtXXH+dO43lr3Iyh3clanHVUd3RzNphqlSePWkwBr5W5Gw4gWgveOw34MNy9g\
LHUfNjZ1KHdUFtfhatNu6tTiXydGaWtOKtPQU4NFX1PPmkM5ZBTpAbg7woeBXTsyoLMXf1t/nIFd\
vPnqQDaX75pg5LV1xy9NBx7XghDg/S1pFk/XkCaN20sCrAU1pbq6GQ0j5iHYglN70mkobqLGwqJh\
6HV0c8TV0Y6MIj1eLg6M7x/Eb8dEalVSbE9/iitr6ObvbtaEUaKvAZQKTP08tdPR0hShNGncXhJg\
N9G1Aqnh4w2rK0uvvxkNI7e8a7KpwSSdhuImam5YPBrTWWuwCPZ2uWKKzzT0uo10164tfXowAN4x\
l9fDrjY9KE0at5mxjRo4cOBt/8zCzz4zHu/R01j42WfatdriYmPhZ59pP00fN33M9PXn4uMvXyss\
tPxZV7neIioKjcad/1B+CtGKFVVUGz9OOW0sqqhu6aG0Wi3xd+f1kgrsJrJU6ZhWWep193vuoWjJ\
EjwnTTKrrjwnTaJy3z4qt22ndPVqPsjK4tNPPyU5OZlwPz+tOjtbUEBcXBy/+tWvmD9//u37glcj\
LfDCSjRWIckpGdbH9tpPEZbUlZRQtGQJdSUl2jV1uk+d/qsrKeGi3kDHZ57mol45b83n2Wep2LqV\
C2+9Tenq1WbvBRC0cCG+L77Iwq+/JiEhgdzcXOLi4jjw0UdceOttDnz0EXFxceTm5pKQkNA6AkyI\
NkBd41IbNUTrJxXYdbLUHdhwDavo008p/nwp9iHB1GXnYOvqgs+zz15RqZV88SWFixZxUW/A76Vf\
89am73l7wwbts3Jzc5n0wQe8NmAAf/rgA3Lz87XHEhISACTIhLjkeispacCwPhJgzaSGlPs99wCW\
pwsv6vXYurqiP/Cj8prsHNxGjmikicKo/SwqKuLL1FSzRycykZT8FJ7ZsAEvvJjIRO1usACfJiYy\
1dubsCeftL7DhYW4ya63lV0aMKyPBFgzqSFVuW8fQQsXmgWGGlAX9QYuvPU2zgOiAXCOjjZ77uWg\
M2Dr6oLb8OEYjh7FY9w47GxtWf3ii0xatIjcvDwmMpGZzGQCE0gggXnMI5RQQLlLbFBQEKtfeIG6\
xR9T6u5+RceiVZ+cL8R1kEqq/ZAAayaXgQOx7eitNVqYBoa6BlZXUoKtqwt1RUVUHTyE6x0DKPni\
S8CIx7hxXNQb8H3xBS4a9BQuWoTrnXei37OHisFKu67Hv79g9YsvMvHtt0nRpTCBCYQSylKWApBB\
Bimk0MnT83KDR6dOFiu8NnVyvhBNIJVU+yEB1gSmVUzh4sVcLC7BITzsqlOC9t7eeE6aRMkXX+D7\
4otcrDJQuGgRAPofD6LfsweH8DBcBgwAwDE8DPe778Jl4EAK3v8nXlP/D18PDxa//TYPPfccCSRo\
4QWQQAI6dHw5eTqRkZHA1cPJ6k/OF0KIq5AAawLTKiZg7lzyL/1sOCVXV1KiVVoXDVUUf/45Dl27\
4Hb33dpz6svLsfX0pPZsOsaaWgAqdu6ky8cfk79wIfo9e6jNP0/aiZM8n38eL7yYxzyzz5nHPGYx\
i9+tXEn0s8/Se9Cgq47d6k/OF0KIq5A2+ibwnDQJ/zm/x3PSJJzCwujyySc4hYVd8bzS1aspXLSI\
wkUfUfXzzwDUnsukJj0d12F3AlB99CgXS5W7uDr17gV2dtSdyyTzV/E4BIfgeuedlE2dytO5OeTr\
9cQSSyihZJDB0zxNBhmEEkossZwvLWXU2LGcOnXKYlu/EEK0Za2qAsvKyuLJJ58kPz8fGxsb4uPj\
mTlzJsXFxTz++ONkZGQQGhrKV199hfdtakhorAnC9DFQmje8pv4fNenpeD78CDXnznGxtpaLej3e\
v3ySquM/c1Gnw2XQINwGD6J03TqoV07CrsvORvef/2A/43kefPFF8qurAbRuwxRS0KFjFrOIJVa7\
fl6nIy42lpRXXqFu8cdU7NqFc89e2Lq44P3EVGncEEK0Wa0qwOzt7XnnnXe44447KC8vZ+DAgYwZ\
M4Zly5YxatQo5s6dy8KFC1m4cCF///vfb8uYGmuCMN2/BZg1ZNSeP09dbi4AVQUF5Ge+xkWdDofw\
MEL++T723t64DR9O9u9/j72XNy79+2Hv5YXb8OE84u3Nh5eqNLjUbRgYyDs9ovnDzp0k1SWZjeP/\
+kcT9uST5B4/TuW27eh37wHQ9p0JIURb1KoCLDAwkMDAQAA6dOhAr169yMnJYc2aNaSkpAAwbdo0\
YmNjb1uANd4EoezfumjQU3VC2Xfi3Ksn7nffhWP37uS//jrOfftSX1xMh3vvo+C997TGjbqSEgwH\
DuB5770UL/mci2VlBL35d3JfeYUXnJy56OPDR0XKjfoC/fxITkmhq4MD4XPnMnXbNm0z8ws+Prz8\
wP0AuET1xalbN8AGWxcXadwQQrRprSrATGVkZHDw4EGGDBlCfn6+FmydOnUi3+QkilutsSYI7yee\
wNbVlYt6A/rde3AbOQKfX/0Ke29vCj74kNpzmXiO+wXer75KxtSpXNTpKPtmNbWZWdReyKfuXCb2\
XbsAUJuZSeZTT2OsUm57PjOqL7aZ5/gqK4svhw/He+P3nP/xRwIO/8Tql15i4ptvMsnBgd+PHg3Y\
UPLFFxQu+gj/Ob+XqksI0S60yiaOiooKHn74Yf7xj3/g4eFh9piNjQ02NjYWX5eYmEhMTAwxMTEU\
FBTc8DgaNkbUlZSQ/9ZbnHvmGarT07Vw835iKv5zfk/A3LmUrl596flqdWYgd+5cas+mg7Mzjj17\
Yti/n7pzmdi4uCghFhICgLGqCjs/P5yjo3HsHMIby5ax8ZFHCXVwpHDRIqX9vmsXQh0c2frbWfza\
1w9bF9dLLfo2Ssu+Xm+xkUOaPIQQbU2rC7Da2loefvhhpk6dyqRLU2ABAQHk5eUBkJeXh7+/v8XX\
xsfHk5qaSmpqKn5+ftf1+aZ/0avrXyVffEHBBx+QM/t3FC/5HP3uPeQvXGh2CK/Ps89Stm49F956\
m5zZv8Nj3Dj85/weWxdnKrdtx9bLC6qquFhRAYBNhw4YDQZcBg2iy6eJ2Li5AXBRr6fq0CEM+1Mp\
XrqMzqNHUXXokDI4Z2dqz2VS/Pnn+Pr64D/n93R8+ikcwsNwG343tq4uFC766IpDgk2/ixqwEmZC\
CGvXqqYQjUYjzz77LL169WL27Nna9fHjx7N8+XLmzp3L8uXLmTBhwi0bg6Xbn1zUX96I7DJoEDYO\
9gTMnXvFIbxq1aXfs4eydevxe+nXl0LChopdO6k6pMN95Agcg4IoT07BkJoKRiO6VV9rpyE6dO6M\
rZMTVYcPYx8YqJ1mX755C7WZmTh07aJMS17qMMycPp3as+kULl5MwNy5VO7bp53TaOm7eE6aJKdz\
CCHahFYVYLt27eLf//43ffv2JTo6GoA33niDuXPn8thjj7FkyRK6du3KV199dcvGYPoXvenRUEo4\
2TRoTTea/fR+4gkq9+3DsD+Vyn0/4F2iPNfW1YWqQ4dxHXYn9l7KKR0uAweS+/IrXKxSNjyrXAfe\
gb2XN1WHD1OXl0fZ6tX4z/k9nT9RNjoHzJ1rtgfNd8YMajIycAgJQbdqFZXbtlMWFYX3E09wUa/H\
98UXzb5Lw+8ohBDWqlUF2N13343RaLT42JYtW27LGCw1bdh7e+P30ktXPFdt4jANAht7BwAM+1Mp\
+eIL/F56Cfd77qFy3z6cIiIoXLQIw9EjuERFUZuZecV72jo5c9GgxyUmBsfwMFzvGKAFUJdPPtGe\
p04LXtQbqD2Xie7cf3C98051FJc2VStNHQ33gsnpHEKItqBVBZi1UYOgOj2d3LlzcerWTWm06NLl\
UjgpzSYVW7dSuW07LlFRuI0ccenXfen47DMYDv+EU2Qkti7O2Do7A1C46CMADKmpuI0cQcOtyHUl\
JeTM/h36PXvo+Mwz+L74IupBwRVbt5oFqqUqS06oF0K0BRJgzXC1v/jzFy6kctt2atIz8H3xBTzG\
jUP39dfoD/5IdXq62ZSdh07H+Zpa6kqKqUnPwJCaSoe4WK0iqk5PR7duHXXnMnHo0sXiqfelq1ej\
33Nps7KLM95PTKV09WrsvLy0Kc/GAkrWwIQQbYEEWDOoe60u6vVmU4oBc+eSlZVF7dl0bF1dsfPy\
ojw5mdqz6eQvXEiXTz7RgkU9sFcNIIfwMK3pApRqre5cJm4jR+A7YwaFixebPQ5KEF7UG7ho0AM2\
FH32mbIZ+lIzybUCStbAhBBtQatro2/dbMx+qu3odl5ehH75pbYPq+SLL6k9m45DeBi+M2aYtbJX\
btuO65130vGZp3GJiaH2bDpl69Zpn6AeHBwwdy6FixdTuW07FVu3mo3C3tsb7yemUn3mDIWLFmkH\
B6vNJKaHD1uiTn3K9KEQwppJBdYM3k9Mxdb18hFNDSsdW1cXLrz1Nr4vvqAFyNVa2dXTOgypqVwO\
xsvhUrRkCZXbtuM2csRVb1SpPh4wd67Z2pc0aQgh2gMJsGZoGAwNp+IaBpSla6avbxiIpiy9V2OP\
OzUILGnUEEK0dTbGq/WtW7mYmBhSU1Nv+ee01qAoWrKEC2+9LWcjCiGa5Xb93XkzSAV2g1prR580\
aggh2joJsBvUWoNC1sGEEG2ddCFep4YH+baW6cPbeVBvSVUJS48upaRKDgUWQtx+EmDXyfR099bk\
do4r6XQS7x54l6TTSbf8s4QQoiGZQrxOt2vqsLlNIrdzSnNixESzn0IIcTtJgF2n27XG1Nwmkdu5\
9uXt7M3TUU/fls8SQoiGZAqxlbvWqRrXS9avhGgZ+rJS9q/9Bn1ZaUsPxepJgLVyt+rYJ1m/EqJl\
HEvZzPYvl3IsZXNLD8XqyRRiOyXrV0LcXvqyUo6lbCZ84BAA+sSObuERWT+pwNopdf3K2/nmVHaN\
TUnKdKVoLxqbHlQrr7MHfmDQ+Idx9fC8rvcRl0mAiZuisSnJ5kxXStiJ2+1mhkVj04N9YkczYurT\
Taq8ZJqxaWQKUdywkqoSDHUGZvSfYXFKsjnTlWrYAdLhKG4LNSwABo1/uFmvVacF+8SOxtXDUwsn\
SyHl6uHZ5Pdv7H3EZa2uAnvmmWfw9/cnKipKu1ZcXMyYMWPo3r07Y8aMoeQ2nDIhmqakqoRXd77K\
4sOLcbF3wdvZ26yKKioqsjhdWVRUZLHamhgxkdkDZ5uFnVRl4lZqTmXUUMNKSQ0pS9ODzan0Gnsf\
cVmrC7CnnnqKDRs2mF1buHAho0aNIi0tjVGjRrFw4cIWGl3b19SwUJ+34sQKduTsIMwjjNjOscDl\
Kuqp2U/Rr18/Tp06ZfbaU6dO0a9fP56a/dQVU4uWwk46JsWtZCksmho2TQ0/fVkpGz56j+8+/8Ti\
tGBRUdH1Db6da3UBNmLECDp27Gh2bc2aNUybNg2AadOmkZSU1AIjazsaC6mGYXG156rPO3jhIEMD\
h5Jels7MrTNJL01nYsREwn8IZ+3iteTm5jJk+BD2H9lPSVUJC9YtIDY2ltzcXNYuXkv4D+HXnFq0\
VJU19fsI0VSmoXWz16COpWzm43//h/eT9+IYEmr2mPoPuvnz51/XWNuzVhdgluTn5xMYGAhAp06d\
yM/Pb+ERWbfGKpqGYaE+99Wdr2oBoa55DQwYyN68vfTs2JMwjzAtxObPn8/axWu199Rd0DF29FgS\
Pk/gL7/8C3l5edpjaxev5anZTzUaPtfqmJQKTTRVU7oE1TWtEVOfJnzgEPav/YainGyLrzu0YR3b\
v1zKoQ3rrnh/018n7dnPpuNpFJeVc/+4X2izEqdOnSIuLo7c3FwSEhKaHGLS5KGwuiYOGxsbbGxs\
LD6WmJhIYmIiAAUFBbdzWFbFtKmipKqEpNNJTIyYiLeztxYWalUT2zmW3Tm72ZGzg5e2voSLnQuh\
nqGsPLmS/n79AThaeJS/3fU3/rzrz6TlprHlsy3mn8dEUi6k8P6L7+OFF7HEkkSS9vi3K77li//7\
gpfufumGv48QjWmsYcO0cUKdVty/9hu2f7mUrONHSD+Yqr1OX1bKoQ3ryDx+BACjzZXvD7D9y6Uk\
rvh/fLZylXYtLy+PuLg4PvjHezz//AwKiou1xxISEgCuGWTS5KGwigALCAggLy+PwMBA8vLy8Pf3\
t/i8+Ph44uPjAeWuosIyb2dvJkZMJOl0EoY6A4sPL8ZQZ8DF3kULMrWq2X9+P6Geoew9v5fDBYcB\
yK3MBcDeVvnPJzU/lR8v/Mjy+5eTdDqJk5+c5J3n3qFOV8dEJjKTmUxgAgkkMI95hBIKQBJJePh5\
8NtPf4udux0lVSXXtS9NzmQU19KUTcRqF2HDrsLaqipqq6vxCe5CTXUV+rJSDm5cx95vVgAQNiCG\
0H4DWb1wPkMemsydD0+hprqK0H4D8Y3szbpP/2X2OROZSEpuCg8/9jheeDGRiWb/oPv000956aWX\
8PHxsfg99q/5hgvnznDP0zPafZOHVUwhjh8/nuXLlwOwfPlyJkyY0MIjsi6W1ohM29VnD5wNYDYN\
F9s5ljCPMHbk7CCjLAOA/n79GdppKG/c/QazB85m3p3zeKrPUwz0H0hxVTGZZZnsP7+fKcOn8Nd/\
/5XAwEBSSCGDDEIJZSlLCSWUDDJIIQV7L3t+++lvCe0WyuLDi2UKUNwyDTcRA1dMCaqNFurUnBp6\
RhtIXbeaMz/uY+/XKziWshkbo/KaLlHR3PfCLH7470rSD6byw39XYrSBvV+vYMOidyk8dZwP5r1K\
UFAQgPYPuvd4j1BCeY/3mMlMJjIRgKCgIJKTky2Gl/o9UtetJvPIYbb9+7Nb9wdmJVpdBTZlyhRS\
UlIoLCwkJCSEhIQE5s6dy2OPPcaSJUvo2rUrX331VUsP06pY2ltlOu2mtr6rFRhASlYK6WXpDA8e\
zpxBc/gu/TsApvScgrezN108urDixApOFJ/gwIUDHLhwgJUnVlJVX0XtxVr+OPyP7HxxJ9/+6VsS\
SGApl6dVEkhAh46Zr8/kt/f+VrsuU4DiVmk45WZpKvFYymbSD6YSNiBGq8S2f7mUOx+eQtiAGNIP\
ptIlKpqa6ip63jUSB2dn7f08/ALw6hTEkIcmc+7wAQB0+bmEDYjhvqlP0m/UvcTFxZGSm8IEJmj/\
oAO0f9AFBgaSnJxMZGSkNm5L+8z0paVcOHeGkb987vb84bVirS7AVqxYYfH6li1bLF4XlzVcz1JZ\
WiNqOO3WcO2ru1d3wjzCmN5vOl5OXhwtPMqOnB242LsQ2zlW6TgsSwfAy9ELXY2OqvoqAAx1Bp76\
91Psfns3Xngxj3lm45zHPGYxi1Wvr+KF0S8QGRl51SnAq30nIZqj4SZi00CzNL1ouik5fOAQamqq\
qa2qor6+jr1fr8DRydmskjv8/XoATu/bzYWMs/SJHUN5YQEjf/kcrh6ehAC/f3Yas/+24Kr/oHty\
8FB83FzNxt0waF09PBn5y2duzR+SFWp1ASau39VOsWjOGpH6Hl5OXuiqdbyy4xU6u3dm7/m9DO00\
lNyKXJ787kl01Tq8HL2I8I4gNT+V/n79KTIU4enkyb4j+0hfmE6dro5YYrVpQ9M1sFhiScpNYkTs\
CGYmzuT50c9bDKgVJ1Zoa3QvRL9wc/6gRLtnGmhqo0ZtVRUOzs5XPGf/2m848L/Ldzj3DgohfOAQ\
Lfh8QkJx9/HDxgayTx7nfNoJvINCKMnN5uSubejLy9j+3Xre/3Zzo/+gW/Z9Mt3f/Tu/fucD7TFp\
1micBFgb0rDSMq1eAIu/bhgaEyMmsv/8fnbk7AAgpyKHnIochgcPJ8o3isWHFwPgbOeMrkbHoE6D\
GBEyQmsGca9118IL0BanU0hBh45ZzDLrQszPy+cvv/wL7v9zv+4uRCFuhBoONdVVWsfhfS/M0hok\
+sSOpqa6CkNZGed+OkhJbjZHt37PmR/3UZKbjYd/ABVFStezu7cPYQNiGPLQZHJPHkNfWsrmb/4f\
i1P2UmaoZiL3X/0fdIYkXv9yFWOnv6RNIzbn+Kn2SAKsDWlYaZlWZA27DdUOw9fvft0sxLydvXn9\
7tdZcWIFhjoDAC72LkzpOQVdtY6k00nkVeYR4h5CVX0V5yvO08m9E/eH3Q/A2tNr8R7pTcGay9sY\
kkgiMDCQt//xNjNfmElSUZLZuKN/Ec24vuMsfqcpPaeYrc0JcT0ariWZUkNCX1ZK/pk00g+msv6f\
bxLUozcD7h2Hq4cndz06lf1rv0F3PpcufaNJ27+H0vw8vINCcHZzp+yCsje1Y3BnRkx96lKn4Fns\
Onhq4QXX/gfd+fx8YmNjWf7mG9w1/qF232V4LRJgbZhpRbbixAqz67tzlb1dL29/mTdHvAmYV2UN\
p+tKqkp4a/9b5FUqm5BPl54GILsiG1BCzlBnIKcyh26PdQPQQszey54nP3iSNbZrCJoTROabmVQX\
K/+DjvllDFWjqkjJSiHMM8zs89Qxq40jQlwvS00bpqGmPmfkL5+jvq6WzCOHyTxymPwzaVo1prbU\
55z8mdL8PLw6BdLtjsF07tOfte++Rn1tLcU5WRzasI7UdcqUY2D3ntw3bChfbdmmjSWJJDoFBLBu\
yTri4+NJyk0yG+uDI+/mp/99TXl2hlklKK4kAdaGmVZkppWMt7M3A/wHsDdvL3vz9mrV1rJjy0gv\
TafQUMicQXMAeGv/W0zvN50PD33I3ry9DAwYSJcOXdiVuwsbbJQW+upiunt15w87/gCALbYEPBRA\
tH80u1bvInhOMEdsjpBZlkmnrp2Y/9/5vPDIC9w16S4+/PuHpGSlENs5Vts4nZKVolWMoGyUblgp\
CtEcltaSTEOtprqKvV+voKa6iqAevck8chivgCDSD6ay9t0F2AABEZE4ODmRefQQXaKiqa+vI3Xd\
ak6n7qW+thaA8pIisi5tbgYoLSxgsI87hkHR/G//IQA8XJx4a/Zv8K+v4tv/reWBX4wnN1fZWzlv\
3jxenj2LDR+9R/rBVI6lbNaqw6tVkO2ZBFg70XB6cUrPKRy8cJC9eXsBOFF0AoCtmVsprSnlSOER\
3OzdyKnMIbs8W+s4rLtYx9nSs1zQXwCgvLacvXl7ya/MR1ejw9nOmbLaMoYGDqVnfE+y+mfh29GX\
zPJM7bip6o7VnPn5DD4+PtretO/Sv2Px4cXa+tuM/jOY0X8GBy8cZEfODpJOJ8lmZXFTmYaaehSU\
jRGi7xuHo5MzPiGhbFj8Ljk/HwUg++ej9Ikdg4uHJx18/TiWsgkA3flcgntFUV5UQNmFfCoKC3Du\
4EFVeRn6kiLsnZwZGRZMVWUl+zKyeX7kYOwry9j+5VK6REXzzpxZzH7zXeLj47UTOO57YRYHN66j\
tqrK7GxGaP4tX9oyCbB2SG3u+OOQP2rVj6HOADbwcMTD/GX3X9BV69BV6wh2C2ZI4BAqaiooqCrg\
cMFh3B3cAfB38aebVzeifKMI9wjnjX1vcGfgnXyb8S219bUcLTyKvbs9uhodQwOHap9n2jyirtOp\
gWWoM9DNS5mCnNJzClN6TjFrPhHielgKANMGiej7xpnt6wI48O1qDGWlOLm54xXQCXsnZ7KO/4Sh\
rJScE0cZ+sgUDGVl5Kefoa66CuPFiwAE94rCt3NXCrPOAUZyfj6Gh38AD4/04+7uObg5OeLbpSuO\
Ls7a8VRL/vYX7n/2eYpystn2788Y+cvncHRyNjuWaugjU8za/qUas5KTOMTNpYZGSlYKT0c9TUpW\
CsuOLaNnx568f/B9DPUGnGydACirKWPlyZXo6/QAONk6UVFbAYCfix/Lji3Dxd6FtWfXUlJdQnF1\
McODh3PgwgFS81Pp0qELAAP8BxDmGXbFobzq4cHqFOeyY8s4UXyCxYcXMytlFitOrLjmHjA5jV5c\
S/jAIYQNiNH2ejVGDTv/rt3w6hRIdWUFTm7u5Px8FBf3Djh36EDctOe569GpePr5cz7tBPlnT1Ne\
WIC7rx+VxcXavrCLdfUEdu9JWHQMnv7+uDk5AlCUdQ6f4C4Edu8JQFlWBvqyUrYu/Zj0g6lsXfqx\
dqCw0Qb2fKPsPXP18JSDfE1IBdbGWdoI3LDdXv25O3e31pRRfbEabydvSqqVUAj1DEVfq+flQS+z\
4IcFZFZkYm9nz/Dg4dzhfwfFVcUY6gwEugaSVZHFQ90eIk+fx7Te0/jPif9oXYqNjU0dR3FVMXvz\
9nIg/wAH8g/gYu/S6PSh3MVZqK5WnZw98INW7VhqjDCt0MIHDiHr+BEiBg/jwrkz6M7nUV9bR5e+\
/ck8opwHWpSdQaeICEoLLtDB14/62lr0pTpsUE7gcHb30KYeAfLSlCn64F5RVBYXk/3zUbJ/PkqX\
vsqB2Nk/H+XgxnX4dw0n88ghLtYp21D6xI7m4MZ1xIybpJ3DKHvDLpMAa+Ms/eV+tVM4YjvH8udd\
fya/Mp/YzrGMCx/HP378B0ajkXnD5mldgh92+JA3fniD8upyduTsQF+r58CFAwwPHs5/z/wXgAv6\
C2SWZ5JVnkVORQ5RvlFXTAe+uvNVbb+Zeriw+pja1diUFno5jV6orrZW1Cd2tHaivNoYYco0FNQj\
pQAyjxzGOyiEnBNHGfiLSdRUVaPXlRDUow/HUjZrlZZnQCCU6nDz9sHDN4CL9XXkpZXh5OaOnYMD\
el0JHv4BdOndlz3frFA6GAfdSfdBwyi9cIHS/DxsjDBowsMU5WRq4wTlXEX1KCv1BBBZB1NIgLVx\
jf3lrlZAauffxIiJjOoyincPvEuQexBdPLowImQEsZ1jtbMQ7w+7n5SsFAb4D9C6BG1sbLT1q2D3\
YDLKMgj1CCXzZCY5FTnaZ5kGFsCOnB0MDx6uhZdp0Dbn1A05jV6orladuHp4ct8Ls8za5htWa2oo\
mB4h1Sm8O7XV1XDHYM6fPsX5S5XU7lVfEBzZiz6xY8g5cYy4adM59P3/tLMU0w+maqdxdOreE72u\
hLDoGKLvG8f5s8peM0dHJ37470ptP1mPu0ZaHKc6ls69+2r3J5P1L4UEWBtn+pd7wyk7NTR25+xm\
7/m9GOoMTOk5BUOdAUOdQTvGyfRkjg3pG0gvS2do4FAm95hMRlkGv47+NZ/89Ak7cnYwe+BsXh36\
Kuml6WSUZWCoM3C44DC7c3eTWZ6pBZaq4fShVFHiRjR2ckXDxw5uXKe1zg+4d5zFMHNwdmbPN0oF\
lH1pStDZvQMdfPzY880KuvTtj+58LkWX9mwdS9lMUI8+AGancZxPO4GDoxPHUjYz5KHJAOjLy0g/\
mIpXQBAludmcPfADPsEhZp+vnpbv0qGD2f3JQLoRQQKsXWlY5ZitOZ3fy8ELB7VmincPvMvQTkOZ\
0X8G94fdT5RvlNZ2H+YRxt68vQwLGsarQ19l6dGlWjWl7ucy1BnYm7eXGf1n4OHooT1uup9LrbxM\
K0DZ6yVuJdOqS70lio2x8alHUCogn+Au2gkc5ZeOjvLv2o3QfneYBd+uVV+SfjCVgG7duevRqejL\
SrF3diL35HEyjxy+fLL9pfWviEFDcfX0tLim1XBcsv5lTgKsHVEDSw2ZiRETtRPoz+jOaPutTM9D\
HBY8jDDPMF6IfsHilGNJVQmGOgMz+s/Q1rhM2+IB5gyaw6BOg64IKNObZqoVnkwFilvJNBAstc5f\
LRhcOnTA1dOT0vw8wgbEED32F5QXFRIxeBjBPZROQvVGl1lHlY3MakC6enji6OSshZdagQ15aLJZ\
+KnvoZ6Mf/bAD1fcgFPORjQnAdaOqNOJS48uNavE1PMPTacXTX/f8PWA1tCx9OhSFh9ezOyBs6+Y\
ClQD6mpdhOpz7/C/A1CCVYhbybSCaRgGloLBNPBM7868ZelHlF3IZ8Oid5ny2ltae/very/fpTn6\
vsvne6qvNdrAucMHSD+YSufefa/4TPXz1IaTq41LKCTA2qHm3B+sue9l+rrYzrHsP7/fYjCZrscl\
nU5iR84OBnUaZHYeohA3W3MrmIaBp66JqXT5uVpXo3pqfV1VNfbOThjKy83W1RyclY3JQx+Zwoip\
T1tsJjGdsuzcu69MFV6DBFgbZ2kf2M3o2jN936u913fp37EjZwdRvlFXdBWarsdJA4dorSzdCFMN\
KaONMk2oHvdkemr99i+Xaifbw5XrV6YdhA3XudTP8wkOuV1f02pJgLVxt2qT742+r2loNSVQ5c7M\
orVwdHLWbrOihtX5s+an1sOVVZTpbVtMW+GlMeP6SYC1cY3d5PJaxzM19jxLVVPD1zR2L6/mVoFy\
2kb71lrO/7PUFdhwg7Rp1Wapimr4Hg2rvNbyXa2B1QTYhg0bmDlzJvX19Tz33HPMnTu3pYdkFRq7\
yeWNHM9kKYDUfWOGOgMvRL9wUzcYyzRj+3Y7T2PXl5VyaMM6jDZolZaqYbV0tY3HjblWxSUnzzed\
VQRYfX09L774Ips2bSIkJIRBgwYxfvx4evfu3dJDszpNDYLWFhhy2kb7djun2Y6lbNYaNdSjm1SW\
mkCa2xhyrefLlGLTWUWA7du3j4iICMLDwwGYPHkya9askQC7Dk0NgusJjMamDIW4Ebdz/5Npy3tL\
hIjs9Wo6qwiwnJwcOnfurP0+JCSEH374oQVHJCyRKkm0Ba4engx7bGpLD0M0gVUEWFMlJiaSmJgI\
QEFBQQuPRgghxK1kFTe0DA4OJisrS/t9dnY2wcHBVzwvPj6e1NRUUlNT8fPzu51DFEIIcZtZRYAN\
GjSItLQ00tPTqampYeXKlYwfP76lhyWEEKIFWcUUor29PR9++CH33nsv9fX1PPPMM/Tp06elhyWE\
EKIFWUWAATzwwAM88MADLT0MIYQQrYRVTCEKIYQQDUmACSGEsEoSYEIIIaySBJgQQgirJAEmhBDC\
KkmACSGEsEoSYEIIIaySBJgQQgirJAEmhBDCKkmACSGEsEoSYEIIIaySBJgQQgirJAEmhBDCKkmA\
CSGEsEoSYEIIIaySBJgQQgirJAEmhBDCKkmACSGEsEoSYEIIIaySBJgQQgir1GoCbNWqVfTp0wdb\
W1tSU1PNHluwYAERERH06NGDjRs3ttAIhRBCtCb2LT0AVVRUFKtXr2b69Olm148fP87KlSs5duwY\
ubm5jB49mlOnTmFnZ9dCIxVCCNEatJoKrFevXvTo0eOK62vWrGHy5Mk4OTkRFhZGREQE+/bta4ER\
CiGEaE1aTYBdTU5ODp07d9Z+HxISQk5OTguOSAghRGtwW6cQR48ezfnz56+4/vrrrzNhwoQbfv/E\
xEQSExMBKCgouOH3E0II0Xrd1gDbvHlzs18THBxMVlaW9vvs7GyCg4MtPjc+Pp74+HgAYmJirm+Q\
QgghrEKrn0IcP348K1eupLq6mvT0dNLS0hg8eHBLD0sIIUQLazUB9t///peQkBD27NnDgw8+yL33\
3gtAnz59eOyxx+jduzf33XcfixYtkg5EIYQQ2BiNRmNLD+JWiImJuWI/mRBCiMZZ09+draYCE0II\
IZpDAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJ\
AkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwIIYRVkgATQghhlSTAhBBCWCUJMCGEEFZJAkwI\
IYRVkgATQohbwFBRw4/fn8NQUdPSQ2mzWk2AzZkzh549e9KvXz8eeughdDqd9tiCBQuIiIigR48e\
bNy4seUGKYQQTfTz7jz2rD7Dz7vzWnoobVarCbAxY8Zw9OhRfvrpJyIjI1mwYAEAx48fZ+XKlRw7\
dowNGzbwwgsvUF9f38KjFUKIxvUaFsidk7rRa1jgNZ8r1dr1aTUBNnbsWOzt7QEYOnQo2dnZAKxZ\
s4bJkyfj5OREWFgYERER7Nu3ryWHKoQQ1+Ti7sgdY7vi4u54zedKtXZ97Ft6AJZ8/vnnPP744wDk\
5OQwdOhQ7bGQkBBycnJaamhCCHHTqVVaU6o1cdltDbDRo0dz/vz5K66//vrrTJgwQfu1vb09U6dO\
bfb7JyYmkpiYCEBBQcGNDVYIIW4TtVoTzXNbA2zz5s2NPr5s2TLWrVvHli1bsLGxASA4OJisrCzt\
OdnZ2QQHB1t8fXx8PPHx8QDExMTcpFELIdozQ0UNP+/Oo9ewwCZNB94OrXFMLaHVrIFt2LCBN998\
k7Vr1+Lq6qpdHz9+PCtXrqS6upr09HTS0tIYPHhwC45UCNGeqOtTR5Kzr9pocbUmDPV6SX5ls5o0\
rtXUIWtmilazBvbrX/+a6upqxowZAyiNHB9//DF9+vThscceo3fv3tjb27No0SLs7OxaeLRCiLai\
YTXT8PfqupShvIb96zOoq65n8C/CzV5bV13P/vUZAGZTgWrQ5JwqIfNosfZ4UVERPj4+V4wl+1we\
F07WUFtdT6qF91PJmpmi1QTY6dOnr/rYq6++yquvvnobRyOEaC/UkAElLBr+Xl2f+uF/ZwHIPaOj\
JL+S9MOFWtDEPBhqsWW+17BADOU15J0ppVOEB2H9fZk/fz6ffvopycnJREZGas89deoUdw8bQUzo\
vbz6hz/RJaojYf19LY5Z1swUrSbAhBDidjNU1FBXXU/Mg6FXVDUNw6hfXAgXzpWRebSYnavSyDxa\
TPSYznSJ6kjk4AC8A9yueH8Xd0eK8yrJP1sGwNzfv0riF+8DEBcXp4XYqVOniIuLo6Aon++K/kXQ\
114M8JzATtIY/VTvdr3O1ZhWswYmhBC328+789i/PgMHJzstJBru31LXowBGP9WbQQ+G4h3oxqAH\
Q3FwtCPzaDHphwuv+hkxD4Ti6efC/oLVWngB5ObmEhcXx/r164mLiyM3N1d7bMnKf7Ij4/+RebSY\
n5KzMVTUsO9/Z/nhf2dls7MJCTAhRLulnpYR1t/3qk0Tpg0Taqgd3qR0RncfHNDoVB9A3ulScrLO\
8+22r82uT2Qi+lw948aNQ5+rZyITzR7fun8NFVWl2HA5aFPXZ7T7xg1TMoUohGi31Grrx+/Pac0W\
6pSd2qChhpM6pWi89NrsUyXkntGRc0JnNtVnuQnkDu6J38B9D4yluLSAiUxkJjOZwAQSSGAe8wgl\
FIAkkggKCuK7dRupK+igfW5ddT1GpHHDlASYEMLq3ei+qF7DArVOQfV9Ni87rnUO9hoWqF3v0qcj\
R1KyyUsrBcDZzZ7Mo8V8vTCVsAF+gFKh1VbX0y8uRHvdz7vhxfveYtGGOaSUpjCBCYQSylKWApBB\
BimkEBQUZNbgoX63vnEhZt9N9oJJgAkh2oCGnYONMf2LX31tr2GBjH6qt0nY5JF5tJguUR3pNSyQ\
AxvPcXhTFvryGgqzyqmurAOgg48z5UVVAJQVVnF4Uxaefi4A2tTfntVnqK1WDiAfN3U4XpEL+N3f\
niOBBC28ABJIQIeO+c+/S+egUG2spkFq+t1+Ss4mdX0GtdX1DLnU1t/eSIAJIaxec/ZFmYYdYBZ8\
akA0fL+MQ0qTRlFWBd4BbuSc0AHQJaojpfkG3Do6Ul5QxUWjkfOnywjp6Y2+oobCnyvoFOFBVUUN\
R7fl4h9t5O3P/oIXXsxjntm45jGPWcxi3sI5ONV2ZNrL92tB6hXgok1lanvPapRQtLmuP7G2QZo4\
hBBWrzknv5ve5uRqtzwxfb8jydmUFhjw8HXGM8CFopwKAIJ7euHq7kj2iRI6dnLnod8N5IHn+zHo\
wVCMGDm2LZf8s2WcP11GUU4l+bosnn9lCnl5ucQSSyihZJDB0zxNBhmEEkossZTqi5j7zrMsf/M7\
wvr70iWqI7p8AztXpWnhtWf1GYwoAdp9cMCt+CO1ClKBCSHaFTWcGk4l/vj9uSvWkwwVNWQeL1Je\
18GBY9uUVvfACE9ssME/zIMuUR0JjPDkx+/PEdbfl+yTJeSdLsWvqxsOjvb4hXpQXFTIP//3O0r1\
ynslkQRACino0DGLWcQSq11XQyyy32ZGP9Vbm0ZUm0pyTpVgA2QeLSataz72Tnbtci1MAkwI0S6Z\
HvPk39XD4tFNPyVnk59eDoCdvS0xD4ZSV1NP+qFCSgsMFGaXU1WhrIdlHi3m2I4cygqUNTF7R3uC\
I70xAp4dOjKs14N8d+Bf2nsnkYSnqw9/+r/3WbL+7yTlJZmN7/6Rj9DrjlB+3p3H3Y92Jz2y0Gx9\
zr+rB3dO6kZtdX2T1//aGgkwIUS7ZNp5GHApDLRzDxusM3XwdcY/1EPrKiwtMGDvaEtVRR1eAS7E\
PBAKoDVbuHd0AiPa+YidIjx4MGYadvY2rPthOQAdPfx48f63iO4RxYYXNjJ69GgKivMBmHz/dO7u\
8hg/rD1LzgmdWaOG6fqc2rbvcKkCa29sjEaj8dpPsz4xMTGkpqa29DCEEK3Y1Q7y1ZfXcHhTFv3H\
dMbB0Y6cUyXkpZUS0tObEVMi2b7iFNknSnB2s6eqso5BD4ZSU1PP2R8LKC+q0roTA8I9sHe0pUNH\
Z07sPk/PYQF8tvIDNv/wX2aMeZMAr84AeAW4UOdWzMyEaYy98yFmzngZB0c7DJeaP4K6e3Lf9L5N\
miK80fZ6a/q7U5o4hBDtVsPmD3VaUe06dHS0w8HJTtvzlX2ihJ2r0vDp7A5AVaVSgRkqlMBTwstJ\
a623d7Ql54SO3DQdABXFNdzd5TH+889veXDqcKJGBuHsZo8u34BtuTd/ePRTxg15mkObsjBeGh9A\
blopPyVnW/wODW+90p5utSIBJoRo10wDoNewQKU1vsBAl6iO9I0LIay/Lx5+zgB4+DqTebSYgowy\
+o/pTEhPb3T5BopyKwEICPdAbWz39HNh5JQeBPf0oqygCg8/Z/qPVl7j6qxMR3bwcdFCcPjjkYR0\
7aSFnw3QNy6EkJ7e2u8taRhYV+usbItkDUwI0a413AStbmgO6+/LkeRscs/oKCuooktUR7wD3Ti8\
KYvctFKCIr3xvVSJeQW4XKrSjJQXVeHkas/dj3fn2M5cSi8YACgrqOJISjbZJ0rIPlGCSwdHs/Ws\
n3fnocs34OHnTHi0n3byxtjn+ph1SzbUcM9ae7rVigSYEKJdu9om6LR9+VoTRkhPb/y7ehA5OIC6\
mnoyDhdyeEsmtVUXAQjs5smdk7qR8ZMy9Vitr+NISrbW1AFK9RbzQCj+XT2wAcL6+5oFk768hg4+\
zpQVVGHveOXp+FfTngKrIQkwIUS71jAA1COaOkV4EDUymJL8SjwDXEhdn0HeGR022FCpu3xqvYef\
s1YthfX3JfmLE2BUbqPSwceZ0/svUK2vo6ywitRvM7RDf9UDhFXqCffQvk/XaA4JMCGEMKGGx/nT\
ZdjZK00YxnqlWTvnhI5OER74dXVHX1qNe0dnRk3rDcAP/zuLDRAc6U3q+gwyjxVTXlRFtb6O4J5e\
2GCjbUa+Y2zXKyq/2up66mrqcXC0o29cyBXjksN7ryQBJoQQJtTwMAJ1NfXknNDhH+pBcKS3dvuU\
Oyd1MzvNo7a6XtsIHdTdk5gHQ7WTMrwCXBgyPpysY8V06uZptlZlesr9tQ7kNd14LXdpVrSaAPvz\
n//MmjVrsLW1xd/fn2XLlhEUFITRaGTmzJl8++23uLq6smzZMu64446WHq4Qoo1ycXdk8KUwMVTU\
aM0W6j6xn5KzqauupyS/kp2r0sg8WsygB0MJ7ul1qWW+lK59fek1LJC8M6Vknyhh39p0sk+UcOek\
bmbvc/7S43DtUzQa3vKlva57mWo1bfRz5szhp59+4tChQ4wbN46//vWvAHz33XekpaWRlpZGYmIi\
M2bMaOGRCiGsScN9Us3RcJ+Yi7sjDk527F+foYVXSE/luKiRU3oQ82Aogx4M1QKvUzdPAHw6u5u1\
tv+8O4/U9RlknyjRbtnSlLGMfqp3u2mRb4pWU4F5eHhov66srMTGRpmJXrNmDU8++SQ2NjYMHToU\
nU5HXl4egYHy/0AhxLU1515hTaGGR1h/X9IjC6mrrmf/+gwcnOzMpgENFTXYADEPhtKvwc0oew0L\
xFBeQ2FWBXc/2r3J04HtuePQklYTYACvvvoq//rXv/D09CQ5ORmAnJwcOnfurD0nJCSEnJwcCTAh\
RJM0515hTWEaIt5j3TBU1GinwZv6eXce+9dnaNOGDRXnVZJ9ooT0w4V4j3Wz+FnSuNG42zqFOHr0\
aKKioq74vzVr1gDw+uuvk5WVxdSpU/nwww+b/f6JiYnExMQQExNDQUHBzR6+EMIKNedeYTfz/Rs7\
EeNIcrY2/Wj6eHs+Fup63NYKbPPmzU163tSpU3nggQdISEggODiYrKzL+yOys7MJDg62+Lr4+Hji\
4+MB5UBKIYRoiltR6Via7lM/p+bSKfedunmafV7D6c6bXT22Na2miSMtLU379Zo1a+jZsycA48eP\
51//+hdGo5G9e/fi6ekp04dCiJuqKZXOjTSDNPwcR0c77pzUjX4N9ns1rNpudfVo7VrNGtjcuXM5\
efIktra2dO3alY8//hiABx54gG+//ZaIiAhcXV1ZunRpC49UCNHWNKXSuRnNIA3v5dWQNGk0j9wP\
TAghmqC9NFRY09+draYCE0KI1kyqo9an1ayBCSGEEM0hASaEEMIqSYAJIYSwShJgQgghrJIEmBBC\
CKskASaEEMIqSYAJIYSwSm12I7Ovry+hoaG37fMKCgrw8/O7bZ93K8l3ab3a0veR79I6ZWRkUFhY\
2NLDaJI2G2C3mzXtXr8W+S6tV1v6PvJdxI2SKUQhhBBWSQJMCCGEVZIAu0nU+5C1BfJdWq+29H3k\
u4gbJWtgQgghrJJUYEIIIaySBNgNmDNnDj179qRfv3489NBD6HQ67bEFCxYQERFBjx492LhxY8sN\
shlWrVpFnz59sLW1vaKjyhq/z4YNG+jRowcREREsXLiwpYfTLM888wz+/v5ERUVp14qLixkzZgzd\
u3dnzJgxlJSUtOAImy4rK4u4uDh69+5Nnz59eP/99wHr/T5VVVUMHjyY/v3706dPH+bNmwdAeno6\
Q4YMISIigscff5yamuu/c7NoIqO4bhs3bjTW1tYajUaj8eWXXza+/PLLRqPRaDx27JixX79+xqqq\
KuPZs2eN4eHhxrq6upYcapMcP37ceOLECePIkSON+/fv165b4/epq6szhoeHG8+cOWOsrq429uvX\
z3js2LGWHlaTbdu2zXjgwAFjnz59tGtz5swxLliwwGg0Go0LFizQ/ntr7XJzc40HDhwwGo1GY1lZ\
mbF79+7GY8eOWe33uXjxorG8vNxoNBqNNTU1xsGDBxv37NljfPTRR40rVqwwGo1G4/Tp040fffRR\
Sw6zXZAK7AaMHTsWe3vlnqBDhw4lOzsbgDVr1jB58mScnJwICwsjIiKCffv2teRQm6RXr1706NHj\
iuvW+H327dtHREQE4eHhODo6MnnyZNasWdPSw2qyESNG0LFjR7Nra9asYdq0aQBMmzaNpKSkFhhZ\
8wUGBnLHHXcA0KFDB3r16kVOTo7Vfh8bGxvc3d0BqK2tpba2FhsbG7Zu3cojjzwCWNf3sWYSYDfJ\
559/zv333w9ATk4OnTt31h4LCQkhJyenpYZ2w6zx+1jjmK8lPz+fwMBAADp16kR+fn4Lj6j5MjIy\
OHjwIEOGDLHq71NfX090dDT+/v6MGTOGbt264eXlpf2Dti3892YN7Ft6AK3d6NGjOX/+/BXXX3/9\
dSZMmKD92t7enqlTp97u4TVbU76PaP1sbGywsbFp6WE0S0VFBQ8//DD/+Mc/8PDwMHvM2r6PnZ0d\
hw4dQqfT8dBDD3HixImWHlK7JAF2DZs3b2708WXLlrFu3Tq2bNmi/Q8wODiYrKws7TnZ2dkEBwff\
0nE21bW+jyWt+ftcjTWO+VoCAgLIy8sjMDCQvLw8/P39W3pITVZbW8vDDz/M1KlTmTRpEmDd30fl\
5eVFXFwce/bsQafTUVdXh729fZv4780ayBTiDdiwYQNvvvkma9euxdXVVbs+fvx4Vq5cSXV1Nenp\
6aSlpTF48OAWHOmNscbvM2jQINLS0khPT6empoaVK1cyfvz4lh7WDRk/fjzLly8HYPny5VZTMRuN\
Rp599ll69erF7NmztevW+n0KCgq0jmODwcCmTZvo1asXcXFxfP3114B1fR+r1tJdJNasW7duxpCQ\
EGP//v2N/fv3N06fPl177LXXXjOGh4cbIyMjjd9++20LjrLpVq9ebQwODjY6Ojoa/f39jWPHjtUe\
s8bvs379emP37t2N4eHhxtdee62lh9MskydPNnbq1Mlob29vDA4ONn722WfGwsJC4z333GOMiIgw\
jho1ylhUVNTSw2ySHTt2GAFj3759tf+trF+/3mq/z+HDh43R0dHGvn37Gvv06WNMSEgwGo1G45kz\
Z4yDBg0yduvWzfjII48Yq6qqWnikbZ+cxCGEEMIqyRSiEEIIqyQBJoQQwipJgAkhhLBKEmBCCCGs\
kgSYEEIIqyQBJoQQwipJgAkhhLBKEmBCCCGskgSYEEIIqyQBJoQQwipJgAkhhLBKEmBCCCGskgSY\
EEIIqyQBJoQQwipJgAkhhLBKEmBCCCGskgSYEEIIqyQBJoQQwipJgAkhhLBKEmBCCCGs0v8HuVN4\
ZSD5FvUAAAAASUVORK5CYII=\
"
  frames[1] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAAkjElEQVR4nO3dfXxU5Z338c/IECQgBsJTJqEmIQxCMKAGUGs1U4hW5YYoinjT\
u4iUsatuWeytsrvdJtnVwrrbontXWwcVae0LXmoxWGhhRRLrAxpDBSoUJmKyJpmQhiFBIYQ8cO4/\
4gwZEyCEJGfOzPf9TzLXOWfyO7yY+c51netcYzMMw0BERMRiLjK7ABERke5QgImIiCUpwERExJIU\
YCIiYkkKMBERsSQFmIiIWJICTERELEkBJiIilqQAExERS1KAiYiIJSnARETEkhRgIiJiSQowERGx\
JAWYiIhYkgJMREQsSQEmIiKWpAATERFLUoCJiIglKcBERMSSFGAiImJJCjAREbEkBZiIiFiSAkxE\
RCxJASYiIpakABMREUtSgImIiCUpwERExJIUYCIiYkkKMBERsSQFmIiIWJICTERELEkBJiIilqQA\
ExERS1KAiYiIJdnNLqC3DB8+nOTkZLPLEBGxlPLycg4fPmx2GV0SsQGWnJxMSUmJ2WWIiFhKZmam\
2SV0mYYQRUTEkhRg0mV+v/+82kVEepMCTLokLy+PjIwMvF5vSLvX6yUjI4O8vDxzChORqKUAk3PK\
y8sjPz8fn8+Hy+UKhpjX68XlcuHz+cjPz1eIiUifUoDJWQXCKyAQYps3bw6GV4BCTET6kgJMzsjv\
97N69eqQthxyaPA1MGvWLBp8DeSQE7J99erVuiYmIn1CARaNjvvhvafbfp5FfHw8hYWFOBwOoC28\
lrKUVawimWRWsYqlLA2GmMPhoLCwkPj4+N4+AxERBVhU2vUyvPmTtp/n4HQ6Kdy8AcfwSymiiHLK\
SSaZNawhmWTKKaeIomB4OZ3OPjgBEREFWHQJ9Lyct0L2v8KU73bc1kmvzHn8AzzZTdRTTz75Idvy\
yaeeejwej8JLRPqUAiyaBHpe3j/AN5fCoHioLYXf3gXvPXXGXpl30DW434whjjhyyQ3ZlksuccTh\
drs7TLEXEelNCrBoMuW7p3tex/1QuALWzYPS/4b9m+DG5ae3fdUb83q9uG67A9/ho2SRFRw2XMSi\
4HBiFlkdptiLiPS2iF0LUToxKL6t53XcD7+7Dz4ramsfcCkc+Qxq9sKamyHlRvjoefxHj+Na8kxw\
qnwBBQAUUUQ99SxjGVlkBdsDIbZnzx5N5BBpx+/3d/qaOFO7dI16YNFo18unw6v/YDj5Rdvv+38P\
h0th50tw6RjiTx1hycIFIYcWUECsI5ZNmzYR64gNhlfAkiVL9IIUaUer2PQeBVg0mvJdSM1q+735\
GGCEbj/VAkcroGQ1edlx5Oaevu4VmG142223hUyxB8jNzdWLUaQdrWLTuzSEGE2O+6H4OcAGY755\
uhd2Nk0nyMv7N6DtJuX2U+WdTieFhYW4XC6WLFmiF6FIO2daxcbj8eB2uzusYhM4RrrOZhiGce7d\
rCczM1PfB9becT8U/KBtwgaA7SIwTp37uBsfA9c/ARrHF+kqv99PRkZGSEjlkBO8fhxHXMj1Y2gb\
3QiH68dWeu/UEGK02PVyW3hd9s224cNrl577GPtAmHZ/8OGZXlhmv+BEwo1WsekbYRVgjY2NTJs2\
jcmTJ5Oenh689lJWVsb06dNJS0vj7rvvpqmpyeRKLaL9zcmBKfTJ17cNHe76zdmPtQ+ESXOh4Ujb\
dPvCn55z6SkROS0wxO5wOLSKTS8JqwAbMGAA27dvZ/fu3ezatYstW7bwwQcf8Nhjj7Fs2TI+/fRT\
hg4dygsvvGB2qdbQfsmowBT6affD8HHQcBgujoNLvpqEYet/+rjULPjmD9uO++9/grdXwtv/3qWl\
p0TkNKfTicfj6fYqNkeON/Hc2wc5clwf2jsTVgFms9kYPHgwAM3NzTQ3N2Oz2di+fTt33nknAAsX\
LqSgoMDEKi2k/Y3LAYPi4e51bSHWWA8jvnrhGM1twXXjcpj7YlvQZf8r3PTTtrYbHwt9HhE5J6/X\
i9vt7vYqNq+WVLDij/t5taSiL8q1nLCbhdja2srVV1/Np59+yoMPPsjYsWOJi4vDbm8rNSkpiaqq\
KpOrtIhAr+vrRoyDRVuh2APNx2F0BvSPhWnutmMCAse6/rFv6hWJIO2nyueQExw2zCefXHKDq9gU\
+ApwuVydDiPelTkm5KeECqseGEC/fv3YtWsXlZWVFBcXs3///i4f6/F4yMzMJDMzk9ra2l6s0kIC\
18FqS0MX6x0UDzGx8P7/g/4D29qKn9N1LpGz+PqQXuDxwdpjPPrqbibnb+XRV3ezfddBrpx+fcgq\
Nk/zNMtYRjnlLGMZT/N0h1Vsvv5desMGxXD/jWMZNiimT8/TKsKuBxYQFxeHy+Vix44d1NfX09LS\
gt1up7KyksTExE6PcbvduN1uoG0qqHD6Olj5u6en0Ad6VoEhwaaGtutcADGDOu+1iUS5g7XHcP+6\
hIO1x1lX/DnTUobx9oFaar48yZNb9tP61Q1Jr+yspOBj6J8+E95bFzy+gAIcDgcve17G7XZT4CsI\
eX6tYnP+wirAamtr6d+/P3FxcZw4cYI333yTxx57DJfLxWuvvcb8+fNZu3Ytc+bMMbtU6wiElPPW\
thmIX78eFlgbEQBD17lEvnLkeBOvllRwV+YYhg2K4ScFn3Cw9jgA5f4Gyv0NwX1bv3Y3bdMpiLu+\
bRm2o1+FWGzciOAwYWABgEAPTavYdE9YBVh1dTULFy6ktbWVU6dOMW/ePGbNmsXEiROZP38+P/7x\
j7nyyitZvHix2aVaR/vrYCPO0LMaFK/rXCJfs/b9cp5+q5SGplaWZTsZO3Iw7x3s+hD7QLsNrl/A\
5KRLKdn6Ozb+catWselhYRVgGRkZfPzxxx3aU1NTKS4uNqEiiznubxsynPLd0MkYItINRujP81yz\
qKnVwP2tFH7wL09jND7eYXjQ6XSGxcobVhZWASYXKHC9C3QdS+QCLbwuhdgYe3AG4MHaY1067vbJ\
o/n9X2poOWVQ+rdjbRMwzvCBUuF1YRRgkSRw/UrXsUQuWGAGYMC/5kziH3+3B5vNxnenf4N/27SP\
oyeaedA1lg0f+7hh3Aj+IdvJsEExPDTzGI9v2sePZ0008QwinxbzFRGRICu9d4bdfWAiIiJdoQAT\
ERFLUoCJiIglKcBERMSSFGAiImJJCjAREbEkBZiIiFiSAkxERCxJASYiIpakAOtFX/9yus7aW+rq\
8L/wAi11dX1VlohIRFCA9ZK8vDwyMjLwer0h7V6vl4yMjODXJxzdsIG//cd/cnTDBhOqFBGxLi3m\
2wvy8vLIz88HwOVyBb/Ezuv1Br/ELrD9x0vbVo2/9I47TKtXRMSKFGA9rH14Afh8PlwuFx6PB7fb\
HfwGViC4n77MTkTk/GkIsRPdvS7l9/tZvXp1SFsOOTT4Gpg1axYNvgZyyAnZvnr16jNeKxMRkTNT\
gHWiu9el4uPjKSwsxOFwAG3htZSlrGIVySSzilUsZWkwxBwOB4WFhfpSOxGRbtAQYicC16O6c13K\
6XRSWFiIy+WiyFfEHOaQTDJrWANAOeUUURQML6fT2aO1i4hEC/XAOmEfOpT4xYuxDx3areOdTice\
j4d66sknP2RbPvnUU4/H41F4iYhcAAVYL/B6vbjdbuKII5fckG255BJHHG63u8MUexER6ToFWA9r\
P1U+iyySSaacchaxiHLKSSaZLLKCsxMVYiIi3aNrYBegpa6Ooxs2cOkdd2AfOhS/3x8ML4ACCgAo\
ooh66lnGMrLICrYHQmzPnj2ayCEicp4UYF3QUldH3csv03zoEMc+/JDB06fTf/RoTjU2cuSFFzn2\
7nsk/vxnxMfHs2TJkpD7wAoowOFw8LLnZdxuNwW+gpDnXrJkicJLRKQbwirAKioq+N73vkdNTQ02\
mw23283SpUs5cuQId999N+Xl5SQnJ/PKK68wtJsTLM5XS10dVT/6EQ3v7wi2Ha38anp9v34ANOzY\
Qd3Lv2XE3z8UvCk5EGLtZxsGZicGemi5ubm6iVlEpJvC6hqY3W7nZz/7Gfv27eODDz7gmWeeYd++\
faxcuZIZM2ZQWlrKjBkzWLlyZZ/U01JXh2/58pDwCtHaGvy1fvPm4I3PeXl55ObmdpgqHwgxh8Oh\
8BIRuUA2wzAMs4s4kzlz5vDQQw/x0EMPUVRUREJCAtXV1WRlZXHgwIGzHpuZmUlJScl5/82TZWXU\
rFzJqOXL+WLTZg4/8wz9Ro+mtaYGzvFPNWBSOv0uGcLon/wLA1JS8Pv9nQ4PnqldRMRs3X3vNENY\
DSG2V15ezscff8z06dOpqakhISEBgNGjR1NTU9Nrf7dm5UqOv/0naoCBkyYB0HroUJeOPfnJXgA+\
X/x9Un732hlDSuElInLhwjLAjh07xty5c3nqqacYMmRIyDabzYbNZuv0OI/Hg8fjAaC2trZbf3vU\
8uXUfPWzX1wcYKNuwwZaq6u7/BwtPh9HN2wgfvHibtUgIiLnFlbXwACam5uZO3cuCxYs4I6vlnIa\
NWoU1V8FSHV1NSNHjuz0WLfbTUlJCSUlJYwYMaJbf39ASgrfeO45BqSkAHBR7EAGXXft6R1iYk7/\
3r8/AP0SEhg4dSq2r/6m3eHQ16OIiPSysAowwzBYvHgxEyZM4OGHHw62z549m7Vr1wKwdu1a5syZ\
0yf1BBb1bfG1hWfstdeSurGA4Q8+yPAHH+Cy3/yaQTfewGUvvkDyb35Nyq/XMujGG/jGC893exkq\
ERHpmrCaxPHuu+/yrW99iyuuuIKLLmrL1p/+9KdMnz6defPm8fnnn3PZZZfxyiuvMGzYsLM+V09c\
iAzcqDz429/m2PbtwRuWu3pcV/cXEQkXmsTRTddffz1nytO33nqrj6s5vagvwIDzuJ4V6LkBug4m\
ItJLwirAIsWFfB2LiIh0jQKsF7TvuYmISO8Iq0kcIiIiXaUAExERS1KAiYiIJSnARETEkhRgIiJi\
SQowERGxJAWYiIhYkgJMREQsSQEmIiKWpAATERFLUoCJiIglKcBERMSSFGAiImJJCjAREbEkBZiI\
iFiSAkxERCxJASYiIpakABMREUtSgImIiCUpwERExJIUYBGqrrGONZ+soa6xzuxSRER6RdgF2H33\
3cfIkSOZNGlSsO3IkSNkZ2czbtw4srOzqavTm/K5FHxawM93/pyCTwvOul93g87v959Xu4hITwu7\
ALv33nvZsmVLSNvKlSuZMWMGpaWlzJgxg5UrV5pUnXXkpOXw8NUPk5OWc9b9uhp07eXl5ZGRkYHX\
6w1p93q9ZGRkkJeXd/4Fi4icLyMMlZWVGenp6cHHTqfT8Pl8hmEYhs/nM5xO5zmf4+qrr+61+iLJ\
kRNHjBf/8qJx5MSRLu2fm5trAAZgOBwO48CBA4ZhGMaBAwcMh8MR3Jabm9uLVYuIYRjG4cOHz6u9\
K6z03hl2PbDO1NTUkJCQAMDo0aOpqakxuaLIMfTioSyatIihFw895755eXnk5+cHH/t8PlwuF5s3\
b8blcuHz+YLb8vPz1RMT6UUaCQnDIcRzsdls2Gy2Trd5PB4yMzPJzMyktra2jyuLbH6/n9WrV4e0\
5ZBDg6+BWbNm0eBrIIeckO2rV6/WNTGRXhD4MBn4EBkIMa/XG/wwGRUfIs3uAnZGQ4jhqf0wYQ45\
RiGFxhrWGMkkG2tYYxRSaOSQ02F4UUR6TvthfNoN52/atClkGJ9uDudb6b3TEj2w2bNns3btWgDW\
rl3LnDlzTK4oOjmdTgoLC3E4HBRRRDnlJJPMGtaQTDLllFNEEQ6Hg8LCQpxOp9kli0QUjYSECrsA\
u+eee7j22ms5cOAASUlJvPDCCyxfvpw333yTcePGsW3bNpYvX252mVHL6XTi8Xiop5588kO25ZNP\
PfV4PB6Fl0gviI+PD36IhLbwWspSVrGKZJJZxSqWsjQYYoEPk/Hx8SZW3XtshmEYZhfRGzIzMykp\
KTG7jIgTGGNv8DUEXzQB5ZSzjGXEOmLVAxPpRb35OrTSe2fY9cDEXHWNdTy761me3fVsh5ubvV4v\
WVlZ+Hw+ssgKDhsuYlFwODGLrA4XlkXk/DR8cZSP3vgdDV8c7XS7RkLaqAcWxeoa6yj4tICctJzg\
NPpndz3LL3f/EoDJIyZz5cgrAWg91spT//spqqurg8fnkEMRRdRTTxxxZJFFAQXB7Q6Hgz179kTs\
8IVIb3n/ld+y43fruHbuPVw3b0GH7eqBtVEPLAoFlo9at39dcBWOusY6fvLuT3j+L88H99tdu5uX\
9r7ES3tf4jf/8xum5kwNeZ4CChiYMJClzyxlYMLAkPACWLJkicJLpIv8VZVsWJmHv6oS46s7hQxb\
x97Y7j/v5Pprr9VICOqBRaVAL+ve9HsZdvEwxsWNY/k7yzna1PlwhQ0bd4+/m6EXD+XA+gP817//\
F0DIbMP2958A5ObmRv49KCLd1PDFUfYWbSM9ayaxQy4F4NXH/5nP/7KbS4aPYMjwUQweFk/NZ6Uk\
Xp7O3qI3+cakKVzzfxZz9dWZ1B45Enyunh4JsdJ7p3pgUWz/kf1cNfIqfvjWD88YXgAGBuVHy/nl\
7l9SNr2MEXNGEDcyLmR4ov0Ue4WXyNntLdrGn367hr1F24I9rEviRwLw5eFaqvZ/woH336b+kI+/\
/mk7AJ9/sov31jzHrTdcH/JcBRQQ64hl06ZNxDpio2okRD2wKFTXWMejf3qUD6o/ICE2geqG6nMe\
M3nEZOwX2dlZs5NrRl/D8knLGZs4tsN+fr8/Yl8sIj0l0ANLvXo629f8ks//spv+A2NpPtHQpeP3\
nophzWuvAz0/EmKl9071wKLQ0IuHBidnnOJUl47ZXbsbG20D81eOurLT8AIUXiJdEDvkUqbOnstn\
Oz/k87/sBuhyeI24LJXrEuJZ9uCDHRYNiLaRELvZBYg57rn8HgbaB+I75mP9gfVn3XeQfRDHW47T\
fKqZv5v8d9xz+T19VKVIZGrfA2s4epSqA/s4XPl5l0LsS38tjce+JG3IpXzw3ruMSU4J2e50OqNm\
9q96YFEmMAMRYNGkRTww5QHuTb+XzFGZ3Jp8K4PtgwEY1G9Q8JjjLceBtl7YQPvALq1cLyJnFrgG\
tv/9t/lb+WdUl+5n4rdc9B8Ye9bjhiYkMjbzGi4efAknvjjKoT1/7nS/aAgvUA8s6gS+wBIIfo3K\
jzJ/FNxe11jHP7/7z7xT9Q4AmaMymTS87duxB9oHnvMLMkXk3NKzZgLQ3NjI55/sAiD2kiEseOLn\
vPR/H4BTHYf240YnkJo5nZ2/38A1d95DzICLg88TrRRgUSZrTBYfHfqIrDFZnW4fevFQnrj+Cdbt\
Xwe0DTWqxyXSswLXwAL3dhk2mHLzLPYWbYNTp7APuJiWk40MdSRx8w/+gQ9fX0/ZxyX0HzCAGxYs\
Cpl+H80UYFGmqKKId6reYeroqaRcmtLpPkMvHsoDUx7o28JEolDskEtDVtoI9KhSr57OZzs/DAbV\
dx5Y1uG+MVGARZ3AEKCGAkXCT6BnBhCfmNRpu5ymAIsyQy8eyqJJi8wuQ0TkgmkWoohIN5xrxXjp\
fQowEZFuaL8clJhDQ4giIt0QmHAR7VPZzaQAExHpBk2sMJ+GEEVExJIUYCIiYkkKMBERsSQFmIiI\
WJICTERELMkyAbZlyxbGjx9PWloaK1euNLscERExmSUCrLW1lQcffJA//vGP7Nu3j3Xr1rFv3z6z\
yxIRERNZIsCKi4tJS0sjNTWVmJgY5s+fz8aNG80uS0RETGSJAKuqqmLMmDHBx0lJSVRVVZlYkYiI\
mC2iVuLweDx4PB4AamtrTa5GRER6kyV6YImJiVRUVAQfV1ZWkpiY2GE/t9tNSUkJJSUljBgxoi9L\
FBGRPmaJAJs6dSqlpaWUlZXR1NTE+vXrmT17ttlliYiIiSwxhGi32/nFL37BzTffTGtrK/fddx/p\
6elmlyUiIiayRIAB3Hrrrdx6661mlyEiImHCEkOIIiIiX6cAExERS1KAiYiIJSnARETEkhRgIiJi\
SQowERGxJAWYiIhYkgJMREQsSQEmIiKWpAATERFLUoCJiIglKcBERMSSFGAiImJJCjAREbEkBZiI\
iFiSAkxERCxJASYiIpakABMREUtSgImIiCUpwERExJIUYCIiYkkKMBERsSQFmIiIWJICTERELCls\
AuzVV18lPT2diy66iJKSkpBtK1asIC0tjfHjx7N161aTKhQRkXBiN7uAgEmTJrFhwwbuv//+kPZ9\
+/axfv169u7di8/nY+bMmXi9Xvr162dSpSIiEg7Cpgc2YcIExo8f36F948aNzJ8/nwEDBpCSkkJa\
WhrFxcUmVCgiIuEkbALsTKqqqhgzZkzwcVJSElVVVSZWJCIi4aBPhxBnzpzJoUOHOrQ/8cQTzJkz\
54Kf3+Px4PF4AKitrb3g5xMRkfDVpwG2bdu28z4mMTGRioqK4OPKykoSExM73dftduN2uwHIzMzs\
XpEiImIJYT+EOHv2bNavX8/JkycpKyujtLSUadOmmV2WiIiYLGwC7PXXXycpKYkdO3Zw2223cfPN\
NwOQnp7OvHnzmDhxIt/5znd45plnNANRRESwGYZhmF1Eb8jMzOxwP5mIiJydld47w6YHJiIicj4U\
YCIiYkkKMBERsSQFmIiIWJICTERELEkBJiIilqQAExERS1KAiYiIJSnARETEkhRgIiJiSQowERGx\
JAWYiIhYkgJMREQsSQEmIiKWpAATERFLUoCJiIglKcBERMSSFGAiImJJCjAREbEkBZiIiFiSAkxE\
RCxJASYiIpakABMREUsKmwB75JFHuPzyy8nIyOD222+nvr4+uG3FihWkpaUxfvx4tm7dal6RIiIS\
NsImwLKzs/nkk0/Ys2cPTqeTFStWALBv3z7Wr1/P3r172bJlCw888ACtra0mVysiImYLmwC76aab\
sNvtAFxzzTVUVlYCsHHjRubPn8+AAQNISUkhLS2N4uJiM0sVEZEwEDYB1t6LL77ILbfcAkBVVRVj\
xowJbktKSqKqqsqs0kREJEzY+/KPzZw5k0OHDnVof+KJJ5gzZ07wd7vdzoIFC877+T0eDx6PB4Da\
2toLK1ZERMJanwbYtm3bzrr9pZdeYtOmTbz11lvYbDYAEhMTqaioCO5TWVlJYmJip8e73W7cbjcA\
mZmZPVS1iIiEo7AZQtyyZQtPPvkkb7zxBrGxscH22bNns379ek6ePElZWRmlpaVMmzbNxEpFRCQc\
9GkP7GweeughTp48SXZ2NtA2keNXv/oV6enpzJs3j4kTJ2K323nmmWfo16+fydWKiIjZbIZhGGYX\
0RsyMzMpKSkxuwwREUux0ntn2AwhiohI9/n9/vNqjwQKMBERi8vLyyMjIwOv1xvS7vV6ycjIIC8v\
z5zCepkCTETEwvLy8sjPz8fn8+FyuYIh5vV6cblc+Hw+8vPzIzLEFGAiIhYVCK+AQIht3rw5GF4B\
kRhiCjAREQvy+/2sXr06pC2HHBp8DcyaNYsGXwM55IRsX716dURdE1OAiYhYUHx8PIWFhTgcDqAt\
vJaylFWsIplkVrGKpSwNhpjD4aCwsJD4+HgTq+5ZCjAREYs5cayJP//3/zDGkRwMsSKKKKecZJJZ\
wxqSSaaccoooCoaX0+k0u/QepQATEbGYv75fzY4NB/nr+9U4nU48Hg/11JNPfsh++eRTTz0ejyfi\
wgsUYCIipgv0qE4cazprW8CE6xK49o6xpEwezusvFrLk+0uII45cckP2yyWXOOJwu90dpthHAgWY\
iIjJ2veoAj78/Wfs2HCQD3//GSeONfHh7z+j+KvfBw6O4aqbLqNoczGL/v4uqg9Vk0VWcNhwEYuC\
w4lZZHWYYh8pwmYtRBGRaHLiWBN/fb+aCdclMOG6BE582UTFviOkTB5O4/Fm9r3TNgV+79s+9r9f\
TWuzETzOHtOPg/srePS/FnC0oW1WYQEFABRRRD31LGMZWWQF2wMhtmfPnoiZyKEemIiICb7e6yrf\
c5jK/XW88fQuXv/PP2OcOr1vILwAPnnbx643K/iyAqan3RLynAUUEOuIZdOmTcQ6YoPhFbBkyZKI\
CS9QgImImCJwHWvCdQn89f1q6mtOMCDWzrEjJ0PC62xunfo93N9dGnw8In4Uf9y0ldtuuy1kij1A\
bm5uxN3IrCFEERETBK5jQVuYtZxs5a8fVHOyoaXLz3Fp/MU899RTJIyN49lf/IofzPx3WmovAcDp\
dFJYWIjL5WLJkiURF16gABMRMd3AwTHYB/TjmP/keR03ZmLbcGBeXh7fX3Q/fzvQxITrEoLbnU5n\
RF3z+joNIYqIhIGUycOJGzUQAFsX3plHpQ7BHtMvOM0+6bIErrrpMgYOjgnZL1LDCxRgIiJhoWz3\
YeprTvCNScO4J3c6CWmXBrdde8dYJmePAWBUyiVMyR5D/5h+lGwuD5l6H200hCgiEgYCQ38Trktg\
4OAYbvnBFewprMTWblvsJTHBSR+V++v4xqRhIUOG0UYBJiISBtpP6gg8nv6/UkP2aT/pI/Dz60OG\
0UQBJiJiMV8Pu2ila2AiImJJCjAREbEkBZiIiFiSAkxERCxJASYiIpakABMREUtSgImIiCXZDMMw\
zr2b9QwfPpzk5OQO7bW1tYwYMaLvCzJJNJ2vzjUyRdO5gvnnW15ezuHDh037++cjYgPsTDIzMykp\
KTG7jD4TTeerc41M0XSuEH3neyE0hCgiIpakABMREUuKugBzu91ml9Cnoul8da6RKZrOFaLvfC9E\
1F0DExGRyBB1PTAREYkMURNgjzzyCJdffjkZGRncfvvt1NfXB7etWLGCtLQ0xo8fz9atW80rsoe8\
+uqrpKenc9FFF3WYzRRp5wqwZcsWxo8fT1paGitXrjS7nB533333MXLkSCZNmhRsO3LkCNnZ2Ywb\
N47s7Gzq6upMrLDnVFRU4HK5mDhxIunp6Tz99NNAZJ5vY2Mj06ZNY/LkyaSnp5ObmwtAWVkZ06dP\
Jy0tjbvvvpumpiaTKw1jRpTYunWr0dzcbBiGYTz66KPGo48+ahiGYezdu9fIyMgwGhsbjc8++8xI\
TU01WlpazCz1gu3bt8/Yv3+/ceONNxofffRRsD0Sz7WlpcVITU01Dh48aJw8edLIyMgw9u7da3ZZ\
Pertt982du7caaSnpwfbHnnkEWPFihWGYRjGihUrgv+frc7n8xk7d+40DMMwvvjiC2PcuHHG3r17\
I/J8T506ZXz55ZeGYRhGU1OTMW3aNGPHjh3GXXfdZaxbt84wDMO4//77jWeffdbMMsNa1PTAbrrp\
Juz2tu/vvOaaa6isrARg48aNzJ8/nwEDBpCSkkJaWhrFxcVmlnrBJkyYwPjx4zu0R+K5FhcXk5aW\
RmpqKjExMcyfP5+NGzeaXVaPuuGGGxg2bFhI28aNG1m4cCEACxcupKCgwITKel5CQgJXXXUVAJdc\
cgkTJkygqqoqIs/XZrMxePBgAJqbm2lubsZms7F9+3buvPNOIHLOtbdETYC19+KLL3LLLbcAUFVV\
xZgxY4LbkpKSqKqqMqu0XhWJ5xqJ59QVNTU1JCS0fa386NGjqampMbminldeXs7HH3/M9OnTI/Z8\
W1tbmTJlCiNHjiQ7O5uxY8cSFxcX/LAdLf+fu8tudgE9aebMmRw6dKhD+xNPPMGcOXOCv9vtdhYs\
WNDX5fWorpyrRAebzYbNZjO7jB517Ngx5s6dy1NPPcWQIUNCtkXS+fbr149du3ZRX1/P7bffzv79\
+80uyVIiKsC2bdt21u0vvfQSmzZt4q233gq+ABITE6moqAjuU1lZSWJiYq/W2RPOda6dseq5nk0k\
nlNXjBo1iurqahISEqiurmbkyJFml9RjmpubmTt3LgsWLOCOO+4AIvt8AeLi4nC5XOzYsYP6+npa\
Wlqw2+1R8/+5u6JmCHHLli08+eSTvPHGG8TGxgbbZ8+ezfr16zl58iRlZWWUlpYybdo0EyvtPZF4\
rlOnTqW0tJSysjKamppYv349s2fPNrusXjd79mzWrl0LwNq1ayOm120YBosXL2bChAk8/PDDwfZI\
PN/a2trgbOgTJ07w5ptvMmHCBFwuF6+99hoQOefaa8yeRdJXxo4dayQlJRmTJ082Jk+ebNx///3B\
bY8//riRmppqOJ1O4w9/+IOJVfaMDRs2GImJiUZMTIwxcuRI46abbgpui7RzNQzD2Lx5szFu3Dgj\
NTXVePzxx80up8fNnz/fGD16tGG3243ExETj+eefNw4fPmx8+9vfNtLS0owZM2YYfr/f7DJ7xDvv\
vGMAxhVXXBF8rW7evDkiz3f37t3GlClTjCuuuMJIT0838vPzDcMwjIMHDxpTp041xo4da9x5551G\
Y2OjyZWGL63EISIilhQ1Q4giIhJZFGAiImJJCjAREbEkBZiIiFiSAkxERCxJASYiIpakABMREUtS\
gImIiCUpwERExJIUYCIiYkkKMBERsSQFmIiIWJICTERELEkBJiIilqQAExERS1KAiYiIJSnARETE\
khRgIiJiSQowERGxJAWYiIhY0v8HCo1t5olki8UAAAAASUVORK5CYII=\
"
  frames[2] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAAZd0lEQVR4nO3df2xb9b3/8Vdo1Eq5/AiEBuKkLG1dZ6lp2q2hbNLUxWsDG42S\
QFnJ1D8yQDZfbUNV+Yqtf0yyLdEl271TqbTubs4Yi6Yr8h2IJVOyFVGa7DtNRV3QoJfmts5YIiVx\
qVKDt6HQNg2f+0dlU5MG2hLnnI/zfEgV9TlOeR+B8/Q5/uS0wBhjBACAZa5zegAAAK4FAQMAWImA\
AQCsRMAAAFYiYAAAKxEwAICVCBgAwEoEDABgJQIGALASAQMAWImAAQCsRMAAAFYiYAAAKxEwAICV\
CBgAwEoEDABgJQIGALASAQMAWImAAQCsRMAAAFYiYAAAKxEwAICVCBgAwEoEDABgJQIGALASAQMA\
WImAAQCsRMAAAFYiYAAAKxEwAICVCBgAwEoEDABgJQIGALASAQMAWKnQ6QFy5dZbb1VlZaXTYwCA\
VUZHR3XmzBmnx7gieRuwyspKDQ4OOj0GAFiltrbW6RGuGJcQkSWZTF7VdgBwCgFDRiQSUU1NjeLx\
eNb2eDyumpoaRSIRZwYDgMsgYJB0MV7RaFSJREKBQCATsXg8rkAgoEQioWg0SsQAuAYBQyZeaemI\
9fX1ZeKVRsQAuAUBW+SSyaQ6OjqytjWrWVOJKTU0NGgqMaVmNWft7+jo4DMxAI4jYItcSUmJ+vv7\
5fF4JF2M1y7t0j7tU6UqtU/7tEu7MhHzeDzq7+9XSUmJg1MDAAGDJJ/Pl4nYgAY0qlFVqlLP6llV\
qlKjGtWABjLx8vl8To8MAAQMF/l8PsViMaWUUlTRrH1RRZVSSrFYjHgBcA0CBkkXVxuGQiEVq1hh\
hbP2hRVWsYoVCoVmLbEHAKcQMGQtla9TXeay4cN6OHM5sU51s5bYA4CT8vZWUrgyyWQya6l8t7ol\
SQMaUEop7dZu1akusz0dsWPHjrGQA4CjOANb5EpKShQMBrO2datbRZ4i9fb2qshTlIlXWjAYJF7A\
VeAWbblBwKBIJKJw+MPPvdKrDbdt25a1xF6SwuEwP8gMXAVu0ZZDJk9t3LjR6RHc5fAPjQnfdPGf\
cwiHw8bj8ZiTJ09mbT958qTxeDwmHA7ndkYgz4TDYSPJSMp6baVfU+l9bnpt2fS9s8AYY5xNaG7U\
1tby16lcKnLTJb//x5xPSyaTl708ONd2AJf30Vu0SRevbsRiMYVCoaxbtEnuubph0/dOLiEiy1yR\
Il7AleMWbQuDgAHAPOMWbQvDVQE7e/asNm3apPXr18vv92cWFoyMjOjuu++W1+vVQw89pPPnzzs8\
qYUi//jwF4Cc4xZtueeqgC1btkyHDx/WG2+8oddff10HDx7Uq6++qu9973vavXu3/va3v+nmm2/W\
M8884/SoAPCJuEVbbrkqYAUFBbr++uslSdPT05qenlZBQYEOHz6sBx98UJLU2tqq7u5uB6cEgCvD\
Ldpyy1UBk6SZmRlt2LBBpaWlqq+v1+rVq1VcXKzCwos3DamoqNDExITDUwLAx+MWbbnnultJLVmy\
RK+//rpSqZTuv/9+nThx4oq/NhaLKRaLSZImJydzNSIAfCxu0bYwXHcGllZcXKxAIKAjR44olUrp\
woULkqTx8XGVl5df9mtCoZAGBwc1ODio5cuXL+S4AJDBLdoWhqsCNjk5qVQqJUl6//339fLLL6u6\
ulqBQEAvvPCCJKmzs1NNTU0OTgkA0luT76lyT1/m10dxi7bcc9UlxFOnTqm1tVUzMzP64IMPtGPH\
DjU0NGjt2rVqaWnR97//fX3uc5/To48+6vSoABa5p3qHPvE56Sh1dHRkLZVPL7EPBAIKBoPE6xpx\
KykAuAZvTb6nLT/+Y+bxaPu2OZ9r0y3abPre6aozMACwxerl139stC7FLdpyw1WfgQEAcKUIGADA\
SgQMAGAlAgYAsBIBAwBYiYABAKxEwAAAViJgAAArETAAgJUIGADASgQMAGAlAgYAsBIBu0bJZPKq\
tgMA5hcBuwaRSEQ1NTWKx+NZ2+PxuGpqavi7fQBgARCwqxSJRBSNRpVIJBQIBDIRi8fjCgQCSiQS\
ikajRAwAcoyAXYV0vNLSEevr68vEK42IAUBuEbArlEwm1dHRkbWtWc2aSkypoaFBU4kpNas5a39H\
RwefiQFAjhCwK1RSUqL+/n55PB5JF+O1S7u0T/tUqUrt0z7t0q5MxDwej/r7+/kbVwEgRwjYVfD5\
fJmIDWhAoxpVpSr1rJ5VpSo1qlENaCATL5/P5/TIAJC3CNhV8vl8isViSimlqKJZ+6KKKqWUYrEY\
8QKAHCNgVykejysUCqlYxQornLUvrLCKVaxQKDRriT0AYH4RsKtw6VL5OtVlLhs+rIczlxPrVDdr\
iT0AYP4VOj2ALZLJZNZS+W51S5IGNKCUUtqt3apTXWZ7OmLHjh1jIQcA5ICrzsDGxsYUCAS0du1a\
+f1+7d+/X5L0zjvvqL6+XmvWrFF9fb3efffdBZ+tpKREwWAwa1u3ulXkKVJvb6+KPEWZeKUFg0Hi\
BQA54qqAFRYW6sc//rGGhob06quv6sCBAxoaGlJ7e7u2bNmi4eFhbdmyRe3t7Y7MF4lEFA5/+LlX\
erXhtm3bspbYS1I4HOYHmQEgh1x1CbGsrExlZWWSpBtuuEHV1dWamJhQT0+PBgYGJEmtra2qq6vT\
D3/4w5zM8D+frc78/t8aGnTHf/x71v50lDo6OrKWyqeX2AcCAQWDQeIFADlWYIwxTg9xOaOjo9q8\
ebPefPNN3XHHHUqlUpIkY4xuvvnmzOO51NbWanBw8Kr/vZcGTJKqT/zPZZ+XTCYve3lwru0AYINr\
/d7pBFedgaW999572r59u55++mndeOONWfsKCgpUUFBw2a+LxWKKxWKSpMnJyU89x781NMy5b65I\
ES8AWBiuC9j09LS2b9+unTt36oEHHpAk3XbbbTp16pTKysp06tQplZaWXvZrQ6GQQqGQpIvvIq7F\
XGdcAAB3cdUiDmOMHn30UVVXV+uJJ57IbG9sbFRnZ6ckqbOzU01NTU6NCABwCVedgf35z3/Wr3/9\
a61bt04bNmyQJP3gBz/Qnj17tGPHDj3zzDP6zGc+o9/85jfODgoAcJyrAvalL31Jc60peeWVVxZ4\
GgCAm7nqEiIAAFeKgAEArETAAABWImAAACsRMACAlQgYAMBKBAwAYCUCBgCwEgEDAFiJgAEArETA\
AABWImAAACsRMACAlQgYAMBKBAwAYCUCBgCwEgEDAFiJgAEArETAAABWImAAACsRMACAlQgYAMBK\
BAwAYCUCBgCwkusC9sgjj6i0tFR33nlnZts777yj+vp6rVmzRvX19Xr33XcdnBAA4AauC9g3v/lN\
HTx4MGtbe3u7tmzZouHhYW3ZskXt7e0OTbe4JJPJq9oOAAvJdQHbvHmzbrnllqxtPT09am1tlSS1\
traqu7vbgckWl0gkopqaGsXj8azt8XhcNTU1ikQizgwGIGOxv8l0XcAu5/Tp0yorK5Mk3X777Tp9\
+rTDE+W3SCSiaDSqRCKhQCCQiVg8HlcgEFAikVA0GiVigIN4kynJuNDIyIjx+/2ZxzfddFPW/uLi\
4st+3c9//nOzceNGs3HjRnPHHXfkcsS8FQ6HjaSsXx6Px/T29hqPxzNrXzgcdnpkYNG59HXq8XjM\
yZMnjTHGnDx5Mut1ei2vz40bN87ztLljRcB8Pp9JJBLGGGMSiYTx+Xyf+GfY9B/BLc6cOTMrUs1q\
NsUqNpJMsYpNs5pnxe3MmTNOjw4sGrl+k2nT904rLiE2Njaqs7NTktTZ2ammpiaHJ8pPJSUl6u/v\
l8fjkSQ1q1m7tEv7tE+VqtQ+7dMu7VKzmiVJHo9H/f39KikpcXBqYPFIJpPq6OjI2tasZk0lptTQ\
0KCpxFTm9ZnW0dGRt5+JuS5g3/jGN/TFL35RJ0+eVEVFhZ555hnt2bNHL7/8stasWaNDhw5pz549\
To+Zt3w+XyZiAxrQqEZVqUo9q2dVqUqNalQDGsjEy+fzOT0ysGjwJjNbgTHGOD1ELtTW1mpwcNDp\
MazV19enhoaGTLzSHtbDGtWoent7tW3bNgcnBBav9IKqqcRUJl5poxrVbu1Wkafomt5k2vS903Vn\
YHBePB5XKBRSsYoVVjhrX1hhFatYoVBo1uonAAvD5/MpFosppZSiimbtiyqqlFKKxWJ5f4WEgCHL\
pUvl61SXuWyYPvOqVKXqVDdriT2AhcObzIsIGDKSyWQmXpLUrW7t137t1u7MZYn92q9udUtSJmL5\
+gEx4Ea8yfwQAUNGSUmJgsFg1rZudavIU6Te3l4VeYoy8UoLBoN5+wEx4Da8ycxGwBahdZ3rMr8+\
KhKJKBz+8JJEehXTtm3bslY/SVI4HF4cP+0PuARvMrMVOj0A3CcdpY6OjqxVTOkl9oFAQMFgkHgB\
Dki/7qLRi4s3Lv2RlvTrM32Glu9vMllGvwhdeub1363/PefzksnkZd+5zbUdwKc39c9/6D+DOzOP\
/+//673s8yKRyKw3mdKHn5Fd65tMm753cgaGOc0VKeIF5M7xgUNX9LxIJKLHH3981uvR5/Pp2LFj\
i+J1SsAWoY876wLgLH/dVv3//3r2k58o3mQSMABwkaIbb5rzsiGysQoRAGAlAgYAsBIBAwBYiYAB\
AKxEwAAAViJgAAArETAAgJUIGADASgQMAGAlAgYAsBIBAwBYiYABAKxEwAAAViJgAAArETAAgJWs\
CdjBgwdVVVUlr9er9vZ2p8cBADjMioDNzMzo29/+tv7whz9oaGhIzz33nIaGhpweCwDgICsCdvTo\
UXm9Xq1atUpLly5VS0uLenp6nB4LAOAgKwI2MTGhFStWZB5XVFRoYmLCwYkAAE4rdHqA+RSLxRSL\
xSRJk5OTDk8DAMglK87AysvLNTY2lnk8Pj6u8vLyWc8LhUIaHBzU4OCgli9fvpAjAgAWmBUBu+uu\
uzQ8PKyRkRGdP39eXV1damxsdHosAICDrLiEWFhYqJ/85Ce69957NTMzo0ceeUR+v9/psQAADrIi\
YJJ033336b777nN6DACAS1hxCREAgI8iYAAAKxEwAICVCBgAwEoEDABgJQIGALASAQMAWImAAQCs\
RMAAAFYiYAAAKxEwAICVCBgAwEoEDABgJQIGALASAQMAWImAAQCsRMAAAFYiYAAAKxEwAICVCBgA\
wEoEDABgJQIGALASAQMAWImAAQCs5JqAPf/88/L7/bruuus0ODiYta+trU1er1dVVVV66aWXHJoQ\
AOAmhU4PkHbnnXfqxRdf1GOPPZa1fWhoSF1dXTp+/LgSiYS2bt2qeDyuJUuWODQpAMANXHMGVl1d\
raqqqlnbe3p61NLSomXLlmnlypXyer06evSoAxMCANzENQGby8TEhFasWJF5XFFRoYmJCQcnAgC4\
wYJeQty6davefvvtWdv37t2rpqamT/3nx2IxxWIxSdLk5OSn/vMAAO61oAE7dOjQVX9NeXm5xsbG\
Mo/Hx8dVXl5+2eeGQiGFQiFJUm1t7bUNCQCwgusvITY2Nqqrq0vnzp3TyMiIhoeHtWnTJqfHAgA4\
zDUB++1vf6uKigodOXJE27Zt07333itJ8vv92rFjh9auXauvfvWrOnDgACsQAQAqMMYYp4fIhdra\
2lk/TwYA+Hg2fe90zRkYAABXg4ABAKxEwAAAViJgAAArETAAgJUIGADASgQMAGAlAgYAsBIBAwBY\
iYABAKxEwAAAViJgAAArETAAgJUIGADASgQMAGAlAgYAsBIBAwBYiYABAKxEwAAAViJgAAArETAA\
gJUIGADASgQMAGAlAgYAsJJrAvbkk0/qs5/9rGpqanT//fcrlUpl9rW1tcnr9aqqqkovvfSSc0MC\
AFzDNQGrr6/Xm2++qWPHjsnn86mtrU2SNDQ0pK6uLh0/flwHDx7Ut771Lc3MzDg8LQDAaa4J2D33\
3KPCwkJJ0he+8AWNj49Lknp6etTS0qJly5Zp5cqV8nq9Onr0qJOjAgBcwDUBu9Qvf/lLfe1rX5Mk\
TUxMaMWKFZl9FRUVmpiYcGo0AIBLFC7kv2zr1q16++23Z23fu3evmpqaMr8vLCzUzp07r/rPj8Vi\
isVikqTJyclPNywAwNUWNGCHDh362P2/+tWv1Nvbq1deeUUFBQWSpPLyco2NjWWeMz4+rvLy8st+\
fSgUUigUkiTV1tbO09QAADdyzSXEgwcP6kc/+pF+97vfqaioKLO9sbFRXV1dOnfunEZGRjQ8PKxN\
mzY5OCkAwA0W9Azs43znO9/RuXPnVF9fL+niQo6f/exn8vv92rFjh9auXavCwkIdOHBAS5YscXha\
AIDTCowxxukhcqG2tlaDg4NOjwEAVrHpe6drLiECAK5dMpm8qu35gIABgOUikYhqamoUj8eztsfj\
cdXU1CgSiTgzWI4RMACwWCQSUTQaVSKRUCAQyEQsHo8rEAgokUgoGo3mZcQIGABYKh2vtHTE+vr6\
MvFKy8eIETAAsFAymVRHR0fWtmY1ayoxpYaGBk0lptSs5qz9HR0defWZGAEDAAuVlJSov79fHo9H\
0sV47dIu7dM+VapS+7RPu7QrEzGPx6P+/n6VlJQ4OPX8ImAAYCmfz5eJ2IAGNKpRVapSz+pZVapS\
oxrVgAYy8fL5fE6PPK8IGABYzOfzKRaLKaWUoopm7YsqqpRSisVieRcviYABgNXi8bhCoZCKVayw\
wln7wgqrWMUKhUKzltjnAwIGAJa6dKl8neoylw0f1sOZy4l1qpu1xD5fuOZeiACAK5dMJrOWyner\
W5I0oAGllNJu7Vad6jLb0xE7duxY3izk4AwMACxUUlKiYDCYta1b3SryFKm3t1dFnqJMvNKCwWDe\
xEviDAwAXO3A/zmc+f23f/aVrH3pH0xO/zDzpasN+/v7s87QwuFw3v0gMwEDAIulo9TR0ZG1VP7S\
iAWDwbyLl0TAAMB6kUhEjz/++KzLgz6fL68+8/ooAgYALvbRy4ZzmStS+RoviUUcAABLETAAgJUI\
GADASgQMAGAlAgYAsBIBAwBYiYABAKxUYIwxTg+RC7feeqsqKyudHiPnJicntXz5cqfHWFAc8+LA\
MTtjdHRUZ86ccXSGK5W3AVssamtrNTg46PQYC4pjXhw4ZnwSLiECAKxEwAAAViJglguFQk6PsOA4\
5sWBY8Yn4TMwAICVOAMDAFiJgFnq+eefl9/v13XXXTdr1VJbW5u8Xq+qqqr00ksvOTRhbhw8eFBV\
VVXyer1qb293epyceOSRR1RaWqo777wzs+2dd95RfX291qxZo/r6er377rsOTjj/xsbGFAgEtHbt\
Wvn9fu3fv19Sfh/32bNntWnTJq1fv15+v1/hcFiSNDIyorvvvlter1cPPfSQzp8/7/CkLmZgpaGh\
IXPixAnz5S9/2fzlL3/JbD9+/LipqakxZ8+eNX//+9/NqlWrzIULFxycdP5cuHDBrFq1yrz11lvm\
3Llzpqamxhw/ftzpsebdH//4R/Paa68Zv9+f2fbkk0+atrY2Y4wxbW1t5rvf/a5T4+VEIpEwr732\
mjHGmH/+859mzZo15vjx43l93B988IH517/+ZYwx5vz582bTpk3myJEj5utf/7p57rnnjDHGPPbY\
Y+anP/2pk2O6GmdglqqurlZVVdWs7T09PWppadGyZcu0cuVKeb1eHT161IEJ59/Ro0fl9Xq1atUq\
LV26VC0tLerp6XF6rHm3efNm3XLLLVnbenp61NraKklqbW1Vd3e3A5PlTllZmT7/+c9Lkm644QZV\
V1drYmIir4+7oKBA119/vSRpenpa09PTKigo0OHDh/Xggw9Kyr9jnm8ELM9MTExoxYoVmccVFRWa\
mJhwcKL5k8/H9klOnz6tsrIySdLtt9+u06dPOzxR7oyOjuqvf/2r7r777rw/7pmZGW3YsEGlpaWq\
r6/X6tWrVVxcrMLCQkmL6//xa1Ho9ACY29atW/X222/P2r537141NTU5MBHcoKCgQAUFBU6PkRPv\
vfeetm/frqefflo33nhj1r58PO4lS5bo9ddfVyqV0v33368TJ044PZJVCJiLHTp06Kq/pry8XGNj\
Y5nH4+PjKi8vn8+xHJPPx/ZJbrvtNp06dUplZWU6deqUSktLnR5p3k1PT2v79u3auXOnHnjgAUmL\
47glqbi4WIFAQEeOHFEqldKFCxdUWFi4qP4fvxZcQswzjY2N6urq0rlz5zQyMqLh4WFt2rTJ6bHm\
xV133aXh4WGNjIzo/Pnz6urqUmNjo9NjLYjGxkZ1dnZKkjo7O/PuDNwYo0cffVTV1dV64oknMtvz\
+bgnJyeVSqUkSe+//75efvllVVdXKxAI6IUXXpCUf8c875xeRYJr8+KLL5ry8nKzdOlSU1paau65\
557MvqeeesqsWrXK+Hw+8/vf/97BKedfX1+fWbNmjVm1apV56qmnnB4nJ1paWsztt99uCgsLTXl5\
ufnFL35hzpw5Y77yla8Yr9drtmzZYpLJpNNjzqs//elPRpJZt26dWb9+vVm/fr3p6+vL6+N+4403\
zIYNG8y6deuM3+830WjUGGPMW2+9Ze666y6zevVq8+CDD5qzZ886PKl7cScOAICVuIQIALASAQMA\
WImAAQCsRMAAAFYiYAAAKxEwAICVCBgAwEoEDABgJQIGALASAQMAWImAAQCsRMAAAFYiYAAAKxEw\
AICVCBgAwEoEDABgJQIGALASAQMAWImAAQCsRMAAAFb6X/2FVZjzo/xdAAAAAElFTkSuQmCC\
"
  frames[3] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAAZEElEQVR4nO3dcUzU9/3H8ddVoglpO1oqlgO3U49jiKJbEbdkcdyUdqsEtDrL\
4h+sGq6/bGuM/aWbfyy5u6QOtmSxJnNZjnUd2R/ltzYdLLBhtMKyLDaOZtZVfnqsgwQ4bPD0fltD\
VcTv7w9zV69oK47j+/0cz0diAp8v2Pclhed9v/e5ry7LsiwBAGCY++weAACAe0HAAABGImAAACMR\
MACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAw\
EgEDABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGImAA\
ACMRMACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGyrF7gEx55JFH5PF47B4DAIwyPDys\
ixcv2j3GXcnagHk8HvX399s9BgAYpbKy0u4R7hqXEJEmHo/Pah0A7ELAkBIKhVRRUaFoNJq2Ho1G\
VVFRoVAoZM9gAHAbBAySbsYrHA4rFovJ7/enIhaNRuX3+xWLxRQOh4kYAMcgYEjFKykZse7u7lS8\
kogYAKcgYAtcPB5Xa2tr2to2bdNkbFK1tbWajE1qm7alHW9tbeU1MQC2I2ALXH5+vnp7e+V2uyXd\
jNc+7dMhHZJHHh3SIe3TvlTE3G63ent7lZ+fb+PUAEDAIMnn86Ui1qc+DWtYHnn0il6RRx4Na1h9\
6kvFy+fz2T0yABAw3OTz+RSJRJRQQmGF046FFVZCCUUiEeIFwDEIGCTd3G0YCASUpzwFFUw7FlRQ\
ecpTIBCYscUeAOxCwJC2Vb5a1anLhs/omdTlxGpVz9hiDwB2ytpbSeHuxOPxtK3yHeqQJPWpTwkl\
tF/7Va3q1HoyYmfOnGEjBwBbcQa2wOXn56upqSltrUMdynXnqqurS7nu3FS8kpqamogXMAvcoi0z\
CBgUCoUUDH70uldyt+HWrVvTtthLUjAY5I3MwCxwi7YMsrLUY489ZvcIxgkGg5bb7bbOnz+ftn7+\
/HnL7XZbwWDQnsEAQwWDQUuSJSntZyv5M5U85qSfLZN+d7osy7LsTWhmVFZW8s+p3Cr0mVs+/r87\
flk8Hr/t5cE7rQO4vY/fok26eXUjEokoEAik3aJNcs7VDZN+d3IJEWnuFCniBdw9btE2PwgYAMwx\
btE2PxwVsCtXrqiqqkrr1q1TeXl5amPB0NCQNm7cKK/Xq6efflrXrl2zeVIDhf7voz8AMo5btGWe\
owK2ZMkSnThxQu+8845Onz6tnp4evfXWW/rBD36g/fv36x//+Iceeughvfzyy3aPCgCfilu0ZZaj\
AuZyuXT//fdLkqampjQ1NSWXy6UTJ05o586dkqTGxkZ1dHTYOCUA3B1u0ZZZjgqYJE1PT2v9+vUq\
KChQTU2NVq1apby8POXk3LxpSHFxscbGxmyeEgA+GbdoyzzH3Upq0aJFOn36tBKJhLZv365z587d\
9fdGIhFFIhFJ0sTERKZGBIBPxC3a5ofjzsCS8vLy5Pf7dfLkSSUSCV2/fl2SNDo6qqKiott+TyAQ\
UH9/v/r7+7V06dL5HBcAUrhF2/xwVMAmJiaUSCQkSR9++KGOHTumsrIy+f1+vf7665KktrY21dfX\
2zglANzkOdCd+vNx3KIt8xx1CXF8fFyNjY2anp7WjRs3tGvXLtXW1mr16tVqaGjQD3/4Q33hC1/Q\
3r177R4VAD5VMkqtra1pW+WTW+z9fr+ampqI1z3iVlIAcI9uPfMabtl6x68z6RZtJv3udNQZGACY\
5JOidStu0ZYZjnoNDACAu0XAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETA\
AABGImAAACMRMACAkQjYPYrH47NaBwDMLQJ2D0KhkCoqKhSNRtPWo9GoKioq+Ld9AGAeELBZCoVC\
CofDisVi8vv9qYhFo1H5/X7FYjGFw2EiBgAZRsBmIRmvpGTEuru7U/FKImIAkFkE7C7F43G1tram\
rW3TNk3GJlVbW6vJ2KS2aVva8dbWVl4TA4AMIWB3KT8/X729vXK73ZJuxmuf9umQDskjjw7pkPZp\
Xypibrdbvb29/IurAJAhBGwWfD5fKmJ96tOwhuWRR6/oFXnk0bCG1ae+VLx8Pp/dIwNA1iJgs+Tz\
+RSJRJRQQmGF046FFVZCCUUiEeIFABlGwGYpGo0qEAgoT3kKKph2LKig8pSnQCAwY4s9AGBuEbBZ\
uHWrfLWqU5cNn9EzqcuJ1aqescUeADD3cuwewBTxeDxtq3yHOiRJfepTQgnt135Vqzq1nozYmTNn\
2MgBABngqDOwkZER+f1+rV69WuXl5Tp8+LAk6dKlS6qpqVFJSYlqamp0+fLleZ8tPz9fTU1NaWsd\
6lCuO1ddXV3Kdeem4pXU1NREvAAgQxwVsJycHP30pz/VwMCA3nrrLR05ckQDAwNqaWnR5s2bNTg4\
qM2bN6ulpcWW+UKhkILBj173Su423Lp1a9oWe0kKBoO8kRkAMshRlxALCwtVWFgoSXrggQdUVlam\
sbExdXZ2qq+vT5LU2Nio6upq/fjHP87IDP/7+bLUx2Xn/nfG8WSUWltb07bKJ7fY+/1+NTU1ES8A\
yDCXZVmW3UPczvDwsDZt2qR3331Xn/3sZ5VIJCRJlmXpoYceSn1+J5WVlerv75/1f/fTApYUj8dv\
e3nwTusAYIJ7/d1pB0edgSV98MEH2rFjh1566SU9+OCDacdcLpdcLtdtvy8SiSgSiUiSJiYmMjrj\
nSJFvABgfjguYFNTU9qxY4d2796tp556SpK0bNkyjY+Pq7CwUOPj4yooKLjt9wYCAQUCAUk3n0Xc\
i0866wIAOIejNnFYlqW9e/eqrKxMzz//fGq9rq5ObW1tkqS2tjbV19fbNSIAwCEcdQb2l7/8Rb/5\
zW+0du1arV+/XpL0ox/9SAcOHNCuXbv08ssv63Of+5x++9vf2jsoAMB2jgrYV77yFd1pT8mbb745\
z9MAAJzMUZcQAQC4WwQMAGAkAgYAMBIBAwAYiYABAIxEwAAARiJgAAAjETAAgJEIGADASAQMAGAk\
AgYAMBIBAwAYiYABAIxEwAAARiJgAAAjETAAgJEIGADASAQMAGAkAgYAMBIBAwAYiYABAIxEwAAA\
RiJgAAAjETAAgJEIGADASI4L2J49e1RQUKA1a9ak1i5duqSamhqVlJSopqZGly9ftnHChSMej89q\
HQDmk+MC9u1vf1s9PT1pay0tLdq8ebMGBwe1efNmtbS02DTdwhEKhVRRUaFoNJq2Ho1GVVFRoVAo\
ZM9gAFIW+pNMxwVs06ZNevjhh9PWOjs71djYKElqbGxUR0eHDZMtHKFQSOFwWLFYTH6/PxWxaDQq\
v9+vWCymcDhMxAAb8SRTkuVAQ0NDVnl5eerzz3zmM6mPb9y4kfb5nTz22GMZmCz7BYNBS1LaH7fb\
bXV1dVlut3vGsWAwaPfIwIJz68+p2+22zp8/b1mWZZ0/fz7t5/Refj5N+t3puDOwT+NyueRyuW57\
LBKJqLKyUpWVlZqYmJjnycwXj8fV2tqatrZN2zQZm1Rtba0mY5Papm1px1tbWxfM5QrACZJXSJKS\
V0q6u7tTV0iSsv1KiREBW7ZsmcbHxyVJ4+PjKigouO3XBQIB9ff3q7+/X0uXLp3PEbNCfn6+ent7\
5Xa7Jd2M1z7t0yEdkkceHdIh7dO+VMTcbrd6e3uVn59v49TAwsGTzHRGBKyurk5tbW2SpLa2NtXX\
19s8Ufby+XypiPWpT8MalkcevaJX5JFHwxpWn/pS8fL5fHaPDCwYPMlM57iAfetb39KXv/xlnT9/\
XsXFxXr55Zd14MABHTt2TCUlJTp+/LgOHDhg95hZzefzKRKJKKGEwgqnHQsrrIQSikQixAuwAU8y\
P+KyLMuye4hMqKysVH9/v91jGCm523AyNpl6Zpc0rGHt137lunOz/ocDcLLu7m7V1tam4pX0jJ7R\
sIbV1dWlrVu3zvrvNel3p+POwGCvW7fKV6s69Ywu+UPhkUfVqp6xxR7A/IlGowoEAspTnoIKph0L\
Kqg85SkQCGT9zycBQ0o8Hk/bxdShDh3WYe3X/tSZ12EdVoc6JH20+ylbXyAGnIgnmR8hYEjJz89X\
U1NT2lqHOpTrzlVXV5dy3bmpeCU1NTVl7QvEgNPwJDMdAUOaUCikYPCjSxLJF4K3bt2atvtJkoLB\
YFa/xwRwGp5kfozd76TOFJPeTT7f1vx6TerPnQSDwbR3+Ccl3+nPHTgA+3AnjpvYhbgArW1bm/r4\
741/v+PXxePx2z5zu9M6gLnx06drUx//9/903fZrQqGQWltbZ+wGTr5G1tTUdE9XSEz63Zlj9wBw\
rjtFingB9guFQnruuedm/Dz6fD6dOXNmQfycErAF6JPOugCYY6E/ySRgAOAwd7psiHTsQgQAGImA\
AQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABgJAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICR\
CBgAwEgEDABgJGMC1tPTo9LSUnm9XrW0tNg9DgDAZkYEbHp6Wt/97nf1xz/+UQMDA3r11Vc1MDBg\
91gAABsZEbBTp07J6/Vq5cqVWrx4sRoaGtTZ2Wn3WAAAGxkRsLGxMS1fvjz1eXFxscbGxmycCABg\
txy7B5hLkUhEkUhEkjQxMWHzNACATDLiDKyoqEgjIyOpz0dHR1VUVDTj6wKBgPr7+9Xf36+lS5fO\
54gAgHlmRMA2bNigwcFBDQ0N6dq1a2pvb1ddXZ3dYwEAbGTEJcScnBz97Gc/0xNPPKHp6Wnt2bNH\
5eXldo8FALCREQGTpCeffFJPPvmk3WMAABzCiEuIAAB8HAEDABiJgAEAjETAAABGImAAACMRMACA\
kQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAwEgED\
ABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCTHBOy1115TeXm57rvvPvX396cda25ultfr\
VWlpqY4ePWrThAAAJ8mxe4CkNWvW6I033tCzzz6btj4wMKD29nadPXtWsVhMW7ZsUTQa1aJFi2ya\
FADgBI45AysrK1NpaemM9c7OTjU0NGjJkiVasWKFvF6vTp06ZcOEAAAncUzA7mRsbEzLly9PfV5c\
XKyxsTEbJwIAOMG8XkLcsmWLLly4MGP94MGDqq+v/4///kgkokgkIkmamJj4j/8+AIBzzWvAjh8/\
PuvvKSoq0sjISOrz0dFRFRUV3fZrA4GAAoGAJKmysvLehgQAGMHxlxDr6urU3t6uq1evamhoSIOD\
g6qqqrJ7LACAzRwTsN/97ncqLi7WyZMntXXrVj3xxBOSpPLycu3atUurV6/W17/+dR05coQdiAAA\
uSzLsuweIhMqKytnvJ8MAPDJTPrd6ZgzMAAAZoOAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABg\
JAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABgJAIGADASAQMAGImAAQCMRMAA\
AEYiYAAAIxEwAICRCBgAwEgEDABgJAIGADCSYwL2wgsv6POf/7wqKiq0fft2JRKJ1LHm5mZ5vV6V\
lpbq6NGj9g0JAHAMxwSspqZG7777rs6cOSOfz6fm5mZJ0sDAgNrb23X27Fn19PToO9/5jqanp22e\
FgBgN8cE7PHHH1dOTo4k6Utf+pJGR0clSZ2dnWpoaNCSJUu0YsUKeb1enTp1ys5RAQAO4JiA3epX\
v/qVvvGNb0iSxsbGtHz58tSx4uJijY2N2TUaAMAhcubzP7ZlyxZduHBhxvrBgwdVX1+f+jgnJ0e7\
d++e9d8fiUQUiUQkSRMTE//ZsAAAR5vXgB0/fvwTj//6179WV1eX3nzzTblcLklSUVGRRkZGUl8z\
OjqqoqKi235/IBBQIBCQJFVWVs7R1AAAJ3LMJcSenh795Cc/0e9//3vl5uam1uvq6tTe3q6rV69q\
aGhIg4ODqqqqsnFSAIATzOsZ2Cf53ve+p6tXr6qmpkbSzY0cv/jFL1ReXq5du3Zp9erVysnJ0ZEj\
R7Ro0SKbpwUA2M1lWZZl9xCZUFlZqf7+frvHAACjmPS70zGXEAEA9y4ej89qPRsQMAAwXCgUUkVF\
haLRaNp6NBpVRUWFQqGQPYNlGAEDAIOFQiGFw2HFYjH5/f5UxKLRqPx+v2KxmMLhcFZGjIABgKGS\
8UpKRqy7uzsVr6RsjBgBAwADxeNxtba2pq1t0zZNxiZVW1urydiktmlb2vHW1tasek2MgAGAgfLz\
89Xb2yu32y3pZrz2aZ8O6ZA88uiQDmmf9qUi5na71dvbq/z8fBunnlsEDAAM5fP5UhHrU5+GNSyP\
PHpFr8gjj4Y1rD71peLl8/nsHnlOETAAMJjP51MkElFCCYUVTjsWVlgJJRSJRLIuXhIBAwCjRaNR\
BQIB5SlPQQXTjgUVVJ7yFAgEZmyxzwYEDAAMdetW+WpVpy4bPqNnUpcTq1U9Y4t9tnDMvRABAHcv\
Ho+nbZXvUIckqU99Siih/dqvalWn1pMRO3PmTNZs5OAMDAAMlJ+fr6amprS1DnUo152rrq4u5bpz\
U/FKampqypp4SZyBAYCjHfmvE6mPv/uLr6UdS74xOflm5lt3G/b29qadoQWDwax7IzMBAwCDJaPU\
2tqatlX+1og1NTVlXbwkAgYAxguFQnruuedmXB70+XxZ9ZrXxxEwAHCwj182vJM7RSpb4yWxiQMA\
YCgCBgAwEgEDABiJgAEAjETAAABGImAAACMRMACAkVyWZVl2D5EJjzzyiDwej91jZNzExISWLl1q\
9xjzise8MPCY7TE8PKyLFy/aOsPdytqALRSVlZXq7++3e4x5xWNeGHjM+DRcQgQAGImAAQCMRMAM\
FwgE7B5h3vGYFwYeMz4Nr4EBAIzEGRgAwEgEzECvvfaaysvLdd99983YsdTc3Cyv16vS0lIdPXrU\
pgkzo6enR6WlpfJ6vWppabF7nIzZs2ePCgoKtGbNmtTapUuXVFNTo5KSEtXU1Ojy5cs2Tjj3RkZG\
5Pf7tXr1apWXl+vw4cOSsvtxX7lyRVVVVVq3bp3Ky8sVDAYlSUNDQ9q4caO8Xq+efvppXbt2zeZJ\
HcyCcQYGBqxz585ZX/3qV62//vWvqfWzZ89aFRUV1pUrV6x//vOf1sqVK63r16/bOOncuX79urVy\
5Urrvffes65evWpVVFRYZ8+etXusjPjTn/5kvf3221Z5eXlq7YUXXrCam5sty7Ks5uZm6/vf/75d\
42VELBaz3n77bcuyLOtf//qXVVJSYp09ezarH/eNGzesf//735ZlWda1a9esqqoq6+TJk9Y3v/lN\
69VXX7Usy7KeffZZ6+c//7mdYzoaZ2AGKisrU2lp6Yz1zs5ONTQ0aMmSJVqxYoW8Xq9OnTplw4Rz\
79SpU/J6vVq5cqUWL16shoYGdXZ22j1WRmzatEkPP/xw2lpnZ6caGxslSY2Njero6LBhsswpLCzU\
F7/4RUnSAw88oLKyMo2NjWX143a5XLr//vslSVNTU5qampLL5dKJEye0c+dOSdn3mOcaAcsiY2Nj\
Wr58eerz4uJijY2N2TjR3Mnmx3Y33n//fRUWFkqSHn30Ub3//vs2T5Q5w8PD+tvf/qaNGzdm/eOe\
np7W+vXrVVBQoJqaGq1atUp5eXnKycmRtPD+P5+tHLsHwO1t2bJFFy5cmLF+8OBB1dfX2zARnMLl\
csnlctk9RkZ88MEH2rFjh1566SU9+OCDacey8XEvWrRIp0+fViKR0Pbt23Xu3Dm7RzIKAXOo48eP\
z/p7ioqKNDIykvp8dHRURUVFczmWbbL5sd2NZcuWaXx8XIWFhRofH1dBQYHdI825qakp7dixQ7t3\
79ZTTz0laWE8bknKy8uT3+/XyZMnlUgkdP36deXk5Cy4/89ni0uIWaSurk7t7e26evWqhoaGNDg4\
qKqqKrvHmhMbNmzQ4OCghoaGdO3aNbW3t6uurs7useZNXV2d2traJEltbW1ZdxZuWZb27t2rsrIy\
Pf/886n1bH7cExMTSiQSkqQPP/xQx44dU1lZmfx+v15//XVJ2feY55zdu0gwe2+88YZVVFRkLV68\
2CooKLAef/zx1LEXX3zRWrlypeXz+aw//OEPNk4597q7u62SkhJr5cqV1osvvmj3OBnT0NBgPfro\
o1ZOTo5VVFRk/fKXv7QuXrxofe1rX7O8Xq+1efNmKx6P2z3mnPrzn/9sSbLWrl1rrVu3zlq3bp3V\
3d2d1Y/7nXfesdavX2+tXbvWKi8vt8LhsGVZlvXee+9ZGzZssFatWmXt3LnTunLlis2TOhd34gAA\
GIlLiAAAIxEwAICRCBgAwEgEDABgJAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgE\
DABgJAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABgpP8HJ8GK0OGDmdcAAAAA\
SUVORK5CYII=\
"
  frames[4] = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\
bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsT\
AAALEwEAmpwYAAAZEElEQVR4nO3dcUzU9/3H8ddVoglpO1oqlgO3U49jiKJbEbdkcdyUdqsEtDrL\
4h+sGq6/bGuM/aWbfyy5u6QOtmSxJnNZjnUd2R/ltzYdLLBhtMKyLDaOZtZVfnqsgwQ4bPD0fltD\
VcTv7w9zV69oK47j+/0cz0diAp8v2Pclhed9v/e5ry7LsiwBAGCY++weAACAe0HAAABGImAAACMR\
MACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAw\
EgEDABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGImAA\
ACMRMACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGyrF7gEx55JFH5PF47B4DAIwyPDys\
ixcv2j3GXcnagHk8HvX399s9BgAYpbKy0u4R7hqXEJEmHo/Pah0A7ELAkBIKhVRRUaFoNJq2Ho1G\
VVFRoVAoZM9gAHAbBAySbsYrHA4rFovJ7/enIhaNRuX3+xWLxRQOh4kYAMcgYEjFKykZse7u7lS8\
kogYAKcgYAtcPB5Xa2tr2to2bdNkbFK1tbWajE1qm7alHW9tbeU1MQC2I2ALXH5+vnp7e+V2uyXd\
jNc+7dMhHZJHHh3SIe3TvlTE3G63ent7lZ+fb+PUAEDAIMnn86Ui1qc+DWtYHnn0il6RRx4Na1h9\
6kvFy+fz2T0yABAw3OTz+RSJRJRQQmGF046FFVZCCUUiEeIFwDEIGCTd3G0YCASUpzwFFUw7FlRQ\
ecpTIBCYscUeAOxCwJC2Vb5a1anLhs/omdTlxGpVz9hiDwB2ytpbSeHuxOPxtK3yHeqQJPWpTwkl\
tF/7Va3q1HoyYmfOnGEjBwBbcQa2wOXn56upqSltrUMdynXnqqurS7nu3FS8kpqamogXMAvcoi0z\
CBgUCoUUDH70uldyt+HWrVvTtthLUjAY5I3MwCxwi7YMsrLUY489ZvcIxgkGg5bb7bbOnz+ftn7+\
/HnL7XZbwWDQnsEAQwWDQUuSJSntZyv5M5U85qSfLZN+d7osy7LsTWhmVFZW8s+p3Cr0mVs+/r87\
flk8Hr/t5cE7rQO4vY/fok26eXUjEokoEAik3aJNcs7VDZN+d3IJEWnuFCniBdw9btE2PwgYAMwx\
btE2PxwVsCtXrqiqqkrr1q1TeXl5amPB0NCQNm7cKK/Xq6efflrXrl2zeVIDhf7voz8AMo5btGWe\
owK2ZMkSnThxQu+8845Onz6tnp4evfXWW/rBD36g/fv36x//+Iceeughvfzyy3aPCgCfilu0ZZaj\
AuZyuXT//fdLkqampjQ1NSWXy6UTJ05o586dkqTGxkZ1dHTYOCUA3B1u0ZZZjgqYJE1PT2v9+vUq\
KChQTU2NVq1apby8POXk3LxpSHFxscbGxmyeEgA+GbdoyzzH3Upq0aJFOn36tBKJhLZv365z587d\
9fdGIhFFIhFJ0sTERKZGBIBPxC3a5ofjzsCS8vLy5Pf7dfLkSSUSCV2/fl2SNDo6qqKiott+TyAQ\
UH9/v/r7+7V06dL5HBcAUrhF2/xwVMAmJiaUSCQkSR9++KGOHTumsrIy+f1+vf7665KktrY21dfX\
2zglANzkOdCd+vNx3KIt8xx1CXF8fFyNjY2anp7WjRs3tGvXLtXW1mr16tVqaGjQD3/4Q33hC1/Q\
3r177R4VAD5VMkqtra1pW+WTW+z9fr+ampqI1z3iVlIAcI9uPfMabtl6x68z6RZtJv3udNQZGACY\
5JOidStu0ZYZjnoNDACAu0XAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETA\
AABGImAAACMRMACAkQjYPYrH47NaBwDMLQJ2D0KhkCoqKhSNRtPWo9GoKioq+Ld9AGAeELBZCoVC\
CofDisVi8vv9qYhFo1H5/X7FYjGFw2EiBgAZRsBmIRmvpGTEuru7U/FKImIAkFkE7C7F43G1tram\
rW3TNk3GJlVbW6vJ2KS2aVva8dbWVl4TA4AMIWB3KT8/X729vXK73ZJuxmuf9umQDskjjw7pkPZp\
Xypibrdbvb29/IurAJAhBGwWfD5fKmJ96tOwhuWRR6/oFXnk0bCG1ae+VLx8Pp/dIwNA1iJgs+Tz\
+RSJRJRQQmGF046FFVZCCUUiEeIFABlGwGYpGo0qEAgoT3kKKph2LKig8pSnQCAwY4s9AGBuEbBZ\
uHWrfLWqU5cNn9EzqcuJ1aqescUeADD3cuwewBTxeDxtq3yHOiRJfepTQgnt135Vqzq1nozYmTNn\
2MgBABngqDOwkZER+f1+rV69WuXl5Tp8+LAk6dKlS6qpqVFJSYlqamp0+fLleZ8tPz9fTU1NaWsd\
6lCuO1ddXV3Kdeem4pXU1NREvAAgQxwVsJycHP30pz/VwMCA3nrrLR05ckQDAwNqaWnR5s2bNTg4\
qM2bN6ulpcWW+UKhkILBj173Su423Lp1a9oWe0kKBoO8kRkAMshRlxALCwtVWFgoSXrggQdUVlam\
sbExdXZ2qq+vT5LU2Nio6upq/fjHP87IDP/7+bLUx2Xn/nfG8WSUWltb07bKJ7fY+/1+NTU1ES8A\
yDCXZVmW3UPczvDwsDZt2qR3331Xn/3sZ5VIJCRJlmXpoYceSn1+J5WVlerv75/1f/fTApYUj8dv\
e3nwTusAYIJ7/d1pB0edgSV98MEH2rFjh1566SU9+OCDacdcLpdcLtdtvy8SiSgSiUiSJiYmMjrj\
nSJFvABgfjguYFNTU9qxY4d2796tp556SpK0bNkyjY+Pq7CwUOPj4yooKLjt9wYCAQUCAUk3n0Xc\
i0866wIAOIejNnFYlqW9e/eqrKxMzz//fGq9rq5ObW1tkqS2tjbV19fbNSIAwCEcdQb2l7/8Rb/5\
zW+0du1arV+/XpL0ox/9SAcOHNCuXbv08ssv63Of+5x++9vf2jsoAMB2jgrYV77yFd1pT8mbb745\
z9MAAJzMUZcQAQC4WwQMAGAkAgYAMBIBAwAYiYABAIxEwAAARiJgAAAjETAAgJEIGADASAQMAGAk\
AgYAMBIBAwAYiYABAIxEwAAARiJgAAAjETAAgJEIGADASAQMAGAkAgYAMBIBAwAYiYABAIxEwAAA\
RiJgAAAjETAAgJEIGADASI4L2J49e1RQUKA1a9ak1i5duqSamhqVlJSopqZGly9ftnHChSMej89q\
HQDmk+MC9u1vf1s9PT1pay0tLdq8ebMGBwe1efNmtbS02DTdwhEKhVRRUaFoNJq2Ho1GVVFRoVAo\
ZM9gAFIW+pNMxwVs06ZNevjhh9PWOjs71djYKElqbGxUR0eHDZMtHKFQSOFwWLFYTH6/PxWxaDQq\
v9+vWCymcDhMxAAb8SRTkuVAQ0NDVnl5eerzz3zmM6mPb9y4kfb5nTz22GMZmCz7BYNBS1LaH7fb\
bXV1dVlut3vGsWAwaPfIwIJz68+p2+22zp8/b1mWZZ0/fz7t5/Refj5N+t3puDOwT+NyueRyuW57\
LBKJqLKyUpWVlZqYmJjnycwXj8fV2tqatrZN2zQZm1Rtba0mY5Papm1px1tbWxfM5QrACZJXSJKS\
V0q6u7tTV0iSsv1KiREBW7ZsmcbHxyVJ4+PjKigouO3XBQIB9ff3q7+/X0uXLp3PEbNCfn6+ent7\
5Xa7Jd2M1z7t0yEdkkceHdIh7dO+VMTcbrd6e3uVn59v49TAwsGTzHRGBKyurk5tbW2SpLa2NtXX\
19s8Ufby+XypiPWpT8MalkcevaJX5JFHwxpWn/pS8fL5fHaPDCwYPMlM57iAfetb39KXv/xlnT9/\
XsXFxXr55Zd14MABHTt2TCUlJTp+/LgOHDhg95hZzefzKRKJKKGEwgqnHQsrrIQSikQixAuwAU8y\
P+KyLMuye4hMqKysVH9/v91jGCm523AyNpl6Zpc0rGHt137lunOz/ocDcLLu7m7V1tam4pX0jJ7R\
sIbV1dWlrVu3zvrvNel3p+POwGCvW7fKV6s69Ywu+UPhkUfVqp6xxR7A/IlGowoEAspTnoIKph0L\
Kqg85SkQCGT9zycBQ0o8Hk/bxdShDh3WYe3X/tSZ12EdVoc6JH20+ylbXyAGnIgnmR8hYEjJz89X\
U1NT2lqHOpTrzlVXV5dy3bmpeCU1NTVl7QvEgNPwJDMdAUOaUCikYPCjSxLJF4K3bt2atvtJkoLB\
YFa/xwRwGp5kfozd76TOFJPeTT7f1vx6TerPnQSDwbR3+Ccl3+nPHTgA+3AnjpvYhbgArW1bm/r4\
741/v+PXxePx2z5zu9M6gLnx06drUx//9/903fZrQqGQWltbZ+wGTr5G1tTUdE9XSEz63Zlj9wBw\
rjtFingB9guFQnruuedm/Dz6fD6dOXNmQfycErAF6JPOugCYY6E/ySRgAOAwd7psiHTsQgQAGImA\
AQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABgJAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICR\
CBgAwEgEDABgJGMC1tPTo9LSUnm9XrW0tNg9DgDAZkYEbHp6Wt/97nf1xz/+UQMDA3r11Vc1MDBg\
91gAABsZEbBTp07J6/Vq5cqVWrx4sRoaGtTZ2Wn3WAAAGxkRsLGxMS1fvjz1eXFxscbGxmycCABg\
txy7B5hLkUhEkUhEkjQxMWHzNACATDLiDKyoqEgjIyOpz0dHR1VUVDTj6wKBgPr7+9Xf36+lS5fO\
54gAgHlmRMA2bNigwcFBDQ0N6dq1a2pvb1ddXZ3dYwEAbGTEJcScnBz97Gc/0xNPPKHp6Wnt2bNH\
5eXldo8FALCREQGTpCeffFJPPvmk3WMAABzCiEuIAAB8HAEDABiJgAEAjETAAABGImAAACMRMACA\
kQgYAMBIBAwAYCQCBgAwEgEDABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCQCBgAwEgED\
ABiJgAEAjETAAABGImAAACMRMACAkQgYAMBIBAwAYCTHBOy1115TeXm57rvvPvX396cda25ultfr\
VWlpqY4ePWrThAAAJ8mxe4CkNWvW6I033tCzzz6btj4wMKD29nadPXtWsVhMW7ZsUTQa1aJFi2ya\
FADgBI45AysrK1NpaemM9c7OTjU0NGjJkiVasWKFvF6vTp06ZcOEAAAncUzA7mRsbEzLly9PfV5c\
XKyxsTEbJwIAOMG8XkLcsmWLLly4MGP94MGDqq+v/4///kgkokgkIkmamJj4j/8+AIBzzWvAjh8/\
PuvvKSoq0sjISOrz0dFRFRUV3fZrA4GAAoGAJKmysvLehgQAGMHxlxDr6urU3t6uq1evamhoSIOD\
g6qqqrJ7LACAzRwTsN/97ncqLi7WyZMntXXrVj3xxBOSpPLycu3atUurV6/W17/+dR05coQdiAAA\
uSzLsuweIhMqKytnvJ8MAPDJTPrd6ZgzMAAAZoOAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABg\
JAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABgJAIGADASAQMAGImAAQCMRMAA\
AEYiYAAAIxEwAICRCBgAwEgEDABgJAIGADCSYwL2wgsv6POf/7wqKiq0fft2JRKJ1LHm5mZ5vV6V\
lpbq6NGj9g0JAHAMxwSspqZG7777rs6cOSOfz6fm5mZJ0sDAgNrb23X27Fn19PToO9/5jqanp22e\
FgBgN8cE7PHHH1dOTo4k6Utf+pJGR0clSZ2dnWpoaNCSJUu0YsUKeb1enTp1ys5RAQAO4JiA3epX\
v/qVvvGNb0iSxsbGtHz58tSx4uJijY2N2TUaAMAhcubzP7ZlyxZduHBhxvrBgwdVX1+f+jgnJ0e7\
d++e9d8fiUQUiUQkSRMTE//ZsAAAR5vXgB0/fvwTj//6179WV1eX3nzzTblcLklSUVGRRkZGUl8z\
OjqqoqKi235/IBBQIBCQJFVWVs7R1AAAJ3LMJcSenh795Cc/0e9//3vl5uam1uvq6tTe3q6rV69q\
aGhIg4ODqqqqsnFSAIATzOsZ2Cf53ve+p6tXr6qmpkbSzY0cv/jFL1ReXq5du3Zp9erVysnJ0ZEj\
R7Ro0SKbpwUA2M1lWZZl9xCZUFlZqf7+frvHAACjmPS70zGXEAEA9y4ej89qPRsQMAAwXCgUUkVF\
haLRaNp6NBpVRUWFQqGQPYNlGAEDAIOFQiGFw2HFYjH5/f5UxKLRqPx+v2KxmMLhcFZGjIABgKGS\
8UpKRqy7uzsVr6RsjBgBAwADxeNxtba2pq1t0zZNxiZVW1urydiktmlb2vHW1tasek2MgAGAgfLz\
89Xb2yu32y3pZrz2aZ8O6ZA88uiQDmmf9qUi5na71dvbq/z8fBunnlsEDAAM5fP5UhHrU5+GNSyP\
PHpFr8gjj4Y1rD71peLl8/nsHnlOETAAMJjP51MkElFCCYUVTjsWVlgJJRSJRLIuXhIBAwCjRaNR\
BQIB5SlPQQXTjgUVVJ7yFAgEZmyxzwYEDAAMdetW+WpVpy4bPqNnUpcTq1U9Y4t9tnDMvRABAHcv\
Ho+nbZXvUIckqU99Siih/dqvalWn1pMRO3PmTNZs5OAMDAAMlJ+fr6amprS1DnUo152rrq4u5bpz\
U/FKampqypp4SZyBAYCjHfmvE6mPv/uLr6UdS74xOflm5lt3G/b29qadoQWDwax7IzMBAwCDJaPU\
2tqatlX+1og1NTVlXbwkAgYAxguFQnruuedmXB70+XxZ9ZrXxxEwAHCwj182vJM7RSpb4yWxiQMA\
YCgCBgAwEgEDABiJgAEAjETAAABGImAAACMRMACAkVyWZVl2D5EJjzzyiDwej91jZNzExISWLl1q\
9xjzise8MPCY7TE8PKyLFy/aOsPdytqALRSVlZXq7++3e4x5xWNeGHjM+DRcQgQAGImAAQCMRMAM\
FwgE7B5h3vGYFwYeMz4Nr4EBAIzEGRgAwEgEzECvvfaaysvLdd99983YsdTc3Cyv16vS0lIdPXrU\
pgkzo6enR6WlpfJ6vWppabF7nIzZs2ePCgoKtGbNmtTapUuXVFNTo5KSEtXU1Ojy5cs2Tjj3RkZG\
5Pf7tXr1apWXl+vw4cOSsvtxX7lyRVVVVVq3bp3Ky8sVDAYlSUNDQ9q4caO8Xq+efvppXbt2zeZJ\
HcyCcQYGBqxz585ZX/3qV62//vWvqfWzZ89aFRUV1pUrV6x//vOf1sqVK63r16/bOOncuX79urVy\
5Urrvffes65evWpVVFRYZ8+etXusjPjTn/5kvf3221Z5eXlq7YUXXrCam5sty7Ks5uZm6/vf/75d\
42VELBaz3n77bcuyLOtf//qXVVJSYp09ezarH/eNGzesf//735ZlWda1a9esqqoq6+TJk9Y3v/lN\
69VXX7Usy7KeffZZ6+c//7mdYzoaZ2AGKisrU2lp6Yz1zs5ONTQ0aMmSJVqxYoW8Xq9OnTplw4Rz\
79SpU/J6vVq5cqUWL16shoYGdXZ22j1WRmzatEkPP/xw2lpnZ6caGxslSY2Njero6LBhsswpLCzU\
F7/4RUnSAw88oLKyMo2NjWX143a5XLr//vslSVNTU5qampLL5dKJEye0c+dOSdn3mOcaAcsiY2Nj\
Wr58eerz4uJijY2N2TjR3Mnmx3Y33n//fRUWFkqSHn30Ub3//vs2T5Q5w8PD+tvf/qaNGzdm/eOe\
np7W+vXrVVBQoJqaGq1atUp5eXnKycmRtPD+P5+tHLsHwO1t2bJFFy5cmLF+8OBB1dfX2zARnMLl\
csnlctk9RkZ88MEH2rFjh1566SU9+OCDacey8XEvWrRIp0+fViKR0Pbt23Xu3Dm7RzIKAXOo48eP\
z/p7ioqKNDIykvp8dHRURUVFczmWbbL5sd2NZcuWaXx8XIWFhRofH1dBQYHdI825qakp7dixQ7t3\
79ZTTz0laWE8bknKy8uT3+/XyZMnlUgkdP36deXk5Cy4/89ni0uIWaSurk7t7e26evWqhoaGNDg4\
qKqqKrvHmhMbNmzQ4OCghoaGdO3aNbW3t6uurs7useZNXV2d2traJEltbW1ZdxZuWZb27t2rsrIy\
Pf/886n1bH7cExMTSiQSkqQPP/xQx44dU1lZmfx+v15//XVJ2feY55zdu0gwe2+88YZVVFRkLV68\
2CooKLAef/zx1LEXX3zRWrlypeXz+aw//OEPNk4597q7u62SkhJr5cqV1osvvmj3OBnT0NBgPfro\
o1ZOTo5VVFRk/fKXv7QuXrxofe1rX7O8Xq+1efNmKx6P2z3mnPrzn/9sSbLWrl1rrVu3zlq3bp3V\
3d2d1Y/7nXfesdavX2+tXbvWKi8vt8LhsGVZlvXee+9ZGzZssFatWmXt3LnTunLlis2TOhd34gAA\
GIlLiAAAIxEwAICRCBgAwEgEDABgJAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgE\
DABgJAIGADASAQMAGImAAQCMRMAAAEYiYAAAIxEwAICRCBgAwEgEDABgpP8HJ8GK0OGDmdcAAAAA\
SUVORK5CYII=\
"


    /* set a timeout to make sure all the above elements are created before
       the object is initialized. */
    setTimeout(function() {
        anime412bc04d7f74f1e9ef729c5e2e62c22 = new Animation(frames, img_id, slider_id, 500.0,
                                 loop_select_id);
    }, 0);
  })()
</script>
</div>
</div>
<p>So the best we could do with this would be to move all this to the GPU. Now the problem is that calling something the GPU 1500 times from Python is a really bad idea because there’s this kind of huge communication overhead of this of flow of control and data switching back between the CPU and the GPU. It’s the kernel launching overhead. It’s bad news. So you don’t want to have a really big fast python loop that inside it calls cuda code. GPU code. So we need to make all of this run without the loop, which we could do with broadcasting. So let’s roll up our sleeves and try to get the broadcast version of this working. So generally speaking, the way we tend to do things with broadcasting on a GPU is we create batches or mini batches. So to create batches or mini batches, we don’t just call them batches. Nowadays, we create a batch size. So let’s say we’re going to do a batch size of five, so we’re going to do five at a time. All right, so how do we do five at a time? This is only doing one at a time. How do we do five at a time last before it’s final data and this time little x for our testing. So I’ve got to do everything ahead of time. Little tests as we always do. This is not now X[0] anymore, but it’s X colon bs (X[:bs]), so it’s the first five. This is now the first five items. Okay, so little x is now a five by two metrics. This is how mini batch the first five items as before. Our data itself is 1500 by two. All right. So we need a distance calculation.</p>
<p>But previously our distance calculation, previously a distance calculation only worked if Little x was a single number and it returned just the distance is from that to everything in Big X. But we need something that’s actually going to be return a Matrix right. We’ve got let’s say we’ve got five by two in little x and then in big X we’ve got something much bigger not to scale, obviously we’ve got 1500 by two. And what is the distance between these two things? Well, if you think about it, there’s going to be a distance between item one and item one, but there’s also going to be a distance between item one, item two, and there’s going to be a distance between let’s use a different color for the next one, item two and item one, right? So the output of this is actually going to be a matrix. The distances are actually going to give us a matrix where I mean, it doesn’t matter which way around, we do what we can decide, but if we it this way around for each of the five things in the mini batch, there will be 1500 distances. The distance between every one. So we’re going to need to do a broadcasting to do this calculation. So this is a function that we’re going to create and it’s going to create this, as you can see, five by 1500 output. But let’s say how we get it. So can we do X minus x? No, we can’t. Why is that? That’s because big X is 1500 by two and little x is five by two.</p>
<p>So it’s going to look at remember our roles right to left these compatible? Yes they are They’re the same these compatible. No, they’re not. Okay. Because they’re different. So that’s not possible to do What if though we want it to What if we insert in big X and axis at the start here and in little x we add an axis in the middle here then now these are compatible because you’ve got they’re the same because I should use arrows really? They are compatible because one of them is a one. And these are compatible because one of them is a one as well. So they are all compatible. And what it’s going to do is it’s going to do the subtraction between these directly and it’s going to copy this across all 1500 rows. It will copy it. This is going to be copied and then this across five rows, and then this will be copied across these 1500 rows because what broadcasting does, it’s not really copying, but it’s effectively copying. And so that gives us it can now subtract them and that gives us what we wanted, which is five by 1500 and then also by two because there’s both the x and the y. So that’s why this works. That’s what this is doing here. It’s taking the subtraction, it’s squaring them, and then summing over that last shortest axis, summing over the X and the Y squids and then take square root. I don’t know why as it touched that square root, we could just put dot square root at the end. But same, same. In fact, it’s worth mentioning that. So most things that can do on tensors, you can either write torch. as a function or you can write it as a method. Generally speaking, both should be fine. Not everything, but most things work in both ways. Okay, so now we’ve got this matrix, which is five by 1500. And the nice thing is that our Gaussian kernel doesn’t actually have to be changed to get the weights, believe it or not. And the reason for that is now how do we get the source code? I could move back up there or I can just type Gaussian question mark, question mark and see it. And the nice thing is that this is just this is a scalar, so it broadcasts over anything and then this is also just a scalar. So this is all going to work fine without any fiddling around. Okay, so now we’ve got a 5, 1500 a weight. So that’s the weight for each of the five things. There are mini batch each of the 1500 things, each of them as compared to. And then we’ve got the shape of the data itself, X.shape, which is the 1500 points. So now we want to apply each one of these weights to each of these columns. So we need to add a unit access to the end set at a unit, access to the end, we could say colon, comma, colon, common, none, but dot, dot, dot means all of the axes up until however many you need. So in this case, the last one comma None[…,None].</p>
</section>
<section id="gpu-batched-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="gpu-batched-algorithm">GPU batched algorithm</h2>
<p>To truly accelerate the algorithm, we need to be performing updates on a batch of points per iteration, instead of just one as we were doing.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb56-2">X <span class="op" style="color: #5E5E5E;">=</span> data.clone()</span>
<span id="cb56-3">x <span class="op" style="color: #5E5E5E;">=</span> X[:bs]</span>
<span id="cb56-4">x.shape,X.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([5, 2]), torch.Size([1500, 2]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><span class="kw" style="color: #003B4F;">def</span> dist_b(a,b): <span class="cf" style="color: #003B4F;">return</span> (((a[<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">-</span>b[:,<span class="va" style="color: #111111;">None</span>])<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>).<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">2</span>)).sqrt()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">dist_b(X, x)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 0.000,  3.899,  4.834,  ..., 17.628, 22.610, 21.617],
        [ 3.899,  0.000,  4.978,  ..., 21.499, 26.508, 25.500],
        [ 4.834,  4.978,  0.000,  ..., 19.373, 24.757, 23.396],
        [ 3.726,  0.185,  4.969,  ..., 21.335, 26.336, 25.333],
        [ 6.273,  5.547,  1.615,  ..., 20.775, 26.201, 24.785]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">dist_b(X, x).shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 1500])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">X[<span class="va" style="color: #111111;">None</span>,:].shape, x[:,<span class="va" style="color: #111111;">None</span>].shape, (X[<span class="va" style="color: #111111;">None</span>,:]<span class="op" style="color: #5E5E5E;">-</span>x[:,<span class="va" style="color: #111111;">None</span>]).shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([1, 1500, 2]), torch.Size([5, 1, 2]), torch.Size([5, 1500, 2]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">weight <span class="op" style="color: #5E5E5E;">=</span> gaussian(dist_b(X, x), <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb65-2">weight</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[    0.199,     0.030,     0.011,  ...,     0.000,     0.000,     0.000],
        [    0.030,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],
        [    0.011,     0.009,     0.199,  ...,     0.000,     0.000,     0.000],
        [    0.035,     0.199,     0.009,  ...,     0.000,     0.000,     0.000],
        [    0.001,     0.004,     0.144,  ...,     0.000,     0.000,     0.000]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">weight.shape,X.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([5, 1500]), torch.Size([1500, 2]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">weight[...,<span class="va" style="color: #111111;">None</span>].shape, X[<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([5, 1500, 1]), torch.Size([1, 1500, 2]))</code></pre>
</div>
</div>
<p>So this is going to add an access to the end. So this is going to turn this is going to turn weight dot shape from five comma 1500 to 5 comma 1500 from a one. And this is going to add an access to the start. Remember, it’s the same as X[None] = X[None,:,:]. And so let’s check our rules left, right to left. These are compatible because one of them is one. These are compatible because they’re both the same. And these are compatible because one of them is one. Okay? So it’s going to be copying each weight across to each of the X and Y, which is what we want. We want to we want to weight both of those components and it’s going to copy each of the 1500 points sorry, each of the point five times, because we do in fact want to wait every one of the five things now, mini batches, the separate set of weights for each of them. So that sounds perfect. So that’s how I think through these calculations. Okay. So we can now do that multiplication, which is going to give us something of five by 1500 by two, because we end up with the maximum of our ranks. And then we sum up over those 1500 points and that’s going to give us now five new data points. Now, something that you might notice here is that we’ve got a product and a sum</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">num <span class="op" style="color: #5E5E5E;">=</span> (weight[...,<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">*</span>X[<span class="va" style="color: #111111;">None</span>]).<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb71-2">num.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 2])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1">num</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[367.870, 386.231],
        [518.332, 588.680],
        [329.665, 330.782],
        [527.617, 598.217],
        [231.302, 234.155]])</code></pre>
</div>
</div>
<p>And when you say a product and a sum that tells you maybe we should use einsum. So in this case, we’ve got our weight, we’ve got five by 1500. So let’s call those i and j As for the five and 1500 we’ve got, the X is 1500 by two. Now we want to take the product of that and that so wanted to use the same name for this row. So he use j again. And then k is the number of rows, that’s the two. And then we want to end up with ik. So einsum, exactly the same result. That’s great. But you might recognize this. That’s exactly the einsum Something we had just before when we were doing matrix multiplication. Oh, that is a matrix multiplication. We’ve just re-invented matrix multiplication using this rather nifty. So we could also just use that. And so, you know, again, this is like what I was playing around with this morning as I started to look at this and I was thinking like, Oh, you know, can we simplify this? I don’t like this kind of like messing around of axes and summing over dimensions and whatnot. And so it’s nice to get things down to Einstein or better still, get down to matrix multipliers. It’s just clearer, you know, it’s stuff that we recognize because we use them all the time they all work performance would be pretty similar. I suspect. Okay, so now that we’ve got that, we then need to do our sum and we’ve got our five points. This is our five denominators. So we’ve got our numerator that we calculated up here for our weighted for our weighted average. The denominator is just the sum of the weights, remember.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">torch.einsum(<span class="st" style="color: #20794D;">'ij,jk-&gt;ik'</span>, weight, X)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[367.870, 386.231],
        [518.332, 588.680],
        [329.665, 330.782],
        [527.617, 598.218],
        [231.302, 234.155]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1">weight<span class="op" style="color: #5E5E5E;">@</span>X</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[367.870, 386.231],
        [518.332, 588.680],
        [329.665, 330.782],
        [527.617, 598.218],
        [231.302, 234.155]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">div <span class="op" style="color: #5E5E5E;">=</span> weight.<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb79-2">div.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 1])</code></pre>
</div>
</div>
<p>And so numerator, divided by denominator is our answer. So again, we’ve gone through every we’ve checked out all the dimensions all along the way. So nothing’s going to surprise us. Don’t try and write a function like this. Just bang from scratch. Right. You’re going to drive yourself crazy. Instead, do it step by step. So here’s our meanshift algorithm, clone the data, go through five iterations, and now go from 0 to n and batch size at a time. So Python has something called slices so we can create a slice of X starting at one up to i + batch size. Right. Unless it’s gone past, in which case use n.&nbsp;And so then we’re just copying and pasting each of the lines of code that we had before. Actually had us copy the cells and merge them. Of course I don’t actually copy and paste because it’s slow and boring and there’s my final step to create the new X[s]. And so notice here s is not a single thing. It’s a slice of things you might not have seen slice before, but this is just internally what Python’s doing when he is :, And it’s very convenient when you to use the same slice multiple times. Okay, so let’s do that using Cuda. I would run it first without cuda, but I mean, I’ve done all the steps before, so it should be fine so puppet on the GPU and run meanshift and let’s see how long that takes. It takes one millisecond and previously without GPU, it took 400 milliseconds. And you know, the other thing we should probably think about doing is looking at other batch sizes as well because now we’re looping over batches, right? So if we make the batch size bigger that for loop, it’s going to do less looping. So what if we make that 16? Will that be any faster?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1">num<span class="op" style="color: #5E5E5E;">/</span>div</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[26.376, 27.692],
        [26.101, 29.643],
        [28.892, 28.990],
        [26.071, 29.559],
        [29.323, 29.685]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><span class="kw" style="color: #003B4F;">def</span> meanshift(data, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">500</span>):</span>
<span id="cb83-2">    n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(data)</span>
<span id="cb83-3">    X <span class="op" style="color: #5E5E5E;">=</span> data.clone()</span>
<span id="cb83-4">    <span class="cf" style="color: #003B4F;">for</span> it <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb83-5">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, n, bs):</span>
<span id="cb83-6">            s <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(i, <span class="bu" style="color: null;">min</span>(i<span class="op" style="color: #5E5E5E;">+</span>bs,n))</span>
<span id="cb83-7">            weight <span class="op" style="color: #5E5E5E;">=</span> gaussian(dist_b(X, X[s]), <span class="fl" style="color: #AD0000;">2.5</span>)</span>
<span id="cb83-8"><span class="co" style="color: #5E5E5E;">#             weight = tri(dist_b(X, X[s]), 8)</span></span>
<span id="cb83-9">            div <span class="op" style="color: #5E5E5E;">=</span> weight.<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">1</span>, keepdim<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb83-10">            X[s] <span class="op" style="color: #5E5E5E;">=</span> weight<span class="op" style="color: #5E5E5E;">@</span>X<span class="op" style="color: #5E5E5E;">/</span>div</span>
<span id="cb83-11">    <span class="cf" style="color: #003B4F;">return</span> X</span></code></pre></div>
</div>
<p>Although each iteration still has to launch a new cuda kernel, there are now fewer iterations, and the acceleration from updating a batch of points more than makes up for it.</p>
<p>Oh, I see. Thank you. People on YouTube pointing out that I’m passing batch size, so I actually need to put it here. All right. So if we used a batch size of five, I wonder is missing. Oh, look at that. I’ve totally made it slow now. And in 57 milliseconds. Haha. Okay. 64, All right. Finally, that makes much more sense. Okay, so the bigger, bigger is better. And I guess we could actually do all 5000 at once. Probably nice. All right. Thank you YouTube friends, for solving that bizarre mystery. Okay. All right. So that’s pretty great. I mean, you know, to say that we can you optimize a meanshift like actually google for this to see if it’s been done before. And it’s the kind of thing that people, like write papers about. So I think it’s great that we can do it so easily with PyTorch. And it’s the kind of thing that previously had been considered, you know, a very challenging academic problem to solve. So maybe you can do something similar with some of these. Now, I haven’t told you what these are. So part of the homework is to go read about them and learn about them. dbscan, funnily enough, actually is an algorithm that I accidentally invented and then discovered a year later had already been invented.</p>
<p>LSH comes up all the time, so that’s great. And in fact I have a strong feeling and I’ve been thinking about this for a while, that something like LSH could be used to speed this whole thing up a lot. Because if you think about it and again, maybe already this already exists, I don’t know. But if you think about it, when we did that distance calculation, the vast majority of the the weights or nearly zero. And so it seems pointless to create that big you know kind of eventually 1500 by 1500 matrix. That’s like it’d be much better if we just found the ones that were like pretty close by and just took their average. And so you want an optimized nearest neighbors, basically. And so this is an example of something that can give you a, an, a kind of a fast nearest neighbors algorithm or, you know, there are things like. kd trees and trees and stuff like that. So if you want to, like have a bonus bonus, invent a new meanshift algorithm which picks only the closest points to avoid the quadratic time. All right. So not very often you get an assignment, which is to invent a new meanshift algorithm, I guess a super, super bonus. Super, super bonus. Publish a paper that describes it. All right. You definitely get four points. If you do that, we’ll give you a number of points equal to the impact factor of the journal. You get it published in. Okay. So what I want to do now is move on to calculus, which for some of us may not be our favorite topic. Yeah, that’s funny. I found out that I in some version here already, I didn’t notice. Okay. Or is ahead of his time. That guy. Let’s talk about calculus. If you’re not super comfortable with derivatives and what they are and why we care. called The Essence of Calculus, which I strongly recommend watching. It’s just a pleasure, actually, to watch, as is everything that is on 3blue1brown.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">data <span class="op" style="color: #5E5E5E;">=</span> data.cuda()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">X <span class="op" style="color: #5E5E5E;">=</span> meanshift(data).cpu()</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">5</span> _<span class="op" style="color: #5E5E5E;">=</span>meanshift(data, <span class="dv" style="color: #AD0000;">1250</span>).cpu()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6.06 ms ± 746 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">plot_data(centroids<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span>, X, n_samples)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Homework:</strong> implement k-means clustering, dbscan, locality sensitive hashing, or some other clustering, fast nearest neighbors, or similar algorithm of your choice, on the GPU. Check if your version is faster than a pure python or CPU version.</p>
<p>Bonus: Implement it in APL too!</p>
<p>Super bonus: Invent a new meanshift algorithm which picks only the closest points, to avoid quadratic time.</p>
<p>Super super bonus: Publish a paper that describes it :D</p>
<p>Where do we start. So the good news is just like you don’t have to know much linear algebra at all, you basically just need to know about matrix multiplication. You also don’t need to know much calculus at all. Just derivatives. So let’s think about like what derivatives are. So I’m going to borrow actually the same starting point that when 3blue1brown uses one of their videos is to consider a car, and we’re going to see how far away from home it is at various time points. Okay. So after a minute, let’s say after a second, it’s traveled five meters and then after 2 seconds, it’s traveled ten meters. Okay. And after 3 seconds, you can probably guess it’s traveled 15 meters. So there’s this concept here of a I got it the wrong way around. Obviously. So time, distance. Okay. So there’s this concept of Yeah, of like location. It’s like how far how far if you traveled at a particular point in time. So we can look at one of these points and find out how far that car has gone. We could also take two points and we can say, where did it start at the start of those two points and where did it finish at the end of those two points. And we can say between those two points, how much time passed and how far did they travel? In 2 seconds. They traveled ten meters. So we could now also say, all right, well, the slope of something, let’s rise over, run. You’ll see.ten meters in 2 seconds. And notice we don’t just divide the numbers. We also divide the units. We get five meters per second. So this here is now changed the dimensions entirely. We’re now not looking at distance, but we’re looking at speed or velocity. And it’s equal to rise over run. It’s equal to the rate of change. And what it says really is as time the X-axis goes up by one second, what happens to the distance in meters as one second passes? How does the number of meters change?</p>
<p>And so maybe these aren’t points at all. Maybe there’s a function that it’s a continuum of points, and so you can do that for the function. So the function is a function of time. Distance is a function of time. And so we could say, what’s the slope of that function? And we can get the slope from point A to point B using over run. So from T one to T two the amount of distance, that’s the amount of time that’s passed is T2 minus T1. That’s how much time has passed that say this is t one, this is t two and the distance that they’ve traveled while they’ve moved from wherever they are at the end to wherever they were at the start. So that’s the change in distance, divided by the change in time, Change in distance, divided by change in time. Okay, So that’s why. So another way. Now the thing is, when we talk about calculus, we talk about finding a slope, but we talk about finding a slope of something that’s more often more tricky than this, right? We have slopes of things that look more like this and we say, what’s this slope absent Terrible attempt at drawing? Let’s maybe put it over here because I’m left handed. What’s this slope now? What does it mean to have like the idea of a velocity at an exact moment in time? It doesn’t mean anything, you know, at an exact moment in time, you’re just like it’s frozen. Right? What’s happening exactly now? But what you can do is you can say, well, what’s the change in time between a bit before a point and bit after a point? And what’s the change in distance between a bit before our point and a bit after our point? And so you can do the same kind of rise over run the thing, right? But you can make that distance between T2 and T1 smaller and smaller and smaller. So let’s rewrite this in a slightly different way. Let’s call the denominator the distance between T1 plus a little bit I’ll call it d, it’s that minus T1. So this is T2 = T1+d, right?</p>
<p>It’s T1 pluss a little bit. So we say oh his T1. Let’s add a little bit and notice that we when we write it this well let’s actually, let’s do the rest of it. So now f52 becomes f of t one plus a little bit and this is the same. And now notice here that t one plus t minus t one. We can delete all that because it just comes out to d.&nbsp;So this is another way of calculating the slope of our function. And as you get smaller and smaller and smaller, we’re kind of getting a triangle that’s tinier and tinier and tinier and it still makes sense it still that some time has passed and the car has moved, right? But it’s just smaller and smaller amounts of time. Now, if you did calculus at that college or at school, you might have done all this stuff messing around with limits, Epsilon Delta and blah, blah, blah. I’ve got really good news. It turns out you can actually just think of this d as a really small number where d is the difference if, uh, it’s. And so when we calculate the slope, we can write it in a slightly different way as the change in dY divided by the change in dX, this here is the change in dY, and this here is the change in dX. And so in other words, this here is a very small number. A very small number, and this here is the result in the function of changing by that very small number. And this way of thinking about calculus is known as the calculus of in infinitesimal. and it’s how Leibniz its originally developed it. And it’s been turned into a whole theory nowadays. And the reason I talk about it here is because when we do calculus, you’ll see me doing stuff all the time where. I act like the dx is a really small number. And when I was at school I was told I wasn’t allowed to do that. I’ve since learned that it’s totally fine, do that.</p>
<p>So, for example, next lesson, we’re going to be looking at the chain role, which looks like this. The dy/dx equals to dy/du * du/xd And I’m just going to say, Oh, these two small numbers can cancel out. And that’s why obviously they’re the same thing and that’s all going to work out nicely. So what do you know? What would be very helpful would be if before the next lesson, if you’re not totally up to date with your, you know, remembering all the stuff you did in high school about calculus is watch the 3blue1brown Course, we are not going to be looking. I don’t think at all that integration. So you don’t have to worry about that. Also we are not going to on the whole be doing any derivatives by hand. So for example, there are rules such as to why the dy/dx, if y equals x squared is 2x, these kind of rules, you’re not really going to have to learn because PyTorch is going to do them all for you. The one that we care about is going to be the chain role that we’re going to learn about that next time. Okay. I hope I don’t get beaten to a bloody pulp the next time I walk into a mathematician’s conference, I suspect I might. But hopefully I get away with this. I think it’s safe. We’ll see how we go. So thanks, everybody very much for joining me and really look forward to seeing you next time where we’re going to do backpropagation from scratch. We’ve already learned to multiply matrices, so once we’ve got backpropagation as well, we’ll be ready to train a neural network. All right. Thanks So.</p>


</section>
</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing Stable Diffusion from Scratch 4/index.html</guid>
  <pubDate>Sun, 19 Mar 2023 20:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 3</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index.html</link>
  <description><![CDATA[ 



<p>Things you should know and practice after this lecture : <br> 1- Einsum <br> 2- 2 ways you can do einsum / matmul in pytorch <br> 3- Using GPU / Cuda <br> 4- args and kwargs <br></p>
<p>All credits go to fastai and all mistakes most likely is mine. This notebook connected to pass two notebooks. So the code is here but I remove most of explanation. You can check past two posts. Enjoy learning …</p>
<section id="matrix-multiplication-from-foundations" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication-from-foundations">Matrix multiplication from foundations</h2>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pickle, gzip, math, os, time, shutil, matplotlib <span class="im" style="color: #00769E;">as</span> mpl, matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
</section>
<section id="get-data" class="level2">
<h2 class="anchored" data-anchor-id="get-data">Get data</h2>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb2-2">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb2-3">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-4">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<p><a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve">urlretrieve</a> - (read the docs!)</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<div class="cell" data-outputid="1a0b666b-dbec-4d43-ba2f-906a72022364" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 Mar 18 22:05 mnist.pkl.gz</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="46954d1b-2e2d-4d17-8aba-7116c0b443e3" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">lst1 <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(x_train[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb7-2">vals <span class="op" style="color: #5E5E5E;">=</span> lst1[<span class="dv" style="color: #AD0000;">200</span>:<span class="dv" style="color: #AD0000;">210</span>]</span>
<span id="cb7-3">vals</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>[0.0,
 0.0,
 0.0,
 0.19140625,
 0.9296875,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125]</code></pre>
</div>
</div>
<div class="cell" data-outputid="71f7b4f9-25e3-4c2a-842c-14d43aef599a" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="bu" style="color: null;">len</span>(lst1)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>784</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> chunks(x, sz):</span>
<span id="cb11-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">len</span>(x), sz): <span class="cf" style="color: #003B4F;">yield</span> x[i:i<span class="op" style="color: #5E5E5E;">+</span>sz]</span></code></pre></div>
</div>
<div class="cell" data-outputid="04901837-77b1-4380-ae43-452451809395" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;">list</span>(chunks(vals, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],
 [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]</code></pre>
</div>
</div>
<div class="cell" data-outputid="0debd072-2c30-4033-db94-c5a9aeb8684b" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb14-2">plt.imshow(<span class="bu" style="color: null;">list</span>(chunks(lst1, <span class="dv" style="color: #AD0000;">28</span>)))<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><a href="https://docs.python.org/3/library/itertools.html#itertools.islice">islice</a></p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;">from</span> itertools <span class="im" style="color: #00769E;">import</span> islice</span></code></pre></div>
</div>
<div class="cell" data-outputid="82051e57-d1b3-40ec-b733-39f274036246" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(vals)</span>
<span id="cb16-2">islice(it, <span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>&lt;itertools.islice at 0x7fc00c934040&gt;</code></pre>
</div>
</div>
<div class="cell" data-outputid="268b5d89-cb31-4b34-ab8f-c9430926d354" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>[0.0, 0.0, 0.0, 0.19140625, 0.9296875]</code></pre>
</div>
</div>
<div class="cell" data-outputid="3ce1be4d-923c-4ef5-898d-cb28af774cf3" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]</code></pre>
</div>
</div>
<div class="cell" data-outputid="52d1cfae-4172-43fb-f4d6-87d5f18fc75a" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>[]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(lst1)</span>
<span id="cb24-2">img <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">iter</span>(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">28</span>)), []))</span></code></pre></div>
</div>
<div class="cell" data-outputid="d0439002-711d-4325-c1de-47340d2a196d" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">plt.imshow(img)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Use this link to learn more about <a href="https://docs.python.org/3.10/library/functions.html?highlight=iter#iter">iter</a></p>
</section>
<section id="matrix-and-tensor" class="level2">
<h2 class="anchored" data-anchor-id="matrix-and-tensor">Matrix and tensor</h2>
<div class="cell" data-outputid="bfa1af01-8818-40f2-823e-efdf42340a79" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">img[<span class="dv" style="color: #AD0000;">20</span>][<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>0.98828125</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="kw" style="color: #003B4F;">class</span> Matrix:</span>
<span id="cb28-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, xs): <span class="va" style="color: #111111;">self</span>.xs <span class="op" style="color: #5E5E5E;">=</span> xs</span>
<span id="cb28-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, idxs): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.xs[idxs[<span class="dv" style="color: #AD0000;">0</span>]][idxs[<span class="dv" style="color: #AD0000;">1</span>]]</span></code></pre></div>
</div>
<div class="cell" data-outputid="50d5123c-de82-474d-adb9-75b942524b2b" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">m <span class="op" style="color: #5E5E5E;">=</span> Matrix(img)</span>
<span id="cb29-2">m[<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.98828125</code></pre>
</div>
</div>
<p>Now we can use pytorch.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb31-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor</span></code></pre></div>
</div>
<div class="cell" data-outputid="fcea7c13-e037-40fc-edfe-ed911efbe43d" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">tensor([<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>tensor([1, 2, 3])</code></pre>
</div>
</div>
<div class="cell" data-outputid="264cd4f5-f507-4173-e397-34689667e17d" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">x_train,y_train,x_valid,y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, (x_train,y_train,x_valid,y_valid))</span>
<span id="cb34-2">x_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>torch.Size([50000, 784])</code></pre>
</div>
</div>
<div class="cell" data-outputid="b691fd0b-c566-46c2-eeb5-ed42c7a8c685" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">x_train.<span class="bu" style="color: null;">type</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>'torch.FloatTensor'</code></pre>
</div>
</div>
<p><a href="https://pytorch.org/docs/stable/tensors.html">Tensor documentation</a></p>
<div class="cell" data-outputid="a333e27b-0d9d-4a5c-a99f-660d0e4b2b3b" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">imgs <span class="op" style="color: #5E5E5E;">=</span> x_train.reshape((<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb38-2">imgs.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>torch.Size([50000, 28, 28])</code></pre>
</div>
</div>
<div class="cell" data-outputid="5ec757b6-ff21-474c-a849-0f4c1a7bf6a6" data-execution_count="27">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">plt.imshow(imgs[<span class="dv" style="color: #AD0000;">0</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>vector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor</p>
<div class="cell" data-outputid="88abcbdb-49fc-4519-cbd4-82790666c8a0" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">imgs[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor(0.9883)</code></pre>
</div>
</div>
<p>Use destructring again. n number of images. c is full number of colums (784)</p>
<div class="cell" data-outputid="e88c7e29-2753-4155-dc2d-726b0e7b0153" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">n,c <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb43-2">y_train, y_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))</code></pre>
</div>
</div>
<p>in y_train we can find min and max of it.</p>
<div class="cell" data-outputid="f10bbc01-1515-416b-b25e-1457c521389b" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="bu" style="color: null;">min</span>(y_train),<span class="bu" style="color: null;">max</span>(y_train)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(tensor(0), tensor(9))</code></pre>
</div>
</div>
<div class="cell" data-outputid="56844e30-a201-4350-e5ff-0fe3f5203550" data-execution_count="31">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">y_train.<span class="bu" style="color: null;">min</span>(), y_train.<span class="bu" style="color: null;">max</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>(tensor(0), tensor(9))</code></pre>
</div>
</div>
</section>
<section id="random-numbers" class="level2">
<h2 class="anchored" data-anchor-id="random-numbers">Random numbers</h2>
<p>Based on the Wichmann Hill algorithm used before Python 2.3.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">rnd_state <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span></span>
<span id="cb49-2"><span class="kw" style="color: #003B4F;">def</span> seed(a):</span>
<span id="cb49-3">    <span class="kw" style="color: #003B4F;">global</span> rnd_state</span>
<span id="cb49-4">    a, x <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30268</span>)</span>
<span id="cb49-5">    a, y <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30306</span>)</span>
<span id="cb49-6">    a, z <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30322</span>)</span>
<span id="cb49-7">    rnd_state <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(x)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">int</span>(y)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">int</span>(z)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="2ec95d64-f852-419a-dafe-eb69c2008ccd" data-execution_count="33">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">seed(<span class="dv" style="color: #AD0000;">457428938475</span>)</span>
<span id="cb50-2">rnd_state</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>(4976, 20238, 499)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><span class="kw" style="color: #003B4F;">def</span> rand():</span>
<span id="cb52-2">    <span class="kw" style="color: #003B4F;">global</span> rnd_state</span>
<span id="cb52-3">    x, y, z <span class="op" style="color: #5E5E5E;">=</span> rnd_state</span>
<span id="cb52-4">    x <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">171</span> <span class="op" style="color: #5E5E5E;">*</span> x) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30269</span></span>
<span id="cb52-5">    y <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">172</span> <span class="op" style="color: #5E5E5E;">*</span> y) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30307</span></span>
<span id="cb52-6">    z <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">170</span> <span class="op" style="color: #5E5E5E;">*</span> z) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30323</span></span>
<span id="cb52-7">    rnd_state <span class="op" style="color: #5E5E5E;">=</span> x,y,z</span>
<span id="cb52-8">    <span class="cf" style="color: #003B4F;">return</span> (x<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30269</span> <span class="op" style="color: #5E5E5E;">+</span> y<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30307</span> <span class="op" style="color: #5E5E5E;">+</span> z<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30323</span>) <span class="op" style="color: #5E5E5E;">%</span> <span class="fl" style="color: #AD0000;">1.0</span></span></code></pre></div>
</div>
<div class="cell" data-outputid="7c01d401-a948-4c69-94c9-1942d66d1ebd" data-execution_count="35">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">rand(),rand(),rand()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)</code></pre>
</div>
</div>
<div class="cell" data-outputid="6ac5c966-92ef-4475-f5c9-e2d9fbdc8ad5" data-execution_count="36">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="cf" style="color: #003B4F;">if</span> os.fork(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In parent: </span><span class="sc" style="color: #5E5E5E;">{</span>rand()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb55-2"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb55-3">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In child: </span><span class="sc" style="color: #5E5E5E;">{</span>rand()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb55-4">    os._exit(os.EX_OK)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In parent: 0.9559050644103264
In child: 0.9559050644103264</code></pre>
</div>
</div>
<div class="cell" data-outputid="2694340f-5952-43c4-8cb3-4171ed37c56f" data-execution_count="37">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="cf" style="color: #003B4F;">if</span> os.fork(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In parent: </span><span class="sc" style="color: #5E5E5E;">{</span>torch<span class="sc" style="color: #5E5E5E;">.</span>rand(<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb57-2"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb57-3">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In child: </span><span class="sc" style="color: #5E5E5E;">{</span>torch<span class="sc" style="color: #5E5E5E;">.</span>rand(<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb57-4">    os._exit(os.EX_OK)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In parent: tensor([0.6953])
In child: tensor([0.6953])</code></pre>
</div>
</div>
<div class="cell" data-outputid="db52f8d5-8f55-4abd-cd02-5ad1fb6997f3" data-execution_count="38">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">plt.plot([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">50</span>)])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="a5b527cb-89cb-4119-8f28-1759c0533f56" data-execution_count="39">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">plt.hist([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">10000</span>)])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>%timeit check the time of excution.</p>
<div class="cell" data-outputid="363d17a6-6f36-46a6-c22f-f661c62841e5" data-execution_count="40">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> <span class="bu" style="color: null;">list</span>(chunks([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">7840</span>)], <span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4.6 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<p>pytorch version is faster.</p>
<div class="cell" data-outputid="35a41cc7-62dc-45b7-d7bc-b16ec9ace12a" data-execution_count="41">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> torch.randn(<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>103 µs ± 42.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
</section>
<section id="matrix-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication">Matrix multiplication</h2>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb65-2">weights <span class="op" style="color: #5E5E5E;">=</span> torch.randn(<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb65-3">bias <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">m1 <span class="op" style="color: #5E5E5E;">=</span> x_valid[:<span class="dv" style="color: #AD0000;">5</span>]</span>
<span id="cb66-2">m2 <span class="op" style="color: #5E5E5E;">=</span> weights</span></code></pre></div>
</div>
<div class="cell" data-outputid="4a007d7c-f416-4c12-e4a0-daaeeafdb2ba" data-execution_count="44">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">m1.shape,m2.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>(torch.Size([5, 784]), torch.Size([784, 10]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="aa7a85f3-c1e0-4add-828f-c3e61a9270dc" data-execution_count="45">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">ar,ac <span class="op" style="color: #5E5E5E;">=</span> m1.shape <span class="co" style="color: #5E5E5E;"># n_rows * n_cols</span></span>
<span id="cb69-2">br,bc <span class="op" style="color: #5E5E5E;">=</span> m2.shape</span>
<span id="cb69-3">(ar,ac),(br,bc)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>((5, 784), (784, 10))</code></pre>
</div>
</div>
<div class="cell" data-outputid="2aa60244-466a-4148-96cb-24d8cf74c602" data-execution_count="46">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">t1 <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb71-2">t1.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>torch.Size([5, 10])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):         <span class="co" style="color: #5E5E5E;"># 5</span></span>
<span id="cb73-2">    <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc):     <span class="co" style="color: #5E5E5E;"># 10</span></span>
<span id="cb73-3">        <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ac): <span class="co" style="color: #5E5E5E;"># 784</span></span>
<span id="cb73-4">            t1[i,j] <span class="op" style="color: #5E5E5E;">+=</span> m1[i,k] <span class="op" style="color: #5E5E5E;">*</span> m2[k,j]</span></code></pre></div>
</div>
<div class="cell" data-outputid="a267c06c-6ac8-468c-8bf3-74fa615bbbf9" data-execution_count="48">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">t1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,
          -3.4375, -11.4696,  -2.1153],
        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,
           2.1577, -15.2772,  -2.6758],
        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,
          -5.4446, -20.6758,  13.5657],
        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,
         -12.9776,  -6.4443,   3.6376],
        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,
          -3.7862,  -8.9827,   5.2797]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="eb090224-1b9a-490f-9762-f52a2c14b324" data-execution_count="49">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">t1.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>torch.Size([5, 10])</code></pre>
</div>
</div>
<div class="cell" data-outputid="3fa3a2d2-2097-47f4-f1d6-bd81e2414a01" data-execution_count="50">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb78-2">t1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],
        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],
        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],
        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],
        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb80-2">np.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb81-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb81-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb81-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb81-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc):</span>
<span id="cb81-6">            <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ac): c[i,j] <span class="op" style="color: #5E5E5E;">+=</span> a[i,k] <span class="op" style="color: #5E5E5E;">*</span> b[k,j]</span>
<span id="cb81-7">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<p>How long does it take to run ? Man it too much. It is o(n^3) and it is so slow</p>
<div class="cell" data-outputid="313b8c7b-62e9-4d8b-c814-8eaa45609202" data-execution_count="53">
<div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><span class="op" style="color: #5E5E5E;">%</span>time _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 621 ms, sys: 1.34 ms, total: 623 ms
Wall time: 629 ms</code></pre>
</div>
</div>
<div class="cell" data-outputid="e0307511-015c-4c04-9822-2e1a18ef5a4c" data-execution_count="54">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">ar<span class="op" style="color: #5E5E5E;">*</span>bc<span class="op" style="color: #5E5E5E;">*</span>ac</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>39200</code></pre>
</div>
</div>
</section>
<section id="numba" class="level2">
<h2 class="anchored" data-anchor-id="numba">Numba</h2>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><span class="im" style="color: #00769E;">from</span> numba <span class="im" style="color: #00769E;">import</span> njit</span></code></pre></div>
</div>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><span class="at" style="color: #657422;">@njit</span></span>
<span id="cb87-2"><span class="kw" style="color: #003B4F;">def</span> dot(a,b):</span>
<span id="cb87-3">    res <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb87-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(a)): res<span class="op" style="color: #5E5E5E;">+=</span>a[i]<span class="op" style="color: #5E5E5E;">*</span>b[i]</span>
<span id="cb87-5">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
</div>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><span class="im" style="color: #00769E;">from</span> numpy <span class="im" style="color: #00769E;">import</span> array</span></code></pre></div>
</div>
<div class="cell" data-outputid="76e35066-5c7e-48d8-a5f5-dc0663d0be35" data-execution_count="58">
<div class="sourceCode cell-code" id="cb89" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><span class="op" style="color: #5E5E5E;">%</span>time dot(array([<span class="fl" style="color: #AD0000;">1.</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>]),array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">4</span>]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 363 ms, sys: 113 ms, total: 476 ms
Wall time: 445 ms</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>20.0</code></pre>
</div>
</div>
<div class="cell" data-outputid="f9e787d2-0484-4699-8a4a-31bed0979f88" data-execution_count="59">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><span class="op" style="color: #5E5E5E;">%</span>time dot(array([<span class="fl" style="color: #AD0000;">1.</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>]),array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">4</span>]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 19 µs, sys: 4 µs, total: 23 µs
Wall time: 26 µs</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>20.0</code></pre>
</div>
</div>
<p>Now only two of our loops are running in Python, not three:</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb95" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb95-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb95-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb95-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb95-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc): c[i,j] <span class="op" style="color: #5E5E5E;">=</span> dot(a[i,:], b[:,j])</span>
<span id="cb95-6">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1">m1a,m2a <span class="op" style="color: #5E5E5E;">=</span> m1.numpy(),m2.numpy()</span></code></pre></div>
</div>
<p>This is the test.</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb97" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">test_close(t1,matmul(m1a, m2a))</span></code></pre></div>
</div>
<p>2000 time faster. We change inner most loop.</p>
<div class="cell" data-outputid="adaccf40-70c2-4e62-b344-e477f91a4812" data-execution_count="64">
<div class="sourceCode cell-code" id="cb99" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> matmul(m1a,m2a)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>383 µs ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
</section>
<section id="elementwise-ops" class="level2">
<h2 class="anchored" data-anchor-id="elementwise-ops">Elementwise ops</h2>
<p><a href="https://tryapl.org/">TryAPL</a></p>
<div class="cell" data-outputid="523f4c95-d4ba-4ced-8ec9-c2b0d30f27bb" data-execution_count="65">
<div class="sourceCode cell-code" id="cb101" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1">a <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="fl" style="color: #AD0000;">10.</span>, <span class="dv" style="color: #AD0000;">6</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb101-2">b <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="fl" style="color: #AD0000;">2.</span>, <span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">7</span>])</span>
<span id="cb101-3">a,b</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))</code></pre>
</div>
</div>
<p>Elementwise addition</p>
<div class="cell" data-outputid="3270a172-d520-4246-c5a8-96ec205dd2d9" data-execution_count="66">
<div class="sourceCode cell-code" id="cb103" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1">a <span class="op" style="color: #5E5E5E;">+</span> b</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>tensor([12., 14.,  3.])</code></pre>
</div>
</div>
<p>Check lecture for awesome implementation of mean.</p>
<div class="cell" data-outputid="b5239b72-3301-4443-ada3-6ab3b8877607" data-execution_count="67">
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1">(a <span class="op" style="color: #5E5E5E;">&lt;</span> b).<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>tensor(0.67)</code></pre>
</div>
</div>
<p>Rank two tensor , aka Matrix.</p>
<div class="cell" data-outputid="1d86b6a4-8564-4768-8e52-5d075e35e4c6" data-execution_count="68">
<div class="sourceCode cell-code" id="cb107" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1">m <span class="op" style="color: #5E5E5E;">=</span> tensor([[<span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>], [<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">6</span>], [<span class="dv" style="color: #AD0000;">7</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">9</span>]])<span class="op" style="color: #5E5E5E;">;</span> m</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])</code></pre>
</div>
</div>
</section>
<section id="frobenius-norm" class="level1">
<h1>Frobenius norm:</h1>
<p><img src="https://latex.codecogs.com/png.latex?%5C%7C%20A%20%5C%7C_F%20=%20%5Cleft(%20%5Csum_%7Bi,j=1%7D%5En%20%7C%20a_%7Bij%7D%20%7C%5E2%20%5Cright)%5E%7B1/2%7D"></p>
<p><em>Hint</em>: you don’t normally need to write equations in LaTeX yourself, instead, you can click ‘edit’ in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click “Download: Other formats” in the top right, then “Download source”; rename the downloaded file to end in <code>.tgz</code> if it doesn’t already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:</p>
<div class="sourceCode" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode latex code-with-copy"><code class="sourceCode latex"><span id="cb109-1"><span class="ss" style="color: #20794D;">$</span><span class="sc" style="color: #5E5E5E;">\|</span><span class="ss" style="color: #20794D;"> A </span><span class="sc" style="color: #5E5E5E;">\|</span><span class="ss" style="color: #20794D;">_F = </span><span class="sc" style="color: #5E5E5E;">\left</span><span class="ss" style="color: #20794D;">( </span><span class="sc" style="color: #5E5E5E;">\sum</span><span class="ss" style="color: #20794D;">_{i,j=1}^n | a_{ij} |^2 </span><span class="sc" style="color: #5E5E5E;">\right</span><span class="ss" style="color: #20794D;">)^{1/2}$</span></span></code></pre></div>
<div class="cell" data-outputid="20dc5fcc-24cd-4b3b-8b9c-5c475f3cbd12" data-execution_count="69">
<div class="sourceCode cell-code" id="cb110" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1">sf <span class="op" style="color: #5E5E5E;">=</span> (m<span class="op" style="color: #5E5E5E;">*</span>m).<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb110-2">sf</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>tensor(285.)</code></pre>
</div>
</div>
<div class="cell" data-outputid="cc40f4ec-6677-4f68-81f8-2f5a6fe7d1bd" data-execution_count="70">
<div class="sourceCode cell-code" id="cb112" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1">sf.sqrt()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>tensor(16.88)</code></pre>
</div>
</div>
<div class="cell" data-outputid="222991c0-a86f-4822-d7f0-433abff41fee" data-execution_count="71">
<div class="sourceCode cell-code" id="cb114" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1">m[<span class="dv" style="color: #AD0000;">2</span>,:],m[:,<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>(tensor([7., 8., 9.]), tensor([3., 6., 9.]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="d837387a-aab0-43eb-d82a-0d59d704f4ae" data-execution_count="72">
<div class="sourceCode cell-code" id="cb116" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1">m[<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>tensor([7., 8., 9.])</code></pre>
</div>
</div>
<p>We can use elementwise operation and get ride of inner loop.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb118" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb118-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb118-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb118-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb118-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc): c[i,j] <span class="op" style="color: #5E5E5E;">=</span> (a[i,:] <span class="op" style="color: #5E5E5E;">*</span> b[:,j]).<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb118-6">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<p>Test to see they are the same.</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb119" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1">test_close(t1,matmul(m1, m2))</span></code></pre></div>
</div>
<div class="cell" data-outputid="34e92552-e24d-4b38-cf3a-5c2c0ae714cc" data-execution_count="75">
<div class="sourceCode cell-code" id="cb120" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.2 ms ± 168 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<p>Now that we wrote it , we can use equivalent of pytorch. (torch.dot)</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb122" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb122-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb122-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb122-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb122-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc): c[i,j] <span class="op" style="color: #5E5E5E;">=</span> torch.dot(a[i,:], b[:,j])</span>
<span id="cb122-6">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb123" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1">test_close(t1,matmul(m1, m2))</span></code></pre></div>
</div>
<div class="cell" data-outputid="8d2c4f35-191f-436a-9f81-f1d0f80d3766" data-execution_count="78">
<div class="sourceCode cell-code" id="cb124" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>831 µs ± 104 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<section id="broadcasting" class="level2">
<h2 class="anchored" data-anchor-id="broadcasting">Broadcasting</h2>
<p>The term <strong>broadcasting</strong> describes how arrays with different shapes are treated during arithmetic operations.</p>
<p>From the <a href="https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html">Numpy Documentation</a>:</p>
<pre><code>The term broadcasting describes how numpy treats arrays with 
different shapes during arithmetic operations. Subject to certain 
constraints, the smaller array is “broadcast” across the larger 
array so that they have compatible shapes. Broadcasting provides a 
means of vectorizing array operations so that looping occurs in C
instead of Python. It does this without making needless copies of 
data and usually leads to efficient algorithm implementations.</code></pre>
<p>In addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.</p>
<p><em>This section was adapted from <a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression">Chapter 4</a> of the fast.ai <a href="https://github.com/fastai/numerical-linear-algebra">Computational Linear Algebra</a> course.</em></p>
<section id="broadcasting-with-a-scalar" class="level3">
<h3 class="anchored" data-anchor-id="broadcasting-with-a-scalar">Broadcasting with a scalar</h3>
<div class="cell" data-outputid="578283fa-c168-4a7a-bc6a-a40add8ac48a" data-execution_count="79">
<div class="sourceCode cell-code" id="cb127" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1">a</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>tensor([10.,  6., -4.])</code></pre>
</div>
</div>
<p>Simplest way of broadcasting.</p>
<div class="cell" data-outputid="bc6c815f-87dc-4d56-8b07-17cede1eed8e" data-execution_count="80">
<div class="sourceCode cell-code" id="cb129" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1">a <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="80">
<pre><code>tensor([ True,  True, False])</code></pre>
</div>
</div>
<div class="cell" data-outputid="1c508e2a-00f8-4547-83c6-f8994d95f5d6" data-execution_count="81">
<div class="sourceCode cell-code" id="cb131" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1">a <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>tensor([11.,  7., -3.])</code></pre>
</div>
</div>
<div class="cell" data-outputid="8ec7ea68-f4ac-4535-8daf-6bf80838fd15" data-execution_count="82">
<div class="sourceCode cell-code" id="cb133" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1">m</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])</code></pre>
</div>
</div>
<p>multiply</p>
<div class="cell" data-outputid="3a2341ba-eec5-402f-da59-986541579178" data-execution_count="83">
<div class="sourceCode cell-code" id="cb135" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>m</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>tensor([[ 2.,  4.,  6.],
        [ 8., 10., 12.],
        [14., 16., 18.]])</code></pre>
</div>
</div>
</section>
<section id="broadcasting-a-vector-to-a-matrix" class="level3">
<h3 class="anchored" data-anchor-id="broadcasting-a-vector-to-a-matrix">Broadcasting a vector to a matrix</h3>
<div class="cell" data-outputid="6be1ce2b-9d70-4e0c-c0e8-289c862627b0" data-execution_count="84">
<div class="sourceCode cell-code" id="cb137" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1">c <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="fl" style="color: #AD0000;">10.</span>,<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">30</span>])<span class="op" style="color: #5E5E5E;">;</span> c</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>tensor([10., 20., 30.])</code></pre>
</div>
</div>
<div class="cell" data-outputid="ff6b72a8-b4f1-410a-add0-085d1d6b6cfb" data-execution_count="85">
<div class="sourceCode cell-code" id="cb139" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1">m</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="b057edb5-23d9-4ba6-f5b3-d6a601a27cd2" data-execution_count="86">
<div class="sourceCode cell-code" id="cb141" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1">m.shape,c.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>(torch.Size([3, 3]), torch.Size([3]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="0f102cfa-4ed5-4abe-b09c-7d999542583a" data-execution_count="87">
<div class="sourceCode cell-code" id="cb143" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1">m <span class="op" style="color: #5E5E5E;">+</span> c</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="719caac4-6678-4324-9bfa-1a2086c40daf" data-execution_count="88">
<div class="sourceCode cell-code" id="cb145" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1">c <span class="op" style="color: #5E5E5E;">+</span> m</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb147" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1">t <span class="op" style="color: #5E5E5E;">=</span> c.expand_as(m)</span></code></pre></div>
</div>
<div class="cell" data-outputid="1b784a77-d186-41c3-e254-59916b7922f7" data-execution_count="90">
<div class="sourceCode cell-code" id="cb148" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1">t</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>tensor([[10., 20., 30.],
        [10., 20., 30.],
        [10., 20., 30.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="4d4f3f08-173f-447e-f505-ac4d9a6fae21" data-execution_count="91">
<div class="sourceCode cell-code" id="cb150" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1">m <span class="op" style="color: #5E5E5E;">+</span> t</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="9b0f8dfb-363d-4ecf-ba25-fa380bcb5f4f" data-execution_count="92">
<div class="sourceCode cell-code" id="cb152" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1">t.storage()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code> 10.0
 20.0
 30.0
[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]</code></pre>
</div>
</div>
<div class="cell" data-outputid="83e4e679-17fc-4d52-b90c-5c7dbae2d206" data-execution_count="93">
<div class="sourceCode cell-code" id="cb154" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1">t.stride(), t.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre><code>((0, 1), torch.Size([3, 3]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="c4cb1bf1-aafe-480c-a0e0-7549a255c5da" data-execution_count="94">
<div class="sourceCode cell-code" id="cb156" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1">c.unsqueeze(<span class="dv" style="color: #AD0000;">0</span>), c[<span class="va" style="color: #111111;">None</span>, :]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="6cad1d78-4e7f-4adf-ed61-23e5db54bb2c" data-execution_count="95">
<div class="sourceCode cell-code" id="cb158" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1">c.shape, c.unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>(torch.Size([3]), torch.Size([1, 3]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="3e2c619d-8ee1-420d-f555-d5a90306ca8f" data-execution_count="96">
<div class="sourceCode cell-code" id="cb160" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1">c.unsqueeze(<span class="dv" style="color: #AD0000;">1</span>), c[:, <span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>(tensor([[10.],
         [20.],
         [30.]]), tensor([[10.],
         [20.],
         [30.]]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="bcb4552c-d7aa-4948-aed3-04f93a5e3b42" data-execution_count="97">
<div class="sourceCode cell-code" id="cb162" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1">c.shape, c.unsqueeze(<span class="dv" style="color: #AD0000;">1</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>(torch.Size([3]), torch.Size([3, 1]))</code></pre>
</div>
</div>
<p>we can avoid : and say c[None].</p>
<div class="cell" data-outputid="660ade84-ada9-4343-8da2-876fdb3a8f0a" data-execution_count="98">
<div class="sourceCode cell-code" id="cb164" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1">c[<span class="va" style="color: #111111;">None</span>].shape,c[...,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="98">
<pre><code>(torch.Size([1, 3]), torch.Size([3, 1]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="9c89375e-b183-44da-a8e0-dadc934816d5" data-execution_count="99">
<div class="sourceCode cell-code" id="cb166" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1">c[:,<span class="va" style="color: #111111;">None</span>].expand_as(m)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="99">
<pre><code>tensor([[10., 10., 10.],
        [20., 20., 20.],
        [30., 30., 30.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="ae5a895b-3c7e-4be9-c5a9-6e6a138b05bb" data-execution_count="100">
<div class="sourceCode cell-code" id="cb168" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1">m <span class="op" style="color: #5E5E5E;">+</span> c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>tensor([[11., 12., 13.],
        [24., 25., 26.],
        [37., 38., 39.]])</code></pre>
</div>
</div>
<p>This adding the vector to each row.</p>
<div class="cell" data-outputid="f1c3425e-20f1-4463-9e75-a35680212b69" data-execution_count="101">
<div class="sourceCode cell-code" id="cb170" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1">m <span class="op" style="color: #5E5E5E;">+</span> c[<span class="va" style="color: #111111;">None</span>,:]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
</section>
<section id="broadcasting-rules" class="level3">
<h3 class="anchored" data-anchor-id="broadcasting-rules">Broadcasting Rules</h3>
<div class="cell" data-outputid="44f96fea-8147-42f4-d766-aea91b9291ef" data-execution_count="102">
<div class="sourceCode cell-code" id="cb172" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1">c[<span class="va" style="color: #111111;">None</span>,:]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="102">
<pre><code>tensor([[10., 20., 30.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="e760ccc4-510f-4af7-a928-12dc405ad37d" data-execution_count="103">
<div class="sourceCode cell-code" id="cb174" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1">c[<span class="va" style="color: #111111;">None</span>,:].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="103">
<pre><code>torch.Size([1, 3])</code></pre>
</div>
</div>
<div class="cell" data-outputid="d375729d-3dbe-43c7-88cc-3aa0df2935ad" data-execution_count="104">
<div class="sourceCode cell-code" id="cb176" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1">c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="104">
<pre><code>tensor([[10.],
        [20.],
        [30.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="33e3fc44-ed3b-4fb8-fea9-cca7bd072bb9" data-execution_count="105">
<div class="sourceCode cell-code" id="cb178" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1">c[:,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="105">
<pre><code>torch.Size([3, 1])</code></pre>
</div>
</div>
<div class="cell" data-outputid="b1a40cb0-8ef3-497b-fa32-6a3be16b15f8" data-execution_count="106">
<div class="sourceCode cell-code" id="cb180" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1">c[<span class="va" style="color: #111111;">None</span>,:] <span class="op" style="color: #5E5E5E;">*</span> c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>tensor([[100., 200., 300.],
        [200., 400., 600.],
        [300., 600., 900.]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="7b183900-e0a5-474d-fda3-0d9fefe53b54" data-execution_count="107">
<div class="sourceCode cell-code" id="cb182" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1">c[<span class="va" style="color: #111111;">None</span>] <span class="op" style="color: #5E5E5E;">&gt;</span> c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre><code>tensor([[False,  True,  True],
        [False, False,  True],
        [False, False, False]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="aa83d303-833f-4864-8397-e2b2e5bedc9b" data-execution_count="108">
<div class="sourceCode cell-code" id="cb184" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1">m<span class="op" style="color: #5E5E5E;">*</span>m</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>tensor([[ 1.,  4.,  9.],
        [16., 25., 36.],
        [49., 64., 81.]])</code></pre>
</div>
</div>
<p>When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the <strong>trailing dimensions</strong>, and works its way forward. Two dimensions are <strong>compatible</strong> when It goes from right to left. - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size</p>
<p>Arrays do not need to have the same number of dimensions. For example, if you have a <code>256*256*3</code> array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:</p>
<pre><code>Image  (3d array): 256 x 256 x 3
Scale  (1d array):             3
Result (3d array): 256 x 256 x 3</code></pre>
<p>The <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules">numpy documentation</a> includes several examples of what dimensions can and can not be broadcast together.</p>
</section>
</section>
<section id="matmul-with-broadcasting" class="level2">
<h2 class="anchored" data-anchor-id="matmul-with-broadcasting">Matmul with broadcasting</h2>
<div class="cell" data-outputid="8941e580-af1d-4703-9a76-59e3dc854049" data-execution_count="109">
<div class="sourceCode cell-code" id="cb187" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1">digit <span class="op" style="color: #5E5E5E;">=</span> m1[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb187-2">digit.shape,m2.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre><code>(torch.Size([784]), torch.Size([784, 10]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="11b812e8-b7aa-4282-c0d0-789ab9d132ec" data-execution_count="110">
<div class="sourceCode cell-code" id="cb189" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1">digit[:,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="110">
<pre><code>torch.Size([784, 1])</code></pre>
</div>
</div>
<div class="cell" data-outputid="a96daa3d-7b0d-415c-eb8a-0e5a11049c38" data-execution_count="111">
<div class="sourceCode cell-code" id="cb191" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1">digit[:,<span class="va" style="color: #111111;">None</span>].expand_as(m2).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<pre><code>torch.Size([784, 10])</code></pre>
</div>
</div>
<div class="cell" data-outputid="7d09c9ea-6157-4f26-f9e2-587cf5265335" data-execution_count="112">
<div class="sourceCode cell-code" id="cb193" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1">(digit[:,<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">*</span>m2).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>torch.Size([784, 10])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb195" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb195-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb195-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb195-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb195-5"><span class="co" style="color: #5E5E5E;">#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version</span></span>
<span id="cb195-6">        c[i]   <span class="op" style="color: #5E5E5E;">=</span> (a[i,:,<span class="va" style="color: #111111;">None</span>] <span class="op" style="color: #5E5E5E;">*</span> b).<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>) <span class="co" style="color: #5E5E5E;"># broadcast version</span></span>
<span id="cb195-7">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<div class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb196" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1">test_close(t1,matmul(m1, m2))</span></code></pre></div>
</div>
<div class="cell" data-outputid="2a076d25-0fd8-4d78-81b2-ccc4892c0b9d" data-execution_count="115">
<div class="sourceCode cell-code" id="cb197" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>231 µs ± 71.6 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<div class="cell" data-outputid="9a937e5d-2a76-47b2-9931-3711df41ad7f" data-execution_count="116">
<div class="sourceCode cell-code" id="cb199" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1">tr <span class="op" style="color: #5E5E5E;">=</span> matmul(x_train, weights)</span>
<span id="cb199-2">tr</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="116">
<pre><code>tensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],
        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],
        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],
        ...,
        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],
        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],
        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="071eaa25-f589-4a9b-cc6d-1a9b5a58533c" data-execution_count="117">
<div class="sourceCode cell-code" id="cb201" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1">tr.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="117">
<pre><code>torch.Size([50000, 10])</code></pre>
</div>
</div>
<div class="cell" data-outputid="b41b7a84-8acb-43aa-a0eb-97be96c95d2f" data-execution_count="118">
<div class="sourceCode cell-code" id="cb203" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><span class="op" style="color: #5E5E5E;">%</span>time _<span class="op" style="color: #5E5E5E;">=</span>matmul(x_train, weights)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1.26 s, sys: 2.76 ms, total: 1.26 s
Wall time: 1.28 s</code></pre>
</div>
</div>
</section>
<section id="einstein-summation" class="level2">
<h2 class="anchored" data-anchor-id="einstein-summation">Einstein summation</h2>
<p>So we’re 5000 times faster than we started out. So another trick that we can use, which I’m a big fan of, is something called Einstein summation. And Einstein summation is a compact representation for representing products and sums. And this is an example of an Einstein summation. And what we’re going to do now is begin to replicate our matrix product with an Einstein summation. And believe it or not, the entire thing can be pushed down to just these characters,(ik,kj-&gt;ikj) which is pretty amazing. So let me explain what’s happening here. The arrow is separating the left hand side from the right hand side. The left hand side is the inputs. The right hand side is the output. The comma is between each inputs. So there are two inputs. The letters are just names that you’re giving to the number of rise in the number of columns. So the first matrix we’re multiplying by has i rows and k columns, the second has k rows and j columns. It’s going to go through a process which creates a new matrix that actually this is not even doing this is not yet doing the matrix multiplication. This is without the sum. This one’s going to create a new matrix that contains i rows, Well, how do we set i faces and k rows and j columns are rank three tensor. So the number of letters is going to be the rank and the rules of how this works is that if you repeat letters between input arrays,(ik,kj) so here’s my inputs (ik,kj) and we’ve got a repeated letter. It means that values along those axes will be multiplied together. So it means that each item across a row will be multiplied by each item down each column to create this i by k by j output tensor. So to remind you, our first matrix is five by 734 that’s m1. Our second matrix is 784 by ten that’s m2. So i is 5, k is 784 and J is 10. So if I do this torch.einsum then I will end up with a i k by k, it’ll be five by 784 by ten. And if you have a look, I’ve run it here on these two tensor and m1 and m2 and the shape of the result is five by 784 by ten. And what it contains is the original five rows of m1 the original ten columns of m2, and then for the other 784 that I mentioned, they all multiplied together because it’s been copied. It’s been copied between the two arguments to the einsum. And so if we now sum up that over this dimension, we get back, if we have a look, it was that we printed this somewhere. Oh, there it is. So what we get back, if we go back to the original matrix multiply, we do. We had 10.94 negative, negative point six, eight, etc. And so now with this Einstein summation version, we’ve got back exactly the same thing because what it’s done is it’s taken each of these columns by rows, multiplied them together to get this five by seven, eight, four by ten, and then add it up that 784 for each one, which is exactly what matrix multiplication does. But we’re going to use one of the two things from Einstein. The second one says if we omit a letter from the output. So the bit on the right of the arrow, it means those values will be summed. So if we remove this K, which gives us i , k and k,j goes to i,j, so we’ve removed the k entirely. That means that sum happens automatically. So if we run this, as you say, we get back again. Matrix multiplication. So Einstein summation notation is, you know, it takes some practice getting used to you, but it’s very convenient and once you get used to it, it’s actually a really nice way of thinking about what’s going on. And as we’ll see in lots of examples, often you can really simplify your code by using just a tiny little Einstein summation, and it doesn’t even have to be a sum, right? You can you don’t have to omit any letters if you’re just doing products. So maybe it’s a bit misnamed. So we can now define a matmul as simply this torch.einsum. So if we now check it, the test_close that the original result is equal to this new matmul. And yes, it is. And let’s see how the speed looks. Okay. And that was for the whole thing. So compared to 600 milliseconds. So as you can see, this is much faster than even the very fast broadcasting approach we used. So this is a pretty good trick is torch.einsum. Some okay, but of course we don’t have to do any of those things because PyTorch already knows how to do matmul. So there’s two ways we can run matmul directly. In PyTorch, you can use a special @ operator. So x_train@weights is the same as matmul train comma weights as you say, test_close or you can say torch.matmul. And interestingly, as you can see here, the speed is about the same as the einsum. So there’s no particular harm that people reason not to do an einsum. So when I say einsum, that stands for Einstein summation notation. All right, let’s go faster. Still. Currently we’re just using my CPU, but I have a GPU. It would be nice to use it. So how does a GPU work at in video? GPU and indeed pretty much all GPU use. The way they work is that they do lots and lots of things in parallel and you have to actually tell the GPU what are all the things you want to do in parallel or one a time. And so what we’re going to do is we’re going to write in Pure Python something that works like a GPU. You expect it won’t actually be in parallel, so it won’t be fast at all. But the first thing we have to do if we’re going to get something working in parallel is we have to create a function that can calculate just one thing even if a thousand other things are happening at the same time, it won’t interact with anything else. And there’s actually a very easy way to think about matrix multiplication in this way, which is what if we try to create something which, just as we’ve done here, fills in a single, the single item of the result?</p>
<p><a href="https://ajcr.net/Basic-guide-to-einsum/">Einstein summation</a> (<a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>einsum</code></a>) is a compact representation for combining products and sums in a general way. The key rules are:</p>
<ul>
<li>Repeating letters between input arrays means that values along those axes will be multiplied together.</li>
<li>Omitting a letter from the output means that values along that axis will be summed.</li>
</ul>
<div class="cell" data-outputid="ef521b79-f4ed-400b-a75d-fadce66bc88f" data-execution_count="119">
<div class="sourceCode cell-code" id="cb205" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1">m1.shape,m2.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="119">
<pre><code>(torch.Size([5, 784]), torch.Size([784, 10]))</code></pre>
</div>
</div>
<p>So far we removed 2 inner loop and we are 5k faster than original for loop. It is more cleaner code and also faster.</p>
<div class="cell" data-outputid="d407d863-e358-4961-8eed-745cc169a56a" data-execution_count="120">
<div class="sourceCode cell-code" id="cb207" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><span class="co" style="color: #5E5E5E;"># c[i,j] += a[i,k] * b[k,j]</span></span>
<span id="cb207-2"><span class="co" style="color: #5E5E5E;"># c[i,j] = (a[i,:] * b[:,j]).sum()</span></span>
<span id="cb207-3">mr <span class="op" style="color: #5E5E5E;">=</span> torch.einsum(<span class="st" style="color: #20794D;">'ik,kj-&gt;ikj'</span>, m1, m2)</span>
<span id="cb207-4">mr.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>torch.Size([5, 784, 10])</code></pre>
</div>
</div>
<div class="cell" data-outputid="0e5a342b-6d61-4a9f-9b73-f8254f787e8a" data-execution_count="121">
<div class="sourceCode cell-code" id="cb209" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb209-1">mr.<span class="bu" style="color: null;">sum</span>(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="121">
<pre><code>tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],
        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],
        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],
        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],
        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])</code></pre>
</div>
</div>
<div class="cell" data-outputid="f1524c31-6fd5-476b-e696-ac8444a3d860" data-execution_count="122">
<div class="sourceCode cell-code" id="cb211" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb211-1">torch.einsum(<span class="st" style="color: #20794D;">'ik,kj-&gt;ij'</span>, m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],
        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],
        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],
        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],
        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb213" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb213-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b): <span class="cf" style="color: #003B4F;">return</span> torch.einsum(<span class="st" style="color: #20794D;">'ik,kj-&gt;ij'</span>, a, b)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb214" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb214-1">test_close(tr, matmul(x_train, weights), eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="9a448671-b412-40ec-cfc2-13cf558197d4" data-execution_count="125">
<div class="sourceCode cell-code" id="cb215" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb215-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">5</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(x_train, weights)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>30.5 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)</code></pre>
</div>
</div>
</section>
<section id="pytorch-op" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-op">pytorch op</h2>
<p>Now that we wrote matmul we can use pytorch matmul version. The speed is almost the same. We can use pytorch’s function or operator directly for matrix multiplication.</p>
<div class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb217" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1">test_close(tr, x_train<span class="op" style="color: #5E5E5E;">@</span>weights, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
<div class="cell" data-outputid="e2e2bd0f-2d37-449a-ec7d-58e5b3156644" data-execution_count="127">
<div class="sourceCode cell-code" id="cb218" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb218-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">5</span> _<span class="op" style="color: #5E5E5E;">=</span>torch.matmul(x_train, weights)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>32 ms ± 4.72 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)</code></pre>
</div>
</div>
<p>So how do we create something that just fills in row zero column zero? Well, what we could do is we could create a new matmul where we’re going to pass in the coordinates of the place that we want to fill in. So I’m going to start by passing that zero comma zero we’ll pass at the matrices We want to multiply and we are passing a tensor that we’ve pre-filled in with zeros to put the result into. So they’re going to say, okay, the result is torch.zeros() rows by columns, cal matmul for location zero comma zero passing in those two matrices and the bunch of zeros matrix ready to put the result in. And if we call that we get the answer in cell zero zero. So here’s an implementation of that. So the implementation is first of all, we’ve been past the zero comma zero coordinates, so let’s de structure them. So hopefully you’ve been experimenting with de structuring that so important. You said all the time into i and j throw in the column, make sure that that is inside the bounds of our output matrix and we’re going to start by start at zero and loop through all of the all of the columns of a in the rows of b for i and j, just like the very innermost loop of our very first Python attempt and then at the end pop that into the output. So here’s something that fills in one piece with a grid successfully. So we could call this row by columns times each time passing in a different grid. And we could do that in parallel because none of those different locations interact with any other location. So something which can calculate a little piece of, of an output on a GPU is called a kernel. So we call this a kernel. And so now we can create something called launch kernel, we pass at the kernel. So that’s the function. So here’s an example launch kernel passing in the function and how many rows and how many columns are there in the output grid. And then give me any arguments that you need to calculate it. So in python *args just says any additional arguments that you pass are going to be put into an array called args. If you do something like C, you might have seen like variadic arguments parameters. It’s a same basic idea. So we’re going to be calling launch kernel.</p>
</section>
<section id="cuda" class="level2">
<h2 class="anchored" data-anchor-id="cuda">CUDA</h2>
<p>How to use GPU instead of CPU. GPU does many more thing at the same time. CPU does not do that. We can compute each cell at the same time because none of those computation interact with other location.</p>
<p>So we’re going to be calling launch kernel. We’re going to be saying launch the kernel matmul using all the rows of a or the columns of b, and then the args which are going to be in star args are going to be m1, the first matrix, m2 the second matrix n res another touched zeros we just created. So launch kernel, it’s going to loop through the rows of a and then for each row of a loop through the columns of b and call the kernel which is matmul on that grid location and passing in m1, m2 and res. So I star args here is going to unpack that and pass them as three separate arguments. And if I run that, run all of that, you’ll see it’s done it, it’s filled in the exact same matrix. Okay. So that’s actually not fast at all. It’s not doing anything in parallel, but it’s the basic idea. So now to actually do it in parallel, we have to use something called Cuda. So Cuda is a programing model for Nvidia GPUs and to program in CUDA from Python. The easiest way currently to do that is be something called Numba. And Numba is a compiler where you’ve seen it actually already for non GPU. It’s a compiler that takes Python code and spits out, you know, compiled fast machine code. If you use its CUDA module, it’ll actually spit out GPU accelerated CUDA code. So rather than using an <span class="citation" data-cites="njit">@njit</span> like before, we now say <span class="citation" data-cites="cuda.jit">@cuda.jit</span> and it behaves a little bit differently but you’ll see that this matmul let me copy the other one over so you can compare cup it, compare it to our Python one, our Python matmul and this <span class="citation" data-cites="cuda.jit">@cuda.jit</span> matmul Look I think identical except for one thing. Instead of passing in the grid, there’s a special magic thing called cuda.gird() And you say how many dimensions just my grid have? And you unpack it so that’s you don’t have to. It’s just a little convenience. That Numba does for you. You don’t have to pass over the grid, it passes it over for you. So it doesn’t need this grid. Other than that, these two are identical, but the decorator is going to compile that into your GPU code. So now we need to create our output tensor just like before, and we need to do something else, which is we have to take our input matrices and our output. So our input tenses, the matrices in this case and the output tensor and we have to move them to the GPU, you I should say, copy them to the GPU.</p>
<div class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb220" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><span class="kw" style="color: #003B4F;">def</span> matmul(grid, a,b,c):</span>
<span id="cb220-2">    i,j <span class="op" style="color: #5E5E5E;">=</span> grid</span>
<span id="cb220-3">    <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">&lt;</span> c.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="kw" style="color: #003B4F;">and</span> j <span class="op" style="color: #5E5E5E;">&lt;</span> c.shape[<span class="dv" style="color: #AD0000;">1</span>]:</span>
<span id="cb220-4">        tmp <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb220-5">        <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(a.shape[<span class="dv" style="color: #AD0000;">1</span>]): tmp <span class="op" style="color: #5E5E5E;">+=</span> a[i, k] <span class="op" style="color: #5E5E5E;">*</span> b[k, j]</span>
<span id="cb220-6">        c[i,j] <span class="op" style="color: #5E5E5E;">=</span> tmp</span></code></pre></div>
</div>
<div class="cell" data-outputid="abf81a4f-b9ab-47bb-8aec-1670023d68ad" data-execution_count="129">
<div class="sourceCode cell-code" id="cb221" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb221-1">res <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb221-2">matmul((<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">0</span>), m1, m2, res)</span>
<span id="cb221-3">res</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="129">
<pre><code>tensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],
        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],
        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],
        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],
        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])</code></pre>
</div>
</div>
<p>*args any additional argument(s) will be put into an array called args. This is the idea.</p>
<div class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb223" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb223-1"><span class="kw" style="color: #003B4F;">def</span> launch_kernel(kernel, grid_x, grid_y, <span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb223-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(grid_x):</span>
<span id="cb223-3">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(grid_y): kernel((i,j), <span class="op" style="color: #5E5E5E;">*</span>args, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span></code></pre></div>
</div>
<div class="cell" data-outputid="4d7e631f-7811-46a5-9a59-e434bb819594" data-execution_count="131">
<div class="sourceCode cell-code" id="cb224" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb224-1">res <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb224-2">launch_kernel(matmul, ar, bc, m1, m2, res)</span>
<span id="cb224-3">res</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="131">
<pre><code>tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],
        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],
        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],
        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],
        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])</code></pre>
</div>
</div>
<p>Now we use cuda so it will do multiply computation at the same time.</p>
<div class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb226" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb226-1"><span class="im" style="color: #00769E;">from</span> numba <span class="im" style="color: #00769E;">import</span> cuda</span></code></pre></div>
</div>
<div class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb227" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb227-1"><span class="kw" style="color: #003B4F;">def</span> matmul(grid, a,b,c):</span>
<span id="cb227-2">    i,j <span class="op" style="color: #5E5E5E;">=</span> grid</span>
<span id="cb227-3">    <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">&lt;</span> c.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="kw" style="color: #003B4F;">and</span> j <span class="op" style="color: #5E5E5E;">&lt;</span> c.shape[<span class="dv" style="color: #AD0000;">1</span>]:</span>
<span id="cb227-4">        tmp <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb227-5">        <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(a.shape[<span class="dv" style="color: #AD0000;">1</span>]): tmp <span class="op" style="color: #5E5E5E;">+=</span> a[i, k] <span class="op" style="color: #5E5E5E;">*</span> b[k, j]</span>
<span id="cb227-6">        c[i,j] <span class="op" style="color: #5E5E5E;">=</span> tmp</span></code></pre></div>
</div>
<p>So cuda.to_device() copies a tensor to the GPU. And so we’ve got three things getting copied to the GPU here and therefore we store the three things over here. Another way I could have written this is I could have said map, which I kind of quite like doing a function which is cuda.to_device to each of these arguments and this would be the same thing. This is going to call CUDA dot device on x_train and put it in here on weights and put it in here and an r and put it in rg. That’s a slightly more convenient way to do it. Okay, so we’ve got our 50,000 by ten output. That’s just all zeros. Of course, that’s just how we created it. And now we’re going to try and fill it in. There is a there’s a particular detail that you don’t have to worry about too much, which is in CUDA They don’t just have a grid, but there’s also a concept of blocks and there’s something we call here TPP, which is threads per block. This is just a detail of the kind of programing model you don’t have to worry about too much. You can just basically copy this. And what it’s going to do is it’s going to call each grid item in parallel and with a number of different processes, basically. So this is just the code which turns the grid into blocks. And so you don’t have to worry too much about the details of that. You just always run it.</p>
<p>Decorator compile it to GPU code.</p>
<div class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb228" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb228-1"><span class="at" style="color: #657422;">@cuda.jit</span></span>
<span id="cb228-2"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b,c):</span>
<span id="cb228-3">    i, j <span class="op" style="color: #5E5E5E;">=</span> cuda.grid(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb228-4">    <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">&lt;</span> c.shape[<span class="dv" style="color: #AD0000;">0</span>] <span class="kw" style="color: #003B4F;">and</span> j <span class="op" style="color: #5E5E5E;">&lt;</span> c.shape[<span class="dv" style="color: #AD0000;">1</span>]:</span>
<span id="cb228-5">        tmp <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb228-6">        <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(a.shape[<span class="dv" style="color: #AD0000;">1</span>]): tmp <span class="op" style="color: #5E5E5E;">+=</span> a[i, k] <span class="op" style="color: #5E5E5E;">*</span> b[k, j]</span>
<span id="cb228-7">        c[i,j] <span class="op" style="color: #5E5E5E;">=</span> tmp</span></code></pre></div>
</div>
<div class="cell" data-outputid="3088f333-3372-415e-e9a9-7f292402f8a9" data-execution_count="135">
<div class="sourceCode cell-code" id="cb229" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb229-1"><span class="op" style="color: #5E5E5E;">!</span>pip install numba</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (0.56.4)
Requirement already satisfied: numpy&lt;1.24,&gt;=1.18 in /usr/local/lib/python3.9/dist-packages (from numba) (1.22.4)
Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba) (63.4.3)
Requirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba) (0.39.1)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb231" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb231-1"><span class="im" style="color: #00769E;">from</span> numba <span class="im" style="color: #00769E;">import</span> cuda</span>
<span id="cb231-2">r <span class="op" style="color: #5E5E5E;">=</span> np.zeros(tr.shape)</span>
<span id="cb231-3">m1g,m2g,rg <span class="op" style="color: #5E5E5E;">=</span> cuda.to_device(x_train),cuda.to_device(weights),cuda.to_device(r)</span></code></pre></div>
</div>
<div class="cell" data-outputid="b7004fae-3041-47e2-a5f0-1fe7244958ae" data-execution_count="137">
<div class="sourceCode cell-code" id="cb232" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb232-1">r.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="137">
<pre><code>(50000, 10)</code></pre>
</div>
</div>
<div class="cell" data-outputid="be5b727f-47dc-456f-f1de-70ee889e0855" data-execution_count="138">
<div class="sourceCode cell-code" id="cb234" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb234-1">TPB <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">16</span></span>
<span id="cb234-2">rr,rc <span class="op" style="color: #5E5E5E;">=</span> r.shape</span>
<span id="cb234-3">blockspergrid <span class="op" style="color: #5E5E5E;">=</span> (math.ceil(rr <span class="op" style="color: #5E5E5E;">/</span> TPB), math.ceil(rc <span class="op" style="color: #5E5E5E;">/</span> TPB))</span>
<span id="cb234-4">blockspergrid</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="138">
<pre><code>(3125, 1)</code></pre>
</div>
</div>
<p>Okay. And so now how do you call the equivalent of launch kernal? it’s it’s a slightly weird way to do it, but it works fine. You call matmul, but because matmul has cuda.jit, it’s got a special thing, which is you have to put something in square brackets afterwards, which is you have to tell it how many blocks per grid. That’s just the result from the previous cell and how many threads per block in each of the two dimensions. So again, you can just copy and paste this from my version, but then you pass in the three arguments to the function. This will be a, c, and c, and this. Okay, this is, this is how you launch a kernel. So this will launch the kernel matmul on the GPU. You at the end of it, rg is going to get filled in. It’s gone. It’s on the GPU, which is not much good to us so we don’t have to copy it back to the CPU, which is called the host copy to host to a run that and it’s done and test_close shows us that result is similar to our original results. So it seems to be working. So that’s great. So I see Sylvor on the YouTube chat is finding that it’s not working on his Mac. That’s right. So this will only work on it in NVIDIA CPU as basically all of the GPU, nearly all the CPU stuff we look at only works on video.</p>
<div class="cell" data-execution_count="139">
<div class="sourceCode cell-code" id="cb236" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb236-1">matmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)</span>
<span id="cb236-2">r <span class="op" style="color: #5E5E5E;">=</span> rg.copy_to_host()</span>
<span id="cb236-3">test_close(tr, r, eps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
<p>Mac GPUs are gradually starting to get a little bit of support from machine learning libraries, but it’s taking quite a while. It’s been, you know, it’s got quite a way to go. As I say this at least towards the end of 2022, if this works for you and later on that’s yeah, that’s great. Okay, so let’s time how fast that is. Okay, so that was 3.6 1 milliseconds. And so if we compare that to the PyTorch matmul on CPU, that was 15 milliseconds. So that’s great. So it’s faster still. So how much faster? Oh, by the way, we can actually go faster than that, which is we can use the exact same code we had from the PyTorch up. But here’s a trick. If you just take your tensor and write .cuda after it, it copies it over to the GPU. If it’s on a if it’s on a Nvidia GPUs, you do the same for weights.cuda. So these are two cuda versions and now I can do the whole thing. And this will actually run on the GPU and then to copy it back to the host, you just say .cpu(). So if we look to see how fast that is, 458 ms .So yeah, that is somebody you just pointed out that I wrote the wrong thing here 1e-3. Okay, so how much faster is that? Well full 458 microseconds original on the whole data set was 663 microseconds. So compared to our broadcast version, we are another 1000 times faster. So overall, this version here, compared to our original version, which was here, the difference in performance is 5 million x , So when you say people say, Yeah, Python can be pretty slow, it can be better to run the stuff on the GPU if possible. We’re not talking about a 20% change, we’re talking about a 5 million x change. So that’s a big deal. And so that’s why you need to be running stuff on the GPU. All right. Some folks on YT are wondering how on earth I’m running cuda when I’m on a mac and given it sets localhost here, that’s because I’m using something called SSH tunneling, which we might get to sometime. I suspect my life coding from the previous course might have covered that already, but this is basically you can use a Jupyter notebook that’s running anywhere in the world from your own machine using something called SSH Tunneling, which is a good thing to look up a OK when a person asks if Einstein summation borrows anything from APL. Oh, yes, it does, actually. So it’s kind of the other way around. Actually. APL borrows it from Einstein notation. I don’t know if you remember I mentioned that Iverson, when he developed APL was heavily influenced by tensor analysis. And so this Einstein notation is very heavily used there. If you’ll notice a key thing that happens in Einstein notation is there’s no loop. You know, there isn’t this kind of sigma, you know, i from here to here and then you put the i inside the function that you’re summing up, everything’s implicit and APL takes that a very long way and, and J takes it even further, which is what Iverson developed after APL and this kind of general idea of of removing the index is very important in APL and it’s become very important in numpy PyTorch TensorFlow and so forth. All right. So finally we know how to multiply matrices. Congratulations. So let’s practice that. That’s practice what we’ve learned. So we’re going to go to zero two main shift to practice this. And so we’re going to try to exercise our kind of tensor manipulation operation muscles in this section. And the key actually endpoint for this is the homework. And so what you need to be doing is getting yourself to a point that you could implement something like this, but for a different algorithm, why do we care about this?</p>
<div class="cell" data-outputid="03ea6df0-2d1b-4363-d059-0f38475217c5" data-execution_count="140">
<div class="sourceCode cell-code" id="cb237" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb237-1"><span class="op" style="color: #5E5E5E;">%%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb237-2">matmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)</span>
<span id="cb237-3">r <span class="op" style="color: #5E5E5E;">=</span> rg.copy_to_host()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>10.9 ms ± 3.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb239" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb239-1">m1c,m2c <span class="op" style="color: #5E5E5E;">=</span> x_train.cuda(),weights.cuda()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb240" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb240-1">r<span class="op" style="color: #5E5E5E;">=</span>(m1c<span class="op" style="color: #5E5E5E;">@</span>m2c).cpu()</span></code></pre></div>
</div>
<div class="cell" data-outputid="02088d3a-ba11-4441-e8de-0c9269adc1f5" data-execution_count="143">
<div class="sourceCode cell-code" id="cb241" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb241-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> r<span class="op" style="color: #5E5E5E;">=</span>(m1c<span class="op" style="color: #5E5E5E;">@</span>m2c).cpu()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.27 ms ± 288 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<p>Our broadcasting version was &gt;500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!</p>


</section>
</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 3/index.html</guid>
  <pubDate>Sat, 18 Mar 2023 20:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing Stable Diffusion from Scratch 2</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index.html</link>
  <description><![CDATA[ 



<section id="matrix-multiplication-from-foundations" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication-from-foundations">Matrix multiplication from foundations</h2>
<p>Important stuff you need to know after this lecture:<br>
1- Matrix Multiplication <br> 2- Numba, how to compile python code to machine code <br> 3- Frobenius Norm <br> 4- Braodcasting Rules <br> 5- expand_as , stride <br> 6- unsqueeze , c[ None , : ] ,c[ : , None]<br> 7- c[None], c[…,None] <br></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pickle, gzip, math, os, time, shutil, matplotlib <span class="im" style="color: #00769E;">as</span> mpl, matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
</section>
<section id="get-data" class="level2">
<h2 class="anchored" data-anchor-id="get-data">Get data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb2-2">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb2-3">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-4">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<p><a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve">urlretrieve</a> - (read the docs!)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 Mar 15 09:50 mnist.pkl.gz</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">lst1 <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(x_train[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb7-2">vals <span class="op" style="color: #5E5E5E;">=</span> lst1[<span class="dv" style="color: #AD0000;">200</span>:<span class="dv" style="color: #AD0000;">210</span>]</span>
<span id="cb7-3">vals</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.0,
 0.0,
 0.0,
 0.19140625,
 0.9296875,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="bu" style="color: null;">len</span>(lst1)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>784</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> chunks(x, sz):</span>
<span id="cb11-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">len</span>(x), sz): <span class="cf" style="color: #003B4F;">yield</span> x[i:i<span class="op" style="color: #5E5E5E;">+</span>sz]</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;">list</span>(chunks(vals, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],
 [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb14-2">plt.imshow(<span class="bu" style="color: null;">list</span>(chunks(lst1, <span class="dv" style="color: #AD0000;">28</span>)))<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><a href="https://docs.python.org/3/library/itertools.html#itertools.islice">islice</a></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;">from</span> itertools <span class="im" style="color: #00769E;">import</span> islice</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(vals)</span>
<span id="cb16-2">islice(it, <span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;itertools.islice&gt;</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.0, 0.0, 0.0, 0.19140625, 0.9296875]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(lst1)</span>
<span id="cb24-2">img <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">iter</span>(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">28</span>)), []))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">plt.imshow(img)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Use this link to learn more about <a href="https://docs.python.org/3.10/library/functions.html?highlight=iter#iter">iter</a></p>
</section>
<section id="matrix-and-tensor" class="level2">
<h2 class="anchored" data-anchor-id="matrix-and-tensor">Matrix and tensor</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">img[<span class="dv" style="color: #AD0000;">20</span>][<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.98828125</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="kw" style="color: #003B4F;">class</span> Matrix:</span>
<span id="cb28-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, xs): <span class="va" style="color: #111111;">self</span>.xs <span class="op" style="color: #5E5E5E;">=</span> xs</span>
<span id="cb28-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, idxs): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.xs[idxs[<span class="dv" style="color: #AD0000;">0</span>]][idxs[<span class="dv" style="color: #AD0000;">1</span>]]</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">m <span class="op" style="color: #5E5E5E;">=</span> Matrix(img)</span>
<span id="cb29-2">m[<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.98828125</code></pre>
</div>
</div>
<p>Now we can use pytorch.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb31-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">tensor([<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([1, 2, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">x_train,y_train,x_valid,y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, (x_train,y_train,x_valid,y_valid))</span>
<span id="cb34-2">x_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([50000, 784])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">x_train.<span class="bu" style="color: null;">type</span>()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'torch.FloatTensor'</code></pre>
</div>
</div>
<p><a href="https://pytorch.org/docs/stable/tensors.html">Tensor documentation</a></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">imgs <span class="op" style="color: #5E5E5E;">=</span> x_train.reshape((<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb38-2">imgs.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([50000, 28, 28])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">plt.imshow(imgs[<span class="dv" style="color: #AD0000;">0</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>vector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">imgs[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.9883)</code></pre>
</div>
</div>
<p>Use destructring again. n number of images. c is full number of colums (784)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">n,c <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb43-2">y_train, y_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))</code></pre>
</div>
</div>
<p>in y_train we can find min and max of it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="bu" style="color: null;">min</span>(y_train),<span class="bu" style="color: null;">max</span>(y_train)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(0), tensor(9))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">y_train.<span class="bu" style="color: null;">min</span>(), y_train.<span class="bu" style="color: null;">max</span>()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(0), tensor(9))</code></pre>
</div>
</div>
</section>
<section id="random-numbers" class="level2">
<h2 class="anchored" data-anchor-id="random-numbers">Random numbers</h2>
<p>Based on the Wichmann Hill algorithm used before Python 2.3.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">rnd_state <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span></span>
<span id="cb49-2"><span class="kw" style="color: #003B4F;">def</span> seed(a):</span>
<span id="cb49-3">    <span class="kw" style="color: #003B4F;">global</span> rnd_state</span>
<span id="cb49-4">    a, x <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30268</span>)</span>
<span id="cb49-5">    a, y <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30306</span>)</span>
<span id="cb49-6">    a, z <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30322</span>)</span>
<span id="cb49-7">    rnd_state <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(x)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">int</span>(y)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">int</span>(z)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">seed(<span class="dv" style="color: #AD0000;">457428938475</span>)</span>
<span id="cb50-2">rnd_state</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(4976, 20238, 499)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><span class="kw" style="color: #003B4F;">def</span> rand():</span>
<span id="cb52-2">    <span class="kw" style="color: #003B4F;">global</span> rnd_state</span>
<span id="cb52-3">    x, y, z <span class="op" style="color: #5E5E5E;">=</span> rnd_state</span>
<span id="cb52-4">    x <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">171</span> <span class="op" style="color: #5E5E5E;">*</span> x) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30269</span></span>
<span id="cb52-5">    y <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">172</span> <span class="op" style="color: #5E5E5E;">*</span> y) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30307</span></span>
<span id="cb52-6">    z <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">170</span> <span class="op" style="color: #5E5E5E;">*</span> z) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30323</span></span>
<span id="cb52-7">    rnd_state <span class="op" style="color: #5E5E5E;">=</span> x,y,z</span>
<span id="cb52-8">    <span class="cf" style="color: #003B4F;">return</span> (x<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30269</span> <span class="op" style="color: #5E5E5E;">+</span> y<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30307</span> <span class="op" style="color: #5E5E5E;">+</span> z<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30323</span>) <span class="op" style="color: #5E5E5E;">%</span> <span class="fl" style="color: #AD0000;">1.0</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">rand(),rand(),rand()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="cf" style="color: #003B4F;">if</span> os.fork(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In parent: </span><span class="sc" style="color: #5E5E5E;">{</span>rand()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb55-2"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb55-3">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In child: </span><span class="sc" style="color: #5E5E5E;">{</span>rand()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb55-4">    os._exit(os.EX_OK)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In parent: 0.9559050644103264
In child: 0.9559050644103264</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="cf" style="color: #003B4F;">if</span> os.fork(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In parent: </span><span class="sc" style="color: #5E5E5E;">{</span>torch<span class="sc" style="color: #5E5E5E;">.</span>rand(<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb57-2"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb57-3">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In child: </span><span class="sc" style="color: #5E5E5E;">{</span>torch<span class="sc" style="color: #5E5E5E;">.</span>rand(<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb57-4">    os._exit(os.EX_OK)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In parent: tensor([0.2262])
In child: tensor([0.2262])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">plt.plot([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">50</span>)])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">plt.hist([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">10000</span>)])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>%timeit check the time of excution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> <span class="bu" style="color: null;">list</span>(chunks([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">7840</span>)], <span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>5.39 ms ± 223 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<p>pytorch version is faster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> torch.randn(<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>86.7 µs ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
</section>
<section id="matrix-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication">Matrix multiplication</h2>
<p>Okay, so let’s move on with our from the foundations now. And so we were working on trying to at least get the start of a forward pass of a linear model or a simple multi-layer perceptron for MNIST going. And we had successfully created a basic tensor. We’ve got some random numbers going. So what we now need to do is we now need to be able to multiply these things together, matrix multiplication. So matrix multiplication to remind you in this case. So we’re doing MNIST, right? So we’ve got about we’re going to use a subset, let’s see. Yeah, Okay. So we’re going to create a matrix called m1, which is just the first five digits, So m1 will be the first five digits. So five rows and. Well, dot, dot dot dot dot dot. And then 780. What is it again. because it’s 28 by 28 pixels and reflect that out. So this is our first matrix and matrix multiplication, and then we’re going to multiply that by some some weights. So the weights are going to be 784 by 10 random numbers. So for every one of thes 784 pixels, each one is going to have a weight. So 784 down here, so 94 by ten. So this first column, for example, is going to tell us all the weights in order to figure out if something’s a zero. And the second column will have all the weights in deciding of the probability of something. So one, so forth, assuming we just doing a linear model. And so then we’re going to multiply these two matrices together. So when we multiply matrices together, we take row one of matrix one and we take column one of matrix two and we take each one in turn. So we take this one and we take this one, we multiply them together and then we take this one and this one and we multiply them together. And we do that for every element wise pair, and then we add them all up and that would give us the value for the very first cell that would go in here. That’s what matrix multiplication is. Okay, so let’s go ahead and create our random numbers for the weights since we’re allowed to use random number generator now and for the bias, but just use a bunch of zeros to start with. So the bias is just what we’re going to add to each one. And so for our matrix multiplication, we’re going to be doing a little mini batch. I’m going to be doing five rows of, as we discussed, five rows of so five, five images flattened out and then multiplied by this weights matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">torch.manual_seed(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb65-2">weights <span class="op" style="color: #5E5E5E;">=</span> torch.randn(<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb65-3">bias <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">m1 <span class="op" style="color: #5E5E5E;">=</span> x_valid[:<span class="dv" style="color: #AD0000;">5</span>]</span>
<span id="cb66-2">m2 <span class="op" style="color: #5E5E5E;">=</span> weights</span></code></pre></div>
</div>
<p>So here are the shapes and one is five by seven, eight four as we saw, and m2 is seven, eight, four by ten. Okay, so keep those in mind. So here’s a handy thing. And one touch shape contains two numbers and I want to pull them out. I want to call the I’m going to think of that as I’m going to actually think of this as like a and b rather than I wanted them to. So this is like a and b, so the number of rows in a and the number of columns in b, if I say equals and one shape that will put five in ar and 784 in ac, So I’ll notice I do this a lot, this restructuring, we talked about it last week too so can do the same for m2 dot shape, put that into b rows and b columns. And so now if I write out ar,ac and br , br , you can again see the same things from the sizes. So that’s a good way to kind of give us the stuff we have to look through. So here’s our results.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">m1.shape,m2.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([5, 784]), torch.Size([784, 10]))</code></pre>
</div>
</div>
<p>get matrix dimsions and put them in variables to make it readable for future looping.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">ar,ac <span class="op" style="color: #5E5E5E;">=</span> m1.shape <span class="co" style="color: #5E5E5E;"># n_rows * n_cols</span></span>
<span id="cb69-2">br,bc <span class="op" style="color: #5E5E5E;">=</span> m2.shape</span>
<span id="cb69-3">(ar,ac),(br,bc)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>((5, 784), (784, 10))</code></pre>
</div>
</div>
<p>So here’s our results. So our resultant tensor, well, we’re multiplying, we’re multiplying together all of these seven, eight, four things and adding them up. So the resultant tensor is going to be five by ten. And then each thing in here is the result of multiplying and adding So the result here is going to start with zeros and there is this is the result and it’s going to contain ar rows, five rows and bc columns, ten columns, five coma ten. Okay, so now we have to fill that in. And so to do a matrix multiplication, so we have to first we have to go through each row one at a time and here we have that go through each row one at a time and then go through each column one at a time. And then we have to go through each pair in that row column one at a time. So it’s going to be a loop in a loop in a So here’s quick over each row, and here we’re going to loop over each column and then here we’re going to loop so each column of c, and then here we’re going to leap over each column of a, which is going to be the same as the number of rows of b, which we can see here. I say ac or br they are seven, eight, four. They’re the same. So it wouldn’t matter whether we day, ac or br, so then our result for that row and that column, we have to add onto it the product of i,k in the first matrix by k,j in the second matrix. So k, it’s going up through those seven, eight, four. And so we’re going to go across the columns and down so across the rows and down the columns, it’s going to go across the row where it goes down this column. So here is the world’s most naive, slow, uninteresting matrix multiplication. And if we run it, okay, it’s done something we have successfully hopefully successfully multiplied the matrices m1 and m2.It’s a little hard to read this, I find because because punch cards used to be 80 columns wide. We still assume screens 80 columns wide. Everything defaults to 80 wide, which is ridiculous, but you can easily change it. So if you say sit print options, you can choose your own line width. Oh, you can say it’s five by ten. We did it before. So if we change the line width, okay, that’s much easier to rate. Now we can see here the five rows and here are the ten columns for that matrix multiplication. I tend to always put this at the top of my notebooks and you can do the same thing for numpy as well. So what I’d like to do this is really important is when I’m working on code, particularly numeric code, I like to do it all step by step and Jupiter. And then what I do is once I’ve got it working is a copy all the cells that have implemented that and I paste them and then I select them all and I hit shift+m to merge. Get rid of anything that prints out stuff I don’t need. And then I put a header on the top, give it a function name, and then I select the whole lot and I hit control or f right square bracket and I’ve turned it into a function, but I still keep the stuff above it. So I can see all the step by step stuff for learning about it later. And so that’s what I’ve done here to create this function. And so this function does exactly the same things we just did, and we can see how long it takes to run by using %time. And it took about half a second, which gosh, that’s a long time to generate such a small matrix. This is just to do five MNIST digits. So that’s not going to be great. We’re going to have to speed that up. I’m actually quite surprised at how slow that is because there’s only 39,200. So, you know, if you look at the how, we’ve got a loop within a loop within a loop, it’s doing 39,200 of these. So Python. Yeah, Python. When you’re just doing python, it is it is slow. So we can’t we can’t do that. That’s why we can’t just write Python.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">t1 <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb71-2">t1.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 10])</code></pre>
</div>
</div>
<p>Go through each row one at a time (5), then each column one at a time (10) and then go through each pair(784). go accross the rows , down the column multiply and add. t1[i,j] += m1[i,k] * m2[k,j]</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):         <span class="co" style="color: #5E5E5E;"># 5</span></span>
<span id="cb73-2">    <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc):     <span class="co" style="color: #5E5E5E;"># 10</span></span>
<span id="cb73-3">        <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ac): <span class="co" style="color: #5E5E5E;"># 784</span></span>
<span id="cb73-4">            t1[i,j] <span class="op" style="color: #5E5E5E;">+=</span> m1[i,k] <span class="op" style="color: #5E5E5E;">*</span> m2[k,j]</span></code></pre></div>
</div>
<p>Default is 80 columns wide because of punch cards and we still do that. (Talking about legacy and network effect , haha)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">t1</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,
          -3.4375, -11.4696,  -2.1153],
        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,
           2.1577, -15.2772,  -2.6758],
        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,
          -5.4446, -20.6758,  13.5657],
        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,
         -12.9776,  -6.4443,   3.6376],
        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,
          -3.7862,  -8.9827,   5.2797]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">t1.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 10])</code></pre>
</div>
</div>
<p>This is only to show data more readable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">torch.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>, sci_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb78-2">t1</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],
        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],
        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],
        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],
        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])</code></pre>
</div>
</div>
<p>Do this on the top of the notebook and make it easier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb80-2">np.set_printoptions(precision<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, linewidth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">140</span>)</span></code></pre></div>
</div>
<p>For numerical programming , Jeremy recommend doing stuff line by line , check the results and dimensions and then when it works , copy all the cells and paste them after those cell and select them all and hit shift+M to merge cells get ride of everything that prints out stuff you dont need put a header on the top (def ….), select the rest of the code and hit control + ] now you have the function. Keep the same none function code above to remember what did you do and how you get there.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb81-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb81-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb81-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb81-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc):</span>
<span id="cb81-6">            <span class="cf" style="color: #003B4F;">for</span> k <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ac): c[i,j] <span class="op" style="color: #5E5E5E;">+=</span> a[i,k] <span class="op" style="color: #5E5E5E;">*</span> b[k,j]</span>
<span id="cb81-7">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<p>How long does it take to run ? Man it too much. It is o(n^3) and it is so slow</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><span class="op" style="color: #5E5E5E;">%</span>time _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 771 ms, sys: 748 µs, total: 772 ms
Wall time: 774 ms</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">ar<span class="op" style="color: #5E5E5E;">*</span>bc<span class="op" style="color: #5E5E5E;">*</span>ac</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>39200</code></pre>
</div>
</div>
</section>
<section id="numba" class="level2">
<h2 class="anchored" data-anchor-id="numba">Numba</h2>
<p>But there is something that kind of lets this write Python we could instead use Numba.Numba is a system that takes python and turns it into basically into machine code and it’s amazingly easy to do. You can basically take a function and write and <span class="citation" data-cites="ngit">@ngit</span> on top. And what it’s going to do is it’s going to look the first time you call this function, it’s going to compile it down to machine code and will run much more quickly. So what I’ve done here is I’ve taken the innermost loop. So just looping through and adding up all these. So I start at zero, go through and add up all those just two vectors and return it, which is called a dot product. And linear algebra, so call it dot and so Numba only works with numpy, it doesn’t work with PyTorch. So we’re just going to use arrays instead of tensers for a moment. Now have a look at this. If I try to do a dot product of one, two, three and two, three, four, it’s pretty easy to do. It took a fifth of a second, which sounds terrible, but the reason it took a fifth of a second is because that’s actually how long it took to compile this and run it. Now that it’s compiled the second time, it just has to call it it’s now 21 microseconds. And so that’s actually very fast. So with Numba we can basically make Python run at C speed. So now the important thing to recognize is if I replace this loop in Python with a called a dot which is running in machine code, then we now have one two loops running in python not three. So our 448 MS, let’s make sure if I run it, run that matmul that should be close to my t1 one. t1 is what we got before.</p>
<p>And so when I’m refactoring or performance improving or whatever, I always like to put every step in the notebook and then test. So this test close comes from fastcore.test and it just checks. The two things are very similar. They might not be exactly the same because of floating point differences, which is fine. Okay, our matmul is working correctly, or at least it’s doing the same thing it did before. So if we now run it, it’s taking 268 micro second, versus 448 milliseconds. So it’s taking, you know, about 2000 times faster just by changing the one in my loop. So really all we’ve done is we’ve had <span class="citation" data-cites="ngit">@ngit</span> to make it 2000 times faster, so Numba is well worth knowing about. I can make your Python code very, very fast. Okay, let’s keep making it faster. So we’re going to use stuff again, which kind of goes back APL. And a lot of people say that learning APL is the thing that’s taught them more about programing than anything else. So it’s probably worth considering learning APL And let’s just look at these various things. You got a is ten six minus four. So remember at APL, we don’t say equals, equals actually means equals. Funny enough we to say set two, we use this arrow and it’s, this is a list of ten, six, four and then b is 287. Okay. And we’re going to add them up a plus b, So what’s going on here? So it’s really important that you can think of a symbol like a as representing a tensor or an array. APL calls them arrays, pytorch call them tensors, Numpy calls them arrays. They’re the same thing. So this is a single that contains a bunch of numbers. This is a single thing that contains a bunch of numbers. This is an operation that applies to arrays or tensors. Now what it does is it works what’s called elsment-wise. It takes each pair ten and two, and that’s them together. Each pair six and eight, add them together. This is element wise addition and Fred is asking in the chat, how do you put in these symbols? If you just mouse over any of them, it will show you how to write it and the one you want is the one at the very bottom, which is the one where it says prefix. Now the prefix is the backtick character. So here it’s saying prefix hyphen gives us times. So we’ve had hyphen. So I’ve of a backtick dash b is a times b for example. So yeah, they all have shortcut keys which you learn pretty quickly. I find, and there’s a fairly consistent kind of system for those shortcut keys too. All right, So we can do the same thing in PyTorch.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><span class="im" style="color: #00769E;">from</span> numba <span class="im" style="color: #00769E;">import</span> njit</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><span class="at" style="color: #657422;">@njit</span></span>
<span id="cb87-2"><span class="kw" style="color: #003B4F;">def</span> dot(a,b):</span>
<span id="cb87-3">    res <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb87-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(a)): res<span class="op" style="color: #5E5E5E;">+=</span>a[i]<span class="op" style="color: #5E5E5E;">*</span>b[i]</span>
<span id="cb87-5">    <span class="cf" style="color: #003B4F;">return</span> res</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><span class="im" style="color: #00769E;">from</span> numpy <span class="im" style="color: #00769E;">import</span> array</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb89" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><span class="op" style="color: #5E5E5E;">%</span>time dot(array([<span class="fl" style="color: #AD0000;">1.</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>]),array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">4</span>]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 479 ms, sys: 145 ms, total: 624 ms
Wall time: 563 ms</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>20.0</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><span class="op" style="color: #5E5E5E;">%</span>time dot(array([<span class="fl" style="color: #AD0000;">1.</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>]),array([<span class="fl" style="color: #AD0000;">2.</span>,<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">4</span>]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 34 µs, sys: 6 µs, total: 40 µs
Wall time: 44.3 µs</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>20.0</code></pre>
</div>
</div>
<p>Now only two of our loops are running in Python, not three:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb95-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb95-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb95-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb95-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc): c[i,j] <span class="op" style="color: #5E5E5E;">=</span> dot(a[i,:], b[:,j])</span>
<span id="cb95-6">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1">m1a,m2a <span class="op" style="color: #5E5E5E;">=</span> m1.numpy(),m2.numpy()</span></code></pre></div>
</div>
<p>This is the test.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb97" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><span class="im" style="color: #00769E;">from</span> fastcore.test <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">test_close(t1,matmul(m1a, m2a))</span></code></pre></div>
</div>
<p>2000 time faster. We change inner most loop.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb99" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> matmul(m1a,m2a)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>455 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<p>So we can do the same thing in PyTorch. It’s a little bit more verbose. PyTorch, which is one reason I often like to do my mathematical fiddling around in APL. I can often do it with less boilerplate, which means I can spend more time thinking, you know, I can see everything on the screen at once. I don’t have to spend as much time trying to like ignore the tensor around bracket square bracket dot com, blah blah. It’s all cognitive load, which I’d rather ignore. But anyway, it does the same thing so I can say a plus b work exactly like APL. So here’s an interesting example. I can go a less than (a &lt; b).float().mean(). So let’s try that one over here less than b. So this is a really important idea, which I think was invented by Ken Iverson, the APL guy, which is that true and false represented zero and one. And because they’re represented by zero and one, we can do things to them. We can add them up and subtract and so forth. That’s a really important idea. So in this case, I want to take the main of them, and I’m going to tell you something amazing, which is that in APL there is no function called mean. Why not? That’s because we can write the mean function, which so that’s four letters mean and we can write the mean function from scratch with four characters. I’ll show you. Here’s the whole mean function we’re going to create a function called mean, and the mean is equal to the sum of a list divided by the of the list. So this here is some divided by count. And so I have now to find a new function called mean, which calculates the mean, mean of a is less than b, there we go. And so, you know in practice, I’m not sure why people would even bother defining a function called mean because it’s just as easy to actually write it’s implementation in APL, in numpy or whatever a python. It’s going to take a lot more than four letters to implement mean. So anyway, you know, it’s a math notation and so being a math notation we can do a lot with little, which I find out folks, I can say everything going on at once anyway. Okay, so that’s how we do the same thing in pytouch. And again, you can say that the less than in both cases, operating element wise. Okay, So a is less than b is saying ten is less than two six is less than eight four is less than seven and gives us back each of those trues and faleses as zeros and onces and according to our YouTube chat, had just exploded as it should. This is why APL is. Yeah life changing.</p>
</section>
<section id="elementwise-ops" class="level2">
<h2 class="anchored" data-anchor-id="elementwise-ops">Elementwise ops</h2>
<p><a href="https://tryapl.org/">TryAPL</a></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb101" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1">a <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="fl" style="color: #AD0000;">10.</span>, <span class="dv" style="color: #AD0000;">6</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb101-2">b <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="fl" style="color: #AD0000;">2.</span>, <span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">7</span>])</span>
<span id="cb101-3">a,b</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))</code></pre>
</div>
</div>
<p>Elementwise addition</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb103" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1">a <span class="op" style="color: #5E5E5E;">+</span> b</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([12., 14.,  3.])</code></pre>
</div>
</div>
<p>Check lecture for awesome implementation of mean.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1">(a <span class="op" style="color: #5E5E5E;">&lt;</span> b).<span class="bu" style="color: null;">float</span>().mean()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.67)</code></pre>
</div>
</div>
<p>Rank two tensor , aka Matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb107" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1">m <span class="op" style="color: #5E5E5E;">=</span> tensor([[<span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>], [<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">6</span>], [<span class="dv" style="color: #AD0000;">7</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">9</span>]])<span class="op" style="color: #5E5E5E;">;</span> m</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])</code></pre>
</div>
</div>
<p>Okay, let’s now go up to higher rank. So this here is a rank one tensor. So a rank one tensor means it’s a list of things, it’s a vector, it’s where else a rank two tensor. It’s like a list of lists. They all have to be the same length lists, or it’s like a rectangular bunch of numbers. And we call it in math, we call it a matrix. So this is how we can create a tensor containing one, two, three, four, five, six, 789. And you can see also, like, what I like to do is I want to print out the thing I just created after I created it. Two ways to do it. You can say put an enter and then write m and that’s going to do that. Or if you want to put it all in the same line, that works too. You just use a semicolon. Neither one is better than the other. They’re just different. So we could do the same thing in APL. Of course in APL it’s going to be much easier. So we’re going to define a matrix called m which is going to be a three by 3 by 3 tensor containing the numbers from 1 to 9. Okay. And there we go. That’s done it in APL, a three by three tensor containing the numbers from 1 to 9. A lot of these ideas from APL you’ll find have made their way into other programing languages. For example, if you use GO you might recognize this. This is the iota character and go uses the word iota to spell it out in a somewhat similar way. A lot of these ideas from APL have found themselves into math notation and in other languages. It’s been around since the late fifties. Okay, so here’s a bit of fun. We’re going to learn about a new thing that looks kind of crazy code for Frobenius Norm and we’ll use that from time to time as we’re doing modeling. And here’s a definition of a four Frobenius norm. It’s the sum over all of the rows and columns of a matrix, and we’re going to take each one and square it. They’re going to add them up and they’re going to take the square root.</p>
<p>And so to implement that in pytouch is as simple as (m*m).sum().sqrt(). So this looks like a pretty complicated thing when you kind of look at it. At first it looks like a lot of squiggly business or if you said this thing here you might be like, what on earth is that? Well, now you know, it’s just a square, some square root. So again, we could do the same thing in APL. So let’s do so in APL. We want the okay, so we got a case called S.F. Now it’s interesting, Apple does this a little bit differently. So dot some by default in PyTorch sums over everything. And if you want to sum over just one dimension, you have to pass in a dimension keyword for very good reasons. APL is the opposite. It just comes across rows or just down columns. So actually we have to say sum up the flattened out version of the Matrix and say flattened out. He is comma. So his sum up the flattened out version of the Matrix. Okay, so that’s our S.F.. Oh, Oh, sorry. And the Matrix is meant to be m times m There you go. So that’s the same thing. Sum up the flattened out and by a matrix and another interesting thing about APL is it always is read right to left. There’s no such thing as operator precedence, which makes life a lot easier. Okay, then we take the square root of that. There isn’t a square root function, so we have to do to the power of 0.5 and there we go. Same thing. All right, You got the idea. Yes. A very interesting question here from Marabou or other bars for norm or absolute value. And I like answer, which is the norm, is the same as the absolute value for scalar. So in this case, you can think of it as absolute value and it’s kind of not needed because it’s being squared anyway. But yes, in this case the norm. Well, in every case for a scale, the norm is the absolute value, which is kind of a cute discovery when you realize it. So thank you for pointing that out. See the. All right. So this is just fiddling around a little bit to kind of get a sense of how these things work. So really importantly, you can index into a matrix and you’ll say rows first and then columns.</p>
</section>
<section id="frobenius-norm" class="level1">
<h1>Frobenius norm:</h1>
<p><img src="https://latex.codecogs.com/png.latex?%5C%7C%20A%20%5C%7C_F%20=%20%5Cleft(%20%5Csum_%7Bi,j=1%7D%5En%20%7C%20a_%7Bij%7D%20%7C%5E2%20%5Cright)%5E%7B1/2%7D"></p>
<p><em>Hint</em>: you don’t normally need to write equations in LaTeX yourself, instead, you can click ‘edit’ in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click “Download: Other formats” in the top right, then “Download source”; rename the downloaded file to end in <code>.tgz</code> if it doesn’t already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:</p>
<div class="sourceCode" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode latex code-with-copy"><code class="sourceCode latex"><span id="cb109-1"><span class="ss" style="color: #20794D;">$</span><span class="sc" style="color: #5E5E5E;">\|</span><span class="ss" style="color: #20794D;"> A </span><span class="sc" style="color: #5E5E5E;">\|</span><span class="ss" style="color: #20794D;">_F = </span><span class="sc" style="color: #5E5E5E;">\left</span><span class="ss" style="color: #20794D;">( </span><span class="sc" style="color: #5E5E5E;">\sum</span><span class="ss" style="color: #20794D;">_{i,j=1}^n | a_{ij} |^2 </span><span class="sc" style="color: #5E5E5E;">\right</span><span class="ss" style="color: #20794D;">)^{1/2}$</span></span></code></pre></div>
<div class="cell">
<div class="sourceCode cell-code" id="cb110" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1">sf <span class="op" style="color: #5E5E5E;">=</span> (m<span class="op" style="color: #5E5E5E;">*</span>m).<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb110-2">sf</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(285.)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb112" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1">sf.sqrt()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(16.88)</code></pre>
</div>
</div>
<p>So really importantly, you can index into a matrix and you’ll say rows first and then columns. And if you say column, it means all the columns. So if I say row two, here it is row two all the columns. Sorry, this is row two. Two sets of zero, APL starts at one, all the columns that’s going to be seven, eight, nine. And you can say I often use comma to print out multiple things and I don’t have to say print in Jupyter. It’s kind of assumed. And so this is just a quick way of printing out the second row. And then here every row column two. So here is every row of column two and here you can see three, six, nine. So one thing very useful to recognize is that for tensors of higher rank than one, such as a matrix, any trailing colomns are optional. So you say this here m2 that’s the same as m2 comma column. It’s really important to remember. so m2 you can say the result is the same. So that means row two every column. Okay, so now with all that in place, we’ve got quite an easy way. We don’t need a number anymore. We can multiply so we can get rid of that innermost loop. So we’re going to get rid of this loop because this is just multiplying together all of the corresponding rows of a with or without sorry, all the corresponding columns of row a With all the corresponding rows of column of b, And so we can just use an element wise operation for that. So here is the i th row of s and here is the j th column of b. And so those are both as we’ve seen just vectors and therefore we can do an element wise multiplication of them and then some of them up and the same as a dot product. So that’s handy. And so again, what do test close. Okay, it’s the same great. And again you’ll see we kind of did all of experimenting first, right, to make sure we understood how it all worked and then put it together. And then if we time it 661 microseconds. Okay. So it’s interesting. It’s actually slower than it really shows you how good number is, but it’s certainly a hell of a lot better our 450 milliseconds. But we know we’re using something that’s kind of a lot more general now. This is exactly the same as dot as we’ve discussed. So we could just use torture, dot, dot, dot, dot, I suppose I should say. And if we run that okay, a little faster, it’s still interestingly, this is still slower than the Numba, which is quite amazing actually. All right. So that’s that one was not exactly a speed up, but it’s kind of more general. It’s just nice. Now we’re going to get something into something really fun, which is broadcasting. And broadcasting is about what if you have arrays with different shapes? So what’s the shape? The shape is the number of or the number of rows and columns or the number of or, as you say, faces rows and columns and so forth. So for example, the shape of m is three by three. So what happens if you multiply or add or do operations to tensors of different shapes? Well, one very simple one, which is if you’ve got a rank one tensor a vector, then you can use any operation with a scalar and it broadcasts that scalar across the tensor. So Ais greater than zero is exactly the same as saying a is greater than tensor zero comma zero comma zero. So it’s basically copying that across three times. That’s not literally making a copy in memory, but it’s acting as if we had said that. And this is the most simple version of broadcasting, okay, It’s broadcasting the zero across the ten and the six and the negative four and. APL does exactly the same thing. a is less than five, so 0 0 1 first time idea. Okay, so we can do plus with a scalar and we can do exactly the same thing with higher than rank one. So two times a matrix is just going to do two. It’s going to be broadcast across all the rows and all the columns. Okay, now gets interesting. So broadcasting dates back to APL, but a really interesting idea is that we can broadcast not just scaler, but we can broadcast vectors across matrices or broadcast any kind of lower ranked tensor across higher ranked tenses or even broadcast together together two tenses of the same rank, but different shapes and a really powerful way. And as I was exploring this, I was trying to I love doing this kind of computer archeology yesterday to find out where the hell this comes from. And it actually turns out from this email message in 1995 that the idea actually comes from a language that I’d never heard of Yorick, which still apparently exists, is Yorick. And so Yorick has talks about broadcasting and controllability. So what happened is this this very obscure language that’s this very powerful idea. And NumPy has, has has happily stolen the idea from Yorick that allows us to broadcast together tensors that don’t appear to match. So let me give an example. Here’s a ten second c, that’s a vector. It’s a rank 1 tensor at ten, 20, 30, and here’s a ten second m, which is a matrix we see in this one before, and one of them is shape three comma three, the other is shape three and. Yet we can add them together. Now, what’s happened when we added it together? Well, what’s happened is ten, and then 10 20 30 got added to four, five, six and then 10 20 30 got added to seven, eight, nine. And hopefully you can see this looks quite familiar. Instead of broadcasting a scalar over a higher rank tensor, this is broadcasting a vector across every row of a matrix and it works both ways. So we can say c plus m gives us exactly the same thing. And so let me explain what’s actually happening here. The trick is to know about this somewhat obscure method called expandas I would expand as does this. This creates a new thing called t, which contains exactly the same thing as c, but expanded it kind of copied over so it has the same shape as m. So here’s what t looks like now. t contains exactly the same thing as c does, but it’s got three copies of it now. And you can say we can definitely add t to m because they match shapes, right? So we can say m plus t, we know we can play m plus t because we’ve already learned that you can do element wise operations on two things that have matching shapes. Now, by the way, this thing t didn’t actually create three copies. Check this out. If we call t.storage, it tells us what’s actually in memory. It actually just contains the numbers ten, 20, 30 but it doesn’t really clever trick. It has a stride of zero across the rows and a size of three comma three. And so what that means is that it acts as if it’s a three by three matrix, and each time it goes to the next row, it actually stays exactly where it is. And this idea of strides is the trick which Numpy and PyTorch and so forth use for all kinds of things where you basically can create, you know, very efficient ways to to do things like expanding or to kind of jump over things and stuff like that, you know, switch between columns and rows, stuff like that. Anyway, the important thing here for us to recognize is that we didn’t actually make a copy. This is totally efficient. That’s all going to be run in C code very fast. So remember this expandas is critical. This is the thing that will teach you to understand how broadcasting works, which is really important for implementing deep learning algorithms or any kind of linear algebra on any python system because the Numpy rules are used exactly the same in jacks, in TensorFlow and PyTorch and so forth. Now I’ll show you a little trick, which is going to be very important in a moment. If we take C, which remember is a vector containing ten, 20, 30 and we say c.unsqueeze(0), then it changes the shape from 3 to 1 comma three. So it changes from a vector of length three to a matrix of one row by three columns. This would turn out to be very important in a moment, and you can see how it’s printed. It’s printed out with two square brackets. Now, I never use one squeeze because I much prefer doing something more flexible, which is if you index into an axis with a special value, none also known as np dot new axis, it does exactly the same thing. It inserts a new axis here. So here we’ll get exactly the same thing. One row by all the columns, straight columns. So this is exactly the same as saying unsqueeze. So this inserts a new unit axis. This is a unit axis, a single row. And this dimension and this does the same thing. So these are the same. So we could do the same thing and say unsqueeze(1), which means now we’re going to unsqueeze into the first dimension. So that means we now have three rows and one column.</p>
<p>So the shape here, the shape is inserting a unit axis in position one. three rows and one column. And so we can do exactly the same thing here. Give us every row and a new unit, axis and position one same thing. Okay, So those two are exactly the same. So this is how we create a matrix with one row. This is how we create a matrix with one column [None, :] vs [: , None] or unsqueeze. We don’t have to say as we’ve done before, non comma colon because you remember trailing colons optional. So therefore just say None is also going to give you a row matrix one. Right. Matrix. This is a little trick here. If you say dot dot, dot that means all of the dimensions and so dot, dot, dot comma None will always insert a unit axis at the end, regardless of what rank a tensor is. So yes, so None and NP new axis mean exactly the same thing. NP new axis is actually a synonym for None if you’ve ever used that. I always use None because why not. Short and simple. So here’s something interesting. If we go see colon comma, None. So let’s go and check out what’s C colonn comma None looks like C colonm common. None is a column. But if we say expandas which is three by three, then it’s going to take that 10 20 30 column and replicate it. 10, 20, 30, 10, 20, 30,, 10, 20, 30, So we could add. So remember like, well, remember, I’ll explain that when you say matrix plus C colon common none, it’s basically going to do this dot expandas for you, so if I want to add this matrix here to m, I don’t need to say don’t expandas I just write this at m+c[:,None]. And so this is exactly the same as doing m plus c, but now, rather than adding the vector to each row, it’s adding the vector to each column. c plus 10, 20, 30, 10, 20, 30, 10, 20, 30. So that’s a really simple way that we now get kind of for free, thanks to this really nifty notation, this initial approach that came from Yorick. So here you can see m plus c None comma column is adding 10, 20, 30 to each row and m plus c column comma None is adding 10 20 30 to each column. All right. So that’s the basic like hand-waving version. So let’s look at like what are the rules and how does it work? Okay, so c[None,:] is one by three, c[:,None]. What if we multiplyc [None,:] by c[:,None]. Well it’s going to do if you think about it, but you definitely should because thinking is very helpful. What is going on here? Okay, so what happens if we go c[None,:] times c[:,None]. So what it’s going to have to do is it’s going to have to take this 10, 20, 30 column vector or three by one matrix, and it’s going to have to make it work across each of these rows. So what it does is expands it to be 10 20 30, , 10 20 30 , 10 20 30 So it’s going to do it just like this and then it’s going to do the same thing for, c[None,:]. So that’s going to become three rows of 10, 20, 30. So we’re going to end up with three rows of 10, 20, 30 times three columns of 10, 20, 30, which gives us our answer. And so this is going to do an outer product. So it’s very nifty that you can actually do an outer product without any special, you know, functions or anything just using broadcasting. And it’s not just out of products. You can do outer Boolean operations and this kind of stuff comes up all the time. Right now, remember, you don’t need the comma colon, so get rid of it. So this is showing us all the places where it’s greater than it’s kind of an outer and outer boolean, if you want to call it that. So this is super nifty and you can do all kinds of tricks with this because it runs very, very fast. So this is going to be accelerated in C.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb114" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1">m[<span class="dv" style="color: #AD0000;">2</span>,:],m[:,<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([7., 8., 9.]), tensor([3., 6., 9.]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb116" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1">m[<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([7., 8., 9.])</code></pre>
</div>
</div>
<p>We can use elementwise operation and get ride of inner loop.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb118" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb118-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb118-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb118-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb118-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc): c[i,j] <span class="op" style="color: #5E5E5E;">=</span> (a[i,:] <span class="op" style="color: #5E5E5E;">*</span> b[:,j]).<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb118-6">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<p>Test to see they are the same.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb119" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1">test_close(t1,matmul(m1, m2))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb120" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.31 ms ± 71.4 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<p>Now that we wrote it , we can use equivalent of pytorch. (torch.dot)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb122" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb122-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb122-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb122-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb122-5">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(bc): c[i,j] <span class="op" style="color: #5E5E5E;">=</span> torch.dot(a[i,:], b[:,j])</span>
<span id="cb122-6">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb123" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1">test_close(t1,matmul(m1, m2))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb124" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>999 µs ± 67.1 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<p>The term <strong>broadcasting</strong> describes how arrays with different shapes are treated during arithmetic operations.</p>
<p>From the <a href="https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html">Numpy Documentation</a>:</p>
<pre><code>The term broadcasting describes how numpy treats arrays with 
different shapes during arithmetic operations. Subject to certain 
constraints, the smaller array is “broadcast” across the larger 
array so that they have compatible shapes. Broadcasting provides a 
means of vectorizing array operations so that looping occurs in C
instead of Python. It does this without making needless copies of 
data and usually leads to efficient algorithm implementations.</code></pre>
<p>In addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.</p>
<p><em>This section was adapted from <a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression">Chapter 4</a> of the fast.ai <a href="https://github.com/fastai/numerical-linear-algebra">Computational Linear Algebra</a> course.</em></p>
<section id="broadcasting-with-a-scalar" class="level3">
<h3 class="anchored" data-anchor-id="broadcasting-with-a-scalar">Broadcasting with a scalar</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb127" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1">a</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([10.,  6., -4.])</code></pre>
</div>
</div>
<p>Simplest way of broadcasting.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb129" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1">a <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([ True,  True, False])</code></pre>
</div>
</div>
<p>How are we able to do <code>a &gt; 0</code>? 0 is being <strong>broadcast</strong> to have the same dimensions as a.</p>
<p>For instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.</p>
<p>Other examples of broadcasting with a scalar:</p>
<p>plus</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb131" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1">a <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([11.,  7., -3.])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb133" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1">m</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])</code></pre>
</div>
</div>
<p>multiply</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb135" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>m</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 2.,  4.,  6.],
        [ 8., 10., 12.],
        [14., 16., 18.]])</code></pre>
</div>
</div>
</section>
<section id="broadcasting-a-vector-to-a-matrix" class="level3">
<h3 class="anchored" data-anchor-id="broadcasting-a-vector-to-a-matrix">Broadcasting a vector to a matrix</h3>
<p>Although broadcasting a scalar is an idea that dates back to APL, the more powerful idea of broadcasting across higher rank tensors <a href="https://mail.python.org/pipermail/matrix-sig/1995-November/000143.html">comes from</a> a little known language called <a href="https://software.llnl.gov/yorick-doc/manual/yorick_50.html">Yorick</a>.</p>
<p>We can also broadcast a vector to a matrix:</p>
<p>lower rank to higher rank. Same rank with different shapes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb137" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1">c <span class="op" style="color: #5E5E5E;">=</span> tensor([<span class="fl" style="color: #AD0000;">10.</span>,<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">30</span>])<span class="op" style="color: #5E5E5E;">;</span> c</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([10., 20., 30.])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb139" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1">m</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb141" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1">m.shape,c.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([3, 3]), torch.Size([3]))</code></pre>
</div>
</div>
<p>add c to each row of m (Matrix).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb143" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1">m <span class="op" style="color: #5E5E5E;">+</span> c</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb145" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1">c <span class="op" style="color: #5E5E5E;">+</span> m</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<p>Trick is we create t so then t could add to m.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb147" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1">t <span class="op" style="color: #5E5E5E;">=</span> c.expand_as(m)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb148" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1">t</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[10., 20., 30.],
        [10., 20., 30.],
        [10., 20., 30.]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb150" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1">m <span class="op" style="color: #5E5E5E;">+</span> t</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<p>We don’t really copy the rows, but it looks as if we did. In fact, the rows are given a <em>stride</em> of 0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb152" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1">t.storage()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code> 10.0
 20.0
 30.0
[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]</code></pre>
</div>
</div>
<p>We only have one line , so we did not copy , make the code efficient.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb154" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1">t.stride(), t.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>((0, 1), torch.Size([3, 3]))</code></pre>
</div>
</div>
<p>You can index with the special value [None] or use <code>unsqueeze()</code> to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).</p>
<p>This is how we create matrix with one row , [None, :]</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb156" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1">c.unsqueeze(<span class="dv" style="color: #AD0000;">0</span>), c[<span class="va" style="color: #111111;">None</span>, :]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))</code></pre>
</div>
</div>
<p>Use unsqueeze to change the shape. We can use this trick for broadcasting.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb158" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1">c.shape, c.unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([3]), torch.Size([1, 3]))</code></pre>
</div>
</div>
<p>This is how we create matrix with one column , [ : , None], easier than unsqueeze but the same.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb160" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1">c.unsqueeze(<span class="dv" style="color: #AD0000;">1</span>), c[:, <span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([[10.],
         [20.],
         [30.]]), tensor([[10.],
         [20.],
         [30.]]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb162" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1">c.shape, c.unsqueeze(<span class="dv" style="color: #AD0000;">1</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([3]), torch.Size([3, 1]))</code></pre>
</div>
</div>
<p>You can always skip trailling ‘:’s. And’…’ means ‘<em>all preceding dimensions</em>’</p>
<p>we can avoid : and say c[None].</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb164" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1">c[<span class="va" style="color: #111111;">None</span>].shape,c[...,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([1, 3]), torch.Size([3, 1]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb166" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1">c[:,<span class="va" style="color: #111111;">None</span>].expand_as(m)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[10., 10., 10.],
        [20., 20., 20.],
        [30., 30., 30.]])</code></pre>
</div>
</div>
<p>basicly does .expand_as for you. This adding the vector to each column.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb168" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1">m <span class="op" style="color: #5E5E5E;">+</span> c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[11., 12., 13.],
        [24., 25., 26.],
        [37., 38., 39.]])</code></pre>
</div>
</div>
<p>This adding the vector to each row.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb170" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1">m <span class="op" style="color: #5E5E5E;">+</span> c[<span class="va" style="color: #111111;">None</span>,:]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[11., 22., 33.],
        [14., 25., 36.],
        [17., 28., 39.]])</code></pre>
</div>
</div>
<p>So here are the rules. Okay? When you operate on two arrays or tensors, numpy and pytorch will compare their shapes. Okay, so remember the shape. This is a shape. You can tell us a shape because we said shape and it goes from right to left. So that’s the traveling dimensions and it checks by the dimensions are compatible. Now they’re compatible if they’re equal, right? So for example, if we say m times, m, then those two shapes are compatible because the because in each case the it’s going to be three, right? So they’re going to be equal. So if that if the shape in that dimension is equal, they’re compatible, or if one of them is one and if one of them is one, then dimension is broadcast to make it the same size as the other. So that’s why the outer product worked. We had a one by three times, a three by one, and so this one got copied three times to make this long and this one got copied three times to make it this long. Okay, so those are the rules. So the arrays don’t have to have the same number of dimensions. So this is an example that comes up all the time. Let’s say you’ve got a 256 by 256 by three array of tensor of RGP values. So you’ve got an image, in other words, a three color image, and you want to normalize. So you want to scale each color in the image by a different value. So this is how we normalize colors. So one way is to you could multiply or divide or whatever, multiply the image by a one dimensional array with three values. So you’ve got a 1D array. So that’s just three. Okay. And then the image is 256 by 256 by three, and we go right to left and we check, are they the same? We say, Yes, they are. And then we keep going left and we say, are they the same? And if missing, we act as if it’s one. And if we get keep going, if it’s missing, we act as if it’s one, this is going to be the same as doing one by one by three. And so this is going to be broadcast. This three, three elements will be brought broadcast over all to 256, but 266 pixels. So this is a super fast and convenient and nice way of normalizing image data with a single expression. And this is exactly how we do it in the fast day library. In fact. So we can use this to dramatically speed up our matrix multiplication. Let’s just grab a single digit just for simplicity. And I really like doing this in Jupyter notebooks. And if you if you build Jupyter notebooks to explain stuff that you’ve learned in this course or ways that you can apply it, consider doing this for your readers, but add a lot more prose. I haven’t added prose here because I want to use my voice. If I, for example, in our book that we published, it’s all written in notebooks and there’s a lot more prose, obviously. But like really, I’d like to show every example all along the way using simple as possible.</p>
</section>
<section id="broadcasting-rules" class="level3">
<h3 class="anchored" data-anchor-id="broadcasting-rules">Broadcasting Rules</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb172" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1">c[<span class="va" style="color: #111111;">None</span>,:]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[10., 20., 30.]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb174" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1">c[<span class="va" style="color: #111111;">None</span>,:].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb176" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1">c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[10.],
        [20.],
        [30.]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb178" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1">c[:,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([3, 1])</code></pre>
</div>
</div>
<p>3 rows times 3 column. This do outer product and other stuff like this.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb180" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1">c[<span class="va" style="color: #111111;">None</span>,:] <span class="op" style="color: #5E5E5E;">*</span> c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[100., 200., 300.],
        [200., 400., 600.],
        [300., 600., 900.]])</code></pre>
</div>
</div>
<p>outer boolean. This run fast.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb182" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1">c[<span class="va" style="color: #111111;">None</span>] <span class="op" style="color: #5E5E5E;">&gt;</span> c[:,<span class="va" style="color: #111111;">None</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[False,  True,  True],
        [False, False,  True],
        [False, False, False]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb184" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1">m<span class="op" style="color: #5E5E5E;">*</span>m</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 1.,  4.,  9.],
        [16., 25., 36.],
        [49., 64., 81.]])</code></pre>
</div>
</div>
<p>When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the <strong>trailing dimensions</strong>, and works its way forward. Two dimensions are <strong>compatible</strong> when It goes from right to left. - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size</p>
<p>Arrays do not need to have the same number of dimensions. For example, if you have a <code>256*256*3</code> array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:</p>
<pre><code>Image  (3d array): 256 x 256 x 3
Scale  (1d array):             3
Result (3d array): 256 x 256 x 3</code></pre>
<p>The <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules">numpy documentation</a> includes several examples of what dimensions can and can not be broadcast together.</p>
</section>
<section id="matmul-with-broadcasting" class="level2">
<h2 class="anchored" data-anchor-id="matmul-with-broadcasting">Matmul with broadcasting</h2>
<p>So let’s just grab a single digit. So here’s the first digit. So its shape is it’s a 784 long vector. Okay. And remember that our weight matrix is 784 by ten. Okay. So if we say digit colon coma None dot shape, then that is a 784 by one row matrix. Okay. So there’s our matrix. And so if we then take that 784 by one and expandas m2, it’s going to be the same shape as our weight matrix. So it’s copied our image data for that digit across all of the ten vectors, representing the ten kind of linear projections we’re doing for our linear model. And so that means that we can take the digit colon comma a None so 784 by one and multiply it by the weights. And so that’s going to get us back 784 by 10 so what it’s doing, remember, is it’s basically looping through each of these 10, 784 long vectors. And for each one of them it’s multiplying it by this digit. So that’s exactly what we want to do in our matrix multiplication. So originally we had when I originally most recently I should say, we had this dot product where we were actually looping over j, which was the columns of b, So we don’t have to do that anymore because we can do it all at once by doing exactly what we just did so we can take the ith and all the columns and add a access to the end. And then just like we did here, multiply it by b and then .sum(). And so that is again exactly the same thing.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb187" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1">digit <span class="op" style="color: #5E5E5E;">=</span> m1[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb187-2">digit.shape,m2.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([784]), torch.Size([784, 10]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb189" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1">digit[:,<span class="va" style="color: #111111;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([784, 1])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb191" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1">digit[:,<span class="va" style="color: #111111;">None</span>].expand_as(m2).shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([784, 10])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb193" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1">(digit[:,<span class="va" style="color: #111111;">None</span>]<span class="op" style="color: #5E5E5E;">*</span>m2).shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([784, 10])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb195" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><span class="kw" style="color: #003B4F;">def</span> matmul(a,b):</span>
<span id="cb195-2">    (ar,ac),(br,bc) <span class="op" style="color: #5E5E5E;">=</span> a.shape,b.shape</span>
<span id="cb195-3">    c <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(ar, bc)</span>
<span id="cb195-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(ar):</span>
<span id="cb195-5"><span class="co" style="color: #5E5E5E;">#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version</span></span>
<span id="cb195-6">        c[i]   <span class="op" style="color: #5E5E5E;">=</span> (a[i,:,<span class="va" style="color: #111111;">None</span>] <span class="op" style="color: #5E5E5E;">*</span> b).<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>) <span class="co" style="color: #5E5E5E;"># broadcast version</span></span>
<span id="cb195-7">    <span class="cf" style="color: #003B4F;">return</span> c</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb196" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1">test_close(t1,matmul(m1, m2))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb197" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">50</span> _<span class="op" style="color: #5E5E5E;">=</span>matmul(m1, m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>185 µs ± 54.5 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)</code></pre>
</div>
</div>
<p>That is another metrics, multiplication, doing it using broadcasting. Now this is like a tricky to get your head around. And so if you haven’t done this kind of broadcasting before, it’s a really good time to pause the video and look carefully at each of these four cells before and understand what did I do there, Why did I do it? What am I showing you? And then experiment with trying to and so remember that we started with m1 zero, right? So just like we have here, a[i , so that’s why we’ve got, i comma colon comma None because this digit is actually m1 zero. So this is like m1 zero colon None. So this line is doing exactly the same thing as this here plus the sum. So let’s check if this matmul is the same as it used to be, yet still working and the speed of it. Okay, not bad. So 137 microseconds. So we’ve now gone from a time from 500 milliseconds to about point 1 milliseconds. Funnily enough, my MacBook Air is an m2, whereas this Mac mini is an m1 that’s a little bit slower. So my error it was a bit faster than 0.1 milliseconds. So overall we’ve got about a 5000 times speed improvement. So that is pretty exciting. And since it’s so fast now, there’s no need to use a mini batch anymore. If you remember, we used a mini batch of five images, but now we can actually use the whole dataset so fast. So now we can do the whole data set. There it is. We’ve now got 15,000 by ten, which is what we want. And so it’s taking us only 656 milliseconds now to do the whole dataset. So this is actually getting to a point now where we could start to create and train some simple models in a reasonable enough time. So that’s good news. All right. I think that’s probably a good time to take a break. We don’t have too much more of this to go, but I don’t want to keep you guys up too late. So hopefully you learned something interesting about broadcasting today. I cannot overemphasize how widely useful is in all deep learning in machine learning code. It comes up all the time. It’s basically our number one most critical kind of foundational operation. So, yeah, take your time practicing it. And also good luck with your diffusion homework from the first half of the lesson. Thanks for joining us and I’ll see you next time.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb199" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1">tr <span class="op" style="color: #5E5E5E;">=</span> matmul(x_train, weights)</span>
<span id="cb199-2">tr</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],
        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],
        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],
        ...,
        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],
        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],
        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb201" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1">tr.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([50000, 10])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb203" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><span class="op" style="color: #5E5E5E;">%</span>time _<span class="op" style="color: #5E5E5E;">=</span>matmul(x_train, weights)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1.92 s, sys: 1.61 ms, total: 1.93 s
Wall time: 2.3 s</code></pre>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing stable diffusion from scratch 2/index.html</guid>
  <pubDate>Tue, 14 Mar 2023 20:30:00 GMT</pubDate>
</item>
<item>
  <title>Writing stable diffusion from scratch</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index.html</link>
  <description><![CDATA[ 



<p>All credit goes to www.fast.ai. All mistakes are mine. In the foundation series, I only write about part of the lecture that is related to writing stable difussion from scratch.Jeremy also talked about the big picture of the stable diffusion model. I will write about those in the big picture series of my blog posts.Almost all of the stuff in the subtitle from lectures.</p>
<section id="matrix-multiplication-from-foundations" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication-from-foundations">Matrix multiplication from foundations</h2>
<p>It’s going to require some serious tenacity and a certain amount of patience,but I think you’re going to learn a lot.A lot of folks I’ve spoken to have said thatprevious iterations of this of the course is like the best course they’ve ever done, and this one’s going to be dramatically better than any previous version we’ve done of this. So hopefully you’ll find that the the hard work and patience pays off. So the goal is to get to stable diffusion from the foundations, which means we have to define what are the foundations. So I have decided to define them as follows : We’re allowed to use Python, we’re allowed to use the Python standard library. So that’s all the stuff that comes with Python by default we’re allowed to use matplotlib because I couldn’t be bothered creating my own plotting library and are allowed to use Jupyter notebooks and nbdev, which is something that creates modules from notebooks. So basically what we’re going to try to do is to yeah, rebuild everything starting from this foundation. Now to be clear, what we are allowed to use are the libraries Once we have re-implemented them correctly. And so if we if we re-implement something from NumPy or from PyTorch or whatever, we’re then allowed to use the Numpy or PyTorch or whatever version, sometimes we’ll be creating things that haven’t been created before, and that’s then going to be becoming our own library and we’re going to be calling that Library Mini AI. So we’re going to be building our own little framework as we go.</p>
<p>The <em>foundations</em> we’ll assume throughout this course are:</p>
<ul>
<li>Python</li>
<li>matplotlib</li>
<li>The Python standard library</li>
<li>Jupyter notebooks and nbdev</li>
</ul>
<p>So, for example, here are some inputs and these inputs all come from the Python standard library, except for these two. Now, to be clear, one challenge we have is that the models we use in stable diffusion, what trained on millions of dollars worth of equipment per month, which we don’t have the time or money. So another we’re going to do is we’re going to create smaller, identical, but smaller versions of them. And so once we’ve got them working, well, then be allowed to use the big Pre-Trained versions. So that’s the basic idea. So we’re going to have to end up with our own VIE our own UNIT, our own clip encoder and, so forth. To some degree. I am assuming that you’ve completed part one of the course to some degree I will cover everything, at least briefly. But if I cover something about deep learning too fast for you to know what’s going on and you get lost, go back and watch part one or go and, you know, Google for that term for stuff that we haven’t In part one, I will go over it very thoroughly and carefully. All right. So I’m going to assume that, you know, the basic idea that which is that we’re going to need to be doing some matrix multiplication.</p>
<p>So we’re going to try to take a deep dive into matrix multiplication today and we’re going to need some input data. And I quite like working with MNIST data, MNIST is hand-written digits. It’s a classic data set they are 28 by 28 pixel grayscale images and so we can download them from this URL. So we use the path libs path object a lot. It’s got part of Python and it basically takes a string and turns it into something that you can treat as a path. For example, you can use slash to mean this file inside this subdirectory. So this is how we create a path object path objects have for example a make directory mkdir method. So I like to get everything set up, but I want to be able to rerun this so lots of times and not have it like give me errors if I run it more than once, if I read a second time, it still works. And in that case that’s because I put this exist_ok = True. How did I know that? I can say because otherwise would try to make the directory. It would already exist in a given error. How do I know what parameters I can pass to make? I just press shift tab. And so when I hit shift tab, it tells me what options there. If I press it a few times, it’ll actually puppet down at the bottom of the screen. Just to remind me I can press escape to get rid of it. Or you can just or else you can just hit tab inside and it’ll list all the things you can type as parametors. As you can see. All right. So we need to grab this URL. And so Python comes with something for doing that, which is the URL lib library that’s part of Python that has something you I will retrieve and something which I’m always a bit surprised is not widely used as people are reading the Python documentation. So you should do that a lot. So if I click on that, here is the documentation for urlretrive, I will retrieve and so I can find exactly what it can take and I can learn about exactly what it does.So I yeah, I read the documentation from the Python docs for every single method I use and I look at every single option that it takes and then I practice with it and to practice with it, I practice inside Jupyter. So if I want this import on its own, I can hit control shift hyphen and it’s going to split it into two cells and then I’ll hit ALT + Enter or Option Enter so I can create something underneath so I can type urlretrieve shift tab. And so there it all is if I’m like way down somewhere in in the notebook and I have no idea where urlretrieve comes from, I can just hit shift enter and it actually tells me exactly where it comes from. And if I want to know more about it, I can just hit question mark shift enter and it’s going to give me documentation and most of all, second question mark and it gives me the full source code and you say it’s not a lot. You know, reading the source code of Python standard library stuff is often quite revealing and you can see exactly how they do it.</p>
<p>We use MNIST (hand-written digits) as our data. It is a classic 28 by 28 pixcel data set. http://yann.lecun.com/exdb/mnist/ We can download them from GitHub URL.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> pathlib <span class="im" style="color: #00769E;">import</span> Path</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pickle, gzip, math, os, time, shutil, matplotlib <span class="im" style="color: #00769E;">as</span> mpl, matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
</section>
<section id="get-data" class="level2">
<h2 class="anchored" data-anchor-id="get-data">Get data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">MNIST_URL<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'</span></span>
<span id="cb2-2">path_data <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'data'</span>)</span>
<span id="cb2-3">path_data.mkdir(exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-4">path_gz <span class="op" style="color: #5E5E5E;">=</span> path_data<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'mnist.pkl.gz'</span></span></code></pre></div>
</div>
<p>That’s a great way to learn more about more about this. So in this case, I’m just going to use a very simple functionality, which is I’m going to say the URL to retrieve and the file name to save it as and again, I’m made it so I can run this multiple times. So it’s only going to do the URL retrieve if the path doesn’t exist. If already downloaded it, I don’t want it downloaded again. So I run that cell and notice that I can put exclamation mark followed by a line of bash. And it actually runs this using bash. If you’re using windows, this this won’t work. And I would very, very strongly if you’re using Windows use WSL and if you used WSL, all of these notebooks will work perfectly. So yeah, do that. All right. It on paperspace or LambdaLabs or something like that, CoLab, etc..</p>
<p><a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve">urlretrieve</a> - (read the docs!)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> urllib.request <span class="im" style="color: #00769E;">import</span> urlretrieve</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> path_gz.exists(): urlretrieve(MNIST_URL, path_gz)</span></code></pre></div>
</div>
<p>So I run that cell and notice that I can put exclamation mark followed by a line of bash.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">-</span>l data</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total 16656
-rw-r--r-- 1 root root 17051982 Mar  6 09:39 mnist.pkl.gz</code></pre>
</div>
</div>
<p>Okay, so this is a zgip file. So thankfully, Python comes with a gzip module. Python comes with quite a lot actually. And so we can open a gzip file gzip.open and we can pass in the path and we say we’re going to read it as binary as opposed to text. Okay. So this is called a context manager. It’s it’s a width clause. And what it’s going to do is it’s going to open up this gzip file. The GC object will be called F and that it runs everything inside the the block. And when it’s done it will close the file. So with blocks can do all kinds of different things.</p>
<p>But in general, with blocks that involve files, it will going to close the file automatically for you. So We can now do that. And so you can see it’s opened up the gzip file and the gzip file contains what’s called pickle objects, pickled objects, It’s basically Python objects that are being saved to disk. It’s the main way that people in pure Python save stuff and it’s part of the standard library. So this is how we load in from that file. Now the file contains a couple of tuples, so when you put a tuple on the left hand side of an equal sign, it’s quite neat. It allows us to put the first couple into two variables called x_train, y_train and the second into x_valid, y_valid.And we’ve added this trick here where you put stuff like this on the left is called D structuring and it’s a super handy way to make your code kind of clear and concise. And lots of languages support that including Python.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> gzip.<span class="bu" style="color: null;">open</span>(path_gz, <span class="st" style="color: #20794D;">'rb'</span>) <span class="im" style="color: #00769E;">as</span> f: ((x_train, y_train), (x_valid, y_valid), _) <span class="op" style="color: #5E5E5E;">=</span> pickle.load(f, encoding<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'latin-1'</span>)</span></code></pre></div>
</div>
<p>Okay, so we’ve now got some data and so we can have a look at it. Now it’s a bit tricky because we’re not allowed to use Numpy according to our rules, but unfortunately this actually comes as Numpy, so I’ve turned it into a list. All right. So I’ve taken the first image and I’ve turned it into a list. And so we can look at a few examples of some values in that list. And here they are. So it looks like the numbers between zero and one and this is what I do, you know, when I learn about a new dataset. So when I started writing this notebook, what you see here other than the pros here is, is what I actually did when I was working with this data. This I wanted to know what it was. So I just grab a little bit of it and look at it. So I kind of got a sense now of what it is now. Interestingly, this image is 784 long list.People already have people freaking out in the comments. No numpy. Yeah, the numpy. Do you say numpy then. NumPy. Why 784 ?What is that. Well that’s because he’s a 28 by 28 images. So it’s just a flat list here of 784 long. So do I turn this 784 long thing into 28 by 28. So I want to take a list of 28 lists of 28, basically because we don’t have matrices. So how do we do that? And so we’re going to be learning a lot of cool stuff in Python here.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">lst1 <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(x_train[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb7-2">vals <span class="op" style="color: #5E5E5E;">=</span> lst1[<span class="dv" style="color: #AD0000;">200</span>:<span class="dv" style="color: #AD0000;">210</span>]</span>
<span id="cb7-3">vals</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.0,
 0.0,
 0.0,
 0.19140625,
 0.9296875,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="bu" style="color: null;">len</span>(lst1)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>784</code></pre>
</div>
</div>
<p>Sorry, I got to start laughing at all the stuff in that chat. The people are quite reasonably freaking out. That’s okay. We’ll get there, I promise. I hope. Otherwise I’ll embarrass myself. All right, So how do I convert a 784 long list into 28 lists? I’m going to use something called chunks. And first of all, I’ll show you what this thing does and then I show you how it works. So vals is currently a list of ten things. So I take vals and I pass it to chunks with five, it creates two lists of five is list number one of five elements. And here’s list number two of five elements. Hopefully you can say it’s doing its chunk defying this list, and this is the length of each chunk. Now, how did you do that? The way I did it is using a very, very useful thing in Python that far too many people don’t know about, just called yield. And what it does is you can see here what a loop it’s going to go through from zero up to the length of my list and it’s going to jump by five at a time. That’s going to go, in this case, 0 to 5. And then it’s going to think of this as being like return for now, it’s going to return the list from zero up to five. So it returns the first bit of the list. But yield doesn’t just return. It kind of like returns a bit and then it continues and it returns a bit more. And so specifically, what yield does is it creates an iterator and iterator is an iterator is basically something you can actually just use it that you can call next on a bunch of times.So what is iterator? Well, iter it is something that I can basically I can call next on and next basically says yield the next thing. So this should yield vals[0,5]. There it is. It did write this vals[0,5]. Now, if I run that again, it’s going to give me a different answer because it’s now up to the second part of this loop. Now it returns the last five. Okay. So this is what a iterator does. Now, if you pass an iterator to Python’s List, it runs through the entire letter, iterater it until it’s finished and creates a list of the results. And what is finished? Looks like this is what finish looks like. If you call next and get stop iteration, that means you’ve run out. And that makes sense, right? Because my loop, there’s nothing left in it. So all of that is to say we now have a way of taking a list and chunkifying it. So what if I now take my full image? Image number one chunkify it into chunks of 28 long and turn that into a list and plot it that we have successfully created an image. So that’s good. Now we are done. But there are other ways to create this iterator. And because iterate is and generators which are closely related are so important. I wanted to show you more about how to do them in Python. It’s one of these things that if you understand this, you will often find that you can throw away huge pieces of enterprise software and basically replace it with an iterator that lets you stream things one bit at a time. It doesn’t store it all in memory. It’s this really powerful thing that once I often find, once I show it to people, they suddenly go like, Oh wow, I know we’ve been using all this third party software and we could have just created a python iterator. Python comes with a whole standard library module called itertools just to make it easier to work with iterators.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">def</span> chunks(x, sz):</span>
<span id="cb11-2">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">len</span>(x), sz): <span class="cf" style="color: #003B4F;">yield</span> x[i:i<span class="op" style="color: #5E5E5E;">+</span>sz]</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;">list</span>(chunks(vals, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],
 [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]</code></pre>
</div>
</div>
<p>In order to convert 784 to 28*28 matrix, we use yeild and chunk function to do this. chunk going through the loop and go through whole list but jump 28 at a time. yeild return and continue. It create a iterator.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">mpl.rcParams[<span class="st" style="color: #20794D;">'image.cmap'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'gray'</span></span>
<span id="cb14-2">plt.imshow(<span class="bu" style="color: null;">list</span>(chunks(lst1, <span class="dv" style="color: #AD0000;">28</span>)))<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I’ll show you one example or something from it at all, such as islice. So let’s grab our values again. These ten values. Well, that was a mistake. I should not have called this iter. Let’s just do that again. Okay, so let’s take these ten values and we can any list and turn it into an iterator by passing it to iter, which I should call it that. So I don’t override this python. That’s not a keyword. But this thing, I don’t want to override. So this is now basically something that I can call. Actually, let’s do this. I’ll show you that. I can call next on it. So if I now go next.it you can see it’s giving me each item one at a time. Okay. So that’s what converting it into an iterator does. I slice convert it into a different kind of iterator? Let’s call this isislice iterator. I write a and so you can see here what it did was it jumped stop here. Yeah. So that’s what it being better so I should query create the iterator and then call next a few times. Sorry, this is what I meant to do. It’s now only returning the first five before it calls stop iteration before it raises stop iteration. So what I does is it grabs the first and things from an iterable, something that you can iterate. Why is that interesting? Because I can pass it to list. For example. Right? And now if I pass it to list again, this iterator has now grabbed the first five things. So it’s now up to thing number six. So if I call it again, it’s the next five things. And if I call it again, then there’s nothing left. And maybe you can see we’ve actually now got this defined, but we can do it with islice. And here’s how we can do it. It’s actually pretty tricky. iter in Python or you can pass it something like a list to create an iterator or you can pass it,this is a really important word, a callable. What’s a callable? A callable is generally speaking, it’s a function. It’s something that you can put parentheses after. Could even be a class, anything you can put parentheses after. You can just think of it for now as a function. So we’ve got a pass it a function and in the second form it’s going to be called until the function returns this value here, which in this case is empty list. And we just saw that islice will return empty list when it’s done. So this here is going to keep calling this function again and again and again. And we’ve seen exactly what happens because we caught it ourselves before. There it is. Until it gets an empty list. So if we do it with 28, then we’re going to get our image again. So we’ve now got two different ways of creating exactly the same thing.If you’ve never used iterate as before, now’s a good time to pause the video and play with them. Right? So for example, you can take this here, right? And if you’ve not seen Lambdas before, they’re exactly the same as functions, but you can define them in line. So let’s, let’s replace that with a function. Okay? So now I’ve turned it into a function and then you can experiment with it. So let’s create our iterator and call F on it.<br>
Well,F and you can say this the first 28 and each time I do it, I’m getting another 28. Now the first few rows are all empty. But finally, look, now I’ve got some values. Call it again. See how each time I’m getting something else. This calling it again and again. And that is the values in a iterator. So that gives you a sense of like how you can use Jupyter to experiment. So what you should do is as soon as you hit something in my code that doesn’t look familiar to you, I recommend pausing the video and experimenting with that in Jupyter. And for example, iter, Most people probably have not used it at all, and certainly very few people have use this to argument form so hit shift tab a few times and now you’ve got at the bottom to the description of what it is or find that more Python iter. Yeah go to the docs. Well that’s not the right but if the docs say API wow crazy that’s terrible let’s try searching here. Yeah okay iter that’s more like it. it so now you’ve got links so it’s like okay it returns an iterated object. What’s that. Well click on it find out that that’s really important to note is that stop exception that we saw so stop iteration exception we saw next already we can find out what iterable is. And here’s an example. And as you can see, it’s using exactly the same approach that we did. But here it’s being used to read from a file. This is really cool. Here’s how to read from a file 64 bytes at a time until you get nothing processing it right so that the docs the python are quite fantastic as long as you use them. If you don’t use them, they’re not very useful at all. And I say Safer in the comments : Our local Haskell programmer appreciating this Haskell illness in Python. So that’s good. It’s not quite Haskell, I’m afraid, but it’s the closest we’re going to come. All right, here we go for time. Pretty good.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">vals</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.0,
 0.0,
 0.0,
 0.19140625,
 0.9296875,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125]</code></pre>
</div>
</div>
<p><a href="https://docs.python.org/3/library/itertools.html#itertools.islice">islice</a></p>
<p>lets grab our 10 values and learn about islice from itertools module in python.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;">from</span> itertools <span class="im" style="color: #00769E;">import</span> islice</span></code></pre></div>
</div>
<p>you can call next in it and give you next 5 items. (5 in islice). So islice only return the first five (use next to understand this).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(vals)</span>
<span id="cb18-2">isit <span class="op" style="color: #5E5E5E;">=</span> islice(it, <span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.0, 0.0, 0.0, 0.19140625, 0.9296875]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">it <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">iter</span>(lst1)</span>
<span id="cb21-2">img <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">iter</span>(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="bu" style="color: null;">list</span>(islice(it, <span class="dv" style="color: #AD0000;">28</span>)), []))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">plt.imshow(img)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Use this link to learn more about <a href="https://docs.python.org/3.10/library/functions.html?highlight=iter#iter">iter</a></p>
</section>
<section id="matrix-and-tensor" class="level2">
<h2 class="anchored" data-anchor-id="matrix-and-tensor">Matrix and tensor</h2>
<p>Okay, so now that we’ve got image, which is a list of lists and each list is 25 long, we can index into it so we can say image 20. Well, let’s do it. Image 20. Okay. Is a list of 28 numbers and then we could index into that. Okay, so we can index into it. Now normally we don’t like to do that for matrices. We would normally rather write it like this : img[20,15] Okay, So that means we’re going to have to create our own class to make that work. So to create a class in Python, you’re write Class. And then you write the name of it and then you write some really weird things. The weird things you write have two underscore is a special word And then two underscore is these things with two underscores Each side are called dunder methods, and they’re all the special magically named methods which have particular meanings to Python, and you’re just going to let them. But they’re all documented in the Python object model. dunder <strong>init</strong>. No, it’s actually terrible search. We probably maybe need to look for object model. Then also absolutely terrible. All right. So maybe try Google Python and it object model. Yeah. Finally. Okay. So what’s your manually for. Oh it’s got data or not object model. And so this is basically where all the documentation is about absolutely everything and I can click done to edit and it tells you basically this is the thing that constructs objects. So any time you want to create a class that you want to, that you want a constructor that’s going to stores and stuff. So in this case it’s going to store image. You have to define dunder init. Python’s slightly weird in that every method you have to put self here for reasons we probably don’t really need to get into right now. And then any parameters. So we’re going to be creating image passing in the thing to store the x’s they’re going to be passing in the Xs. And so here we’re just going to store it inside the self. So once I’ve got this line of code, I’ve now got something that knows how to store stuff, the x’s inside itself. So now I want to be able to call square bracket 20 comma 15. So how do we do that? Well, basically part of the data model, this is a special thing called dunder <strong>getitem</strong>. And when you call square brackets on your object, that’s what Python uses and it’s going to pass across the [20,15]. Yeah that’s indices So we’re now but basically you’re going to return this so the self.x with the first index and the second index. So let’s create that matrix class and run that And you can now see m[20,15] is the same, quick note on, you know, ways in which my code is different to everybody else’s, which it is. It’s somewhat unusual to put definitions of methods on the same line as as the the signature like this. I do a quite a lot for one liners. As I mentioned before, I find it really helps me to be able to see all the code I’m working with on the screen at once. A lot of the world’s best program has actually had that approach as well. It seems to work quite well for some people that are extremely productive. It’s not common in Python, some people are quite against it. So if you’re at work and your colleagues don’t write Python this way, you probably shouldn’t either. But if you can get away with it, I think it works quite well anyway. Okay, so now that we’ve created something that lets us index into things like this, we’re allowed to use because we were allowed to use this one feature in PyTorch. Okay, so we can now do that. And so now to create a tensor, which is basically a lot like our matrix can now pass a list into tensor to get back that tensor version of that list. Or perhaps more interestingly, we could pass in a list of lists. Maybe this gives us a name.that needs to be a list of lists just like we had before. For our image. In fact, let’s do it for our image. Let’s just pass in our image. Yeah. Okay. And so now we should be able to say tens[20,15]. Okay, so we’ve successfully reinvented that. All right. So now we can convert all of our lists into tenses. There’s a convenient way to do this, which is to use the map function in the Python standard library. So shift tab map takes function, and then some iterables, in this case one iterable, and it’s going to apply this function to each of these four things and return those four things. And so then I can put four things on the left to receive those four things. So this is going to call tensor x_train and put it in a x_train so forth. So this is converting all of these lists to tensors and storing them back in the same name. So you can see that x_train now is a tensor. So that means it has a shape property. It 50,000 images in it which each 784 long and you can find out what kind of what kind of stuff it contains by calling x_train.type() . So it contains floats. So this is the tensor class we’ll be using a lot of it. So of course you should read its documentation in I don’t love the PyTorch documentation. Some of it’s good, some of it’s not good. It’s a bit all over the place. So here’s tensor, but it’s well worth scrolling through to get a sense of like this is actually not bad. Right? It tells you how you can construct it. This is how I constructed one before passing it lists of lists. You can also pass at Numpy Array. You can change types, so on and so forth. So you know, it’s well worth reading through and like you’re not going to look at every single method it takes, but you’re kind. If you browse through it, you’ll get a general sense, right? That tensors do just about everything you can think of for numeric programing. At some point you will want to know every single one of these, or at least be aware roughly what exists. So you know what to search for in the docs. Otherwise you will end up recreating stuff from scratch, which is much, much slower than simply reading the documentation to find out it’s there. All right. So instead of instead of calling chunks or islice the thing that is roughly equivalent in a tensor is the reshape method. So reshape. So the reshape our 15,000 by 784 thing we be able to turn it into a 50000,28 by 28 tensors. So I could write here reshape to 50,000 by 28 by 28. But I kind of don’t need to because I could just put -1 here and it can figure out that that must be 50,000, because it knows that I have 50,000 by 784 items so I can figure out. So -1 means just fill this with all the rest.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">img[<span class="dv" style="color: #AD0000;">20</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.09375,
 0.4453125,
 0.86328125,
 0.98828125,
 0.98828125,
 0.98828125,
 0.98828125,
 0.78515625,
 0.3046875,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0,
 0.0]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">img[<span class="dv" style="color: #AD0000;">20</span>][<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.98828125</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="kw" style="color: #003B4F;">class</span> Matrix:</span>
<span id="cb27-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, xs): <span class="va" style="color: #111111;">self</span>.xs <span class="op" style="color: #5E5E5E;">=</span> xs</span>
<span id="cb27-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, idxs): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.xs[idxs[<span class="dv" style="color: #AD0000;">0</span>]][idxs[<span class="dv" style="color: #AD0000;">1</span>]]</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">m <span class="op" style="color: #5E5E5E;">=</span> Matrix(img)</span>
<span id="cb28-2">m[<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.98828125</code></pre>
</div>
</div>
<p>Now we can use this one feature in pytorch.We do it cause it more look like math than original way.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb30-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> tensor</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">tensor([<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([1, 2, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">tens <span class="op" style="color: #5E5E5E;">=</span> tensor(img)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">tens[<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.9883)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">x_train,y_train,x_valid,y_valid <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">map</span>(tensor, (x_train,y_train,x_valid,y_valid))</span>
<span id="cb36-2">x_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([50000, 784])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">x_train.<span class="bu" style="color: null;">type</span>()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'torch.FloatTensor'</code></pre>
</div>
</div>
<p><a href="https://pytorch.org/docs/stable/tensors.html">Tensor documentation</a></p>
<p>you could also do this imgs = x_train.reshape((50000,28,28)), -1 is another way Jeremy prefer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">imgs <span class="op" style="color: #5E5E5E;">=</span> x_train.reshape((<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>))</span>
<span id="cb40-2">imgs.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([50000, 28, 28])</code></pre>
</div>
</div>
<p>Okay. Now what does the word tensor mean? So there’s some very interesting history here, and I’ll try not to get too far into it because I’m a bit overenthusiastic about this stuff. I must admit. I’m very, very interested in the history of tensor programing and array programing, and it basically goes back to a language called APL. APL is a basically originally a mathematical notation that was developed in the mid to late fifties, 1950s. And at first it was used to as a notation for defining how certain new IBM systems what would work. So it was all written out in this, in this notation, it’s kind of like a replacement for mathematical notation that was designed to be more consistent and and kind of more expressive in the early sixties. So the guy who wrote made it was called Kenneth E. Iverson.In the early sixties some implementations that actually allowed this notation to be executed on a computer appeared both in notation and the executable implementations. Slightly confusingly, both called APL. APL has been in constant development ever since that time, and today is one of the world’s most powerful programing languages. And you can try it by going to try APL. And why am I mentioning it here? Because one of the things Kenneth E. Iverson did well, he studied an area of physics called tensor analysis, and as he developed APL, he basically said like, Oh, what if we took these ideas from tensor analysis and put them into a programing language? So in yeah, in APL you, you and you know have been able to for some time can basically you can define a variable and rather than saying equals which is a terrible way to define things really mathematically because that has a very different meaning most of the time in math. Instead we use Arrow to define things. We can say, okay, that’s going to be a a tensor like so, and then we can look at their contents of a and we can do things like, Oh, what if we do a*3 or a-2 and so forth. And as you can see, what it’s doing is it’s taking all the contents of this tensor and it’s multiplying them all by three or subtracting two from all of them, or perhaps more fun we could put in to be a different tensor. And we can now do things like a divided by b, and you can see it’s taking each of a and dividing by each of b. Now, this is very interesting because now we don’t have to write loops anymore. We can just express things directly. We can multiply things by scales even if they’re this is called a rank one tensor. That is to say it’s basically a method called a vector. We can take two and can divide one by the other and so forth. It’s a really powerful idea. Funnily enough, APL didn’t call them tensor even though Kenneth E. Iverson said he got this idea from tensor analysis. APL calls them arrays. NumPy, which was heavily influenced by APL, also calls them arrays. For some reason PyTorch, which very heavily influenced by APL, so by numpy doesn’t call them arrays, it calls them tensors. They’re all the same thing. They are rectangular blocks of numbers. They can be one dimensional like a vector, they can be two dimensional, like a matrix, they can be three dimensional, which is like a bunch of stacked matrices, like a batch of matrices and so forth. If you are interested in APL, which I hope you are, we have a whole APL and a array programing section on our forums and also we’ve prepared a whole set of notes on every single glyph in APL, which also covers all kinds of interesting mathematical concepts like complex direction and magnitude and all kinds of fun stuff like that. That’s all totally optional, but a lot of people who do APL say that they feel like they’ve become a much better programmer in the process. And also you’ll find here at the forums a set of 17 study sessions of an hour or two each covering the entirety of the language, every single glyph. So that’s all like where this stuff comes from. So this, this batch of 50,000 images, is what we call a rank three tensor in PyTorch and in Numpy We would call it an array with three dimensions. Those are the same thing. So what is the rank? The rank is just the number of dimensions. It’s 50,000 images of 28 high by 28 wide. So there are three dimensions. That is the rank of the tensor. So if we then pick out a particular image right, then look at its shape. We could call this a matrix. It’s a 28 by 28 tensor, or we could call it a rank two tensor vector is a rank one tensor in APL, a scalar is a rank zero tensor, and that’s the way it should be. A lot of languages in libraries don’t unfortunately think of it that way. So what is a scalar is a bit dependent on the language. Okay, so we can index into the zeroth image,20 rows and 50s colomn,get back the same number. Okay. So we can take x_train.shape, which is 50,000 by 784 and you can destructure it into N, which is the number of images and C which is the number of the 4 number of columns, for example. And we can also well, this is actually part of the standard library. So reality is mean so we can find out in y_train what’s the smallest number and what’s the maximum number. So that goes from 0 to 9. So you see here it’s not just the number zero, it’s a scalar tensor zero. They act almost the same most of the time. So here’s some example of a bit of the the y_train. So you can see these are basically this is going to be the labels, right? These are our digits and this is its shape. So this is 50,000 of these labels. Okay. And so since we’re allowed to use this in the standard library, well, it also exists in PyTorch. So that means we’re also allowed to use torch.min() and torch.max() properties. All right. So before we wrap up, we’re going to do one more thing. And I don’t know what the we would call kind of anti cheating, but according to our rules, we’re allowed to use random numbers because there is a random number generator in the Python standard library. But we’re going to do random numbers from scratch ourselves. And the reason we’re going to do that is even though according to the rules, we could be allowed to use the standard library one, it’s actually extremely instructive to build our own random number generator from scratch well, at least I think so. Let’s see what you think. So there is no way normally in software to create a random number.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">plt.imshow(imgs[<span class="dv" style="color: #AD0000;">0</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">plt.imshow(imgs[<span class="dv" style="color: #AD0000;">3</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>vector rank one tensor matrix is a rank 2 tensor scalor in APL(depend of programming languages) is rank zero tensor</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">imgs[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">20</span>,<span class="dv" style="color: #AD0000;">15</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.9883)</code></pre>
</div>
</div>
<p>Use destructring again. n number of images. c is full number of colums (784)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">n,c <span class="op" style="color: #5E5E5E;">=</span> x_train.shape</span>
<span id="cb46-2">y_train, y_train.shape</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))</code></pre>
</div>
</div>
<p>in y_train we can find min and max of it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="bu" style="color: null;">min</span>(y_train),<span class="bu" style="color: null;">max</span>(y_train)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(0), tensor(9))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">y_train.<span class="bu" style="color: null;">min</span>(), y_train.<span class="bu" style="color: null;">max</span>()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(0), tensor(9))</code></pre>
</div>
</div>
<p>Here we go further and broke our rule and make it even harder so we gonna do random number from scratch. We use sodu random number generator.</p>
</section>
<section id="random-numbers" class="level2">
<h2 class="anchored" data-anchor-id="random-numbers">Random numbers</h2>
<p>Based on the Wichmann Hill algorithm used before Python 2.3.</p>
<p>So there is no way normally in software to create a random number. Unfortunately, computers, you know, add, subtract, times, logic gates, stuff like that. So how does one create random numbers? Well, you could go to the Australian National University Quantum random number generator, and this looks at the quantum fluctuations of the vacuum and provides an API which will actually hook you in and return quantum random fluctuations of the vacuum. So that’s about that’s the most random thing I’m aware of. So that would be one way to get random numbers and there’s actually an API for that. So that’s a bit of fun. You could do what Cloudflare does. Cloudflare has a huge wall full of larva lamps and it uses the pixels of a camera looking at those larva lamps to generate random numbers. Intel nowadays actually has something in its chips which you can call RDRAND, which will return random numbers on certain intel chips from 2012. All of these things are kind of slow. They can kind of get you one random number from time to time. We want some way of getting lots and lots of random numbers. And so what we do is we use something called a pseudo random number generator. A pseudo random number generator is a mathematical function that you can call lots of times, and each time you call it, it will give you a number that looks random to show you what I mean by that, I’m going to run some code and I’ve created a function which will look at the moment called rand. And if I call rand 50 times and plot it, there’s no obvious relationship between one call and the next. That’s one thing that I would expect to see from my random numbers. I would expect that each time I call rand, the numbers would look quite different to each other. The second thing is rand is meant to be returning uniformly distributed random numbers, and therefore if I call it lots and lots and lots of times and plot its, histogram, I would expect see exactly this, which is each from zero to point one. There’s a few from point one, 2.2, there’s a few 1.2, 2.3. That’s true. It’s a fairly evenly spread thing. These are the two key things I would expect to see an even distribution of random numbers and that there’s no correlation or no obvious correlation from one to the other. So we’re going to try and create a function that has these properties. We’re not going to derive it from scratch. I’m just going to tell you that we have a function here called the Wickman Hill algorithm. This is actually what Python used to use back in before Python 2.3. And the key reason we need to know about this is to understand really well the idea of random state. Random state is a global variable. It’s something which is, or at least it can be. Most of the time when we use it, we use it as a random variable and it’s just basically one or more numbers. So we’re going to start with no random state at all. I’m going to create a function called seed that we’re going to pass something to, and I just smashed the keyboard to create this number. Okay So this is my random number. You could get this from the and quantum vacuum generator or from cloudflare’s larva lamps or from your intel chips RDRAND. You know, in python land, which pretty much always is a number 42, any of those are fine. So you pass in some number or you can pass in the current tick count in nanosecond as there’s various ways of getting some random starting point. And if we pass it into seed, it’s going to do a bunch of modular divisions and create a tupple of three things, and it’s going to store them in this global state. So Rand State now contains three numbers. Okay, so why do we do that? The reason we did that is because now this function, which takes our random state, unpacks it into three things and does again a bunch of multiplications and modules and then sticks and together with various kind of weights, modulo one. So this is how you can pull out the decimal part. This returns random numbers. But the key thing I want you to understand is that we pull out the random state at the start. We do math thingies to it and then we store new random state. And so that means each time I call this, I’m going to get a different number, right? So this is a random number generator. And this is really important because lots of people in the deep learning world screw this up, including me. Sometimes, which is to remember that random number generators rely on the state. So let me show you where that will get you if you’re not careful. If we use a special thing called fork that creates a whole separate copy of this python process in one copy or.fork() returns true and in the other copy it returns false, roughly speaking. So this copy here is this. If I say this, this version here the true version is the original none copied. It’s called the parent and so on. My else here this. So this will only be called by the parent. This will only be called by the copy. It is called the child. And each one I’m calling rand. These are two different random numbers right? Wrong. Yeah. The same number. And why is that? That’s because this process here and this process here are copies of each other, and therefore they each contain the same numbers in random state. So this is something that comes up in deep learning all the time, because in deep learning we often do parallel processing, for example, to generate lots of augmented images at the same time using multiple processes fast.ai used to have a bug in fact, where we failed to correctly initialize the random number generator separately in each process.</p>
<p>And in fact, to this day, at least as as of October 2022 torch.rand by default fails to initialize the random number generator. That’s the same number. Okay, so you got to be careful now. I have a feeling numpy gets it right. Let’s check import Numpy As an np. Okay. And so I don’t then I can’t remember which right now. Okay. NumPy also doesn’t have interesting. What about python. Right. And look at that. So python does actually remember to re initialize the random state and each fork. So you know this is something that like even if you’ve experimented in Python and you think everything’s working well in your data loader or whatever and you switch to PyTorch or numpy and now suddenly everything’s broken. So this is why we’ve spent some time re-implementing random, the random number generator from scratch, partly because it’s fun and interesting and partly because it’s important that you now understand that when you’re calling rand or any random number generator kind the default versions in numpy and PyTorch, this global state is going to be copied. So you’ve got to be a bit careful. Now I will mention our random number generator. Okay, So this is this is called percent timeout percent is a special Jupyter or Ipython function and percent time. It runs a piece of Python code this many times. So to call it ten times. Well, actually today seven loops and each one will be seven times and I’ll take the maiden standard deviation. So here I am going to generate random numbers, long chunks, and if I run that, it takes me 3 milliseconds. Like if I run it using PyTorch, this is the exact same thing in PyTorch. It’s going to take me 73 micro second. So as you can see, although we could use our version, we’re not going to because the PyTorch version is much, much faster. This is how we can create a 784 by ten. And why would we want this? That’s because this is our final layer of our neural net. Or if we’re doing a linear classifier, a linear weights want it to be 784 because that’s 28 by 28 by ten, because that’s the number of possible outputs, the number of possible digits. All right. That is it. So quite the intense lesson. I think we can all agree should keep you busy for a week. And thanks very much for joining and see you next time. Bye everybody.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">rnd_state <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span></span>
<span id="cb52-2"><span class="kw" style="color: #003B4F;">def</span> seed(a):</span>
<span id="cb52-3">    <span class="kw" style="color: #003B4F;">global</span> rnd_state</span>
<span id="cb52-4">    a, x <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30268</span>)</span>
<span id="cb52-5">    a, y <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30306</span>)</span>
<span id="cb52-6">    a, z <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">divmod</span>(a, <span class="dv" style="color: #AD0000;">30322</span>)</span>
<span id="cb52-7">    rnd_state <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">int</span>(x)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">int</span>(y)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">int</span>(z)<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">seed(<span class="dv" style="color: #AD0000;">457428938475</span>)</span>
<span id="cb53-2">rnd_state</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(4976, 20238, 499)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="kw" style="color: #003B4F;">def</span> rand():</span>
<span id="cb55-2">    <span class="kw" style="color: #003B4F;">global</span> rnd_state</span>
<span id="cb55-3">    x, y, z <span class="op" style="color: #5E5E5E;">=</span> rnd_state</span>
<span id="cb55-4">    x <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">171</span> <span class="op" style="color: #5E5E5E;">*</span> x) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30269</span></span>
<span id="cb55-5">    y <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">172</span> <span class="op" style="color: #5E5E5E;">*</span> y) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30307</span></span>
<span id="cb55-6">    z <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">170</span> <span class="op" style="color: #5E5E5E;">*</span> z) <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">30323</span></span>
<span id="cb55-7">    rnd_state <span class="op" style="color: #5E5E5E;">=</span> x,y,z</span>
<span id="cb55-8">    <span class="cf" style="color: #003B4F;">return</span> (x<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30269</span> <span class="op" style="color: #5E5E5E;">+</span> y<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30307</span> <span class="op" style="color: #5E5E5E;">+</span> z<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">30323</span>) <span class="op" style="color: #5E5E5E;">%</span> <span class="fl" style="color: #AD0000;">1.0</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">rand(),rand(),rand()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><span class="cf" style="color: #003B4F;">if</span> os.fork(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In parent: </span><span class="sc" style="color: #5E5E5E;">{</span>rand()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb58-2"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb58-3">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In child: </span><span class="sc" style="color: #5E5E5E;">{</span>rand()<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb58-4">    os._exit(os.EX_OK)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In parent: 0.9559050644103264
In child: 0.9559050644103264</code></pre>
</div>
</div>
<p>Be carefull when you use Pytorch or Numpy</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="cf" style="color: #003B4F;">if</span> os.fork(): <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In parent: </span><span class="sc" style="color: #5E5E5E;">{</span>torch<span class="sc" style="color: #5E5E5E;">.</span>rand(<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb60-2"><span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb60-3">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'In child: </span><span class="sc" style="color: #5E5E5E;">{</span>torch<span class="sc" style="color: #5E5E5E;">.</span>rand(<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb60-4">    os._exit(os.EX_OK)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In parent: tensor([0.2706])
In child: tensor([0.2706])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">plt.plot([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">50</span>)])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">plt.hist([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">10000</span>)])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>%timeit check the time of excution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> <span class="bu" style="color: null;">list</span>(chunks([rand() <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">7840</span>)], <span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>8.57 ms ± 368 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<p>pytorch version is faster.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><span class="op" style="color: #5E5E5E;">%</span>timeit <span class="op" style="color: #5E5E5E;">-</span>n <span class="dv" style="color: #AD0000;">10</span> torch.randn(<span class="dv" style="color: #AD0000;">784</span>,<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>135 µs ± 54.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>fastaipart2</category>
  <category>Stable-Diffusion</category>
  <guid>https://bahmansadeghi.com/posts/Writing stable diffusion from scratch/index.html</guid>
  <pubDate>Sun, 05 Mar 2023 20:30:00 GMT</pubDate>
</item>
<item>
  <title>Welcome To My Blog</title>
  <dc:creator>Bahman Sadeghi</dc:creator>
  <link>https://bahmansadeghi.com/posts/about/index.html</link>
  <description><![CDATA[ 



<p>What I write about here ?</p>
<p>1- Currently I am doing fast AI course part 2. So I do write about it twice a week. One post every mondays one post every Tuesdays.</p>
<p>2- I am trying something I call 52 projects in 52 weeks. I will write about that too (Without schedule).</p>
<p>3- Other stuff that not technical , I will write about it , stuff that interest me. (Without schedule).</p>



 ]]></description>
  <category>About Here</category>
  <category>Indie Projects</category>
  <guid>https://bahmansadeghi.com/posts/about/index.html</guid>
  <pubDate>Sun, 26 Feb 2023 20:30:00 GMT</pubDate>
</item>
</channel>
</rss>
